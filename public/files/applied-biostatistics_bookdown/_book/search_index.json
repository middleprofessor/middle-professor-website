[["index.html", "Statistics for the Experimental Biologist A Guide to Best (and Worst) Practices Preface Linear Models Aren’t t-tests and ANOVA good enough? What is unusual about this book?", " Statistics for the Experimental Biologist A Guide to Best (and Worst) Practices Copyright 2018 Jeffrey A. Walker Draft: 2024-04-16 Preface Linear Models More cynically, one could also well ask “Why has medicine not adopted frequentist inference, even though everyone presents P-values and hypothesis tests?” My answer is: Because frequentist inference, like Bayesian inference, is not taught. Instead everyone gets taught a misleading pseudo-frequentism: a set of rituals and misinterpretations caricaturing frequentist inference, leading to all kinds of misunderstandings. – Sander Greenland We use statistics to learn from data with uncertainty. Traditional introductory textbooks in biostatistics implicitly or explicitly train students and researchers to “discover by p-value” using hypothesis tests (Chapter 6). Over the course of many chapters, the student learns to use a look-up table or flowchart to make a series of binary decisions about their data (“is the independent variable continous or categorical”, “is the response variable normal or not”), choose the correct “test” based on these decisions (“use a Mann-Whitney U test”), compute a test statistic for their data, compute a p-value based on the test statistic, and compare the p-value to 0.05. Textbooks typically give very little guidance about what can be concluded if \\(p &lt; 0.05\\) or if \\(p &gt; 0.05\\), but many researchers conclude, incorrectly, they have “discovered” an effect if \\(p &lt; 0.05\\) but found “no effect” if \\(p &gt; 0.05\\). This book is an introduction to the statistical analysis of data from biological experiments with a focus on the estimation of treatment effects and measures of the uncertainty of theses estimates. Instead of a flowchart of “which statistical test”, this book emphasizes a regression modeling approach using linear models and extensions of linear models. “What what? I learned from the post-doc in my lab that regression was for data with a continuous independent variable and that t-tests and ANOVA were for data with categorical independent variables.” No! This misconception has roots in the history of regression vs. ANOVA and is reinforced by how introductory biostatistics textbooks, and their instructors, choose to teach statistics. Classical linear regression, t-tests and ANOVA are all special cases of a linear model. The different linear models in this text are all variations of the equation for a line \\(Y = mX + b\\) using slightly different notation: \\[\\begin{equation} \\mathrm{E}(Y | X) = \\beta_0 + \\beta_1 X \\tag{0.1} \\end{equation}\\] The chapter An introduction to linear models explains the meaning of this notation. Here, just recognize that this is a regression model, but in modern statistics, we use this to not only estimate the effects of a continuous \\(X\\) variable on some response (classical regression) but also for the estimation of effects of a categorical treatment variable on some response (as in classical t-tests and ANOVA). A regression model with a categorical treatment variable is possible because the treatment variable is recoded into a numeric indicator variable indicating group membership (“wildtype” or “knockout”). Classical t-tests and ANOVA are equivalent to special cases of regression models but the linear model is usually presented in a different way, one that allows simple “paper and pencil math” (addition, subtraction, multiplication, division). The linear model underneath a classical t-tests/ANOVA is some variation of \\[\\begin{equation} \\overline{Y}_k = \\mu + \\alpha_k \\tag{0.2} \\end{equation}\\] where \\(\\mu\\) is the grand mean and \\(\\alpha_k\\) is the difference between the mean of treatment k and the grand mean. In this text, I use linear model for any model that looks like Equation (0.1) and ANOVA model for any model that looks like Equation (0.2) (many statistics textbooks call these “cell-mean models”). It would be more correct to call models that look like Equation (0.1) a regression model but the word “regression” has a lot of baggage because of how it is taught – as a method for data with continuous \\(X\\) (“independent”) variables. If I said, “this is a book of regression models for analyzing your experimental data”, you might quite reasonably, but incorrectly, assume that the book wasn’t really relevant to your experiments because your independent variables are categorical (“WT” vs.”KO”) and not continuous. Table 0.1 is a compact summary of the linear models introduced in this text, which covers a large fraction of the kinds of models researchers would need with experimental data. Aren’t t-tests and ANOVA good enough? For many experiments, the linear models advocated here will give the same p-value as a t-test or ANOVA, which raises the question, why bother with linear models? Some answers include Flexibility. Linear models and extensions of linear models are all variations of \\(Y = \\beta_0 + \\beta_1 X\\). Generalizations of this basic model (see Table 0.1) include linear models with added covariates or multiple factors, generalized least squares models for heterogeneity of variance or correlated error, linear mixed models for correlated error or hierarchical grouping, generalized linear models for counts and other biological data that don’t have normal distributions, generalized additive models for responses that vary non-linearly over time, causal graphical models for inferring causal effects with observational data, multivariate models for multiple responses, and some machine learning models. This book is not a comprehensive source for any of these methods but an introduction to the common foundations of all. Possibility. Issues in inference from linear models, including t-test/ANOVA, occur when the data violate the assumptions of independence, homogeneity of variances, or a normal conditional response. Linear (regression!) models were expanded in different ways to specifically model these violations. Many of these models have no ANOVA equivalent – consequently, researchers using ANOVA are forced to use the wrong model or kludgy alternatives, such as non-parametric tests. Gateway drug. Many of the statistical models used in genomics are variations of the linear models introduced here. There will be a steep learning curve for these methods if your statistical training consists of t-tests, ANOVA, and Mann-Whitney non-parametric tests. Biologically meaningful focus. Linear models encourage looking at, thinking about, and reporting estimates of the size of a treatment effect and the uncertainty of the estimate. The estimated treatment effect is the difference in the response between two treatments. If the mean plasma glucose concentration over the period of a glucose tolerance test is 15.9 mmol/l in the knockout group and 18.9 mmol/l in the wildtype group, the estimated effect is -3.0 mmol/l. The magnitude of this effect is our measure of the difference in glucose tolerance between the two treatments. What is the physiological consequence of this difference? Is this a big difference that would excite NIH or a trivial difference that encourages us to pursue some other line of research? I don’t know the answers to these questions– I’m not a metabolic physiologist. Researchers in metabolic physiology should know the answers, but if they do, they don’t indicate this in the literature. Effect sizes are rarely reported in the experimental bench-biology literature. What is reported are p-values. Small p-values give the researchers confidence that “an effect exists” and an abundance of small p-values from a series of experiments that have rigorously probed a system give the researchers confidence that they have discovered knowledge about some biological mechanism. Extremely small p-values give some researchers the confidence that an effect is large or important. This confidence is unwarranted. P-values are not a useful measure of effect size. P-values are neither necessary nor sufficient for good data analysis. But, a p-value is a useful tool in the data analysis toolkit. If the conduction of the experiment and analysis of the results closely approximate the model underlying the computation of the p-value, then a p-value dampens the frequency that we are fooled by randomness and gives a researcher some confidence in the direction (positive or negative) of an effect. Importantly, the estimation of effects and uncertainty and the computation of a p-value are not alternatives. Throughout this text, linear models are used to compute a p-value in addition to the estimates of effects and their uncertainty. NHST Blues – The emphasis on p-values as the measure to report is a consequence of the “which statistical test?” strategy of data analysis. This practice, known as Null-Hypothesis Significance Testing (NHST), has been criticized by statisticians for many, many decades. Nevertheless, introductory biostatistics textbooks written by both biologists and statisticians continue to organize textbooks around a collection of hypothesis tests, with a great deal of emphasis on “which statistical test?” and much less emphasis on estimation and uncertainty. The NHST/which-statistical-test strategy of learning or doing statistics is easy in that it requires little understanding of the statistical model underneath the tests and its assumptions, limitations, and behavior. The NHST strategy in combination with point-and-click software enables mindless statistics and encourages the belief that statistics is a tool like a word processor is a tool, after all, a rigorous analysis of one’s data requires little more than getting p-values and creating bar plots. Indeed, many PhD programs in the biosciences require no statistics coursework and the only training available to students is from the other graduate students and postdocs in the lab. As a consequence, the biological sciences literature is filled with error bars that imply data with negative values and p-values that have little relationship to the probability of the data given the experimental design. More importantly for science, the reported statistics are often not doing for the study what the researchers and journal editors think they are doing. What is unusual about this book? Real data sets from the recent experimental biology literature. All of the data in this text are from articles in which the researchers have followed open science best practices and made the data freely available by archiving the data with the article. This not only let’s me explain modern statistics relevant to experimental biologist’s experiments using data collected by experimental biologists but also allows me to connect the statistical explanation with the goals of the experiment. A potential downside of using real data is that understanding any analysis requires some level of understanding of the underlying biology and the underlying methods of data collection. I try to give enough background biology to understand the motivation of the experiment and experimental methodology to understand the data. Because I scan many, many articles in experimental biology looking for good archived data to use as examples in this text, I’ve developed an appreciation for the variation in how experimental biologists think about and do statistics. A consequence of this is the recognition of the many ways that researchers use non-best practices and occasionally worst practices. Much of the material in each chapter and one entire chapter (Issues in inference) is devoted to addressing the consequences of these non-best and worst practices. "],["a-table-of-models.html", "A Table of Models including mapping between linear models and classical tests", " A Table of Models including mapping between linear models and classical tests Table 0.1: Linear models and extensions of linear models covered in this text. Design Formula NHST Chapter single factor with 2 levels, different mice in each treatment ~ treatment t-test 10.1.1 single factor with &gt; 2 levels, different mice in each treatment ~ treatment one-way ANOVA 10.1.3 single factor with ≥ 2 levels and 1 continuous covariate, different mice in each treatment ~ treatment + weight ANCOVA 14 two factors, different mice in each treatment combination ~ treatment * genotype two way ANOVA 15 single factor with 2 levels, heterogeneity, different mice in each treatment ~ treatment, weights = varIdent(form = ~ 1 | treatment) Welch t-test 12.2 single factor with ≥ 2 levels, heterogeneity, different mice in each treatment ~ treatment, weights = varIdent(form = ~ 1 | treatment) multiple Welch t-tests 12.2 single factor with 2 levels, all treatments measured in each batch, n = 1 for each batch by treatment combination ~ treatment + (1 | id) paired t-test 12.1.1 single factor with &gt; 2 levels, all treatments measured in each batch, n = 1 for each batch by treatment combination ~ treatment + (1 | id) repeated measures ANOVA (univariate model) 16.7 single factor with &gt; 2 levels, all treatments measured in each batch, n = 1 for each batch by treatment combination ~ treatment + (1 | id), correlation = CorSymm(form = ~ 1 | id), weights = varIdent(form = ~ 1 | treatment) repeated measures ANOVA (multivariate model) 16.7 single factor with ≥ 2 levels, all treatments measured in each batch, n &gt; 1 for each batch by treatment combination ~ treatment + (1 | experiment_id) + (1 | experiment_id:treatment) two-way mixed-effect ANOVA 16.6 single factor with ≥ 2 levels, all treatments measured in each batch, n &gt; 1 for each batch by treatment combination ~ treatment + (treatment | experiment_id) none 16.6 "],["ask1-intro.html", "Chapter 1 Analyzing experimental data with a linear model 1.1 This text is about using linear models to estimate treatment effects and the uncertainty in our estimates. This, raises the question, what is “an effect”?", " Chapter 1 Analyzing experimental data with a linear model 1.1 This text is about using linear models to estimate treatment effects and the uncertainty in our estimates. This, raises the question, what is “an effect”? At the end of this text, I provide an example set of analyses for multiple, related experiments. This example is a goal or target; it’s what you will be working towards as you learn from this text. The data for the analysis come from multiple experiments presented in Figure 2 in the article ASK1 inhibits browning of white adipose tissue in obesity. The chapter preceding the analysis is just enough biology to help you understand the biological importance of each experiment. The data for Figure 2 is from a set of experiments exploring the consequences of adipose-tissue specific deletion of the ASK1 signaling protein on multiple, adverse effects of a high-fat diet in mice, including weight gain, glucose intolerance, and increased liver triacylglycerol levels. I chose the data in Figure 2 of this paper because of the diversity of analyses and plot types. My analyses and plots differ slightly from those of the researchers because I implemented better practices – the stuff of this text. Here, I use one panel from Figure 2 to outline what the statistical analysis of experimental data is all about. Much of this outline will be repeated in “An introduction to linear models” chapter. The goal of the experiments is to estimate the effect of the adipose-specific ASK1 deletion. To understand what I mean by “an effect”, and to understand how we can estimate an effect by fiting a linear model to data, let’s peak at the results of the fit model. Figure 1.1: The effect of adipose-specific ASK1 knockout on liver triglyceride (TG). For this experiment, the researchers want to know if knocking out the ASK1 gene in the adipose tissue cells lowers the liver triglyceride (TG) level in mice fed a high-fat diet. To investigate this, the researchers compared the TG levels in a group of knockout mice (“ASK1Δadipo” – Δ is the del operator and refers to a deletion in genetics) to the TG levels in a group of control mice (“ASK1F/F”). Specifically, they measured the difference in the mean TG of the control group from the mean TG of the knockout group. \\[ \\overline{\\texttt{TG}}_\\texttt{ASK1Δadipo} - \\overline{\\texttt{TG}}_\\texttt{ASK1F/F} \\] The measured difference in the means is the estimate of the effect of ASK1 deletion on liver TG levels. This estimate is -21.6 µmol per g liver. An effect has the same units as the variable compared (µmol per g liver), a magnitude (21.6 units), and a direction (negative). My version of Figure 2i, which shows the results of the experiment, is Fig. 1.1 above. The direction and magnitude of the estimated effect (the measured difference in means) can be mentally reconstructed by comparing the position of the two group means in the lower part of Fig. 1.1. The upper part of Fig. 1.1 explicitly shows the estimate and shows the the uncertainty in the estimate using 95% confidence intervals. The numbers to make this plot come from the coefficient table of the fit model, shown in Table 1.1. The second row of this table are the statistics for the effect of the knockout on TG levels. The value in the “Estimate” column is the estimated effect (the measured difference in means). The value in the “Std. Error” column is the standard error of the difference in means (SE), a measure of uncertainty of the estimate. We use this SE to compute the statistic for a t-test, from which we get the p-value, the probability of observing a t-value as large or larger than our observed value, if the null were true. The “null is true” not only means the true effect is zero but also assumes a long list of conditions, such as, a random treatment assignment and homogeneity of variance. We also use the SE to compute the 95% confidence intervals of the difference. Understanding what a standard error is and how to interpret confidence intervals is paramount to practicing good statistics. This is covered in the chapter Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals). Table 1.1: Coefficient table for linear model fit to exp2i data. Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 61.47 4.98 12.3 0.000 50.37 72.57 treatmentASK1Δadipo -21.60 7.05 -3.1 0.012 -37.30 -5.90 It is hard to overemphasize that what we measure in experiments is estimated effects. The true effect may be larger, smaller, or even in the reverse direction. This text is all about what we can infer about true effects from the statistical analysis of experimental data. This inference requires that we also measure our uncertainty in our estimate of the effect. The measured means in each group are computed from a random sample of mice. If we only cared about the six mice in each group in this experiment, then we would not need to fit a linear model to the data to estimate the effect, we could simply compute each group’s mean and subtract the control mean from the knockout mean. But we care more about something more than these dozen mice because we are trying to discover something general about ASK1 regulation of TG levels in mice, generally (and even in mammals, and especially humans, generally). To make this leap of inference, we use a model to claim that each sample mean is an estimate of the respective population mean. Given this model, we can compute the standard error of each mean and the standard error of the difference in means. A standard error is a measure of the sampling variance of a statistic and, therefore, a measure of the precision of the estimate. The standard error, then, is a measure of uncertainty in the estimate. Here is how to think about precision and uncertainty: if we were to repeat this experiment many, many times, we would generate a long list of mean TG levels for the control mice and a long list of mean TG levels for the knockout mice. The less variable the means in each list, the more precise. By using a model, we do not need to repeat this experiment many times to get a standard error. The model we are going to fit to the Figure 2i data is \\[\\begin{align} y &amp;= \\beta_0 + \\beta_1 x + \\varepsilon\\\\ \\varepsilon &amp;\\sim N(0, \\sigma^2) \\end{align}\\] This is a model of how the Figure 2i data were generated. In this model, \\(y\\) is the liver TG level for some fictional, randomly generated mouse and \\(x\\) is a variable that indicates the condition of the ask1 gene in randomly generated mouse – a value of 0 is given to mice with a functional ASK1 gene and a value of 1 is given to mice with a knocked out gene. \\(\\beta_0\\) is the “true” mean of TG in mice fed a high-fat diet and with a functional ASK1 gene. By “true”, I mean the mean that would be computed if we were to measure TG on an infinite number of these mice (exactly what “these mice” means is a good topic for a campfire discussion). The observed mean of the ASK1F/F group is an estimate of \\(\\beta_0\\). The sum \\(\\beta_0\\) + \\(\\beta_1\\) is the true mean of TG in mice fed a high-fat diet but with a knocked out ASK1 gene. This means that \\(\\beta_1\\) is the true difference in the means, or the true effect. The observed difference in means between the ASK1Δadipo and ASK1F/F groups is an estimate of \\(\\beta_1\\). This difference is the estimated effect. The sum \\(\\beta_0 + \\beta_1 x\\) is the expectation of TG, or expected value of TG in a generated mouse given the generating model. This sum equals the true mean of the infinite set of normal mice if \\(x = 0\\) and equals the true mean of the infinite set of ASK1 knockout mice if \\(x = 1\\). All generated control mice have the same expected value of TG. All generated knockout mice have the same expectated value of TG. \\(\\varepsilon\\) is the error for the randomly generated mouse. It is a random number sampled from a Normal distribution with a mean of zero and a variance of \\(\\sigma^2\\). The variation in generated mice has a systematic component due to variation in \\(x\\) and a random (or stochastic) component due to variation in \\(\\varepsilon\\). By fitting a model to the data we estimate the parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\). It is the estimation of \\(\\sigma\\) that allows us to compute a measure of our uncertainty (a standard error) of our estimates of the means (\\(\\beta_0\\) and \\(\\beta_0 + \\beta_1\\)) and of the difference in the means (\\(\\beta_1\\)). Let’s fit this model to the Figure 2i data using R. exp2i_m1 &lt;- lm(liver_tg ~ treatment, data = exp2i) Robust inference from the model (generalizing from sample to population, including measures of the uncertainty of our estimates, requires that our data approximates the kind of data we’d expect from the data generating model specified above. All rigorous analysis should use specific model checks to evaluate this. First, the “normality check” – we use a quantile-quantile (QQ) plot to see if our data approximate what we’d see if we sampled from a normal distribution. set.seed(1) qqPlot(exp2i_m1, id=FALSE) 95% of quantiles computed from fake samples from the generating model above are inside the two dashed lines in the plot. Our measured quantiles are the open circles. The quantiles from a sample that looks like it was sampled from a normal distribution, or “looks Normal”, should be approximately linear and mostly lie within the dashed lines. Our sample looks good. Regardless, inference is pretty robust to moderate departure from Normal. Second, the “homogeneity check” – we use a spread level plot to see if there is some pattern to the variance, for example if the spread of residuals is noticeably bigger in one group than another, or if the spread increases with the fitted value. spreadLevelPlot(exp2i_m1, id=FALSE) ## ## Suggested power transformation: 1.294553 This looks pretty good. Given these checks, lets move on and look at the table of model coefficients. exp2i_m1_coef &lt;- cbind(coef(summary(exp2i_m1)), confint(exp2i_m1)) Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 61.5 4.98 12.3 0.000 50.4 72.6 treatmentASK1Δadipo -21.6 7.05 -3.1 0.012 -37.3 -5.9 The two values in the column “Estimate” are the estimates of \\(\\beta_0\\) and \\(\\beta_1\\). The top value (61.5) is the mean of the control mice (the units are µmol/g). The mean of the knockout mice is the sum of the two values (39.9 µmol/g). And the effect of ASK1 deletion on TG levels is simply the second value (-21.6 µmol/g). The standard error of the effect is 7.05 µmol/g. We can use the standard error to compute a t-value (-3.1, in the column “t value”). A t-value is a test statistic. The probability (“p value”) of the significance test is 0.012. This if the probability of sampling a t-value as large or larger than the observed t-value, if we were to sample from a null distribution of t-values (a distribution of sampled t values if the true value of \\(\\beta_1\\) was 0). We can also use the standard error to compute a 95% confidence interval of the effect. The lower bound of this interval is -37.3 µmol/g and the upper bound is -5.9 µmol/g. A confidence interval is another way of communicating uncertainty, and the way advocated in this text. In a 95% confidence interval, 95% of similarly constructed intervals (from hypothetical sampling of six mice from the ASK1 normal population and six mice from the ASK1 knockout population) will contain the true mean. Another way to think about a confidence interval is, it is the range of true differences that are compatible with the data, where compatible means “not rejected” by a t-test (a t-test between the estimated effect and any number inside the interval would return a p-value greater than 0.05). Here is how we might report this result in a paper: Mean TG level in ASK1Δadipo mice on a high-fat diet was 21.6 µmol/g less than that in ASK1F/F mice on a high-fat diet (95% CI: -37.3, -5.9, \\(p = 0.012\\)) (Fig.@ref(fig:ask1-exp2i-ggplot_the_model)). "],["part-i-getting-started.html", "Part I: Getting Started", " Part I: Getting Started "],["getting-started-r-projects-and-r-markdown.html", "Chapter 2 Getting Started – R Projects and R Markdown 2.1 R vs R Studio 2.2 Download and install R and R studio 2.3 Open R Studio and modify the workspace preference 2.4 If you didn’t modify the workspace preferences from the previous section, go back and do it 2.5 R Markdown in a nutshell 2.6 Install R Markdown 2.7 Importing Packages 2.8 Create an R Studio Project for this textbook 2.9 Working on a project, in a nutshell 2.10 Create and setup an R Markdown document (Rmd) 2.11 Let’s play around with an R Markdown file", " Chapter 2 Getting Started – R Projects and R Markdown A typical statistical modeling project will consist of: importing data from Excel or text (.csv or .txt) files cleaning data initial exploratory plots analysis model checking generating plots generating tables writing text to describe the project, the methods, the analysis, and the interpretation of the results (plots and tables) The best practice for reproducible research is to use as few software tools for these steps as possible. Too many research projects are not reproducible because the data were cleaned in Excel, and then different parts of the data were separately imported into a GUI statistics software for analysis, and then output from the statistics software was transcribed to Excel to make a table. And other parts of the analysis are used to create a plot in some plotting software. And then the tables and plots are pasted into Microsoft Word to create a report. Any change at any step in this process will require the researcher to remember all the downstream parts that are dependent on the change and to re-do an analysis, or a table, or a plot. Or a team member on the project making asks about how a particular variable was transformed and R studio encourages best practices by creating a project folder that contains all project documents and implementing a version of markdown called R Markdown. An R Markdown document can explicitly link all parts of the workflow so that changes in earlier steps automatically flow into the later steps. At the completion of a project, a researcher can choose “run all” from the menu and the data are read, cleaned, analyzed, plotted, tabled, and put into a report with the text. 2.1 R vs R Studio R is a programming language. It runs under the hood. You never see it. To use R, you need another piece of software that provides a user interface. The software we will use for this is R Studio. R Studio is a slick (very slick) graphical user interface (GUI) for developing R projects. 2.2 Download and install R and R studio Download R for your OS Download R Studio Desktop If you need help installing R and R studio, here is Andy Field’s Installing R and RStudio video tutorial) 2.3 Open R Studio and modify the workspace preference Open R Studio Click on R Studio &gt; Preferences to Click on General in the left menu diable “Restore .RData into workspace at startup” Click on the “Save workspace to .RData on exit” popup menu and choose “Never” What’s going on here? The workspace contains the values of all the objects created by the R code that you’ve run in the working R session. Nothing good comes from this. You want to start each R session with a clean slate, a blank workspace. This means that when you start a new R session, you will need to re-run all your code chunks to start where you left-off at the close of your last R session. This seems tedious but, be warned, bad things will happen if you save the workspace from the last session and re-load this at startup. Trust me. Just don’t do it. 2.4 If you didn’t modify the workspace preferences from the previous section, go back and do it 2.5 R Markdown in a nutshell In this text, we will write code to analyze data using R Markdown. R markdown is a version of Markdown. Markdown is tool for creating a document containing text (like microsoft Word), images, tables, and code that can be output, or knitted, to the three modern output formats: html (web pages), pdf (reports and documents), and microsoft word (okay, this isn’t modern but it is widely used). The R Markdown, or .Rmd, document contains three components: a YAML header, which specifies formatting and styles for the knitted document the code “chunks”, which are blocks of code that do something the space before and after the code chunks which contains text and any output images or tables from the code chunks. 2.6 Install R Markdown Directions for installing R Markdown R Markdown can output pdf files. The mechanism for this is to first create a LaTeX (“la-tek”) file. LaTeX is an amazing tool for creating professional pdf documents. You do not need PDF output for this text, but I encourage you to download and install the tinytex distribution, which was created especially for R Markdown in R Studio. The tinytex distribution is here. 2.7 Importing Packages The R scripts you write will include functions in packages that are not included in Base R. These packages need to be downloaded from an internet server to your computer. You only need to do this once (although you have to redo it each time you update R). But, each time you start a new R session, you will need to load a package using the library() function. Now is a good time to import packages that we will use Open R Studio and choose the menu item “Tools” &gt; “Install Packages”. In the “packages” input box, insert the names of packages to install the package. The names can be separated by spaces or commas, for example “data.table, emmeans, ggplot2”. Make sure that “install dependencies” is clicked before you click “Install”. Packages that we will use in this book are Import and wrangling packages devtools – we use this to install packages that are not on CRAN here – we use this to read from and write to the correct folder janitor – we use the function clean_names from this package readxl – elegant importing from microsoft Excel spreadsheets data.table - we use the data.table way to wrangle data in this text. magrittr – instead of nesting functions, we use the pipe operator from this package stringr – we use this to wrangle character variables forcats – we use this to wrangle factor variables analysis packages emmeans – we use this to compute modeled means and contrasts nlme – we use this for gls models lme4 – we use this for linear mixed models lmerTest – we use this for inference with linear mixed models glmmTMB – we use this for generalized linear models MASS – we will use glm.nb from this package afex – we use this for classic ANOVA linear models car – we use this for model checking DHARMa – we use this for model checking generalized linear models insight – we use this to learn about models graphing and tabling packages ggplot2 – we use this for plotting ggsci – we use this for the color palettes ggthemes – we use this for the colorblind palette ggpubr – we use this to make ggplots a bit easier ggforce – we use this for improved jitter plots dabestr – we use this to make several plot types cowplot – we use this to combine plots knitr – we use this to make kable tables kableExtra – we use this to improve kable tables lazyWeave – we use this for pretty p-values Once these are installed, you don’t need to do this again although there will be additional packages that you might install. You simply need to use the library() function at the start of a markdown script. 2.8 Create an R Studio Project for this textbook Create a project folder within the Documents folder (Mac OS) or My Documents folder (Windows OS). All files associated with this book will reside inside this folder. The name of the project folder should be something meaningful, such as “Applied Biostatistics” or the name of your class (for students in my Applied Biostatistics class, this folder could be named “BIO_413”). Within the project folder, create new folders named “Rmd” – this is where your R markdown files are stored “R” – this is where additional R script files are stored “data” – this is where data that we download from public archives are stored “output” – this is where you will store fake data generated in this class “images” – this is where image files are stored Open R Studio and click the menu item File &gt; New Project… Choose “Existing Directory” and navigate to your project folder Choose “Create Project” Check that a “.Rproj” file is in your project folder Download and move the file ggplot_the_model.R into the R folder. Figure 2.1: Project folder with the .Rproj file and all main folders located at the first level of the project The project directory should look like that in Figure 2.1. Importantly, the project file (“Applied Biostatistics.Rproj”) and the main folders are all located at the first level within the project folder. Bug alert If your .Rproj file is somewhere else (on the desktop, in the data folder, etc.), bad things will happen. 2.9 Working on a project, in a nutshell Wake up, brush teeth, and open the project by double-clicking the .Rproj icon. Alternatively, open R Studio and then use the File &gt; Open Project to open the project. The name of the project will at the top-right of the R Studio window. We always want to work within an open project and the first workflow guarantees this. If we Open R Studio and then open a .Rmd file, we could be working within another project or no project at all. Bad things will happen. Run previous code chunks, in order (top to bottom). Write new code in code chunks and run. When we run code, we add R objects to the workspace. The workspace contains the values of all the objects created by the R code that has been run in the working session. When I save the .Rmd file, these values are not saved, only the text and code chunks in the R Markdown document. This is a feature, not a bug. When we are finished with the session, quit R Studio. If you get a popup window asking if you want to save the workspace, click “No”. Then immediately go back to the section “Open R Studio and modify the workspace preference” above and follow the directions. 2.10 Create and setup an R Markdown document (Rmd) The top-left icon in R Studio is a little plus sign within a green circle. Click this and choose “R Markdown” from the pull-down menu. Give the file a meaningful title. Add your name in the Author text book. After the first time doing this, R Studio will default to your name. R Studio opens a demo document. Delete all text below the first code chunk, starting with the header “## R Markdown” 2.10.1 Modify the yaml header Replace “output: html_document” in the yaml header with the following in order to create a table of content (toc) on the left side of the page and to enable code folding output: html_document: toc: true toc_float: true code_folding: hide 2.10.2 Modify the “setup” chunk All R Markdown documents should start with a setup chunk that loads the packages containing R functions that are not in Base R and defines a few key R objects, such as the name of the data folder. A good practice is to load only the packages used by the chunks in the the document. For a student or researcher new to R and R studio, this can be confusing because they are unlikely to be aware of which functions belong to which package and, therefore, which packages should be loaded. Here, I disregard this best practice and offer a general, all-purpose setup chunk for users of this text. knitr::opts_chunk$set(echo = TRUE) # wrangling packages library(here) # here makes a project transportable library(janitor) # clean_names library(readxl) # read excel, duh! library(data.table) # magical data frames library(magrittr) # pipes library(stringr) # string functions library(forcats) # factor functions # analysis packages library(emmeans) # the workhorse for inference library(nlme) # gls and some lmm library(lme4) # linear mixed models library(lmerTest) # linear mixed model inference library(afex) # ANOVA linear models library(glmmTMB) # generalized linear models library(MASS) # negative binomial and some other functions library(car) # model checking and ANOVA library(DHARMa) # model checking # graphing packages library(ggsci) # color palettes library(ggpubr) # publication quality plots library(ggforce) # better jitter library(cowplot) # combine plots library(knitr) # kable tables library(kableExtra) # kable_styling tables # ggplot_the_model.R packages not loaded above library(insight) library(lazyWeave) # use here from the here package here &lt;- here::here # use clean_names from the janitor package clean_names &lt;- janitor::clean_names # load functions used by this text written by me # ggplot_the_model.R needs to be in the folder &quot;R&quot; # if you didn&#39;t download this and add to your R folder in your # project, then this line will cause an error source_path &lt;- here(&quot;R&quot;, &quot;ggplot_the_model.R&quot;) source(source_path) data_folder &lt;- &quot;data&quot; image_folder &lt;- &quot;images&quot; output_folder &lt;- &quot;output&quot; echo = TRUE tells knitr to display the code within a code chunk when the R markdown file is knitted. knitr::opts_chunk$set(echo = TRUE) sets echo = TRUE for all code chunks in the containing .Rmd file (not all R markdown files in the project!). here &lt;- here::here makes sure that here uses the function from the here package and not some other package. Huh? Let’s back up – R is an open source project and packages are written by independent programmers and scientists and not employees of some central company. When someone develops a package, they create functions that do stuff. Sometimes developers of different packages create functions that have the same name. There is a function name conflict if we load two packages with the same name. Our R session will use the function of the last loaded package as the function assigned to the name. If we want the name to be used with the function in the previously loaded package, then we need to either re-order the library() statements, or simply re-assign the name to the function that we want. This is what here &lt;- here::here does. This script takes the function here from the package “here” and assigns it to the object here. 2.11 Let’s play around with an R Markdown file 2.11.1 Create a “fake-data” chunk Create a new chunk and label it “fake-data”. Insert the following R script and then click the chunk’s run button set.seed(4) n &lt;- 10 fake_data &lt;- data.table( treatment = rep(c(&quot;cn&quot;, &quot;tr&quot;), each = n), neutrophil_count_exp1 = rnegbin(n*2, mu = rep(c(10, 15), each = n), theta = 1), neutrophil_count_exp2 = rnegbin(n*2, mu = rep(c(10, 20), each = n), theta = 1) ) # View(fake_data) This chunk creates fake neutrophil counts in two different experiments. The comment (#) sign before View(fake_data) “comments out” the line of code, so it is not run. View the data by highlighting View(fake_data) and choosing “Run selected line(s)” from the Run menu. 2.11.2 Create a “plot” chunk Create a new chunk and label it “plot”. Insert the following R script and then click the chunk’s run button gg_1 &lt;- ggstripchart(data = fake_data, x = &quot;treatment&quot;, y = &quot;neutrophil_count_exp1&quot;, color = &quot;treatment&quot;, palette = &quot;jco&quot;, add = &quot;mean_se&quot;, legend = &quot;none&quot;) + ylab(&quot;Neutrophil Count (Exp. 1)&quot;) + stat_compare_means(method = &quot;t.test&quot;, label.y = 50, label = &quot;p.format&quot;) + NULL gg_2&lt;- ggstripchart(data = fake_data, x = &quot;treatment&quot;, y = &quot;neutrophil_count_exp2&quot;, color = &quot;treatment&quot;, palette = &quot;jco&quot;, add = &quot;mean_se&quot;, legend = &quot;none&quot;) + ylab(&quot;Neutrophil Count (Exp 2)&quot;) + stat_compare_means(method = &quot;t.test&quot;, label.y = 65, label = &quot;p.format&quot;) + NULL plot_grid(gg_1, gg_2, labels = &quot;AUTO&quot;) Each plot shows the mean count for each group, the standard error of the mean count, and the p-value from a t-test. This statistical analysis and plot are typical of those found in experimental biology journals. This text will teach alterntatives that implement better practices. 2.11.3 Knit the Rmd Knit to an html file Knit to a word document If you’ve installed tinytex (or some other LaTeX distribution), knit to a pdf file "],["part-ii-r-fundamentals.html", "Part II: R fundamentals", " Part II: R fundamentals "],["data.html", "Chapter 3 Data – Reading, Wrangling, and Writing 3.1 Long data format – the way data should be 3.2 Use the here function to construct the file path 3.3 Learning from this chapter 3.4 Working in R 3.5 Data wrangling 3.6 Saving data 3.7 Exercises", " Chapter 3 Data – Reading, Wrangling, and Writing This text avoids canned data sets from R packages because importing and wrangling data is a major obstacle in doing statistics for new R learners. Coding the import of raw data archived in text (or Excel or Google sheets) format is a best practice for increasing replicability. Importing data is difficult for new R learners for two reasons Many experimental biology researchers archive the data in a format that might make sense for analysis in Excel, but doesn’t work for analysis in most other statistical software, including R, without significant post-import processing (or wrangling) the data. This includes data in wide format, where the measures of the response variable for each treatment level is in a different column data in transposed format, where the measures of the response variable are in a row instead of a column (or the measures for each treatment level are in a separate row) missing values that are represented by different things, sometimes in the same table. These things include blank cells, periods, hyphens, the word “NA”, or numbers such as -9999. column headers that have multiple words separated by spaces sheets with a variable number of empty cells that separate values within different groups sheets that contain information about the data using colored blocks of cells or other formats These issues are solved by archiving data in (tidy data format)https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html. The hallmark of tidy data is data in long format, which is introduced in the next section (Section @data-long). Nevertheless, most students and researchers will have colleagues that share non-tidy data and a critical skill for R users is the ability to tidy data. A file on a hard drive has an address, or file path that is specific to both place (the specific computer) and time (the file organization of the computer). If we move a data file to another computer, or even to another directory within the same computer, the address changes. Modern computer users are at least vaguely familiar with the hierarchical organization of file storage (most students in my classes seem to store everything they create on the desktop) but are unaware of the concept of a file path and confused by what this path is. When we use a menu-driven system to open a file, the OS is constructing the file path under the hood. But when coding, we need to give R the file path. There are several imperfect methods for coding the path the the data file. There is one perfect method – the function here from the here package. This is discussed in Use the here function to construct the file path 3.1 Long data format – the way data should be Some best practices for archiving data tables, including archived data at journal websites, are illustrated by the format of the data in Table 3.1.1 Table 3.1: First six rows of the diet data set. subject_id institution diet vo2 lean_mass fat_mass D1 UC Davis HF 129.5653 24.33708 15.862920 D10 UC Davis LF 100.0749 21.08880 3.151200 D11 UC Davis LF 118.3097 22.56076 6.659238 D12 UC Davis LF 107.7487 22.11884 5.351156 D13 UC Davis LF 104.7367 21.17438 5.005616 D14 UC Davis HF 138.6420 24.64885 16.501150 The best practices included in the archived data include The data table contains all measures on each individual. In most of the archived Excel files that I download from journal websites, the researchers have separated each variable into separate tables and exclude all information from these tables other than the treatment group. For example, researchers would typically archive the vo2 values in Table 3.1 in a separate table within an Excel sheet and with the values split by diet group (high fat “HF” and low fat “LF”)(Figure 3.1). Don’t do this. Figure 3.1: The vo2 values separated from the rest of the data and split into separate diet groups. We don’t want to separate each variable into separate tables because we often want to use information from other measures of the same individual in our analysis. For example, for the comparison of \\(\\texttt{vo2}\\) among treatment levels, we would want to add the measure of \\(\\texttt{lean_mass}\\) to the model as a covariate (what is often called an ANCOVA model). To do this, we’d need to know which values of \\(\\texttt{vo2}\\) and values of \\(\\texttt{lean_mass}\\) belong to the same individual. Even if you think you have analyzed the data using a best practice, don’t separate each variable into separate tables. Researchers sometimes fail to recognize best practices. Any re-analysis of published data using best practices often require information from other measures of the same individual. For example, many researchers would compare the ratio of \\(\\texttt{vo2}\\) to \\(\\texttt{total_mass}\\) among treatment levels. This isn’t a best practice and if I want to check the researchers conclusions, I’d need a table like Table 3.1 to re-analyze the data. Even if you have used what is considered a best practice, don’t separate each variable into separate tables. Sometimes a researcher wants to use data from published data sets to parameterize a mechanistic model. Or, use data from published data sets to explore patterns of relationships among variables, for example how fat mass scales with lean mass (or total mass). For these, we need a table like Table 3.1 to re-analyze the data. The researchers organized the table with individuals in rows. A data table with all measures for each individual could organize the data with each row or each column belonging to one individual. The best practice for most analysis in R (and most other statistical software) is to organize individuals to rows (there are exceptions in some R packages that analyze genomics datasets). The researchers organized the data in long format – that is, each column contains all measures of a single variable. In general, do not organize the data in wide format, by splitting the values of a variable into multiple columns, with each column representing the values for a different group. An example of wide format is in Figure 3.1, which is the \\(\\texttt{vo2}\\) subset of data split by the groups in the columne \\(\\texttt{diet}\\). By contrast, the \\(\\texttt{vo2}\\) data are in long format in the original table (Table 3.1). It might be useful to think of long format for a variable as a single column with the values for each group stacked on top of each other. In general, R and almost all other statistical software (one exception is Graphpad Prism) require the data to be in long format. A reason for this is because the long format is consistent with the math underneath the statistical modeling of data. For example, the least squares solution of the linear model is \\[ \\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y} \\] where \\(\\mathbf{X}\\) is the model matrix, a table of numeric values with each row containing the values of the \\(X\\) variables for an individual (categorical variables such as \\(\\texttt{diet}\\) are recoded into one or more numeric indicator variables indicating group membership). The model matrix is in long format. * A second reason is organizational simplicity. Statistical models often include multiple variables measured on the same individual – the example above with \\(\\texttt{lean_mass}\\) added as a covariate is an example. With data in wide format, there is no non-awkward way to match values of \\(\\texttt{lean_mass}\\) with values of \\(\\texttt{vo2}\\) from the same individual. Or, in models with crossed factor variables, for example, an experiment with four groups repesenting all four combinations of the variables \\(\\texttt{genotype}\\) (with levels “WT” and “KO”) and \\(\\texttt{treatment}\\) (with levels “Control” and “HFD”), there is no non-awkward way (at least in a table with a single header row) to indicate that the design is two crossed factors. In wide format, the design appears to be a single factor with four groups. Don’t confuse a data table with multiple columns of different variables with a data table in wide format. Long and wide are relevant to how a single variable is organized – if split into multiple columns the table is wide, if stacked into a single column, the table is long. One exception to long format is a longitudinal or a repeated measures data set (Chapter 17), in which the response variable is measured multiple times on each individual. An example might be a pre-post design where a variable is measured both before and after a treatment is applied or an experiment where the response is measured each week for multiple weeks. In univariate models for the analyses of these data, the multiple measures for an individual are stacked into long format. As a consequence, there are now multiple rows of data for each individual (one for each measure of the variable). In multivariate models for the analyses of these data, the multiple measures for an individual are treated as if they are different variables and are spread out into wide format. The table includes the raw values. Many researchers normalize or standarize the values of a variable by constructing a ratio of the raw value divided by some standardizing value. Almost always, the best practice is to leave the response variable alone (do not make a ratio) and include the standardizing value as a covariate (or offset) in the model. But, if I want to check th conclusions of a particular result in which the researchers used a ratio as the response variable, then I would need the raw measures and the standardizing values. 3.2 Use the here function to construct the file path Importing data into R can be a struggle for new R users and, unfortunately, most online “how to import” sources give easy but superficial methods that don’t follow best practices for increasing reproducibility or do not allow flexible organization of files within a project. (TL;DR – use here() from the here package) df &lt;- read.table(file=\"clipboard\") imports data copied to the clipboard, from an Excel/Sheets file or from an open text file. For this to be semi-reproducible, a comment specifying the filename, worksheet and range that was copied is necessary. More problematic (catastrophically so for reproducibility), is, how does a researcher know that they highlighted and copied the correct range in the Excel sheet? df &lt;- read.csv(file.choose()) opens the familiar “open file” dialog box, which lets the user navigate to the file of choice. For this to be semi-reproducible, a comment specifying the filename to import is necessary. The catastrophic problem (for reproducibility) is, how does a researcher know which file was actually opened during an R session? The researcher might think they opened “walker_maine_bee_data_clean_androscoggin.csv” but mistakenly opened “walker_maine_bee_data_clean_aroostook.csv”. df &lt;- read.table(file=\"my_data.txt\") and df &lt;- read_excel(file=\"my_data.xlsx\") are reproducible because the filename is explicitly specified. But, this method requires that “my_data” is physically located in the same folder as the file containing the R script (the .Rmd file in our case) and this violates the best practice of clean project organization with different folders for the different kinds of files (data, R scripts, images, manuscript text, etc.). R Studio has an elegant import tool in the environment pane that opens a custom dialog box that allows the researcher to navigate to the file, and to specify what part of the file to import, such as the specific sheet and range for an Excel file. This has the same reproducibility issues as #1 and #2 but R Studio includes the equivalent script, which adds all relevant information for reproducility. One then simply copies and pastes this script into a code chunk and voila! The next time the script is run, the data can be imported from the script without using menus and dialog boxes. Except that..the script does not seem to take into account that the working directory of an R Markdown file is not the project folder but the folder containing the R Markdown file and so this two-step method fails. More personally, I’d prefer to run a chunk that quickly opens the data file instead of re-navigating through my file system and re-specifying the sheet and range every time I re-start the project in a new R session. There are at least three solutions to the issues raised above, all requiring some understanding of file paths and directory structure in an operating system. A file such as “my_data.xlsx” has an absolute file path, which is the full address of the file (the filename is something like your house street number). The absolute file path of “my_data.xlsx” might be “/Users/jwalker/Documents/applied-biostatistics/data/my_data.xlsx”. A relative file path is the file path from the working directory. In an R Studio project, the working directory is the project directory, which is the directory containing the .Rproj file. This will be the working directory of the console. Importantly, the working directory of an R Markdown code chunk is the folder containing the saved R Markdown file. An R Studio Notebook is an R Markdown file so the working directory of a notebook code chunk is the folder containing the saved notebook file. If an R Markdown file is located within the rmd folder, which is located within the project folder, then the relative file path to “my_file.xlsx” is “../data/my_file.xlsx”. The “..” tells the file OS to move “up” into the parent directory (which is the project folder) and the “data” tells the file OS to move “down” into the data folder. These are put together into a single address using “/”. The beauty of relative paths is that they remain the same – and so do not break one’s script – if the project folder, and all of its contents including the data folder and the rmd folder, is moved to another location on the hard drive (say into a new “Research” folder). By contrast, the absolute file path changes, which breaks any old script. The three solutions are Create a relative path to the file using something like file_path &lt;- \"../data/my_data.xlsx\". This should always work but it fails on some computers. For example, if the project folder is on a Windows OS (but not Mac OS) desktop, the assigned relative address doesn’t seem to look in the folder containing the file. Create a setup chunk that reroutes the working directory to the project folder using the script # use this in a chuck called &quot;setup&quot; to force the working directory to be # at the level of the project file. knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file()) For this to work, the chunk has to be named “setup”, that is, the text inside the curly brackets at the top of the chunk should be “r setup”. Then, with this chunk, the relative file path is file_path &lt;- \"../data/my_data.xlsx\" if “my_data.xlsx” is immediately inside the data folder which is immediately inside the project folder. This should work on any machine, and should work even if a project folder is moved. Use the function here(). The most robust solution seems to be using the function here() from the here package. The function works something like this data_folder &lt;- &quot;data&quot; # path to data that are imported file_name &lt;- &quot;my_data.xlsx&quot; file_path &lt;- here(data_folder, file_name) # paste together parts of the address my_file &lt;- read_excel(file = file_path) here() creates an absolute path, but one that is created on the fly, and will change (or should change) correctly if the project folder is moved on the same machine, to a cloud drive, or to another machine altogether. 3.3 Learning from this chapter It will be easiest to learn from this chapter by starting with a clean R Markdown document for this chapter. Create a new R Markdown file and save it to the “Rmd” folder of your project. To create and setup the Rmd, follow the steps in Create and setup an R Markdown document (Rmd) Important: The import method given here will not work properly until the Rmd file is saved! Get in the habit of creating the file, saving it immediately, and saving it often. Important: The import method will not work properly unless you are working within your R studio project. If you don’t see the name of the project in the upper right of the R Studio toolbar, then you are not in the project. Open the project before moving forward. A good habit is to open R studio by double clicking the project file icon (and not the R Studio icon or the Rmd document that you are working on). Notes on the setup chunk from Create and setup an R Markdown document (Rmd) As you become a better R programmer, be kind to the future you by loading only the packages necessary for the code in the R Markdown file that you are working on. If your default is to load everything, the future you will be confused why something was installed. Again, we’ve disregarded this best practice in this textbook to make sure that chunks don’t fail because of a missing function. Be kind to the future you by commenting on why a package is loaded; usually this is a specific function from the package here &lt;- here::here is my favorite script ever. What is it doing? One can read this as “assign the function here from the here package to the object here” (this is reading the script right to left). Why do this? It turns out the multiple packages define a function called “here”. If any of these packages are loaded after the here package, then here from here won’t work – it will be replaced by here from the more recently loaded package. To make sure that here uses the function from the here package, I simply reassign here from the here package to the object “here” after loading in all packages. 3.4 Working in R 3.4.1 Importing data 3.4.1.1 Importing Excel files Source article: Lyu, Yang, et al. “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice.” Elife 10 (2021): e59399. file name: elife-59399-fig1-data1-v2.xlsx Let’s open the data for Fig. 1H. Steps Highlight and copy the title of the article, which is “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice” Create a new folder within the “data” folder. Name the folder the title of the paper by pasting the name from the clipboard. This is the “data from” folder, since it contains the data from the publication with the title of the folder. Download the .xlsx file and move it into this folder The code below uses the function read_excel() from the package readxl. More about the amazing power of this package is the tidyverse page and chapter 11 in the R for Data Science book. data_from &lt;- &quot;Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice&quot; file_name &lt;- &quot;elife-59399-fig1-data1-v2.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp1h &lt;- read_excel(file_path, sheet = &quot;H&quot;) exp1h &lt;- clean_names(exp1h) # clean column names exp1h &lt;- data.table(exp1h) # convert to data.table # View(exp1h) This book will consistently uses this protocol for importing downloaded Excel files. For any tidy archived data in Excel, a user would simply copy this chunk, paste it into their Rmd, and modify the stuff inside the quotes to import the data. But this is mindless programming. Don’t do this. Understand what each line in the chunk does. 3.4.1.1.1 View the data Let’s start with the last line, which is commented out # View(exp1h) The comment (or hashtag) symbol (#) tells R to ignore everything in the line following the symbol. Usually a comment symbol is followed by a brief comment on the code. This comment if mostly for the future you – when you come back after a six month hiatus and wonder what a line or block of code is doing, the comment gives you this information. Sometimes a comment is used in front of R code to keep the R code from running. That is the purpose of the comment symbol here. View(exp1h) tells R to open a window showing a spreadsheet view of the data.table exp1h. In R studio, this window is opened as a new tab in the Source editor pane (the pane with your Rmd content). To run this code, highlight “View(exp1h)” but not the comment symbol and click the “Run selected line(s)” item in the Run pop-up menu (right side of pane’s tool bar). We don’t include the comment sign in the highlight because this would tell R to ignore everything after it. We can view the data in exp1h more easily by holding the command key (Mac OS) or the control key (Windows) and clicking the mouse in “exp1h” in any of the lines. Even though I use the mouse click method, I like to have the View(exp1h) at the end of my import chunk to remind me to View the data to make sure everthing imported correctly. 3.4.1.1.2 What does each line in the chunk do? The first three lines in the chunk create the directory path to the file. This path includes three variables data_folder – assigned to “data” in the setup chunk. “data” is a folder within the project folder that contains (or will contain) all datasets for this text. The data come from many different published papers, so the data for each publication gets its own folder within “data”. data_from – the name of the “data from” folder within “data” containing the data files. In this text, these folder names will always be the name of the published paper. filename – the name of the file to read. There may be multiple data files within the publication’s data folder. These are all put together into the absolute path using the function here() from the here package. Take a look at the value of file_path to confirm. The next three lines (starting with exp1h &lt;-) read_excel imports the data and assign this to a data.frame named exp1h clean_names cleans the column names of exp1h data.table converts exp1h to a data.table. A data.table is a data.frame with magical properties. In steps 2 and 3, the functions take the data.frame and process it in some way and then assigned the processed data.frame to an object that has the same name (exp1h). This script can be made slightly more “elegant” using the “pipe” operator %&gt;%. exp1h &lt;- read_excel(file_path, sheet = &quot;H&quot;) %&gt;% clean_names() %&gt;% data.table() This is a single line of code containing three separate operations all piped together. A way to think about this is The read_excel function imports the data from the file located at file_path and assigns this data to exp1h. The pipe operator then sends this to the clean_names function, which cleans the column names of exp1h. The pipe operator then sends this to the data.table function, which convert the data.frame to a data.table. A 3rd way to do this is with nested functions. exp1h &lt;- data.table(clean_names(read_excel(file_path, sheet = &quot;H&quot;))) # convert to data.table Prior to the creation of the pipe operator, I nested functions to make code more compact. However, nested functions are often less readable, and I now mostly avoid these and use the pipe operator instead. Let’s back up to understand the steps, and especially the clean_names step. Here is the first line, which is the line that imports the file. exp1h &lt;- read_excel(file_path, sheet = &quot;H&quot;) Look at the column names (or column headers in Excel lingo) of the imported data using names or colnames (yes, there are elebenty million ways to do anything in R) (names is very general in that it can be used to return the names of the parts of any list, while colnames is specific to matrix-like objects). Type this into the console, not into your R Markdown chunk: names(exp1h) ## [1] &quot;Diet&quot; &quot;Replicate&quot; &quot;Weight (mg/fly)&quot; &quot;TAG (ug/fly)&quot; ## [5] &quot;Protein (ug/fly)&quot; &quot;TAG/Protein&quot; In general, it is bad practice to include spaces, parentheses, and special characters such as /, -, $, or ^, in the column names of a data frame because these symbols have specific meanings in R functions. The best practice for spaces is to replace the space with an underscore, for example the column header ““Weight (mg/fly)” could be weight_mg_per_fly. Some coders separate words with a period (weight.mg.per.fly). Others mash words together into a single word like this weightmgperfly but this should generally be avoided because the result can be hard to read. Finally, some coders use Caps to separate words like this WeightMgPerFly. This is easier to read than simple concatenation but the underscore is the easiest to read. The clean_names from the janitor package is a beautiful function to clean the column names of a data frame including replacing spaces with an underscore and stripping parentheses. The default format is snake_case, which replaces all spaces with underscores and changes any uppercase letter with lowercase. Many coders like to work with all lowercase variable names to avoid having to hit the shift key. I am one of these. Now run the line and look at the column names. exp1h &lt;- clean_names(exp1h) names(exp1h) ## [1] &quot;diet&quot; &quot;replicate&quot; &quot;weight_mg_fly&quot; &quot;tag_ug_fly&quot; ## [5] &quot;protein_ug_fly&quot; &quot;tag_protein&quot; Finally, run the line exp1h &lt;- data.table(exp1h) You won’t see any difference if you View the data or peak at the column names. But you’ve transformed the data frame to a data.table and given it magical properties. Worst Practices – resist the temptation to change the column names in the data file, which reduces reproducibility. Leave original data files original. Always increase reproducibility! colleague blues – Most researchers live in an Excel world and save data in a way that is efficient for computing stuff in Excel but not efficient for statistical analysis using R or other statistical computing software packages (with the exception of Graphpad Prism). Analyzing data will be much less frustrating if the data are saved in a format that facilitates analysis. Best practices for creating data files https://www.youtube.com/watch?time_continue=309&amp;v=Ry2xjTBtNFE – An excellent video introduction to best practices for organizing data in a spreadsheet that will subsequently be analyzed by statistics software. Broman, K. W., &amp; Woo, K. H. (2017). Data organization in spreadsheets (No. e3183v1). https://doi.org/10.7287/peerj.preprints.3183v1 – An excelllent review of best practices for organizing data in a spreadsheet. 3.4.1.1.3 The read_excel function read_excel is a beautifully flexible function because Excel. Data can be in different sheets and there can be different datasets within a single sheet. And, researchers tend to use Excel like a blackboard in that an Excel sheet often contains calculations such as means, standard deviations and t-tests. When using read_excel it is important to send the function enough information to read the correct data. For the exp1h data, if we simply used exp1h &lt;- read_excel(file_path) %&gt;% clean_names() %&gt;% data.table() without specifying the sheet, read_excel defaults to reading the first sheet (“A”), which is not what we wanted. We can specify the exact range to important using the range = argument exp1h &lt;- read_excel(file_path, sheet = &quot;H&quot;, range = &quot;A1:F49&quot;) %&gt;% clean_names() %&gt;% data.table() This isn’t necessary for these data because the “H” sheet contains only a matrix of data and not extraneous information and the read_excel function is smart enough to figure this out. For many of the data sets in wet bench experimental biology, the range argument will be crucial because multiple datasets are archived on a single sheet. We can also specify other arguments. For example, the following code explicitly tells read_excel to use the first row of the data range to be used as the column names and not the first row of data. TRUE is the default argument, so this isn’t necessary for the exp1h data but if the data we want to import doesn’t have column names (and this can be common in archived data sets) then we need to pass the argument “col_names = FALSE”. exp1h &lt;- read_excel(file_path, sheet = &quot;H&quot;, range = &quot;A1:F49&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% data.table() 3.4.1.1.4 A quick plot of the exp1h data Just for fun, let’s use the ggboxplot function from the ggpubr package to reproduce something close to Fig. 1H. We could make a plot that looks exactly like Fig. 1H but this would require some of the wrangling outlined below. ggboxplot(data = exp1h, x = &quot;diet&quot;, y = &quot;tag_protein&quot;, ylab = &quot;TAG/Protein&quot;, xlab = &quot;&quot;, add = &quot;jitter&quot;) Notes on the code to make this plot the names of the columns to use as the x and y axes were passed inside quotes. In some R functions, column names are passed inside quotes and in other R functions, the column names are passed as-is. There is little rhyme or reason for this. 3.4.1.2 Importing text files Source article: Corrigan, June K., et al. “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice.” Elife 9 (2020): e53560.. file name: mmpc_all_phases.csv Let’s open the data for Fig. 1. Steps Copy the title of the paper title, which is “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice” Create a new folder within “data”. Name the folder the title of the paper by pasting from the clipboard. This is the “data from” folder, since it contains the data from the publication with the title of the folder. Click on the link to the file above, highlight all values in the window by clicking inside the window and typing command-a (Mac os) or control-a (Windows) and copy. Paste into a an open, blank text document and save it as “mmpc_all_phases.csv”. If saved elsewhere, move this file into the “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice” folder within the “data” folder. If you don’t have a dedicated text editor such as BBEdit on MacOS, then open a new R (not Rmd!) document in R studio. This will open as a new tab in the Script Editor pane. Paste the data into the R document window. Save it as “mmpc_all_phases.csv”. R Studio will ask to be sure you want the extension to be .csv and not .R. Yes you are sure. If saved elsewhere, move this file into the “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice” folder within the “data” folder. A .csv file is a comma-delimited text file, which means that the entries of a row are separated by commas. A text file is readable by any text editor software and most other kinds of software. Datasets that are stored as text files are typically saved as either .csv (where the entries of a row are separated by commas) or .txt (where the entries are separated by tabs). The base R way to read a .csv file is using read.csv. The read.table function is more versatile, as the delimiter can be specified. The function fread() from the data.table package is fast, smart, and flexible. It is smart in the sense that it guesses what the delimter is. # construct file path data_from &lt;- &quot;A big-data approach to understanding metabolic rate and response to obesity in laboratory mice&quot; file_name &lt;- &quot;mmpc_all_phases.csv&quot; file_path &lt;- here(data_folder, data_from, file_name) # open file and assign to exp1, then pipe to clean_names # to change column names to snake_case exp1 &lt;- fread(file_path) %&gt;% clean_names() Be sure to read the section on importing Excel files above to understand this code. Here, as with the import of the Excel file, the first three lines create the directory path to the file. There is no need to pipe exp1 to the data.table function because fread automatically opens the data as a data.table. As with the imported Excel file above, this code sends the data object to clean_names to change the column labels to snake_case. 3.4.1.2.1 A quick plot of the exp1 data Let’s use the ggscatter function from the ggpubr package to reproduce something close to Fig. 1D. We could make a plot that looks exactly like Fig. 1D but this would require some additional plot arguments. ggscatter(data = exp1[diet == &quot;HF&quot; &amp; acclimation == TRUE], x = &quot;total_mass&quot;, y = &quot;ee&quot;, color = &quot;institution&quot;, palette = pal_okabe_ito_4, add = &quot;reg.line&quot;, ylab = &quot;energy expenditure (kcal/hr)&quot;, xlab = &quot;body mass (g)&quot;) 3.4.1.3 Troubleshooting file import If you get an error that starts with “Error: path does not exist:” then R is not “seeing” your specified file given the path you’ve given it. Make sure that you are working within the project by checking the project name on the right side of the R Studio tool bar. If the project is “None” or you are working within a different project, then open the new project. Make sure you loaded the package here in a “setup” chunk and that you have run the setup chunk Make sure you have assigned data_folder &lt;- \"data\" in the setup chunk and have run the chunk. Make sure your “data” folder is one level inside your project folder. “one level” means it is not buried deeper inside other folders within the project folder. Make sure your “data from” folder (the folder with the title of the publication) is one level inside your “data” folder Make sure your data file is one level inside the correct “data from” folder. Bug alert Make sure you have the name of the “data from …” folder correct in your script. Do not type the name of the folder. Instead, go to the finder and highlight the folder containing the data file, copy the name, return to the R markdown script, type folder &lt;- \"\" and then paste the clipboard (the name of the folder) in between the quote marks. Bug alert Make sure the file name is correct in the script. As with the folder name, I go to the finder and copy the file name and paste it in place. In Windows use ctrl-a instead of ctrl-c to copy the full filename including the extension. More generally, Humans are very good at understanding misspelld and OdDLy capitalized words but the R language (or any computer language) is very literal. R is case sensitive (some programming languages are not). “Prenatal acoustic communication”, “Prenatal Acoustic Communication”, and “prenatal acoustic communication” are all different values. Spelling AND capitalization have to be perfect, not simply close. Spelling includes spaces. A frequent bug is a file name typed as “Prenatal acoustic communication” when the actual name is “Prenatal acoustic communication”. Can you spot the bug? The original (what we need to copy) has two spaces between “acoustic” and “communication” while the incorrect copy has only one. Spelling bugs are avoided by simply copying and pasting names of folders, names of files, column names of data frames, and level names of factors, which leads to a general rule of R scripting… 3.4.1.4 Rule number one in R scripting {# rule1} Always copy and paste any text that will be inserted into quotes Do not try to type it out. You have been warned. 3.5 Data wrangling Data archived in Excel spreadsheets, at least in wet-bench experimental biology projects, are generally not in a format this is readily analyzed in R, or any statistical software other than perhaps Graphpad Prism. Use these examples as templates for how to import and wrangle Excel-archived data in your project. 3.5.1 Reshaping data – Wide to long 3.5.1.1 Wide to long – Adipsin data Source: Adipsin preserves beta cells in diabetic mice and associates with protection from type 2 diabetes in humans Public source – the Adipsin paper is behind a paywall. A public source of the paper from NIH is available. Link to source data Fig. 1k of the Adipsin paper presents a bar plot of the glucose uptake in response to control (GFP) or adipsin treatment. A screenshot of the Excel-archived data is shown above. The data are in wide format. In wide-format, the values of a single variable (here, this is glucose uptake level) are given in separate columns for each treatment level (group). The values for the GFP group are in Column A and the values for the Adipsin group are in Column B. Wide format is efficient for computations in a spreadsheet, such as computing means and standard deviations of columns of data, and for plotting. For most statistical analyses of experimental data in R (and most statistics software), all values of a single variable should be in a single column. This is called long format. I’ve manually rearranged the data from the archived spread sheet into long format by stacking each group’s values into a single column, shown in the screen capture below. All values of glucose uptake are in a single column. In long format, there needs to be a way to identify which values belong to which group and this is achieved here with column “treatment”. In adition to the treatment column. The difference between wide and long also reflects how we think about statistical analysis. When we do a t-test to compare the means of glucose uptake between GFP and Adipsin groups, we might think we have two things: the set of glucose uptake values for the GFP group and the set of values for the Adipsin group. When we fit a linear model, we also have two things, the variable treatment containing treatment level assignment and the variable glucose_uptake containing the glucose uptake values. In wide format, there is nothing to suggest that treatment is a variable. There are many functions to tidy data from wide to long. melt from the data.table package is especially useful. It is data.table’s version of melt from the reshape2 package. The major arguments of data.table::melt are melt(data, id.vars, measure.vars, variable.name, value.name) melt takes the data in the columns listed in measure.vars and stacks these into a single column named value.name. The names of the columns in measure.vars are the values of the elements in a new column named variable.name. The elements of any column in id.vars are repeated p times, where p is the number of columns that were stacked. Let’s melt the three different response variables of the adipsin data and merge them into a single data.table. There are several ways to combine data sets including merge and cbind. We’ll compare these later. file_folder &lt;- &quot;Adipsin preserves beta cells in diabetic mice and associates with protection from type 2 diabetes in humans&quot; fn &lt;- &quot;41591_2019_610_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, file_folder, fn) treatment_levels &lt;- c(&quot;db/db-GFP&quot;, &quot;db/db-Adipsin&quot;) # as separate line fig_1k_wide &lt;- read_excel(file_path, sheet = &quot;Figure 1k&quot;, range = &quot;A3:B9&quot;) fig_1k_wide &lt;- data.table(fig_1k_wide) fig_1k &lt;- melt(fig_1k_wide, measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_uptake&quot;) # or piped -- which do you prefer? fig_1k &lt;- read_excel(file_path, sheet = &quot;Figure 1k&quot;, range = &quot;A3:B9&quot;) %&gt;% data.table() %&gt;% melt(measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_uptake&quot;) # View(fig_1k) # highlight without the comment sign and &quot;run selected lines()&quot; to view A pretty-good-plot using the ggpubr package # put warning=FALSE into the chunk header to supress the warning gg &lt;- ggstripchart(x = &quot;treatment&quot;, y = &quot;glucose_uptake&quot;, add = &quot;mean_se&quot;, data = fig_1k) gg 3.5.1.2 Wide to long – Enteric nervous system data Source: Rolig, A. S., Mittge, E. K., Ganz, J., Troll, J. V., Melancon, E., Wiles, T. J., … Guillemin, K. (2017). The enteric nervous system promotes intestinal health by constraining microbiota composition. PLOS Biology, 15(2), e2000689. Source data Let’s import and reshape the data for figure 2d. Look at the excel file and the data in Fig. 2d. There is a single treament with four levels, but the authors have organized the data in each level in separate columns and used the column header as the level name. Let’s melt the data from wide to long by stacking the four columns into a single column “neutrophil_count” and adding a treatment column identifying the group. folder &lt;- &quot;The enteric nervous system promotes intestinal health by constraining microbiota composition&quot; filename &lt;- &quot;journal.pbio.2000689.s008.xlsx&quot; file_path &lt;- here(data_folder, folder, filename) # figure 2D data sheet_i &lt;- &quot;Figure 2&quot; range_i &lt;- &quot;F2:I24&quot; fig_2d_wide &lt;- read_excel(file_path, sheet=sheet_i, range=range_i) %&gt;% clean_names() %&gt;% data.table() # change column names by replacing without &quot;_donor&quot; in each name # these new column names will become the levels of the treatment factor new_colnames &lt;- c(&quot;gf&quot;, &quot;wt&quot;, &quot;sox10&quot;, &quot;iap_mo&quot;) setnames(fig_2d_wide, old=colnames(fig_2d_wide), new=new_colnames) # wide to long fig_2d &lt;- melt(fig_2d_wide, measure.vars=colnames(fig_2d_wide), variable.name=&quot;treatment&quot;, value.name=&quot;neutrophil_count&quot;) # omit empty rows fig_2d &lt;- na.omit(fig_2d) # re-order factors fig_2d[, treatment := factor(treatment, levels = c(&quot;wt&quot;, &quot;gf&quot;, &quot;sox10&quot;, &quot;iap_mo&quot;))] # View(fig_2d) To learn (instead of just copy and modify), it’s best to do this in steps and not run the whole chunk. At each step, look at the result using View. The script above includes three extra wrangling steps. Changing column names in fig_2d_wide. The column names in wide format will become the treatment level names of the treatment factor after reshaping. It will be easier down the road if these names are shorter and the “_donor” in each name is redundant. The setnames function renames the column names. For these data, the number of measures within the different treatments differs and, as a consequence, there are multiple cells with NA which indicates a missing value. View(fig_2d_wide) (this can be typed in the console) to see this. After reshaping to long format (fig_2d), the rows with missing values become empty rows – there is no useful information in them (View this). To see this, re-run the lines of the chunk up to the line “# omit empty rows”. The na.omit function deletes any row with missing values. Here, this deletes these information-less rows. Be very careful with na.omit. You do not want to delete rows of data that contain information you want. For both analysis and plots, we want to compare values to the control level, which is named “wt” for the fig_2d data. That is, we want “wt” to be the reference level. To achieve this, the levels of the factor treatment need to be re-ordered using the levels argument. (note, I typically do not add “levels =”, but simply pass the list of levels) 3.5.1.3 Wide to long – bee data The example above is pretty easy, because the all columns in the original data frame are melted (stacked). Here is an example in which only a subset of columns are stacked. In addition, only a subset of the remaining columns are retained in the long format data frame. The data are from Panel A of supplement Fig. 8 (https://journals.plos.org/plosbiology/article/file?type=supplementary&amp;id=info:doi/10.1371/journal.pbio.2003467.s019){target=“_blank”} from Source: Kešnerová, L., Mars, R.A., Ellegaard, K.M., Troilo, M., Sauer, U. and Engel, P., 2017. Disentangling metabolic functions of bacteria in the honey bee gut. PLoS biology, 15(12), p.e2003467. Source data folder &lt;- &quot;Data from Disentangling metabolic functions of bacteria in the honey bee gut&quot; filename &lt;- &quot;journal.pbio.2003467.s001.xlsx&quot; # figure 2D data sheet_i &lt;- &quot;S8 Fig&quot; range_i &lt;- &quot;A2:H12&quot; file_path &lt;- here(data_folder, folder, filename) fig_s8a_wide &lt;- read_excel(file_path, sheet=sheet_i, range=range_i) %&gt;% clean_names() %&gt;% data.table() # wide to long stack_cols &lt;- paste0(&quot;replicate&quot;, 1:5) fig_s8a &lt;- melt(fig_s8a_wide, id.vars = c(&quot;media&quot;, &quot;time_h&quot;), measure.vars = stack_cols, variable.name = &quot;Replicate&quot;, value.name = &quot;OD600&quot;) # measure of absorbance at 600nm 3.5.1.4 Wide to long – stacking multiple sets of columns Article: GPR109A mediates the effects of hippuric acid on regulating osteoclastogenesis and bone resorption in mice source data The data are from Fig. 1b folder &lt;- &quot;GPR109A mediates the effects of hippuric acid on regulating osteoclastogenesis and bone resorption in mice&quot; file_name &lt;- &quot;42003_2020_1564_MOESM4_ESM.xlsx&quot; file_path &lt;- here(data_folder, folder, file_name) fig1b_wide &lt;- read_excel(file_path, sheet = &quot;Fig. 1b&quot;, range = &quot;C4:R9&quot;, col_names = FALSE) %&gt;% data.table() treatment_levels &lt;- c(&quot;Wild type&quot;, &quot;GPR109A-/-&quot;) treatment_abbrev &lt;- c(&quot;wt&quot;, &quot;ko&quot;) measures &lt;- c(&quot;BV/TV&quot;, &quot;Tb.Th&quot;, &quot;Tb.Sp&quot;, &quot;Tb.N&quot;, &quot;BMD&quot;, &quot;Cs.Th&quot;, &quot;BV&quot;, &quot;MA&quot;) new_colnames &lt;- paste(rep(measures, each = 2), treatment_abbrev, sep = &quot;_&quot;) setnames(fig1b_wide, old = colnames(fig1b_wide), new = new_colnames) BV/TV_wt BV/TV_ko Tb.Th_wt Tb.Th_ko Tb.Sp_wt Tb.Sp_ko Tb.N_wt Tb.N_ko BMD_wt BMD_ko Cs.Th_wt Cs.Th_ko BV_wt BV_ko MA_wt MA_ko 5.229493 6.697880 0.0379685 0.0436415 0.3383573 0.3507201 1.377323 1.534749 0.0927138 0.1125594 0.1363595 0.1445718 0.2296399 0.2250816 0.3832690 0.4268379 5.171787 7.271222 0.0386800 0.0425357 0.3579566 0.3946835 1.337071 1.709440 0.0930417 0.1129723 0.1387652 0.1430709 0.1946119 0.2422370 0.4216582 0.3553190 4.287585 9.873900 0.0385307 0.0438999 0.4401164 0.3019478 1.112770 2.249185 0.0782485 0.1330846 0.1334085 0.1588262 0.1568608 0.3700646 0.3620063 0.4698913 4.308528 5.557610 0.0374626 0.0372947 0.3576182 0.3328132 1.150088 1.490186 0.0841214 0.1007773 0.1369047 0.1288561 0.1555844 0.2050777 0.3661425 0.4413361 5.175202 6.673029 0.0406120 0.0400175 0.4541035 0.3604716 1.274303 1.667530 0.0921535 0.1189364 0.1299124 0.1438411 0.1521829 0.2224382 0.3578702 0.4556137 To analyze each response, the two treatment levels for each measure need to be stacked into a single column. This is easy using melt from the data.table package. Add this to your chunk. fig1b &lt;- melt(fig1b_wide, measure.vars = list( c(&quot;BV/TV_wt&quot;, &quot;BV/TV_ko&quot;), c(&quot;Tb.Th_wt&quot;, &quot;Tb.Th_ko&quot;), c(&quot;Tb.Sp_wt&quot;, &quot;Tb.Sp_ko&quot;), c(&quot;Tb.N_wt&quot;, &quot;Tb.N_ko&quot;), c(&quot;BMD_wt&quot;, &quot;BMD_ko&quot;), c(&quot;Cs.Th_wt&quot;, &quot;Cs.Th_ko&quot;), c(&quot;BV_wt&quot;, &quot;BV_ko&quot;), c(&quot;MA_wt&quot;, &quot;MA_ko&quot;)), variable.name = &quot;treatment_id&quot;, value.name = measures) fig1b[, treatment := ifelse(treatment_id == 1, treatment_levels[1], treatment_levels[2])] fig1b[, treatment := factor(treatment, levels = treatment_levels)] treatment_id BV/TV Tb.Th Tb.Sp Tb.N BMD Cs.Th BV MA treatment 1 5.229493 0.0379685 0.3383573 1.377323 0.0927138 0.1363595 0.2296399 0.3832690 Wild type 1 5.171787 0.0386800 0.3579566 1.337071 0.0930417 0.1387652 0.1946119 0.4216582 Wild type 1 4.287585 0.0385307 0.4401164 1.112770 0.0782485 0.1334085 0.1568608 0.3620063 Wild type 1 4.308528 0.0374626 0.3576182 1.150088 0.0841214 0.1369047 0.1555844 0.3661425 Wild type 1 5.175202 0.0406120 0.4541035 1.274303 0.0921535 0.1299124 0.1521829 0.3578702 Wild type 3.5.2 Reshaping data – Transpose (turning the columns into rows) 3.5.2.1 Transpose – PI3K inhibitors data Source: Suppression of insulin feedback enhances the efficacy of PI3K inhibitors Source data Figure 3A of this publication is a plot of blood glucose level taken on the same individual mice from four treatment groups over six time periods. Data on a single variable such as blood glucose, taken on the same individual at multiple time points, are known as longitudial data but are often mistakenly called repeated measures data. There are mulitple ways to analyze longitudinal data, some goood, some less good. There are two reasonable ways to archive longitudinal data for analysis in R. The Excel-archived data for Figure 3A is neither. A screen capture of two of the four treatment groups is shown below. In the archived data the individual mice are in columns. The measure at each time point is in rows. And the treatment group is in blocks. Typical data for analysis in R should have the individual mice in rows and each variable in columns (an exception in experimental biology is omics data, such as RNA expression levels. Many packages with functions to analyze these data have the genes on each row and the individual on each column). The Figure 3A data are turned on its side. We need to transpose the data, or rotate the matrix 90 degrees (make the columns rows and the rows columns) to turn the data into wide format. From this we can create a new data.table with the data in long format. folder &lt;- &quot;Suppression of insulin feedback enhances the efficacy of PI3K inhibitors&quot; filename &lt;- &quot;41586_2018_343_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, folder, filename) pi3k_side &lt;- read_excel(file_path, sheet = &quot;Figure 3A (Blood Glucose)&quot;, range = &quot;A2:U7&quot;, col_names = FALSE) %&gt;% data.table() # give columns names as the treatment of each mouse # verify n=5 per group treatment_levels &lt;- c(&quot;Chow&quot;, &quot;Ketogenic&quot;, &quot;Metformin&quot;, &quot;SGLT2i&quot;) colnames(pi3k_side) &lt;- c(&quot;time&quot;, rep(treatment_levels, each = 5)) # transpose # keep colnames in &quot;side&quot; as values of treatment col in &quot;wide&quot; # make values of &quot;time&quot; in &quot;side&quot; the colnames in &quot;wide&quot; pi3k_wide &lt;- transpose(pi3k_side, keep.names = &quot;treatment&quot;, make.names = &quot;time&quot;) # make a baseline column pi3k_wide[, glucose_0 := get(&quot;0&quot;)] # make-up a mouse id for each mouse pi3k_wide[, id := paste(treatment, 1:.N, sep = &quot;_&quot;), by = treatment] # make treatement a factor with &quot;chow&quot; as reference pi3k_wide[, treatment := factor(treatment, treatment_levels)] # make a long version pi3k_long &lt;- melt(pi3k_wide, id.vars = c(&quot;treatment&quot;, &quot;id&quot;, &quot;glucose_0&quot;), variable.name = &quot;time&quot;, value.name = &quot;glucose&quot;) Notes Read the comments on the usage of the keep.names and make.names arguments of transpose. These are powerful. pi3k_wide has column names that are times (in minutes). This presents wrangling problems (column names shouldn’t be numbers. Here it is useful to create the long format data.table with a time column of numbers). For example, the code above creates copies the column “0” into a new column “glucose_0” using glucose_0 := get(\"0\"). Had the code been glucose_0 := \"0\", all values would be the character “0”. Had the code been glucose_0 := 0, all values would be the number 0. get looks for the column with the name of whatever is inside the parentheses. Let’s do a quick plot to examine the data qplot(x = time, y = glucose, data = pi3k_long, color = treatment) + geom_line(aes(group = id)) 3.5.3 Combining data Source Bak, A.M., Vendelbo, M.H., Christensen, B., Viggers, R., Bibby, B.M., Rungby, J., Jørgensen, J.O.L., Møller, N. and Jessen, N., 2018. Prolonged fasting-induced metabolic signatures in human skeletal muscle of lean and obese men. PloS one, 13(9), p.e0200817. Source data The data are from a randomized crossover design where 18 men (9 lean and 9 obese) were measured for multiple metabolic markers at two times: 1) in a post-absorptive state after 12 hours overnight fast, and 2) in a prolonged fasting state after 72 hours of fasting. In addition, at each time point, metabolic markers were measured prior to and after an insulin infusion. Here, we want to reproduce values in Table 2, which are measures of mean blood insulin and metabolite levels after 12 hours and 72 hours fasting in both the lean and obese groups. A difficulty for the analyst is that the response data are in the “Table 2” sheet but the variable containing the assignment to “lean” or “obese” group is in the “Table 1” sheet. To analyze these response, the two datasets need to be combined into a single data frame. The important consideration when combining data is that like is matched with like. For the fasting dataset, “like” is the subject id, and we have some data for each subject id in Table 1 and other data for the same subject ids in Table 2. This means that we essentially want to glue the columns of table 2 to the columns of table 1 in a way that insures that the correct data for each subject id is on the same row. This is a bit more complicated for these data because Table 1 contains 18 data rows, one for each subject id and Table 2 contains 36 data rows, 2 for each subject id, because each subject has data measured at 12 hours and at 72 hours. 3.5.4 Subsetting data It is common to see researchers create multiple subsets of data for further processing. This practice should be be discouraged because the same variables will be in multiple data frames and it can be hard to keep track of any processing of variables in the different datasets. Instead, subset the data at the level of analysis. There are many ways to subset data in R. Experienced users tend to divide up into those using base R, those using the tidyverse packages, or those using data.table. Learn one well. This book uses data.table. Before outlining usage in data.table, let’s back up a bit and review different indexing systems. In Excel, rows are specified (or “indexed”) by numbers and columns by letters. Every cell has an address, for example C2 is the cell in the 2nd row and 3rd column. Notice that in Excel, the column part of the address comes before the row part. In statistics, it is extremely common to use a system where \\(x_{ij}\\) is the value of the element in the ith row and jth column of the matrix X. Notice that in this notatin, the row index (i) comes before the column index (j). In programming languages, including R, it is extremely common to use a system where my_data[i, j] is the value of the element in the ith row and jth column of the matrix-like object named “my_data” (such as a data frame in R). data.table explicitly refers to the row index and column index as i and j. 3.5.4.1 Specifying a subset of rows (“observations” or “cases”) A subset of rows is specified using either a list of row numbers or In a data.table, a subset of rows is specified using either a list of row numbers or a combination of comparison operators (==, !=, &gt;, &lt;, &gt;=, &lt;=, %in%) and Boolean logic operators (&amp;, |, ! – these are “and”, “or”, “not”) as i. Let’s use the pi3k_long data from above to explore this. First, the plot of plasma glucose for all individuals in each treatment group across all time points. qplot(x = time, y = glucose, data = pi3k_long, color = treatment) + geom_line(aes(group = id)) pi3k_long[treatment == \"Chow\",]) is the subset of rows in which entries in the column “treatment” take the value “Chow” using the “is equal” (“==”) operator qplot(x = time, y = glucose, data = pi3k_long[treatment == &quot;Chow&quot;,], color = treatment) + geom_line(aes(group = id)) And the subset of rows in which entries in the column “treatment” take any value but “Chow” using the “not equal” operator (“!=”). qplot(x = time, y = glucose, data = pi3k_long[treatment != &quot;Chow&quot;,], color = treatment) + geom_line(aes(group = id)) The subset of rows in which entries in the column “treatment” take either the value “Chow” or the value “SGLT2i” by combining two “is equal” (“==”) operators using the OR (“|”) boolean operator qplot(x = time, y = glucose, data = pi3k_long[treatment == &quot;Chow&quot; | treatment == &quot;SGLT2i&quot;,], color = treatment) + geom_line(aes(group = id)) The subset of rows in which entries in the column “time” take either the value “30” or the value “60” using the “in a list” operator (%in%). The values in the “time” column look like integers but are actually treatment levels (which act like string or character variables). qplot(x = time, y = glucose, data = pi3k_long[time %in% c(&quot;30&quot;, &quot;60&quot;),], color = treatment) + geom_line(aes(group = id)) The subset of rows in which entries in the column “time_c” are less than or equal to 60 using the “less than or equal to” operator AND the value in the treatment column is in the list (“Chow”, “SGLT2i”). The two comparisons are combined with the AND (“&amp;”) Boolean operator. pi3k_long[, time_c := as.numeric(as.character(time))] qplot(x = time, y = glucose, data = pi3k_long[time_c &lt;= 30 &amp; treatment %in% c(&quot;Chow&quot;, &quot;SGLT2i&quot;),], color = treatment) + geom_line(aes(group = id)) The same result as above but using different operators. I would describe this as, the subset of rows in which entries in the column “time_c” are less than or equal to 60 using the “less than or equal to” operator AND the value in the treatment column is either “Chow” OR “SGLT2i”. The two comparisons are combined with the AND (“&amp;”) Boolean operator. The order of operations is determined by the parentheses, as with all algebra. pi3k_long[, time_c := as.numeric(as.character(time))] qplot(x = time, y = glucose, data = pi3k_long[time_c &lt;= 30 &amp; (treatment == &quot;Chow&quot; | treatment == &quot;SGLT2i&quot;),], color = treatment) + geom_line(aes(group = id)) 3.5.5 Wrangling columns 3.5.5.1 Creating new columns that are functions of values in existing columnes data.table allows math within the index arguments. Example 1: create a new column whose value is a function of values in other columns In this example, I create a column that contains the number of marked cells as a percent of the total cell count. # count to fraction - but probably want to analyze as a count! See xxx figx[, marked_cells_perc := marked_cells/total_cells * 100] Notes \\(\\texttt{marked_cells}\\) and \\(\\texttt{total_cells}\\) are columns in the data.table figx You almost certainly want to analyze the original count column and use \\(\\texttt{total_cells}\\) as an offset in the model. See the section Use a GLM with an offset instead of a ratio of some measurement per total Example 2: create a new column whose value is a function of values in other columns In this example, I create the column \\(\\texttt{glucose_auc}\\), which is the area under the curve of glucose measures at the different time points of glucose tolerance test. This computation requires the auc function below (typically, I put chunks with functions at the top of a R Markdown document). The computation uses the base R apply function. auc &lt;- function(x, y, method=&quot;auc&quot;){ # method = &quot;auc&quot;, auc computed using trapezoidal calc # method = &quot;iauc&quot; is an incremental AUC of Le Floch # method = &quot;pos.iauc&quot; is a &quot;positive&quot; incremental AUC of Le Floch but not Wolever # method = &quot;pb.auc&quot; is AUC of post-time0 values if(method==&quot;iauc&quot;){y &lt;- y - y[1]} if(method==&quot;pos.iauc&quot;){y[y &lt; 0] &lt;- 0} if(method==&quot;pb.auc&quot;){ x &lt;- x[-1] y &lt;- y[-1] } n &lt;- length(x) area &lt;- 0 for(i in 2:n){ area &lt;- area + (x[i] - x[i-1])*(y[i-1] + y[i]) } area/2 } Here is the implementation. The object time_cols is a vector containing the names of the columns containing the data at each time point. time_cols &lt;- c(&quot;time_0&quot;, &quot;time_15&quot;, &quot;time_30&quot;, &quot;time_60&quot;, &quot;time_90&quot;, &quot;time_120&quot;) Y &lt;- figx_wide[, .SD, .SDcols = time_cols] figx_wide[, glucose_auc := apply(Y, 1, auc, x = times)] 3.5.5.2 Change the reference level of a factor Factor levels should always be arranged in a sensible order both for model fitting and for plotting. The first level in the vector is the reference level, which is an important concept for understanding the coefficients of the statistical models fit in this book. treatment_levels &lt;- c(&quot;WT&quot;, &quot;KO&quot;, &quot;KO_drug&quot;) figx[, treatment := factor(treatment, levels = treatment_levels)] 3.5.5.3 Converting a single column with all combinations of a 2 x 2 factorial experiment into two columns, each containing the two levels of a factor 3.5.5.3.1 Example 1 (tstrspl) The example data are analyzed in the chapter Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”) Article source: TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise Public source The data are from Figure 2j. Data source data_from &lt;- &quot;TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise&quot; file_name &lt;- &quot;41586_2020_1992_MOESM4_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) treatment_levels &lt;- c(&quot;WT Resting&quot;, &quot;WT Active&quot;, &quot;Tlr9-/- Resting&quot;, &quot;Tlr9-/- Active&quot;) exp2j_wide &lt;- read_excel(file_path, sheet = &quot;2j&quot;, range = &quot;A5:D13&quot;, col_names = TRUE) %&gt;% data.table() colnames(exp2j_wide) &lt;- treatment_levels exp2j &lt;- melt(exp2j_wide, measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_uptake&quot;) %&gt;% na.omit() # danger! exp2j[, c(&quot;genotype&quot;, &quot;stimulation&quot;) := tstrsplit(treatment, &quot; &quot;, fixed = TRUE)] genotype_levels &lt;- c(&quot;WT&quot;, &quot;Tlr9-/-&quot;) stimulation_levels &lt;- c(&quot;Resting&quot;, &quot;Active&quot;) exp2j[, genotype := factor(genotype, levels = genotype_levels)] exp2j[, stimulation := factor(stimulation, levels = stimulation_levels)] # View(exp2j) Melting the wide data creates a column \\(\\texttt{treatment}\\) containing the column names of the wide data. The column type is character (often called “string” because each the variable is a string of characters). We want to split the string at the space, and put the first part into one new column and the second part into a second new column. The tstrsplit() function from the data.table package does this split for us. The first argument is the name of the column that contains the strings that we want to split. The second argument is the character that we want to split on. c(\"genotype\", \"stimulation\") before := sets the names of the new columns containing. This is a really elegant solution. I replaced the original column names of initial import for several reasons: “Electrical Stimulation” is too long and will make the coefficient names unwieldy. This could be shortened to “Stimulation”, but… I don’t want a factor name to include a level with the same name. I like the factor name \\(\\texttt{stimulation}\\), so I I renamed the level “Electrical Stimulation” to “Active”. We are splitting the column names using the space character ” ” and two of the original column names have two spaces, the 2nd inside what should be a single level (“Electrical Stimulation”) The read_excel function removed a space in the last column name, which is read as “Tlr9-/-Electrical Stimulation”. Here is a peek at the data prior to melting wide to long WT Resting WT Active Tlr9-/- Resting Tlr9-/- Active 5.437741 9.511914 7.969706 7.793486 6.026892 9.901527 9.660112 9.561649 7.731616 13.038920 6.709424 8.055529 6.285710 9.665391 5.259744 7.739374 7.343797 8.590693 8.244721 10.312190 And a peek of the first three and last three rows after the melt. treatment glucose_uptake genotype stimulation WT Resting 5.437741 WT Resting WT Resting 6.026892 WT Resting WT Resting 7.731616 WT Resting Tlr9-/- Active 10.312190 Tlr9-/- Active Tlr9-/- Active 7.943574 Tlr9-/- Active Tlr9-/- Active 9.292673 Tlr9-/- Active 3.5.5.3.2 Example 2 Source: Tauriello, D., Palomo-Ponce, S., Stork, D. et al. TGFβ drives immune evasion in genetically reconstituted colon cancer metastasis. Nature 554, 538–543 doi:10.1038/nature25492 Source data filename: “41586_2018_BFnature25492_MOESM10_ESM.xlsx” sheet: “Fig. 4h-tumours” The analysis of the data in Fig. 4h specifies a single \\(X\\) variable “Treatment” with four levels (or groups): “Con”, “Gal”, “aPD-L1”, and “Gal+aPD-L1”. These levels indicate that the design is actually factorial with two factors, each with two levels. The first factor has levels “no Gal” and “Gal”. The second factor has levels “no aPD-L1”, “aPD-L1”. The single column Treatment “flattens” the 2 X 2 factorial design to a 4 x 1 design. In general, we would want to analyze an experiment like this as factorial model, because this allows us to make inferences about the interaction effect between the two factors. For these inferences, we need a standard error, or a confidence interval, or a p-value of the estimate, which we can easily get from the factorial model. In order to analyze the data with a factorial model, we need to create two new columns – one column is the factor variable containing the two levels of Gal and one column is the factor variable containing the two levels of aPD-L1. gal_levels &lt;- c(&quot;no Gal&quot;, &quot;Gal&quot;) tumor[, gal := ifelse(treatment == &quot;Gal&quot; | treatment == &quot;Gal+aPD-L1&quot;, gal_levels[2], gal_levels[1])] apd_levels &lt;- c(&quot;no aPD-L1&quot;, &quot;aPD-L1&quot;) tumor[, apdl1 := ifelse(treatment == &quot;aPD-L1&quot; | treatment == &quot;Gal+aPD-L1&quot;, apd_levels[2], apd_levels[1])] # re-order factor levels tumor[, gal:=factor(gal, gal_levels)] tumor[, apdl1:=factor(apdl1, apd_levels)] A way to check the results to make sure that our conversion is correct is to compute the sample size for the 2 x 2 combinations, but include the original treatment column in the by list. tumor[!is.na(num_positive_per_mm), .(N=.N), by=.(treatment, gal, apdl1)] ## treatment gal apdl1 N ## 1: Con no Gal no aPD-L1 124 ## 2: Gal Gal no aPD-L1 89 ## 3: aPD-L1 no Gal aPD-L1 101 ## 4: Gal+aPD-L1 Gal aPD-L1 58 That looks good. Bug alert If you break Rule #1, and type in the treatment level “Gal+aPD-L1” as “Gal + aPD-L1”, then you will get new columns containing junk. ## treatment gal apdl1 N ## 1: Con no Gal no aPD-L1 124 ## 2: Gal Gal no aPD-L1 89 ## 3: aPD-L1 no Gal aPD-L1 101 ## 4: Gal+aPD-L1 no Gal no aPD-L1 58 Remember Rule #1. Always copy and paste any text that will be inserted into quotes. This is easily done here by typing unique(tumor$treatment) into the console. This function returns the unique values of the column “treatment” of the data.table “tumor”. unique(tumor$treatment) [1] “Con” “Gal” “aPD-L1” “Gal+aPD-L1” Now, copy the name of a level and paste into your code. Repeat until done. 3.5.6 Missing data Source: Deletion of Cdkn1b in ACI rats leads to increased proliferation and pregnancy-associated changes in the mammary gland due to perturbed systemic endocrine environment Source data Supplement Figure 1F of this paper shows weight as a function of age class and genotype for the whole body and 8 organs. There are some missing weights in the Excel-archived data. These missing data are designated with a minus “-” sign. To import these data in correctly, use the na = argument in the read_excel function. file_folder &lt;- &quot;Deletion of Cdkn1b in ACI rats leads to increased proliferation and pregnancy-associated changes in the mammary gland due to perturbed systemic endocrine environment&quot; file_name &lt;- &quot;journal.pgen.1008002.s008.xlsx&quot; file_path &lt;- here(data_folder, file_folder, file_name) fig_s1f &lt;- read_excel(file_path, sheet = &quot;all weights&quot;, range = &quot;A2:K57&quot;, na = &quot;-&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% data.table() fig_s1f[, genotype := factor(genotype, c(&quot;+/+&quot;, &quot;-/-&quot;))] fig_s1f[, age_class := ifelse(age_at_sac_wks &lt;= 6.0, &quot;4-6&quot;, &quot;8+&quot;)] # View(fig_s1f) Notes In R, a value of “NA” represents missing. The default value for na = is an empty (or blank) cell (not a space but a cell that is empty). na = accepts a list of strings, for example na = c(\"\", \"-99\", \"--\") that will all be read as na. 3.5.6.1 Handling missing data 3.5.6.1.1 Many base R functions used for summary measures require NA handling mean(fig_s1f[, ovary]) # returns &quot;NA&quot; ## [1] NA mean(fig_s1f[, ovary], na.rm = TRUE) # returns the mean ## [1] 0.2489524 sd(fig_s1f[, ovary]) # returns &quot;NA&quot; ## [1] NA sd(fig_s1f[, ovary], na.rm = TRUE) # returns the mean ## [1] 0.151694 sum(fig_s1f[, ovary]) # returns &quot;NA&quot; ## [1] NA sum(fig_s1f[, ovary], na.rm = TRUE) # returns the mean ## [1] 10.456 There are many ways to get the sample size for a particular variable. Be careful if using length() which counts NA as part of the vector of values. 3.5.6.1.2 The !is.na function is useful length(fig_s1f[, ovary]) ## [1] 55 length(fig_s1f[!is.na(ovary), ovary]) ## [1] 42 Notes !is.na(ovary) is taking the subset of rows of fig_s1f for which the value of “ovary” is not NA (!is.na is read “not is.na”) This is especially useful if you are creating your own code uses counts. Here I create a table of means, standard error of the mean, and 95% CIs of the mean for each genotype group. But first, this script generates the wrong N for each group (since there are missing values), although the mean and SD are correct. fig_s1f[, .(mean = mean(spleen, na.rm = TRUE), n = .N, sd = sd(spleen, na.rm = TRUE)), by = genotype] ## genotype mean n sd ## 1: -/- 0.5801333 21 0.13680480 ## 2: +/+ 0.2956667 34 0.04460855 To compute the correct n, which will be necessary for computing the SE and the CI, use !is.na spleen_summary &lt;- fig_s1f[!is.na(spleen), .(mean = mean(spleen), n = .N, sd = sd(spleen)), by = genotype] spleen_summary[, se := sd/sqrt(n)] spleen_summary[, lower := mean + se*qt(.025, (n-1))] spleen_summary[, upper := mean + se*qt(.975, (n-1))] spleen_summary ## genotype mean n sd se lower upper ## 1: -/- 0.5801333 15 0.13680480 0.03532285 0.5043734 0.6558933 ## 2: +/+ 0.2956667 27 0.04460855 0.00858492 0.2780201 0.3133132 3.5.6.1.3 ggplot functions automatically handle missing values with a useful warning. qplot(x = body_wt_g_sac, y = spleen, color = genotype, data = fig_s1f) 3.5.6.1.4 Regression model functions (lm, glm, gls, etc.) handle missing values by default Missing data in regression model functions such as lm are handled using the argument na.action = and the default is “na.omit”, which omits any rows that contain a missing value in one or more of the model variables (it includes rows if these contain missing values only in the columns not included in the model). It’s as if the user took the subset of data including only the columns containing the model variables and then deleted any row with missing values. Here is the coefficient table of the fit model object that did not explictly tell the lm function how to handle missing data. m1 &lt;- lm(spleen ~ body_wt_g_sac + genotype, data = fig_s1f) coef(summary(m1)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.04238009 0.0242993900 1.744081 8.902319e-02 ## body_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13 ## genotype-/- 0.23760586 0.0147600545 16.097898 8.072069e-19 Here is the coefficient table of the fit model object that did explicitly tell lm how to handle missing data, using the argument na.action = \"na.exclude\". These coefficient tables are the same. m2 &lt;- lm(spleen ~ body_wt_g_sac + genotype, data = fig_s1f, na.action = &quot;na.exclude&quot;) coef(summary(m2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.04238009 0.0242993900 1.744081 8.902319e-02 ## body_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13 ## genotype-/- 0.23760586 0.0147600545 16.097898 8.072069e-19 3.5.6.2 But…beware of fitted, predicted, or residual values from regression model functions unless you’ve explictly told the function how to handle missing values Use na.action = \"na.exclude\" if you want to add the fitted (or predicted) values or residuals as new columns in the original data object (fig_sf1). Compare the length of the fitted values vector from models m1 (using the default “na.omit”) and m2 (using the “na.exclude”). length(fitted(m1)) ## [1] 42 length(fitted(m2)) ## [1] 55 There are 55 observations (rows in the data) but only 42 complete rows with no missing values. The vector of fitted values from m1 has 42 fitted values. The vector of fitted values from m2 has 55 elements, the 42 fitted values plus 13 NA elements. This is important if we want to do something like add the fitted values (or residuals, or some function of these) to the original data object (fig_sf1). Here I compute the spleen weights adjusted to the mean body weight of the control (“+/+”) group using the residuals from m1 and m2. mean_x_control &lt;- mean(fig_s1f[genotype == &quot;+/+&quot;, body_wt_g_sac]) b &lt;- coef(m1) fig_s1f[, spleen_adj_m1 := b[1] + b[2]*mean_x_control + b[3]*(as.integer(genotype)-1 + residuals(m1))] fig_s1f[, spleen_adj_m2 := b[1] + b[2]*mean_x_control + b[3]*(as.integer(genotype)-1 + residuals(m2))] # View(fig_s1f) The computation of “spleen_adj_m1” returns a warning that the values of residuals(m1) were recycled (the first 42 elements of the new column were filled with the 42 residuals and the last 13 elements of the new column were filled with the first 13 residuals) – after the first row of missing data, all of these computed adjusted values are wrong. Using residuals(m2), the adjusted values are matched to the correct row and the rows with missing variables do not have an adjusted value (because there is no residual to compute this). 3.6 Saving data For many projects, it is uncommon to save data. I might save simulated data if it takes a long time (tens of minutes to hours or even days) to generate these and I simply want to work with the simulated data in the future and not have to regenerate it. Or I might save wrangled data if it takes a long time to import and wrangle and I want to analyze the wrangled data in the future and not have to re-import and re-wrangle it. If the data will only be used in this or future R projects, the data can be saved as an R object using saveRDS() data_from &lt;- &quot;Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice&quot; outfile_name &lt;- &quot;elife-59399-fig1h.Rds&quot; save_file_path &lt;- here(data_folder, data_from, outfile_name) saveRDS(object = exp1h, file = save_file_path) # to read this use exp1h &lt;- readRDS(save_file_path) Reading a large .Rds file is very fast compared to reading the same data stored as a text file. However, if the data need to be imported into some other software, such as a spreadsheet, then save the data as a text file. # save the data to correct data_from folder data_from &lt;- &quot;Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice&quot; # tab delimited outfile_name &lt;- &quot;elife-59399-fig1h.txt&quot; save_file_path &lt;- here(data_folder, data_from, outfile_name) write.table(exp1h, save_file_path, sep = &quot;\\t&quot;, quote = FALSE) # comma delimited outfile_name &lt;- &quot;elife-59399-fig1h.csv&quot; save_file_path &lt;- here(data_folder, data_from, outfile_name) write.table(exp1h, save_file_path, sep = &quot;,&quot;, quote = FALSE) Look at your project directory to make sure the file is where it should be! We used write.table() to create a tab-delimited text file using sep = \"\\t\" to specify tabs to separate the row elements. “ is the standard character string for a tab. Check in your data_from folder (with the name”Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice”) and open the file in a text editor. 3.7 Exercises Import and pretty-good-plot the data for Figure 2i of the Adipsin paper. You will need to download and archive the Excel file for “Figure 2”. Store this within the “Adipsin preserves beta cells…” folder. The data are the percent of cells staining for NKX6.1, which is a transcription factor protein that regulates beta cell development in the pancreas. Beta cells sense glucose levels in the blood and secrete insulin. Disruption of the insulin signaling system results in Diabetes mellitus. The data are in wide format, with each treatment group in a separate column. The data need to be melted into long format with a new column called “treatment”. This will give you a pretty good plot of the data (if the data object is named “adipsin_fig2i”) ggstripchart(data = adipsin_fig2i, x = &quot;treatment&quot;, y = &quot;nkx6_1&quot;, add = &quot;mean_se&quot;) Import and quick pretty-good-plot the data for Figure 3b of the PI3K paper. You will need to download and archive the Excel file for “Figure 3”. Store this within the “Suppression of insulin feedback enhances…” folder. The data are c-peptide levels in response to the treatments. C-peptide is cleaved from the pro-insulin polypeptide and circulates in the blood and is a marker of how much insulin is being produced by the beta cells of the pancreas. The data are in wide format, with each treatment group in a separate column. The data need to be melted into long format with a new column called “treatment”. Modify the code from exercise 1 to pretty-good-plot the data as in exercise 1. Corrigan, June K., et al. “A big-data approach to understanding metabolic rate and response to obesity in laboratory mice.” Elife 9 (2020): e53560.)↩︎ "],["plotting-models.html", "Chapter 4 Plotting Models 4.1 Pretty good plots show the model and the data 4.2 Working in R", " Chapter 4 Plotting Models So, along the lines of Sarah Susanka’s “Not So Big House,” Kolbert asks the group, “What would a Pretty Good House look like?” – Michael Maines2 Plots should be the focus of both the reader and researcher. Instead of mindless plotting, a researcher should ask a series of questions of every plot What is the point of each element in a plot? Are these the points that I most want to communicate? Are there better practices for communicating these points? Are the points that I want to communicate that are not covered by these elements? The answer to these questions should inform what is and what is not plotted. The result is a pretty good plot. The idea of a pretty good plot is borrowed from the “pretty good house” concept that grew out of a collaborative group of builders and architects in Northern New England. The “pretty good house” combines best practices for building an earth friendly, high performance home at a reasonable cost. There is no pretty good house governing body that awards certificates of achievement but, instead, a set of metrics and a collection of building practices that can achieve these. A typical pretty good plot contains some combination of Modeled effects with confidence intervals. “Effects” are differences between groups in response to treatment – the raison d’etre of an experiment. Modeled means and confidence intervals. Individual data points or a summary distribution of these. 4.1 Pretty good plots show the model and the data The data to introduce best practices in plotting come from Figure 2d and Figure 2e from “ASK1 inhibits browning of white adipose tissue in obesity”, introduced in the introductor chapter (Analyzing experimental data with a linear model) 4.1.1 Pretty good plot component 1: Modeled effects plot Figure 4.1: Effects plot of glucose AUC data Biologists infer the biological consequences of a treatment by interpreting the magnitude and sign of treatment “effects”, such as the differences in means among treatment levels. Why then do we mostly plot treatment level means, where effect magnitude and sign can only be inferred indirectly, by mentally computing differences in means? A pretty good plot directly communicates treatment effects and the uncertainty in the estimates of these effects using an effects plot. Figure ?? is an effects plot of the linear model fit to the glucose tolerance data. The effects plot is “flipped”. The y-axis is the categorical variable – it contains the labels identifying the pair of groups in the contrast and the direction of the difference. In addition to the pairwise comparisons, I include the interaction effect on the y-axis. The x-axis is the continuous variable – it contains the simple effects, which is the difference in means between the two groups identified by the y-axis labels. Additionally, the y-axis includes the estimate of the \\(diet \\time genotype\\) interaction effect. The bars are 95% confidence intervals of the effects (either simple effects or interaction effect), which is the range of values that are compatible with the observed data at the 95% level. We can use the effects and CIs of the effects to evaluate the treatment effects. For example, when on a high fat diet (HFD), the mean, post-baseline plasma glucose level in the ASK1\\(\\Delta\\)adipo is 3.5 mmol/L less than that for the control (ASK1F/F). Differences less than 5.3 mmol/L less than ASK1F/F levels or greater than 1.7 mmol/L less than ASK1F/F levels are not very compatible with the data. It is up to the research community to decide if 1.7 mmol/L or 3.5 mmol/L differences are physiologically meaningful effects. 4.1.2 Pretty good plot component 2: Modeled mean and CI plot Figure 4.2: Response plot The response plot in Figure 4.2 “shows the model” – by this I mean the plot shows the modeled means, represented by the large circles, the modeled 95% confidence intervals of each mean, represented by the error bars, and the model-adjusted individual response values, represented by the small colored dots. What do I mean by modeled means, modeled error intervals, and model-adjusted responses? ask1 diet N Sample mean Sample sigma Sample SE Model mean Model sigma Model SE ASK1F/F chow 10 14.35 1.48 0.47 14.35 1.85 0.59 ASK1F/F HFD 13 19.40 3.60 1.00 18.63 1.85 0.52 ASK1Δadipo chow 8 13.87 1.26 0.44 14.01 1.85 0.66 ASK1Δadipo HFD 14 15.92 1.73 0.46 15.53 1.85 0.49 The modeled means and error intervals are estimated from the statistical model. Many published plots show the sample means and sample error intervals, which are computed within each group independently of the data in the other groups and are not adjusted for any covariates or for any hierarchical structure to the data. A modeled mean will often be equal to the raw mean, but this will not always be the case. Here, the modeled means for the non-reference groups in Figure ?? do not equal the sample means because the modeled means are adjusted for the baseline measures of glucose (Table ??) (specifically, the modeled means are conditional on the baseline being equal to the mean of the baseline in the reference group). For most of the analyses in this text, modeled error intervals are not the same as the sample error intervals and are commonly conspicuously different. For the glucose tolerance data, the modeled error intervals are calculated from a pooled estimate of \\(\\sigma\\) while the sample error intervals are estimated from sample-specific estimates of \\(\\sigma\\) (Table ??). Model-adjusted responses are responses that are adjusted for covariates in the model. If there are no covariates in the model, the model-adjusted responses are the same as the raw response. In the glucose tolerance data, the model-adjusted responses are the modeled, individual response measures if all individuals had the same baseline glucose (the covariate). Modeled means, error intervals, and responses are not commonly plotted but it is these values that are consistent with our inferences from the statistical model. There are many data sets in experimental biology where a plot of sample means, error intervals, and responses give a very distorted view of inference from the model. The response plot in Figure 4.2 also “shows the data” by plotting response values as “jittered” dots. Showing the data allows the reader to get a sense of the underlying sample size and distribution including outliers, which can be used to mentally model check the published statistical analysis. Adding a box plot, violin plot, or dot plot augments the communication of the distributions if there are enough data to justify the addition. allows a reader to see the overlap in individual responses among groups and to evaluate the biological consequences of this overlap. 4.1.3 Combining Effects and Modeled mean and CI plots – an Effects and response plot. Figure 4.3: Effect of diet and ASK1 deletion on post-baseline glucose. Top: effects plot of 2 X 2 simple effects (difference in means) and of the diet X genotype interaction. Bars are 95% confidence intervals of the effects. Unadjusted p-values from the linear model are given. Bottom: response plot of the means and 95% confidence interval of each diet X genotype combination. Combining the effects and response plots into a single plot is an easy solution to issues that arise if only one or the other is used. What are these issues? While a response plot like that in Figure 4.2 is standard in biology, it fails to show the effects, and the uncertainty in the effects, explicitly. To infer the effects from the plot, a reader must perform mental math – either compute the difference or the ratio between pairs of means. This mental math is easy enough if the comparisons are between individual treatment levels but much harder if the comparisons are between pooled sets of treatment levels, for example in a factorial experimental design. The mental math that is excessively difficult is the reconstruction of some kind of error interval of the contrasts, for example the 95% confidence intervals in Figure ?? and it is these intervals that are necessary for a researcher to infer the range of biological consequences that are compatible with the experiment’s results. The inclusion of the p-values for all pairwise comparisons in a response plot gives the significance level of these contrasts, but of the kinds of summary results that we could present (contrasts, error intervals, p-values), the p-values are the least informative. Effects plots are very uncommon in most of biology outside of meta-analysis and clinical medicine more generally. An effects plot alone fails to communicate anything about the sample size or conditional distribution of the data. Equally important, response values are often meaningful and researchers working in the field should be familiar with usual and unusual values. This can be useful for interpreting biological consequences of treatment effects but also for researchers and readers to asses the credibility of the data (for example, I have twice, once in my own data and once in a colleagues data, found mistakes in the measurement of an entire data set of response variable because the plotted values weren’t credible). 4.1.4 Some comments on plot components Several recent criticisms of bar plots have advocated box plots or violin plots as alternatives. Box plots and violin plots are useful alternatives to jittered dots if there are sufficient data to capture the distribution but I wouldn’t advocate replacing the plot of modeled means and confidence intervals with box or violin plots, as these communicate different things. More importantly, box and violin plots do not communicate the treatment effects. Almost all plots in biology report the error bars that represent the sample standard error. As described above, sample standard error bars do not reflect the fit model and can be highly misleading, at least if interpreting as if they do reflect the model. Also, sample standard error bars can explicitly include absurd values or imply absurd confidence intervals. For example, I sometimes see standard error bars cross \\(y=0\\) for a response that cannot be negative, such as a count. Even if the standard error bar doesn’t cross zero, it is common to see standard error bars that imply (but do not explicitly show) 95% confidence intervals that cross zero, again for responses that cannot be negative. A standard error bar or confidence interval that crosses zero implies that negative means are compatible with the data. This is an absurd implication for responses that cannot have negative values (or are “bounded by” zero). Explicit or implicit error bars that cross zero are especially common for count responses with small means. If a researcher plots confidence intervals, these should be computed using a method that avoids absurd implications, such methods include the bootstrap and generalized linear models. Significance stars are okay, the actual p-value is better, effects plots are best. Many researchers add star symbols to a plot indicating the level of significance of a particular paired comparison. Stars are okay in the sense that there is no inferential difference between \\(p = 0.015\\) and \\(p = 0.045\\). There’s also no inferential difference between \\(p = 0.0085\\) and \\(p = 0.015\\), which highlights the weakness of -chotomizing any continuous variable. For this reason, a better, alternative would be to add the actual p-value (as above). A more serious criticism of stars is that it encourages researchers and readers to focus on statistical significance instead of effect size and uncertainty. A more valuable alternative, then, is to report the effects and uncertainty in an effects plot or a combined effects-and-response plot. 4.2 Working in R A reasonable goal of any research project should be a script to generate the final plots entirely within the R environment and not rely on external drawing software to add finishing features. This section covers some of the basics of using R packages to create plots. Later chapters cover some of the details that are specific to the analyses in that chapter. ggplot2 is one of the major plotting environments in R and the one that seems to have the strongest following, especially among new R users. ggplot2 has the ability to generate extremely personalized and finished plots. However, ggplot2 has a long learning curve and until one is pretty comfortable with its implementation of the grammar of graphics, creating a plot with multiple layers (mean points, error intervals, raw data points, p-values, text annotations) and modified aesthetics (axis text, point colors) can often require many hours of googling. ggpubr is an extension to ggplot2 (it calls ggplot2 functions under the hood) and provides many canned functions for producing the kinds of ggplots that are published in biological journals. With one line of script, a researcher can generate a publishable plot. ggplot_the_model and related functions in this chapter are my attempts to create a simple function for creating publication ready plots that highlight effect size and uncertainty. Some of the basics for using ggpubr, ggplot2, and ggplot_the_model are outlined here. More specific examples are in each chapter. 4.2.1 Source data Data source: ASK1 inhibits browning of white adipose tissue in obesity The source data are that for Figure 2E. The response is \\(glucose\\_auc\\) the “area under the curve” of repeated measures of blood glucose during the 120 minutes of a glucose tolerance test. Glucose AUC is a measure of glucose tolerance, the higher the area, the higher the blood glucose over the two hours, and the worse the physiological response to a sudden rise in blood glucose. There are two treatment factor variables: 1) \\(Diet\\), with levels “chow” and “HFD”, where “chow” is normal mouse chow and “HFD” is a high fat diet, and 2) \\(ask1\\), with levels “ASK1F/F” and “ASK1Δadipo” where “ASK1F/F” is the control level and “ASK1Δadipo” is the ASK1 adipose-deletion mouse described in Chapter 1. 4.2.1.1 Import data_from &lt;- &quot;ASK1 inhibits browning of white adipose tissue in obesity&quot; file_name &lt;- &quot;41467_2020_15483_MOESM4_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # the data are in &quot;tranposed&quot; format -- each row contains the n # measures of a treatment level. Read, then transpose # to make the treatment levels the columns fig2e_wide &lt;- read_excel(file_path, sheet = &quot;Source Date_Figure 2&quot;, range = c(&quot;A233:O236&quot;), # lot of NA col_names = FALSE) %&gt;% data.table %&gt;% transpose(make.names = 1) # turn data onto # melt the four columns into a single &quot;glucose_auc&quot; column # and create a new column containing treatment level. y_cols &lt;- colnames(fig2e_wide) # melt fig2e &lt;- melt(fig2e_wide, measure.vars = y_cols, value.name = &quot;glucose_auc&quot;, variable.name = &quot;treatment&quot;) # create two new columns that are the split of treatment fig2e[, c(&quot;ask1&quot;, &quot;diet&quot;) := tstrsplit(treatment, &quot; &quot;, fixed=TRUE)] # since glucose_auc is the only response variable in this # data.table, omit all rows with any NA fig2e &lt;- na.omit(fig2e) # View(fig2e) 4.2.2 How to plot the model The steps throughout the text for plotting the model fit to experimental data are fit the statistical model use the fit model to estimate the modeled means and confidence limits using emmeans from the emmeans package. use the emmeans object to estimate the contrasts of interests using the contrast function from emmeans. Plot the individual points. If covariates are in the model, use the fit model from step 1 to plot the adjusted values of the points. Use the values from step 2 to plot the modeled means and error intervals. If including p-value brackets, use the values from step 3. If you are using ggplot_the_model functions, then steps 4-6 are done for you. Here, I fit a linear model to the fig2e data and compute the use the emmeans and contrast functions without comment. The details of these functions are in the chapters that follow. The fit model and construction of the plots is simplified from those above. 4.2.2.1 Fit the model The response is the glucose AUC, which is the area under the curve of the data from the glucose tolerance test. The model is a factorial linear model with ask1 genotype and diet as the two factors. # glucose_auc is the AUC of the glucose tolerance curves computed using trapezoidal algorithm m1 &lt;- lm(glucose_auc ~ ask1*diet, data = fig2e) 4.2.2.2 Compute the modeled means table of estimated means and confidence intervals Modeled means are computed by passing the model object (m1) to the emmeans function and specifying the columns containing the groups using the specs = argument`. m1_emm &lt;- emmeans(m1, specs = c(&quot;ask1&quot;, &quot;diet&quot;)) m1_emm ## ask1 diet emmean SE df lower.CL upper.CL ## ASK1F/F chow 1691 83.4 41 1523 1859 ## ASK1Δadipo chow 1637 93.2 41 1449 1826 ## ASK1F/F HFD 2257 73.1 41 2110 2405 ## ASK1Δadipo HFD 1871 70.5 41 1728 2013 ## ## Confidence level used: 0.95 4.2.2.3 Compute the contrasts table of estimated effects with confidence intervals and p-values Contrasts among levels, or combinations of levels, are computed by passing the emmeans object (m1.emm) to the contrast function. There are many important variations of this step. This text advocates computing planned comparisons, which requires expert knowledge and forthought. Here I limit the computation to the four simple effects (the effects of one factor in each of the levels of the other factor). m1_simple &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) m1_simple ## diet ask1 contrast estimate SE df lower.CL upper.CL ## chow . ASK1Δadipo - (ASK1F/F) -53.6 125 41 -306.21 199 ## HFD . ASK1Δadipo - (ASK1F/F) -386.7 102 41 -591.85 -182 ## . ASK1F/F HFD - chow 566.5 111 41 342.48 790 ## . ASK1Δadipo HFD - chow 233.3 117 41 -2.67 469 ## t.ratio p.value ## -0.429 0.6703 ## -3.808 0.0005 ## 5.107 &lt;.0001 ## 1.997 0.0525 ## ## Confidence level used: 0.95 Notes I often use m1_pairs as the name of the contrast table. Here I use m1_simple to remind me that I’ve limited the comparison to the four simple effects. If I only compute planned comparisons, I might use m1_planned. 4.2.3 Be sure ggplot_the_model is in your R folder If you skipped Create an R Studio Project for this textbook, then download and move the file ggplot_the_model.R into the R folder in your Project folder. 4.2.4 How to use the Plot the Model functions The philosophy underneath these functions is to use the model fitted by the researcher to make the plots. The functions require information from three objects: the data frame containing the modeled data the modeled means and CIs from emmeans the modeled effects, CIs and p-values from emmeans::contrast. This philosophy strikes a balance between functions in which all of the statistical modeling is hidden and the researcher only sees the output and manually building ggplots. Actually, I strongly encourage researchers to learn how to build these plots and to not rely on canned functions, and I outline this after introducing the ggplot_the_model functions These functions require the fit model object (m1), the emmeans object of modeled means (m1_emm) and the contrast object of modeled effects (m1_simple). 4.2.4.1 ggplot_the_response For the response plot only, use ggplot_the_response. m1_response_plot &lt;- ggplot_the_response( fit = m1, fit_emm = m1_emm, fit_pairs = m1_simple, palette = pal_okabe_ito_blue, y_label = expression(paste(&quot;mmol &quot;, l^-1, &quot; min&quot;)), g_label = &quot;none&quot; ) m1_response_plot ggplot_the_response arguments: fit model object from lm, lmer, nlme, glmmTMB fit_emm object from ‘emmeans’. Or, a data frame that looks like this object, with modeled factor variables in columns 1 and 2 (if a 2nd factor is in the model), a column of means with name “emmean”, and columns of error intervals named “lower.CL” and “upper.CL” fit_pairs object from emmeans:contrast. Or, a data frame that looks like this object. wrap_col = NULL Not used at the moment x_label = “none” A character variable used for the X-axis title. y_label = “Response (units)” A character variable used for the Y-axis title. Use expression(paste()) method for math. g_label = NULL A character variable used for the grouping variable (the 2nd factor) title. Use “none” to remove dots = “sina” controls the plotting of individual points. sina from ggforce package. Alternatives are “jitter” and “dotplot” dodge_width = 0.8 controls spacing between group means for models with a 2nd factor (the grouping variable) adjust = 0.5 controls spread of dots if using dots = \"sina\" contrast_rows = “all” controls which rows of fit_pairs to use for p-value brackets. Use “none” to hide. y_pos = NULL manual control of the y-coordinates for p-value brackets palette = pal_okabe_ito allows control of the color palette. The default pal_okabe_ito palette is a color blind palette. legend_position = “top” controls position of the legend for the grouping variable (the 2nd factor in a two-factor model) flip_horizontal = FALSE controls the orientation of the axes. group_lines = FALSE used for plotting lines connecting group means. Not yet implemented. 4.2.4.2 ggplot_the_effects For the effects plot only, use ggplot_the_effects. m1_effects_plot &lt;- ggplot_the_effects( fit = m1, fit_pairs = m1_simple, effect_label = expression(paste(&quot;Effect (mmol &quot;, l^-1, &quot; min)&quot;)) ) m1_effects_plot ggplot_the_effects arguments fit model object from lm, lmer, nlme, glmmTMB fit_pairs object from emmeans:contrast. Or, a data frame that looks like this object. contrast_rows = “all” controls which rows of fit_pairs to include in plot. show_p = TRUE controls show/hide of p-values effect_label = “Effect (units)” character variable for the title of the effects axis title. 4.2.4.3 ggplot_the_model For the combined response and effects plot, use ggplot_the_model. m1_plot &lt;- ggplot_the_model( fit = m1, fit_emm = m1_emm, fit_pairs = m1_simple, palette = pal_okabe_ito_blue, y_label = expression(paste(&quot;mmol &quot;, l^-1, &quot; min&quot;)), g_label = &quot;none&quot;, effect_label = expression(paste(&quot;Effect (mmol &quot;, l^-1, &quot; min)&quot;)) ) m1_plot ggplot_the_model arguments fit same as for ggplot_the_response fit_emm same as for ggplot_the_response fit_pairs same as for ggplot_the_response wrap_col = NULL same as for ggplot_the_response x_label = “none” same as for ggplot_the_response y_label = “Response (units)” same as for ggplot_the_response g_label = NULL same as for ggplot_the_response effect_label = “Effect (units)” same as for ggplot_the_effect dots = “sina” same as for ggplot_the_response dodge_width = 0.8 same as for ggplot_the_response adjust = 0.5 same as for ggplot_the_response contrast_rows = “all” same as for ggplot_the_response y_pos = NULL same as for ggplot_the_response palette = pal_okabe_ito same as for ggplot_the_response legend_position = “bottom” Except for default, same as for ggplot_the_response flip_horizontal = TRUE Except for default, same as for ggplot_the_response group_lines = FALSE used for plotting lines connecting group means. Not yet implemented. rel_heights = c(1,1) used to control relative heights of the effects and response plot 4.2.4.4 ggplot_the_treatments x_levels &lt;- rbind( ASK1 = c(&quot;F/F&quot;, &quot;Δadipo&quot;, &quot;F/F&quot;, &quot;Δadipo&quot;), Diet = c(&quot;chow&quot;, &quot;chow&quot;, &quot;HFD&quot;, &quot;HFD&quot;) ) # this is the same code as above but hiding # legend position m1_response_plot_base &lt;- ggplot_the_response( fit = m1, fit_emm = m1_emm, fit_pairs = m1_simple, palette = pal_okabe_ito_blue, y_label = expression(paste(&quot;mmol &quot;, l^-1, &quot; min&quot;)), g_label = &quot;none&quot;, legend_position = &quot;none&quot; ) m1_response_plot2 &lt;- ggplot_the_treatments( m1_response_plot_base, x_levels = x_levels, text_size = 3.5, rel_heights = c(1, 0.1) ) m1_response_plot2 If you prefer plus and minus symbols, use minus &lt;- \"\\u2013\" for the minus sign instead of the hyphen “-” minus &lt;- &quot;\\u2013&quot; # good to put this in the setup chunk x_levels &lt;- rbind( Δadipo = c(minus, &quot;+&quot;, minus, &quot;+&quot;), HFD = c(minus, minus, &quot;+&quot;, &quot;+&quot;) ) m1_response_plot2 &lt;- ggplot_the_treatments( m1_response_plot_base, x_levels = x_levels, text_size = 3.5, rel_heights = c(1, 0.1) ) m1_response_plot2 4.2.5 How to generate a Response Plot using ggpubr Steps 1-3 were completed above. 4.2.5.1 Step 4: Plot the individual points Using ggplot I’m going to show how to create the initial, base plot of points using ggplot2 in order to outline very briefly how ggplot2 works. m1_response &lt;- ggplot( data = fig2e, aes(x = treatment, # these 2 lines define the axes y = glucose_auc, color = ask1 # define the grouping variable )) + # surprisingly, the code above doesn&#39;t plot anything # this adds the points as a layer geom_jitter(width = 0.2) + # change the title of the y-axis ylab(expression(paste(&quot;AUC (mmol &quot;, l^-1, &quot; min)&quot;))) + # change the theme theme_pubr() + # these theme modifications need to be added after re-setting # the theme theme( legend.position = &quot;none&quot;, # remove legend axis.title.x = element_blank() # remove the x-axis title ) + # change the colors palette for the points scale_color_manual(values = pal_okabe_ito_blue) + NULL m1_response Notes The ggplot function requires a data frame passed to data containing the data to plot and an aesthetic (aes), which passes the column names that set the x and y axes. These column names must be in the data frame passed to data. color = is an aesthetic that sets the grouping variable used to assign the different colors. The x-axis is discrete but is numeric. The x-axis values are 1, 2, 3, 4. But instead of using these numbers as the labels for the x-axis values, ggplot uses the names of the groups (the four values of the column “treatment”) Using ggpubr m1_response &lt;- ggstripchart( data = fig2e, x = &quot;treatment&quot;, y = &quot;glucose_auc&quot;, color = &quot;ask1&quot;, xlab = &quot;&quot;, #ylab = expression(paste(&quot;AUC (mmol &quot;, l^-1, &quot; min)&quot;)), ylab = &quot;AUC (mmol/min)&quot;, palette = pal_okabe_ito_blue, legend = &quot;none&quot; ) m1_response 4.2.5.2 Step 5: Plot the modeled means and 95% error intervals To add points and error bars to m1_response, we need to tell ggplot the x-axis positions (or coordinates). These positions are the values of the “treatment” column in fig2e. The modeled means and and 95% CIs are in the m1_emm object but there is no “treatment” column, or any column with these values. We, therefore have to make this column before we can add the modeled means and CIs to the plot. # convert m1_emm to a data.table m1_emm_dt &lt;- summary(m1_emm) %&gt;% data.table() # create treatment column # make sure it matches values in the two factor columns m1_emm_dt[, treatment := c(&quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot;, &quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot;)] Now add the modeled means and CIs m1_response &lt;- m1_response + # add layer containing means geom_point(data = m1_emm_dt, aes(y = emmean, color = ask1), size = 3) + # add layer containing error bars geom_errorbar(data = m1_emm_dt, aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = ask1), width = 0.05) + NULL m1_response Notes m1_response generated by ggpubr::stripchart is a ggplot2 object. This means modifications of the plot are implemented by adding these with the “+” sign. The modeled means are in the column “emmean” in the data frame m1_emm_dt. We need to tell geom_point where to find the data using the data = argument. geom_point() (and other geoms) assumes that the points that we want to plot are defined by the same x and y column names used to create the plot – if these don’t exist, we need to state the x and y column names in the aesthetic function aes. Since we created a “treatment” column in m1_emm_dt that contains the x-axis coordiantes (1, 2, 3, 4), we do not need to tell ggplot where to find the x-values. But there is no “glucose_auc” column in m1_emm_dt so we need to tell geom_point() where to find the y values using aes(y = \"emmean\"). Adding the modeled error intervals using geom_errorbar follows the same logic as adding the modeled means. Importantly, and interestingly, y = emmean has to be passed even though no information from this column is used to plot the error bars. Note that column names passed to a ggpubr function must be in quotes but column names passed to a ggplot2 function cannot be in quotes 4.2.5.3 Step 6: Adding p-values p-value brackets are added to a response plot using stat_pvalue_manual from the ggpubr package. This function needs a column of p-values, and a pair of columns that define the left and right x-axis positions of the bracket. # convert m1_simple to a data.table m1_simple_dt &lt;- data.table(m1_simple) # create group1 -- column containing x-position # of the left side of the bracket # need to look at m1_simple_dt to construct this. # &quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot; ,&quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot; m1_simple_dt[, group1 := c(&quot;ASK1Δadipo chow&quot;, &quot;ASK1Δadipo HFD&quot;, &quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot;)] # create group2 -- column containing x-position # of the right side of the bracket # need to look at m1_simple_dt to construct this. # &quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot; ,&quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot; m1_simple_dt[, group2 := c(&quot;ASK1F/F chow&quot;, &quot;ASK1F/F HFD&quot;, &quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot;)] m1_simple_dt[, p_rounded := p_round(p.value, digits = 2)] m1_simple_dt[, p_pretty := p_format(p_rounded, digits = 2, accuracy = 1e-04, add.p = TRUE)] Now add the p-values # simply assigning this to a new plot with new name # because I want to re-use the base plot in the next chunk m1_response_p &lt;- m1_response + stat_pvalue_manual( data = m1_simple_dt, label = &quot;p_pretty&quot;, y.position = c(3300, 3300, 3450, 3600), tip.length = 0.01) m1_response_p Notes on adding p-values to the plot: The y.position argument in stat_pvalue_manual() contains the position on the y-axis for the p-value brackets. I typically choose these values “by eye”. Essentially, I look at the maximum y-value on the plot and then choose a value just above this for the first bracket. This may take some trial-and-error to position the brackets satisfactorily. Use base R indexing to specify a subset. For example m1_response_p &lt;- m1_response + stat_pvalue_manual( data = m1_simple_dt[c(2,4), ], # only rows 2, 4 label = &quot;p_pretty&quot;, y.position = c(3300, 3450), tip.length = 0.01) m1_response_p ggpubr::stat_compare_means automates the process somewhat but the function is too limited for statistics on anything but the simplest experiments. I don’t advocate it’s use. 4.2.5.4 A variation for factorial models The experiment in Fig2e has a factorial design and was analyzed (here, not in the original paper) using a factorial model. The factorial design can be represented in the plot by clustering the levels. dodge_width = 0.4 jitter_width = 0.2 m1_simple_dt[, xmin := c(1-dodge_width/4, 2-dodge_width/4, 1-dodge_width/4, 1+dodge_width/4)] m1_simple_dt[, xmax := c(1+dodge_width/4, 2+dodge_width/4, 2-dodge_width/4, 2+dodge_width/4)] m1_response_fac &lt;- ggstripchart( data = fig2e, x = &quot;diet&quot;, y = &quot;glucose_auc&quot;, color = &quot;ask1&quot;, fill = &quot;ask1&quot;, # ylab = expression(paste(&quot;AUC (mmol &quot;, l^-1, &quot; min)&quot;)), ylab = &quot;AUC (mmol/min)&quot;, palette = pal_okabe_ito_blue, # position = position_dodge(width = dodge_width) position = position_jitterdodge(dodge.width = dodge_width, jitter.width = jitter_width) ) + rremove(&quot;xlab&quot;) + #ggpubr function # add layer containing means geom_point(data = m1_emm_dt, aes(y = emmean, color = ask1), size = 3, position = position_dodge(width = dodge_width)) + # add layer containing error bars geom_errorbar(data = m1_emm_dt, aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = ask1), width = 0.05, position = position_dodge(width = dodge_width)) + # add p-value brackets stat_pvalue_manual( data = m1_simple_dt, label = &quot;p_pretty&quot;, xmin = &quot;xmin&quot;, xmax = &quot;xmax&quot;, y.position = c(3300, 3300, 3450, 3600), tip.length = 0.01, size = 3) + NULL m1_response_fac 4.2.5.5 How to add treatment combinations to a ggpubr plot Many researchers in bench biology insert a grid of treatments below the x-axis of a plot. This is time consuming if we are using some external software to add to this. A much leaner workflow would be to add the treatment grid in the same step as generating the plot itself. Here is a kludgy way to do this using a ggpubr plot. A much more elegant method is described below 4.2.5.5.1 Variant 1 – Fake axes use_this &lt;- FALSE # if false use +/- symbols if(use_this == TRUE){ x_levels &lt;- rbind( &quot;ASK1:&quot; = c(&quot;F/F&quot;, &quot;Δadipo&quot;, &quot;F/F&quot;, &quot;Δadipo&quot;), &quot;Diet:&quot; = c(&quot;chow&quot;, &quot;chow&quot;, &quot;HFD&quot;, &quot;HFD&quot;) ) }else{ minus &lt;- &quot;\\u2013&quot; # good to put this in the setup chunk x_levels &lt;- rbind( &quot;Δadipo:&quot; = c(minus, &quot;+&quot;, minus, &quot;+&quot;), &quot;HFD: &quot; = c(minus, minus, &quot;+&quot;, &quot;+&quot;) ) } x_levels_text &lt;- apply(x_levels, 2, paste0, collapse=&quot;\\n&quot;) x_levels_title &lt;- paste(row.names(x_levels), collapse=&quot;\\n&quot;) x_axis_min &lt;- 0.4 x_axis_max &lt;- 4.5 y_plot_min &lt;- 1200 y_axis_min &lt;- 1350 y_axis_max &lt;- 3500 y_breaks &lt;- seq(1500, 3500, by = 500) y_labels &lt;- as.character(y_breaks) m1_response_final &lt;- m1_response_p + coord_cartesian(ylim = c(y_plot_min, y_axis_max)) + scale_y_continuous(breaks = y_breaks) + theme( axis.line = element_blank(), # remove both x &amp; y axis lines axis.text.x = element_blank(), # remove x-axis labels axis.ticks.x = element_blank() # remove x-axis ticks ) + # add shortened y-axis line that doesn&#39;t extend to treatments geom_segment(aes(x = x_axis_min + 0.01, y = y_axis_min, xend = x_axis_min + 0.01, yend = y_axis_max), size = 0.5) + # add x-axis line above treatments geom_segment(aes(x = x_axis_min, y = y_axis_min, xend = x_axis_max, yend = y_axis_min), size = 0.5) + # add treatment combinations annotate(geom = &quot;text&quot;, x = 1:4, y = y_plot_min, label = x_levels_text) + # add factor names annotate(geom = &quot;text&quot;, x = x_axis_min, y = y_plot_min, label = x_levels_title, hjust = 0) + NULL m1_response_final Notes ggplot_the_treatments should work with any ggplot, including those generated by ggpubr functions, with categorical values on the x-axis. Here, I add the treatment combinations manually. The table of combinations is added inside the plot area (inside the axes). To make this look nice, and not weird, the x and y axes are removed and new lines are inserted to create new axis lines. The bottom of the new y-axis line starts above the treatment table. The new x-axis line is inserted above the treatment table. To include the treatment combination (group) names, add this as the first row of x_levels. Note that if the plot has a grid, this grid will extend into the area occupied by the treatment table using this method. 4.2.6 How to generate a Response Plot with a grid of treatments using ggplot2 Above, I wrote a short script for generating the base response plot using ggplot. In this plot, the x-axis variable (“treatment”) is categorical and ggplot uses the integers 1, 2, 3, 4 as the coordinate values for the position of the four groups on the x-axis. Understanding this mapping is important for adding lines to a plot or annotating a plot with text, both of which requires x,y coordinates to position the feature. While the x-axis is discrete – I cannot add a tick at x = 1.5 – the horizontal dimension of the plot area itself is continuous and I can add points or lines or text at any x coordinate within the plot area. Here I generate the same plot using a continuous x-axis. This requires that I explicitly create a numeric column in the data with the x-axis position for each group. This is easy. The advantage of this is that I now have a continuous x-axis that is more manipulatable than a discrete x-axis. I use this to add treatment levels below the plot 4.2.6.1 First, wrangle the data # make sure treatment is a factor with the levels in the # desired order # fig2e[, treatment_i := as.integer(fig2e$treatment)] # above line stopped working on 08/28/23 with error: # Error in x[[name]] &lt;- value : bad names attribute # so here is a non-data.table alternative fig2e$treatment_i &lt;- as.integer(fig2e$treatment) # convert m1_emm to a data.table m1_emm_dt_i &lt;- summary(m1_emm) %&gt;% data.table() # create treatment_i column m1_emm_dt_i[, treatment_i := 1:4] # convert m1_simple to a data.table m1_simple_dt_i &lt;- data.table(m1_simple) # create group1 -- column containing x-position # of the left side of the bracket # need to look at m1_simple_dt to construct this. # &quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot; ,&quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot; m1_simple_dt_i[, group1 := c(2, 4, 3, 4)] # create group2 -- column containing x-position # of the right side of the bracket # need to look at m1_simple_dt to construct this. # &quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot; ,&quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot; m1_simple_dt_i[, group2 := c(1, 3, 1, 2)] m1_simple_dt_i[, p_rounded := p_round(p.value, digits = 2)] m1_simple_dt_i[, p_pretty := p_format(p_rounded, digits = 2, accuracy = 1e-04, add.p = TRUE)] Notes The first line simply converts the categorical variable treatment into a numeric variable with values 1-4 assigned to the four treatment levels. As above, m1_emm_dt is created and a column with the x-variable name is added As above, m1_simple_dt is created and group1 and group2 columns with the x-axis positions of the two groups in the contrast are created. Here these columns are integers and not group names. 4.2.6.2 Second, generate the plot m1_response_2 &lt;- ggplot( data = fig2e, aes(x = treatment_i, # these 2 lines define the axes y = glucose_auc, color = ask1 # define the grouping variable )) + # add jittered points geom_jitter(width = 0.2) + # add layer containing means geom_point(data = m1_emm_dt_i, aes(y = emmean, color = ask1), size = 3) + # add layer containing error bars geom_errorbar(data = m1_emm_dt_i, aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = ask1), width = 0.05) + # add the p-value brackets stat_pvalue_manual( data = m1_simple_dt_i[c(2,4), ], # only rows 2, 4 label = &quot;p_pretty&quot;, y.position = c(3300, 3450), tip.length = 0.01) + # change the title of the y-axis ylab(expression(paste(&quot;AUC (mmol &quot;, l^-1, &quot; min)&quot;))) + # change the theme theme_pubr() + # these theme modifications need to be added after re-setting # the theme theme( legend.position = &quot;none&quot;, # remove legend axis.title.x = element_blank() # remove the x-axis title ) + # change the colors palette for the points scale_color_manual(values = pal_okabe_ito_blue) + NULL m1_response_2 Notes This code is exactly the same as that above but uses an x-variable that is numeric instead of a factor. 4.2.6.3 Third, add the grid of treatments below the x-axis minus &lt;- &quot;\\u2013&quot; # good to put this in the setup chunk # I&#39;ve added x-axis group names in addition to the grid x_levels &lt;- rbind( c(&quot;&quot;, &quot;Control&quot;, &quot;Δadipo&quot;, &quot;HFD&quot;, &quot;Δadipo+HFD&quot;), c(&quot;Δadipo:&quot;, minus, &quot;+&quot;, minus, &quot;+&quot;), c(&quot;HFD: &quot;, minus, minus, &quot;+&quot;, &quot;+&quot;) ) x_breaks_text &lt;- apply(x_levels, 2, paste0, collapse=&quot;\\n&quot;) x_breaks &lt;- c(0.5, 1:4) m1_response_2 &lt;- m1_response_2 + coord_cartesian(xlim = c(0.5, 4.5)) + scale_x_continuous(breaks = x_breaks, labels = x_breaks_text, expand = c(0, 0)) + theme(axis.ticks.x = element_blank()) + # remove x-axis ticks NULL m1_response_2 Notes This is a good plot. A pretty good plot would include the effects. 4.2.7 How to generate an Effects Plot The effects plot is built using ggplot2 instead of ggpubr because the plot is “flipped” – the y-axis is the categorical variable and the x-axis is the continuous variable. The base plot with the estimates (but not the error bars) could be made using ggpubr (see below) but the subsequent functions to modify the plot are awkward because of inconsistencies in the designation of the x and y-axis (old or new?). # use the m1_simple_dt object created above # create nice labels for the contrasts m1_simple_dt[, contrast_pretty := c(&quot;ASK1Δadipo - Control&quot;, &quot;ASK1Δadipo - HFD&quot;, &quot;HFD - Control&quot;, &quot;ASK1Δadipo+HFD - ASK1Δadipo&quot;)] # make sure the levels of contrast_pretty are in the order of that in # m1_simple_dt m1_simple_dt[, contrast_pretty := factor(contrast_pretty, levels = contrast_pretty)] m1_effects &lt;- ggplot(data = m1_simple_dt, aes(x = estimate, y = contrast_pretty)) + geom_point() + xlab(&quot;Effects (mmol/L)&quot;) + # add error bars geom_errorbar(aes(x = estimate, xmin = lower.CL, xmax = upper.CL), width = 0.02) + # add zero effect line geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = pal_okabe_ito_blue[1]) + # add p-values annotate(geom = &quot;text&quot;, x = m1_simple_dt$estimate + c(-30,0,0,0), y = 1:4 + 0.2, label = m1_simple_dt$p_pretty, size = 3) + theme_pubr() + rremove(&quot;ylab&quot;) + #remove y-axis, ggpubr function NULL m1_effects Notes c(-30,0,0,0) is added to the x coordinate to shift the first p-value to the left of the zero-effect line. how would this be done with ggpubr? # use the modified m1_simple_dt object created in the previous chunk # use ggpubr::ggerrorplot with no error m1_effects_pubr &lt;- ggerrorplot(data = m1_simple_dt, y = &quot;estimate&quot;, x = &quot;contrast_pretty&quot;, ylab = &quot;Effects (mmol/L)&quot;, desc_stat = &quot;mean&quot;, # mean only! orientation = &quot;horizontal&quot;) + # remove y-axis title rremove(&quot;ylab&quot;) + #ggpubr function # add error bars using ggplot function # note using original (not horizontal) axis designation geom_errorbar(aes(y = estimate, ymin = lower.CL, ymax = upper.CL), width = 0.02) + # add zero effect line using ggplot function # note using original (not horizontal) axis designation geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = pal_okabe_ito_blue[1]) + # add p-values # note using original (not horizontal) axis designation annotate(geom = &quot;text&quot;, y = m1_simple_dt$estimate + c(-30,0,0,0), x = 1:4 + 0.3, label = m1_simple_dt$p_pretty, size = 3) + NULL m1_effects_pubr 4.2.8 How to combine the response and effects plots 4.2.8.1 Using the ggpubr response plot The cowplot::plotgrid() function is used generally to arrange multiple plots into a single figure. Here I use it to combine the response and effects subplots into a single plot. The response plot is m1_response created using ggpubr. # modify the response and effects plots for consistency # change group names for consistency groups_pretty &lt;- c(&quot;Control&quot;, &quot;ASK1Δadipo&quot;, &quot;HFD&quot;, &quot;ASK1Δadipo+HFD&quot;) m1_bottom &lt;- m1_response + scale_x_discrete(labels = groups_pretty) + coord_flip() # rotate to horizontal m1_top &lt;- m1_effects + # assign to new plot object scale_x_continuous(position=&quot;top&quot;) # move x-axis to &quot;top&quot; m1_plot &lt;- plot_grid(m1_top, m1_bottom, nrow = 2, align = &quot;v&quot;, axis = &quot;lr&quot;, rel_heights = c(1, 1)) m1_plot Notes The response plot was flipped in this code. If you know that you want it in this orientation, simply build it with this orientation to avoid the kinds of axis designation ambiguities highlighted above with the ggpubr effects plot. the align and axis arguments are used to force the plot areas to have the same width. rel_heights = c() adjusts the relative heights of the top and bottom plot. This typically requires fiddling. The placement of the p-values looks different than it does in the standalone effects plot. To improve the look in the combined plot, fiddle with the placement argument (the x and y positions) in the chunk that generates the effects plot. 4.2.8.2 Using a response plot with a treatment grid Here, I re-build the response plot in a horizontal orientation tab &lt;- &quot;\\u0009&quot; # good to put this in the setup chunk minus &lt;- &quot;\\u2013&quot; # good to put this in the setup chunk minus_pad &lt;- paste0(minus, &quot; &quot;) plus_pad &lt;- &quot;+ &quot; x_levels &lt;- rbind( c(minus_pad, plus_pad, minus_pad, plus_pad), c(minus_pad, minus_pad, plus_pad, plus_pad) ) x_breaks_text &lt;- apply(x_levels, 2, paste0, collapse = &quot;&quot;) x_breaks_text &lt;- c(x_breaks_text, &quot;Δadipo HFD&quot;) x_breaks &lt;- c(1:4, 4.5) m1_response_horiz &lt;- ggplot( data = fig2e, aes(y = treatment_i, # these 2 lines define the axes x = glucose_auc, color = ask1 # define the grouping variable )) + # add jittered points geom_jitter(width = 0.2) + # add layer containing means geom_point(data = m1_emm_dt_i, aes(x = emmean, color = ask1), size = 3) + # add layer containing error bars geom_errorbar(data = m1_emm_dt_i, aes(x = emmean, xmin = lower.CL, xmax = upper.CL, color = ask1), width = 0.05) + # change the title of the y-axis xlab(expression(paste(&quot;AUC (mmol &quot;, l^-1, &quot; min)&quot;))) + # change the theme theme_pubr() + # these theme modifications need to be added after re-setting # the theme theme( legend.position = &quot;none&quot;, # remove legend axis.title.y = element_blank(), # remove the x-axis title axis.ticks.y = element_blank() # remove y-axis ticks ) + # change the colors palette for the points scale_color_manual(values = pal_okabe_ito_blue) + # add the treatment coord_cartesian(ylim = c(0.5, 4.5)) + scale_y_continuous(breaks = x_breaks, labels = x_breaks_text, expand = c(0, 0)) + NULL m1_response_horiz m1_plot &lt;- plot_grid(m1_top, m1_response_horiz, nrow = 2, align = &quot;v&quot;, axis = &quot;lr&quot;, rel_heights = c(1, 1)) m1_plot 4.2.9 How to add the interaction effect to response and effects plots In the experiment for Figure 2E, a good question to ask is, is the effect of ASK1Δadipo conditional on diet? For example, a scenario where ASK1Δadipo lowers AUC about the same amount in both Chow and HFD mice (that is, the effect of ASK1Δadipo is not conditional on diet) has a different underlying biological explanation of the control of browning than a scenario where ASK1Δadipo lowers AUC only in HFD mice. To pursue this, we need an estimate of the interaction effect. In general, if an experiment has a factorial design, we always want estimates of the interaction effects 4.2.9.1 Adding interaction p-value to a response plot Wrangle m1_coef &lt;- coef(summary(m1)) ixn_estimate &lt;- m1_coef[&quot;ask1ASK1Δadipo:dietHFD&quot;, &quot;Estimate&quot;] p_ixn &lt;- m1_coef[&quot;ask1ASK1Δadipo:dietHFD&quot;, &quot;Pr(&gt;|t|)&quot;] %&gt;% p_round(digits = 3) %&gt;% p_format(digits = 3, accuracy = 1e-04) p_ixn_text &lt;- paste0(&quot;interaction p = &quot;, p_ixn) m1_response_ixn &lt;- m1_response_fac + # add line connecting group means # comment out all lines to remove geom_line(data = m1_emm_dt, aes(y = emmean, group = ask1, color = ask1), position = position_dodge(width = dodge_width)) + # add p-value annotate(geom = &quot;text&quot;, x = 1.5, y = 2300, label = p_ixn_text) + NULL m1_response_fac m1_response_ixn 4.2.9.2 Adding interaction effect to effects plot 4.2.9.2.1 Add the interaction effect to the contrast table # convert m1_simple to data.table m1_simple_dt &lt;- data.table(m1_simple) # get interaction p-value using emmeans::contrast() m1_ixn &lt;- contrast(m1_emm, interaction = c(&quot;revpairwise&quot;), by = NULL) %&gt;% summary(infer = TRUE) %&gt;% data.table() setnames(m1_ixn, old = names(m1_ixn)[1:2], new = names(m1_simple_dt)[1:2]) m1_ixn[, contrast := &quot;ask1:diet&quot;] # note that column order does not need to be the same m1_effects_dt &lt;- rbind(m1_simple_dt, m1_ixn) m1_effects_dt[, contrast_pretty := c(&quot;Δadipo - Control&quot;, &quot;Δadipo+HFD - HFD&quot;, &quot;HFD - Control&quot;, &quot;Δadipo+HFD - Δadipo&quot;, &quot;Interaction&quot;)] m1_effects_dt[, p_round := p_round(p.value, digits = 2)] m1_effects_dt[, p_pretty := p_format(p_round, digits = 2, accuracy = 1e-04, add.p = TRUE)] # don&#39;t forget this imperative step! # otherwise your p-values and factor labels won&#39;t match! m1_effects_dt[, contrast_pretty := factor(contrast_pretty, levels = contrast_pretty)] m1_effects &lt;- ggplot(data = m1_effects_dt, aes(x = estimate, y = contrast_pretty)) + geom_point() + xlab(&quot;Effects (mmol/L)&quot;) + # add error bars geom_errorbar(aes(x = estimate, xmin = lower.CL, xmax = upper.CL), width = 0.02) + # add zero effect line geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = pal_okabe_ito_blue[1]) + # add p-values annotate(geom = &quot;text&quot;, x = m1_effects_dt$estimate + c(-30,0,0,0,0), y = 1:5 + 0.2, label = m1_effects_dt$p_pretty, size = 3) + theme_pubr() + rremove(&quot;ylab&quot;) + #remove y-axis, ggpubr function NULL m1_effects m1_top &lt;- m1_effects + scale_x_continuous(position=&quot;top&quot;) # move x-axis to &quot;top&quot; m1_plot &lt;- plot_grid(m1_top, m1_response_horiz, nrow = 2, align = &quot;v&quot;, axis = &quot;lr&quot;, rel_heights = c(1, 0.9)) m1_plot Notes This is a pretty good plot “The Pretty Good House - Finding the right balance between construction cost and energy performance”. https://www.greenbuildingadvisor.com/article/the-pretty-good-house↩︎ "],["part-iii-some-fundamentals-of-statistical-modeling.html", "Part III: Some Fundamentals of Statistical Modeling", " Part III: Some Fundamentals of Statistical Modeling "],["uncertainty.html", "Chapter 5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals) 5.1 Standard errors are used to compute p-values and confidence intervals 5.2 Background 5.3 Simulations – using fake data as an intuition pump 5.4 Bootstrapped standard errors 5.5 Confidence Intervals 5.6 Standard error of a difference between means 5.7 Confidence limits of a difference between means 5.8 Hidden code", " Chapter 5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals) Uncertainty is the stuff of science. What do we mean by uncertainty in statistics? Uncertainty is the error in estimating a parameter, such as the mean of a sample, or the difference in means between two experimental treatments, or the predicted response given a certain change in conditions. Uncertainty emerges because of variability and is measured with a variance or its square root, which is a standard deviation. The standard deviation of a statistic is called a standard error of the statistic. Standard errors are to statistics of experimental data as atoms are to matter – everything is built from them. 5.1 Standard errors are used to compute p-values and confidence intervals Let’s revisit Table 1.1 from the experiment presented in the introductory chapter Analyzing experimental data with a linear model. Table 5.1: Coefficient table for linear model fit to exp2i data. Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 61.47 4.98 12.3 0.000 50.37 72.57 treatmentASK1Δadipo -21.60 7.05 -3.1 0.012 -37.30 -5.90 The values in the second row give the inferential statistics for the estimate of the treatment effect. The estimate is in the column “Estimate”. To guide our inference about this value, we use the standard error of the estimate in the column “Std. Error”. We use this standard error to compute a t-value, which is given in the column “t value” and, from this, we can compute a p-value. This is the classic “t-test” that all experimental biology researchers are familiar with, although computed using the linear model and not the way presented in most introductory statistics textbooks (the two ways give numerically equivalent results). We also use the standard error to compute the boundaries of the 95% confidence interval of the estimate, which are given in the “2.5 %” and “97.5 %” columns. The importance of confidence intervals in statistical inference is less well appreciated among researchers in experimental biology. A p-value is a tool to help us infer if there is an effect. A confidence interval is a tool to help us infer the range of effects that are compatible with the data. This chapter uses simulation to pump our intuition of standard errors and confidence intervals. We wait until the next chapter to focus on p-values. 5.2 Background In any introductory statistics class, students are introduced to two measures of variability, the “standard deviation” and the “standard error.” These terms are absolutely fundamental to statistics. Yet, many biology researchers confuse these terms and certainly, introductory students do too. When a research biologist uses the term “standard deviation,” they are probably referring to the sample standard deviation which is a measure of the variability of a sample. When a research biologist uses the term “standard error,” they are probably referring to the standard error of a mean, but it could be the standard error of another statistic, such as a difference between means or a regression slope. An important point to remember and understand is that all standard errors are standard deviations. This will make more sense soon. 5.2.1 Sample standard deviation The sample standard deviation is a measure of the variability of a sample. For example, were we to look at a histological section of skeletal muscle we would see that the diameter of the fibers (the muscle cells) is variable. We could use imaging software to measure the diameter of a sample of 100 cells and get a distribution like this The mean of this sample is 69.4µm and the standard deviation is 2.8 µm. The standard deviation is the square root of the variance, and so computed by \\[\\begin{equation} s_y = \\sqrt{\\frac{\\sum_{i=1}^n{(y_i - \\overline{y})^2}}{n-1}} \\tag{5.1} \\end{equation}\\] Memorize this equation. To understand the logic of this measure of variability, note that \\(y_i - \\overline{y}\\) is the deviation of the \\(i\\)th value from the sample mean, so the numerator is the sum of squared deviations. The numerator is a sum over \\(n\\) items and the denominator is \\(n-1\\) so the variance is (almost!) an averaged squared deviation. More variable samples will have bigger deviations and, therefore, bigger average squared deviations. Since the standard deviation is the square root of the variance, a standard deviation is the square root of an average squared deviation. This makes it similar in value to the averaged deviation (or average of the absolute values of the deviations since the average deviation is, by definition of a mean, zero). 5.2.1.1 Notes on the variance and standard deviation Variances are additive but standard deviations are not. This means that the variance of the sum of two independent (uncorrelated) random variables is simply the sum of the variances of each of the variables. This is important for many statistical analyses. The units of variance are the square of the original units, which is awkward for interpretation. The units of a standard deviation is the same as that of the original variable, and so is much easier to interpet. For variables that are approximately normally distributed, we can map the standard deviation to the quantiles of the distribution. For example, 68% of the values are within one standard deviation of the mean, 95% of the values are within two standard deviations, and 99% of the values are within three standard deviations. 5.2.2 Standard error of the mean A standard error of a statistic is a measure of the precision of the statistic. The standard error of the mean is a measure of the precision of the estimate of the mean. The standard error of a difference in means is a measure of the precision of the estimate of the difference in means. The smaller the standard error, the more precise the estimate. The standard error of the mean (SEM) is computed as \\[\\begin{equation} SEM = \\frac{s_y}{\\sqrt{n}} \\tag{5.2} \\end{equation}\\] The SEM is often denoted \\(s_{\\bar{y}}\\) to indicate that it is a standard deviation of the mean (\\(\\bar{y}\\)). 5.2.2.1 The standard error of the mean can be thought of as a standard deviation of an infinitely long column of re-sampled means In what sense is a standard error a standard deviation? This is kinda weird. If we sample 100 cells in the slide of muscle tissue and compute the mean diameter, how can the mean have a standard deviation? There is only one mean! To understand how the SEM is a standard deviation, imagine that we sample \\(n\\) values from \\(N(\\mu, \\sigma^2)\\) (a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean of our sample is an estimate of \\(\\mu\\) and the standard deviation of sample is an estimate of \\(\\sigma\\)) an infinite number of times and each time, we write down the mean of the new sample. This infinitely large sample of means is the sampling distribution of the mean. The standard deviation of the sampling distribution of the mean is the standard error of the mean. Our observed SEM is an estimate of this true value because our observed standard deviation is an estimate of \\(\\sigma\\). 5.2.2.2 A standard deviation can be computed for any statistic – these are all standard errors. The SEM is only one kind of standard error. A standard deviation can be computed for any statistic – these are all standard errors. For some statistics, such as the mean, the standard error can be computed directly using an equation, such as that for the SEM (equation (5.2)). For other statistics, there is no equation, and we need to use a computer intensive method known as the bootstrap to compute a standard error. We will return to the bootstrap in Section 5.4. 5.2.2.3 Notes on standard errors The units of a standard error are the units of the measured variable. A standard error is proportional to sample variability (the sample standard deviation, \\(s_y\\)) and inversely proportional to sample size (\\(n\\)). Sample variability is a function of both natural variation (there really is variation in diameter among fibers in the quadriceps muscle) and measurement error (imaging software with higher resolution can measure a diameter with less error). Since the SEM is a measure of the precision of estimating a mean, this means this precision will increase (or the SEM will decrease) if 1) an investigator uses methods that reduce measurement error and 2) an investigator computes the mean from a larger sample. This last point (the SEM decreases with sample size) seems obvious when looking at equation (5.2), since \\(n\\) is in the denominator. Of course \\(n\\) is also in the denominator of equation (5.1) for the sample standard deviation but the standard deviation does not decrease as sample size increases. First this wouldn’t make any sense – variability is variability. A sample of 10,000 cell diameters should be no more variable than a sample of 100 cell diameters (think about if you agree with this or not). Second, this should also be obvious from equation (5.1). The standard deviation is the square root of an average and averages don’t increase with the number of things summed since both the the numerator (a sum) and denominator increase with \\(n\\). 5.3 Simulations – using fake data as an intuition pump 5.3.1 Using Google Sheets to generate fake data to explore the standard error In statistics we are interested in estimated parameters of a population using measures from a sample. The goal in this section is to use Google Sheets (or Microsoft Excel) to use fake data to discover the behavior of sampling and to gain some intuition about uncertainty using standard errors. 5.3.1.1 Steps Open Google Sheets In cell A1 type “mu”. mu is the greek letter \\(\\mu\\) and is very common notation for the poplation value (the TRUE value!) of the mean of some hypothetical measure. In cell B1, insert some number as the value of \\(\\mu\\). Any number! It can be negative or positive. In cell A2 type “sigma”. sigma is the greek letter \\(\\sigma\\). \\(\\sigma^2\\) is very common (universal!) notation for the population (TRUE) variance of some measure or parameter. Notice that the true (population) values of the mean and variance are greek letters. This is pretty standard in statistics. In cell B2, insert some positive number (standard deviations are the positive square roots of the variance). In cell A8 type the number 1 In cell A9 insert the equation “=A8 + 1”. What is this equation doing? It is adding the number 1 to to the value in the cell above, so the resulting value should be 2. In Cell B8, insert the equation “=normsinv(rand())*$B$2 + $B$1”. The first part of the equation creates a random normal variable with mean 0 and standard deviation 1. multiplication and addition transform this to a random normal variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) (the values you set in cells B1 and B2). copy cell B8 and paste into cell B9. Now Highlight cells A9:B9 and copy the equations down to row 107. You now have 100 random variables sampled from a infinite population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). In cell A4 write “mean 10”. In cell B4 insert the equation “=average(B8:B17)”. The resulting value is the sample mean of the first 10 random variables you created. Is the mean close to \\(\\mu\\)? In cell A5 write “sd 10”. In cell B5 insert the equation “stdev(B8:B17)”. The result is the sample standard deviation of the first 10 random variables. Is this close to \\(\\sigma\\)? In cell A6 write “mean 100”. In cell B6 insert the equation “=average(B8:B107)”. The resulting value is the sample mean of the all 100 random variables you created. Is this mean closer to \\(\\mu\\) than mean 10? In cell A7 write “sd 100”. In cell B7 insert the equation “=stdev(B8:B107)”. The resulting value is the sample standard deviation of the all 100 random variables you created. Is this SD closer to \\(\\sigma\\) than sd 10? The sample standard deviation is a measure of the variability of the sample. The more spread out the sample (the further each value is from the mean), the bigger the sample standard deviation. The sample standard deviation is most often simply known as “The” standard deviation, which is a bit misleading since there are many kinds of standard deviations! Remember that your computed mean and standard deviations are estimates computed from a sample. They are estimates of the true values \\(\\mu\\) and \\(\\sigma\\). Explore the behavior of the sample mean and standard deviation by re-calculating the spreadsheet. In Excel, a spreadsheet is re-calculated by simultaneously pressing the command and equal key. In Google, command-R recalculates but is painfully slow. Instead, if using Google Sheets, just type the number 1 into a blank cell, and the sheet recalculates quickly. Do it again. And again. Each time you re-calculate, a new set of random numbers are generated and the new means and standard deviations are computed. Compare mean 10 and mean 100 each re-calculation. Notice that these estimates are variable. They change with each re-calculation. How variable is mean 10 compared to mean 100? The variability of the estimate of the mean is a measure of uncertainty in the estimate. Are we more uncertain with mean 10 or with mean 100? This variability is measured by a standard deviation. This standard deviation of the mean is also called the standard error of the mean. Many researchers are loose with terms and use “The” standard error to mean the standard error of the mean, even though there are many kinds of standard errors. In general, “standard error”” is abbreviated as “SE.” Sometimes “standard error of the mean” is specifically abbreviated to “SEM.” The standard error of the mean is a measure of the precision in estimating the mean. The smaller the value the more precise the estimate. The standard error of the mean is a standard deviation of the mean. This is kinda weird. If we sample a population one time and compute a mean, how can the mean have a standard deviation? There is only one value! And we compute this value using the sample standard deviation: \\(SEM = \\frac{SD}{\\sqrt{N}}\\). To understand how the SEM is a standard deviation, Imagine recalculating the spread sheet an infinite number of times and each time, you write down the newly computed mean. The standard error of the mean is the standard deviation of this infinitely long column of means. 5.3.2 Using R to generate fake data to explore the standard error note that I use “standard deviation” to refer to the sample standard deviation and “standard error” to refer to the standard error of the mean (again, we can compute standard errors as a standard deviation of any kind of estimate) 5.3.2.1 part I In the exercise above, you used Google Sheets to generate \\(p\\) columns of fake data. Each column had \\(n\\) elements, so the matrix of fake data was \\(n \\times p\\) (it is standard in most fields to specify a matrix as rows by columns). This is much easier to do in R and how much grows exponentially as the size of the matrix grows. To start, we just generate a \\(n \\times p\\) matrix of normal random numbers. set.seed(1) # so everyone gets the same result! # R script to gain some intuition about standard deviation (sd) and standard error (se) # you will probably need to install ggplot2 using library(ggplot2) n &lt;- 6 # sample size p &lt;- 100 # number of columns of fake data to generate # this is a step-by-step method for constructing the matrix N &lt;- n * p # the total number of fake values to draw from a random distribution fake_data &lt;- rnorm(N, mean = 0, sd = 1) # create a vector of N fake values fake_data &lt;- matrix(fake_data, nrow = n, ncol = p) # put into a matrix object # the three step-by-step lines above can be replaced # with a more compact code using the pipe operator fake_data &lt;- rnorm(n * p, mean = 0, sd = 1) %&gt;% matrix(nrow = n, ncol = p) The compacted code (the last line of code, which is wrapped and so looks like two lines) is the cool thing about R. In one line I’m creating a dataset with \\(n\\) rows and \\(p\\) columns. Each column is a sample of the standard normal distribution which by definition has mean zero and standard deviation of 1. But, and this is important, any sample from this distribution will not have exactly mean zero and standard deviation of 1, because it’s a sample – the mean and standard deviation will have some small error from the truth. The line has two parts to it (connected by the magritter pipe operator): first I’m using the function rnorm() (for random normal) to create a vector of \\(n \\times p\\) random, normal deviates (draws from the random normal distribution) and then I’m organizing these into a matrix using the function matrix(). To compute the vector of means, standard deviations, and standard errors for each column of fake_data, use the apply() function. means &lt;- apply(fake_data,2,mean) # the apply function is super useful sds &lt;- apply(fake_data,2,sd) sems &lt;- sds/sqrt(n) apply() is a workhorse in many R scripts and is often used in R scripts in place of a for-loop (see below) because it takes fewer lines of code. The SEM is the standard deviation of the mean, so let’s see if the standard deviation of the means is close to the true standard error. We sampled from a normal distribution with SD=1 so the true standard is 1/sqrt(n) ## [1] 0.4082483 and the standard deviation of the \\(p\\) means is sd(means) ## [1] 0.4150002 Questions how close is sd(means) to the true SE? change p above to 1000. Now how close is sd(means) to the true SE? change p above to 10,000. Now how close is sd(means) to the true SE? 5.3.2.2 part II - means This is a visualization of the spread, or variability, of the sampled means, which is the sampling distribution of the means. qplot(means, bins = 30) Compute the mean of the means mean(means) ## [1] -0.05790466 Questions Remember that the true mean is zero. How close, in general, are the sampled means to the true mean. How variable are the means? How is this quantified? change n to 100, then replot. Are the means, in general, closer to the true mean? How variable are the means now? Is the mean estimated with \\(n=100\\) closer to the truth, in general, then the mean estimated with \\(n=6\\)? Redo with \\(n=10000\\) 5.3.2.3 part III - how do SD and SE change as sample size (n) increases? mean(sds) ## [1] 1.025274 Questions what is the mean of the standard deviations when n=6 (set p=1000) what is the mean of the standard deviations when n=100 (set p=1000) when n = 1000? (set p=1000) when n = 10000? (set p=1000) how does the mean of the standard deviations change as n increases (does it get smaller? or stay about the same size) repeat the above with SEM mean(sems) ## [1] 0.4185663 Congratulations, you have just done a Monte Carlo simulation! 5.3.2.4 Part IV – Generating fake data with for-loops A for-loop is used to iterate a computation. n &lt;- 6 # sample size n_sim &lt;- 10^5 # number of iterations of loop (equivalent to p) means &lt;- numeric(n_sim) sds &lt;- numeric(n_sim) sems &lt;- numeric(n_sim) for(i in 1:n_sim){ y &lt;- rnorm(n) # mean=0 and sd=1 are default so not necessary to specify means[i] &lt;- mean(y) sds[i] &lt;- sd(y) sems[i] &lt;- sd(y)/sqrt(n) } sd(means) ## [1] 0.4077471 mean(sems) ## [1] 0.3887001 Questions What do sd(means) and mean(sems) converge to as n_sim is increased from 100 to 1000 to 10,000? Do they converge to the same number? Should they? What is the correct number? Question number 4 is asking what is E[SEM], the “expected standard error of the mean”. There is a very easy formula to compute this. What is it? 5.3.2.5 Part V – the sampling distribution of means is asymptotically Normal (wait, what?) let’s review a standard error. The standard error of a mean is the standard deviation of the sampling distribution of the mean – that is, the standard deviation of the means taken from an infinite number of samples. A fascinating result in statistics is the sampling distribution of a mean is asymptotically Normal regardless of the distribution of the sample. What do I mean by this? Let’s take a positively skewed distribution, say the counts of marked cells. The data are generated by a negative binomial distribution. \\[ counts \\sim NB(\\mu, \\theta) \\] Fig. 5.1 is a fakesample of cell counts from 10,000 fake mice. set.seed(1) n &lt;- 10^4 counts &lt;- rnegbin(n, mu = 10, theta = 1) qplot(counts, bins = 30) Figure 5.1: Positively skewed distribution of cell counts. Let’s use a for-loop to sample this distribution (the NB distribution, not the sample from this distribution above) 1000 times and plot the distribution of the 1000 means. But here, we will simulate a more realistic experiment by using \\(n=10\\) as our sample size. set.seed(1) n_sim &lt;- 1000 n &lt;- 10 means &lt;- numeric(n_sim) for(i in 1:n_sim){ counts &lt;- rnegbin(n, mu = 10, theta = 1) means[i] &lt;- mean(counts) } qplot(means, bins = 30) Figure 5.2: Distribution of 1000 means of samples from a non-normal distribution with n = 10. The distribution of count means in Fig. 5.2 is still positively skewed but less than the distribution of counts in Fig. 5.1. Let’s repeat this with a bigger sample size (\\(n = 1000\\)) set.seed(1) n_sim &lt;- 1000 n &lt;- 1000 means &lt;- numeric(n_sim) for(i in 1:n_sim){ counts &lt;- rnegbin(n, mu = 10, theta = 1) means[i] &lt;- mean(counts) } qplot(means, bins = 30) Figure 5.3: Distribution of 1000 means of samples from a non-normal distribution with n = 1000. The distribution of count means in Fig. 5.3 looks Normal-ish. What Figures 5.2 and 5.3 demo is that, as the sample size increases, the sampling distribution gets closer and closer to Normal. The “approach” to Normal is rapid when we go from really small samples (\\(n=5\\)) to small samples (\\(n=30\\)) but slows as we go from big samples (\\(n=100\\)) to really big samples (\\(n=1000\\)). That is, the approach is asymptotic. The rate of the approach depends on the sampling distribution of the sample – the more non-normal the sampling distribution of the sample, the higher the sample size needed for the sampling distribution of means to be nearly-normal. This result, that the sampling distribution of means from any distribution is asymptotically normal, is the Central Limit Theorem and the consequences of this are huge. Confidence intervals and p-values are functions of sampling distributions of means (or differences in means) and when we say that a p-value assumes Normality, it is really the Normality of the sampling distribution of the means and not the sampling distribution of the data that matters. But, because the sampling distribution of means is dependent on sampling size, it means that inference assuming Normality will be less-correct if our sample size is small. And sample sizes in experimental biology tend to be very small. 5.4 Bootstrapped standard errors The bootstrap is certainly one of the most valuable tools invented in modern statistics. But, it’s not only a useful tool for applied statistics, it’s a useful tool for understanding statistics. Playing with a parametric bootstrap will almost certainly induce an “aha, so that’s what statisticians mean by …” moment. A bootstrapped standard error of a statistic is the empirical standard deviation of the statistic from a finite number of samples. The basic algorithm for a bootstrap is (here “the statistic” is the mean of the sample) sample \\(n\\) values from a probability distribution compute the mean repeat step 1 and 2 many times for a bootstrapped standard error, compute the standard deviation of the set of means saved from each iteration of steps 1 and 2. The probability distribution can come from two sources: A parametric bootstrap uses samples from a parametric probability distribution, such as a Normal distribution or a poisson distribution (remember, these are “parametric” because the distribution is completely described by a set of parameters). A good question is why bother? In general, one would use a parametric bootstrap for a statistic for which there is no formula for the standard error, but the underlying data come from a parametric probability distribution. A non-parametric bootstrap uses resamples from the data. The data are resampled with replacement. “Resample with replacement” means to sample \\(n\\) times from the full set of observed values. If we were to do this manually, we would i) write down each value of the original sample on its own piece of paper and throw all pieces into a hat. ii) pick a paper from the hat, add its value to sample \\(i\\), and return the paper to the hat. iii) repeat step ii \\(n\\) times, where \\(n\\) is the original sample size. The new sample contains some values multiple times (papers that were picked out of the hat more than once) and is missing some values (papers that were not picked out in any of the \\(n\\) picks). A good question is, why bother? A non-parametric bootstrap assumes no specific parametric probability distribution but it does assume the distributio of the observed sample is a good approximation of the true population distribution (in which case, the probability of picking a certain value is a good approximation to the true probability). 5.4.1 An example of bootstrapped standard errors Data from: Fernández, Álvaro F., et al. “Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice.” Nature 558.7708 (2018): 136-140. Experiment: Figure 3b data file name: 41586_2018_162_MOESM5_ESM.xlsx See Hidden Code for the chunk to import the exp3b dataset. Let’s compute the standard error of the mean of \\(\\texttt{positive_nuclei_per_area}\\) for the WT group using both a parametric and a nonparametric bootstrap. To implement the algorithm above using easy-to-understand code, I’ll first extract the set of \\(\\texttt{positive_nuclei_per_area}\\) values for the WT group and assign it to its own variable. wt_nuclei &lt;- exp3b[genotype == &quot;WT&quot;, positive_nuclei_per_area] I outlined the data.table way to subset data in Subsetting data but here I review what the line of code above does. exp3b[genotype == \"WT\", ] indexes the rows (that is, returns the row numbers) that satisfy the condtion genotype = “WT”. Or, put another way, it selects the subset of rows that contain the value “WT” in the column “genotype”. exp3b[, positive_nuclei_per_area] indexes the column labeled “positive_nuclei_per_area”. Combined, these two indices extract the values of the column “positive_nuclei_per_area” in the subset of rows that contain the value “WT” in the column “genotype”. The resulting vector of values is assigned to the variable “wt_nuclei”. 5.4.1.1 parametric bootstrap set.seed(1) # so we all get the same results! # we&#39;ll use these as parameters for parametric bootstrap n &lt;- length(wt_nuclei) mu &lt;- mean(wt_nuclei) sigma &lt;- sd(wt_nuclei) n_boot &lt;- 1000 # number of bootstrap iterations, or p means &lt;- numeric(n_boot) # we will save the means each iteration to this for(iter in 1:n_boot){ # this line sets up the number of iterations, p fake_sample &lt;- rnorm(n, mean = mu, sd = sigma) means[iter] &lt;- mean(fake_sample) } se_para_boot &lt;- sd(means) se_para_boot ## [1] 0.504529 5.4.1.2 non-parametric bootstrap set.seed(1) # so we all get the same results! n_boot &lt;- 1000 # number of bootstrap iterations, or p means &lt;- numeric(n_boot) # we will save the means each iteration to this # inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean inc &lt;- 1:n for(iter in 1:n_boot){ # inc is the set of rows to include in the computation of the mean. means[iter] &lt;- mean(wt_nuclei[inc]) # re-sample 1:n to construct inc for the next iteration inc &lt;- sample(1:n, replace = TRUE) } se_np_boot &lt;- sd(means) se_np_boot ## [1] 0.5102687 The parametric bootstrapped SEM is 0.5. The non-parametric bootstrapped SEM is 0.51. Run these several times to get a sense how much variation there is in the bootstrapped estimate of the SEM given the number of iterations. Compute the parametric standard error using equation (5.2) and compare to the bootstrapped values. ## [1] 0.5235885 5.5 Confidence Intervals 5.5.1 Just the math Here I introduce a confidence interval (or CI) of a sample mean but the concept is easily generalized to any statistic. A 95% confidence interval of the mean has a 95% probability of including the true mean, that is, the value of true mean is between the lower and upper bounds of the interval. This probability is a long-run frequency in the sense that if a population is sampled many times, and the 95% CIs are constructed for each sample, 95% of the CIs will contain the mean. A longer outline of the interpretation of a confidence interval is in the next section. The SE of a statistic is used to construct the lower and upper boundary of a confidence interval. For the CI of a mean, the formula is \\[ CI = \\overline{y} + t^*SE \\] where \\(t^*\\) (or critical value) is a quantile of the t-distribution with \\(n-1\\) degrees of freedom. For the lower bound of the 95% CI, we want the quantile at 0.025 (2.5%). For the upper bound of the 95% CI, we want the quantile at 0.975 (97.5%). For the wt_nuclei sample, with \\(n = 20\\), the lower and upper \\(t^*\\) are t_lower = qt(0.025, df = (n - 1)) t_upper = qt(0.975, df = (n - 1)) ## t_lower t_upper ## -2.093024 2.093024 And, the lower and upper 95% CIs of the mean of wt_nuclei are se_wt &lt;- sd(wt_nuclei)/sqrt(n) # CIs lower &lt;- mean(wt_nuclei) + t_lower * se_wt upper &lt;- mean(wt_nuclei) + t_upper * se_wt ## 2.5% 97.5% ## 1.106817 3.298584 The function qt maps a probability to a t-value – this is the opposite of a t test, which maps a t-value to a probability. Sending \\(\\alpha/2\\) and \\(1 - \\alpha/2\\) to qt returns the bounds of the confidence interval on a standardized scale. Multiplying these bounds by the standard error o wt_nuclei transforms the standardized bounds onto the scale of the wt_nuclei counts. We can check our manual computation with the linear model confint(lm(wt_nuclei ~ 1)) ## 2.5 % 97.5 % ## (Intercept) 1.106817 3.298584 5.5.2 Interpretation of a confidence interval Okay, so what is a confidence interval? A confidence interval of the mean is a measure of the uncertainty in the estimate of the mean. A 95% confidence interval has a 95% probability (in the sense of long-run frequency) of containing the true mean. It is not correct to state that “there is a 95% probability that the true mean lies within the interval”. These sound the same but they are two different probabilities. The first (correct interpretation) is a probability of a procedure – if we re-do this procedure (sample data, compute the mean, and compute a 95% CI), 95% of these CIs will contain the true mean. The second (incorrect interpretation) is a probability that a parameter (\\(\\mu\\), the true mean) lies within some range. The second (incorrect) interepretation of the CI is correct only if we also assume that any value of the mean is equally probable (Greenland xxx), an assumption that is absurd for almost any data. Perhaps a more useful interpretation of a confidence interval is, a confidence interval contains the range of true means that are compatible with the data, in the sense that a \\(t\\)-test would not reject the null hypothesis of a difference between the estimate and any value within the interval (this interpretation does not imply anything about the true value) (Greenland xxx). The “compatibility” interpretation is very useful because it implies that values outside of the interval are less compatible with the data. 5.5.3 Bootstrap confidence interval set.seed(1) # so we all get the same results! n_boot &lt;- 1000 # number of bootstrap iterations, or p means &lt;- numeric(n_boot) # we will save the means each iteration to this # inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean inc &lt;- 1:n for(iter in 1:n_boot){ # inc is the set of rows to include in the computation of the mean. means[iter] &lt;- mean(wt_nuclei[inc]) # re-sample 1:n to construct inc for the next iteration inc &lt;- sample(1:n, replace = TRUE) } ci_boot_wt &lt;- quantile(means, c(0.025, 0.975)) ci_boot_wt ## 2.5% 97.5% ## 1.287197 3.306559 5.5.4 A plot of a “parametric” CI vs. bootstrap CI of the means The dataset in exp3b was specifically chosen to highlight differences between a bootstrap CI and a CI computed using Eq. (5.2) for the standard error of a mean. I’ll refer to this latter CI as a “parametric” CI – It’s the CI one gets if we fit a linear model to just the treatment group. See if you can follow what’s going on here. Create a function to compute the 95% parametric CI of a mean. By “parametric” CI, I mean the CI using Eq. (5.2) for the SEM. I create a function because anytime we recycle code to compute something with a different input, it is best practice to create a function to compute this something. # a simple ci function, that can&#39;t handle missing ci_mean &lt;- function(y, alpha = 0.05){ n &lt;- length(y) ci &lt;- c( &quot;2.5%&quot; = mean(y) + qt(alpha/2, df = (n - 1)) * sd(y)/sqrt(n), &quot;97.5%&quot; = mean(y) + qt((1 - alpha/2), df = (n - 1)) * sd(y)/sqrt(n) ) return(ci) } Create a function to compute the 95% non-parametric bootstrap CI of a mean # a simple bootstrap function boot_ci_mean &lt;- function(y, alpha = 0.05, n_boot = 1000, seed = 1){ set.seed(seed) n_boot &lt;- 1000 # number of bootstrap iterations means &lt;- numeric(n_boot) # we will save the means each iteration to this # inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean inc &lt;- 1:n for(iter in 1:n_boot){ # inc is the set of rows to include in the computation of the mean. means[iter] &lt;- mean(y[inc]) # re-sample 1:n to construct inc for the next iteration inc &lt;- sample(1:n, replace = TRUE) } ci_boot &lt;- quantile(means, c(alpha/2, (1-alpha/2))) return(ci_boot) } Create a table of the parametric and bootstrap CIs of the mean of the WT and KI groups wt_nuclei &lt;- exp3b[genotype == &quot;WT&quot;, positive_nuclei_per_area] ki_nuclei &lt;- exp3b[genotype == &quot;KI&quot;, positive_nuclei_per_area] exp3b_ci &lt;- exp3b[, .(positive_nuclei_per_area = mean(positive_nuclei_per_area), ci = ci_mean(positive_nuclei_per_area), ci_boot = boot_ci_mean(positive_nuclei_per_area)), by = genotype] exp3b_ci[, boundary := rep(c(&quot;lower&quot;, &quot;upper&quot;), 2)] exp3b_ci_wide &lt;- dcast(exp3b_ci, genotype + positive_nuclei_per_area ~ boundary, value.var = c(&quot;ci&quot;, &quot;ci_boot&quot;)) Plot gg1 &lt;- ggplot(data = exp3b, aes(x = genotype, y = positive_nuclei_per_area)) + geom_point(position = position_jitter( width = 0.1, seed = 1), alpha = 0.3) + geom_point(data = exp3b_ci_wide, aes(y = positive_nuclei_per_area), size = 1.5) + geom_errorbar(data = exp3b_ci_wide, aes(ymin = ci_lower, ymax = ci_upper), width = 0.1) + ylab(&quot;Positive nuclei per area&quot;) + ggtitle(&quot;Parametric CI&quot;) + theme_pubr() gg2 &lt;- ggplot(data = exp3b, aes(x = genotype, y = positive_nuclei_per_area)) + geom_point(position = position_jitter( width = 0.1, seed = 1), alpha = 0.3) + geom_point(data = exp3b_ci_wide, aes(y = positive_nuclei_per_area), size = 1.5) + geom_errorbar(data = exp3b_ci_wide, aes(ymin = ci_boot_lower, ymax = ci_boot_upper), width = 0.1) + ylab(&quot;Positive nuclei per area&quot;) + ggtitle(&quot;Bootstrap CI&quot;) + theme_pubr() plot_grid(gg1, gg2, ncol = 2) The difference between the parametric and bootstrap CI is most easily seen by comparing the CIs in the KI treatment group. The parametric CI is symmetric about the mean. This has to be true because of how it is computed. The non-parametric bootstrap CI is asymmetric about the mean – it is longer on the more positive side and shorter on the less positive side. This asymmetry reflects the positive-skew of the underlying distribution. A nice feature of a bootstrap CI compared to a parametric CI computed as the sample CI is that the bootstrap CI will never contain impossible values. A parametric CI fit by a linear model might have a negative lower bound, which would be imply that negative counts are consistent with the data. Negative counts aren’t really consistent with anything because negative counts are impossible. 5.6 Standard error of a difference between means Let’s apply the concepts of standard error and confidence interval to a more important statistic for experimental biologists – the difference between the means of two groups. The standard error of the difference between the means of two groups is \\[ s_{\\bar{y}_2 - \\bar{y}_1} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\] The standard error of the difference between means is the square root of the sum of the squared standard errors of the mean of each variable. To understand this, recall that when we add two, independent, random variables, the variance of the summed variable is the sum of the variances of the independent variables (okay, but what is the variance of the difference between two random variables? This is no different because a difference is just the addition of the negative of the second variable). 5.6.1 The standard error of a difference between means is the standard deviation of the sampling distribution of the difference Let’s peak at a sampling distribution of the difference in means of simulated control (“Cn”) and treated (“Tr”) groups. The fake data are simulated to look like those in exp2i, introduced in the introductory chapter Analyzing experimental data with a linear model. set.seed(1) # so we all get the same result! n_sim &lt;- 10^4 # set mean, sd and difference to look like exp2i from ch. 1 m1 &lt;- lm(liver_tg ~ treatment, data = exp2i) s_obs &lt;- summary(m1)$sigma sigma &lt;- s_obs + rnorm(1, mean = 0, sd = s_obs/sqrt(12)) b_obs &lt;- coef(m1) mu_1 &lt;- b_obs[1] + rnorm(1, mean = 0, sd = sigma/sqrt(12)) # cn mean = b0 mu_2 &lt;- b_obs[1] + b_obs[2] + rnorm(1, mean = 0, sd = sqrt(2*sigma^2/6)) # tr mean = b0 + b1 delta &lt;- mu_2 - mu_1 n &lt;- nrow(exp2i)/2 diff &lt;- numeric(n_sim) sed &lt;- numeric(n_sim) diff_obs &lt;- b_obs[2] # observed value sed_obs &lt;- sqrt(2*s_obs^2/n) # observed value for(sim_i in 1:n_sim){ Cn &lt;- rnorm(n, mean = mu_1, sd = sigma) Tr &lt;- rnorm(n, mean = mu_2, sd = sigma) diff[sim_i] &lt;- mean(Tr) - mean(Cn) sed[sim_i] &lt;- sqrt(sd(Cn)^2/n + sd(Tr)^2/n) } qplot(diff, bins = 30) Figure 5.4: Sampling distribution of the difference in means for fake data modeled to look like the data in exp2i from Chapter 1. Fig. 5.4 is a sample (N = \\(10^4\\)) from the sampling distribution of the difference in means for fake data modeled to look like the data in exp2i from Chapter 1.The standard deviation of this sample is sd(diff) ## [1] 5.761664 This is pretty close to the true standard deviation of the distribution, which is sqrt(sigma^2/n + sigma^2/n) ## [1] 5.77137 This is the expected standard error of the difference in means if we were to sample \\(n = 6\\) individuals from the “Cn” population (\\(\\mu=\\) 61.9966034, \\(\\sigma=\\) 9.9963055 and \\(n = 6\\) individuals from the “Tr” population (\\(\\mu=\\) 35.043945, \\(\\sigma=\\) 9.9963055. The observed difference and standard error and the first ten fake differences and standard errors from the \\(10^4\\) fake data sets are.. Estimate SE -21.60000 7.045487 (obs) -31.98518 6.353331 -33.51950 4.447695 -29.13858 5.087926 -25.81587 3.440894 -25.62529 6.855294 -25.16365 6.872344 -22.38114 4.336371 -30.93056 5.593599 -14.81727 4.020610 -24.85262 4.207291 5.7 Confidence limits of a difference between means Let’s compute the 95% confidence interval of the difference in means of the exp2i data. Cn &lt;- exp2i[treatment == &quot;ASK1F/F&quot;, liver_tg] Tr &lt;- exp2i[treatment == &quot;ASK1Δadipo&quot;, liver_tg] diff_1 &lt;- mean(Tr) - mean(Cn) s_1 &lt;- sd(Tr) s_2 &lt;- sd(Cn) se &lt;- sqrt(s_1^2/n + s_2^2/n) # 2n - 2 df because there are 2n values and # 2 parameters fit (a df is lost for each fit parameter) t_lower = qt(0.025, df = (2*n - 2)) t_upper = qt(0.975, df = (2*n - 2)) lower &lt;- diff_1 + t_lower * se upper &lt;- diff_1 + t_upper * se Estimate Std. Error 2.5 % 97.5 % -21.6 7.045487 -37.29832 -5.901676 Now compare these values to the “treatmentASK1Δadipo” row of the coefficient table of the linear model fit to the exp2i data. # fit the model m1 &lt;- lm(liver_tg ~ treatment, data = exp2i) # output table model_table &lt;- cbind(coef(summary(m1)), confint(m1)) # print model_table %&gt;% kable() %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 61.46667 4.981912 12.337968 0.0000002 50.36628 72.567058 treatmentASK1Δadipo -21.60000 7.045487 -3.065792 0.0119261 -37.29832 -5.901676 5.7.1 95% of 95% CIs of the difference include the true difference A 95% CI of the difference (the estimate of the true effect) has a 95% probability of “covering the true effect”, where the probability is in the sense of long-term frequency – if we repeated an experiment 1000 times, we would expect 950 of the measured 95% CIs to include the true effect. Let’s check the frequency of 95% CIs that cover the true effect in the \\(10^4\\) fake data sets generated above. First, compute the CIs for each data set. lower &lt;- diff + qt(0.025, df = (2*n - 2)) * sed upper &lt;- diff + qt(0.975, df = (2*n - 2)) * sed ci_table &lt;- data.table( sim = factor(1:n_sim), estimate = diff, &quot;lower&quot; = lower, &quot;upper&quot; = upper ) As a check, let’s plot the first 10 estimates of the true effect and the CI of the estimate from the fake data simulation. gg &lt;- ggplot(data = ci_table[1:10,], aes (x = estimate, y = sim)) + geom_vline(aes(xintercept = delta), linetype = &quot;dashed&quot;) + geom_point(size = 2) + geom_segment(aes(x = lower, xend = upper, y = sim, yend = sim)) + theme_pubr() gg The frequency of CIs that cover the true effect is (sum(delta &gt; lower &amp; delta &lt; upper))/n_sim * 100 ## [1] 95.1 Pretty good! 5.8 Hidden code 5.8.1 Import exp3b data_from &lt;- &quot;Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice&quot; file_name &lt;- &quot;41586_2018_162_MOESM5_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp3b &lt;- read_excel(file_path, sheet = &quot;Fig. 3b&quot;, range = &quot;I4:K50&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% clean_names() setnames(exp3b, old = &quot;area_mm2&quot;, new = &quot;area&quot;) genotype_levels &lt;- c(&quot;WT&quot;, &quot;KI&quot;) exp3b[, genotype := rep(genotype_levels, c(20, 26))] exp3b[, genotype := factor(genotype, levels = genotype_levels)] exp3b[, positive_nuclei_per_area := positive_nuclei/area] "],["p-values.html", "Chapter 6 P-values 6.1 A p-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution 6.2 Pump your intuition – Creating a null distribution 6.3 A null distribution of t-values – the t distribution 6.4 P-values from the perspective of permutation 6.5 Parametric vs. non-parametric statistics 6.6 frequentist probability and the interpretation of p-values 6.7 Some major misconceptions of the p-value 6.8 What the p-value does not mean 6.9 On using p-values in experimental biology 6.10 Better reproducibility 6.11 Multiple testing and controlling for false positives 6.12 Recommendations 6.13 Problems", " Chapter 6 P-values A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm.3 A p-value is a measure of the compatibility between observed data and the null model. Here, “compatibility” is a probability, specifically, the probability of sampling a test-statistic as or more extreme than the observed test statistic, if all the assumptions used to compute the p-value are true. To deconstruct what this means, and the implications of the meaning, let’s use the exp2i data from Figure 2i in the study on the browning of white adipose tissue in mice that was introduced in the introductory chapter # Analyzing experimental data with a linear model. Data source: ASK1 inhibits browning of white adipose tissue in obesity fig_2i_m1 &lt;- lm(liver_tg ~ treatment, data = fig_2i) The chunk above fits a linear model with liver_tg as the response variable and treatment as the single \\(X\\)-varaiable. Figure 6.1 is the plot of the modeled means, 95% confidence intervals of the mean, and p-value of the significance test of the effect of treatment on liver triacylglycerol. Figure 6.1: UCP1 expression, relative to the mean level in the control group. Mean (circle) and 95% confidence interval (line) are shown. Unadjusted p-values are from linear model with sh-RNA treatment and LPS treatment fully crossed. The coefficients of the model, and the standard error, 95% confidence interval, test-statistic, and p-value of each coefficient are shown in Table ?? Recall from Chapter ?? that, with this model, the coefficient for the intercept term is the mean liver_tg for the control group, which is the estimate of the true mean for a mice with functional ASK1 protein. And the coefficient for the treatmentASK1Δadipo term is the difference in means between the knockout and control group, which is the estimate for the true effect of knocking out ASK1 in the adipose tissue. The p-value for this “effect” term is 0.012. How do we interpret this number? Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 61.5 4.98 12.3 0.000 50.4 72.6 treatmentASK1Δadipo -21.6 7.05 -3.1 0.012 -37.3 -5.9 6.1 A p-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution The test statistic in the table above is a t-value. For this specific model, this t-value is precisely the t-value one would get if they executed a classical Student t-test on the two groups of liver TG values. Importantly, this is not generally true. For many of the models in this text, a t-value is computed but this is not the t-value that would be computed in a classical t-test. When we do a t-test, we get a p-value. The probability returned in a t-test is \\(p = \\mathrm{prob}(t \\ge t_{obs} | H_0)\\). Read this as “the p-value is the probability of observing a t-value that is greater than or equal to the observed t-value, given the null model is true.” Probability, in this text, is a long run frequency of sampling. The specific probability associated with the effect of treatment on liver TG is the long-run frequency of observing a t-value as big or bigger than the observed t-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a t-value as big or bigger than the observed t-value” and “null is true”. A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a t-value. Unfortunately, google sheets doesn’t have a t-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now “fill right” or copy and paste these functions into 999 new columns. You now have 1000 t-tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected t-value will also be zero (why?) and the distribution of these 1000 t-values will look normal but the tails are a little fuller. This row of t-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a t-value in your head, say 0.72 (remember that t-values will largely range from about -3 to +3 although the theoretical range is \\(-\\infty\\) to \\(+\\infty\\). What is the probability of observing a t of 0.72 or bigger if the null is true? Look at the row of t-values! Count the number of \\(t \\ge 0.72\\) and then divide by the total number of t-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and t-values). Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact t is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding t as big, or bigger, than our observed t. Second, the t-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the \\(t \\ge 0.72\\) and the \\(t \\le -0.72\\) and then add these two counts to compute the frequency of as extreme or more extreme values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count \\(t \\ge 0.72\\) but take advantage of the beautiful mathematical properties of the theoretical t distribution, which allows us to compute the frequentist probability (expected long range frequency) given the t-value and the degrees of freedom using the t-distribution. Now what do I mean with the phrase “null is true”? Most people equate “null is true” with “no difference in means” but the phrase entails much more than this. Effectively, the phrase is short for “all assumptions used to compute” the p-value (see the Sander Greenland quote at the start of this chapter). A p-value is based on modeling the real data with a theoretical sample in which all the values were randomly sampled from the same distribution and the assignment of the individual values to treatment was random. Random sampling from the same distribution has three important consequences. First, random assignment to treatment group means that the expected means of each group are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment also means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. Here is what is super important about this: a low p-value is more likely if any one of these consequences is untrue about our data. A low p-value could arise from a difference in true means, or it could arise from a difference in true variances, or it could arise if the \\(Y\\) values are not independent of each other. This is why we need certain assumptions to make a p-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low p-value is evidence of unequal means. Let’s summarize: A pretty good definition of a p-value is: the long-run frequency of observing a test-statistic as large or larger than the observed statistic, if the null were true. A more succinct way to state this is \\[\\begin{equation} p = \\mathrm{prob}(t \\ge t_o | H_o) \\end{equation}\\] where t is a hypothetically sampled t-value from a null distribution, \\(t_o\\) is the observed t-value, and \\(H_o\\) is the null hypothesis. Part of the null hypothesis is the expected value of the parameter estimated is usually (but not always) zero – this can be called the nil null. For example, if there is no ASK1 deletion effect on liver TG levels, then the expected difference between the means of the control and knockout mice is zero. Or, \\[\\begin{equation} \\mathrm{E}(\\bar{Y}_{knockout} - \\bar{Y}_{control} | H_o) = 0.0 \\end{equation}\\] 6.2 Pump your intuition – Creating a null distribution The mean liver_tg in the knockout treatment is 21.6 µmol less than the mean liver_tg in the control treatment. This is the measured effect, or the observed differences in means. How confident are we in this effect? Certainly, if the researchers did the experiment comparing two control (ASK1F/F) groups, instead of a control and treatment group, they would measure some difference in their means simply because of sampling (that is which mice were sampled for the experiment). So let’s reframe the question: are the observed differences unusually large compared to a distribution of differences that would occur if there were no effect? That is, if the “null were true”. To answer this, we compare our observed difference to this null distribution. This comparison gives the probability (a long-run frequency) of “sampling” a random difference from the null distribution of differences that is as large, or larger, than the observed difference. What is a null distribution? It is the distribution of a statistic (such as a difference in means, or better, a t-value) if the null were true. Here, I hope to pump your intuition by generating a null distribution that is relevant to the ASK1 liver TG data. See if you can understand the script before reading the explanation below. seed &lt;- 1 n_rep &lt;- 10^5 # number of replicate experiments mu &lt;- mean(fig_2i[treatment == &quot;ASK1F/F&quot;, liver_tg]) sigma &lt;- sd(fig_2i[treatment == &quot;ASK1F/F&quot;, liver_tg]) n &lt;- nrow(fig_2i[treatment == &quot;ASK1F/F&quot;,]) d_null &lt;- numeric(n_rep) for(rep in 1:n_rep){ sample_1 &lt;- rnorm(n, mean = mu, sd = sigma) sample_2 &lt;- rnorm(n, mean = mu, sd = sigma) d_null[rep] &lt;- mean(sample_2) - mean(sample_1) } qplot(d_null) Figure 6.2: Null distribution for the difference in means of two samples from the same, inifinitely large population with a true mean and standard deviation equal to the observed mean and standard deviation of the ASK1 liver TG data. What have we done above? using the rnorm function, we’ve simulated an infinitely large population of mice that have a distribution of liver TG values similar to that of the mice assigned to the control (ASK1F/F) group. The true mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) of the simulated TG level are equal to the observed mean and standard deviation of the TG levels of the control mice. randomly sample 6 values from this population of simulated liver TG values and assign to \\(\\texttt{sample_1}\\). We sample 6 values because that is the sample size of our control in the experiment. randomly sample 6 values from the same population of simulated liver TG values and assign to \\(\\texttt{sample_1}\\). compute the difference of the means: \\(\\bar{Y}_{sample\\_2} - \\bar{Y}_{sample\\_1}\\). repeat 1-3 100,000 times, each time saving the difference in means. plot the distribution of the 100,000 differences using a histogram The distribution of the differences is a null distribution. Notice that the mode of the null distribution is at zero, and the mean (0.01768) is close to zero (if we had set \\(n\\) to infinity, the mean would be precisely zero). The expected difference between the means of two random samples from the same population is, of course, zero. Don’t gloss over this statement if that is not obvious. The tails extend out to a little more than +20 and -20. What this means is that it would be uncommon to randomly sample a value from this distribution of differences as or more extreme than our observed difference, -21.6. By “more extreme”, I mean any value more negative than -21.6 or more postive than 21.6. So it would be uncommon to sample a value from this distribution whose absolute value is as or more extreme than 21.6. How uncommon would this be? diff_obs &lt;- fig_2i_m1_coef[&quot;treatmentASK1Δadipo&quot;, &quot;Estimate&quot;] null_diff_extreme &lt;- which(abs(d_null) &gt; abs(diff_obs)) n_extreme &lt;- length(null_diff_extreme) (p_d_null = n_extreme/n_rep) ## [1] 0.00566 In the 100,000 runs, only 566 generated data with an absolute difference as large or larger than 21.6 (an “absolute difference” is the absoute value of the difference). The frequency of differences as large or larger than our observed difference is 0.00566. This frequency is the probability of sampling a difference as or more extreme than the observed difference “under the null”. It is a p-value, but it is not the p-value in the coefficient table. This is because the p-value in the coefficient table is computed from a distribution of t-values, not raw differences. This raises the question, what is a t-distribution, and a t-value, more generally? 6.3 A null distribution of t-values – the t distribution A t-test is a test of differences between two values. These could be the difference between the means of two samples (a “two-sample” t-test) the difference between a mean of a sample and some pre-specified value (a “one-sample” t-test) the difference between a coefficient from a linear model and zero A t-test compares an observed t-value to a t-distribution. The null distribution introduced above was a distribution of mean differences under the null. A distribution of mean differences under the null is very specific to the mean and standard deviation of the population modeled and the sample size of the experiment. This isn’t generally useful, since it will be unique to every study (at least it wasn’t generally useful prior to the time of fast computers. One could, and some statisticians do, compute p-values using the algorithm above). A t-distribution is a way of transforming a null distribution of mean differences, which is unique to the study, into a distribution that is a function of sample size only. A t-distribution is a distribution of t-values under the null, where a t-value is a difference standardized by its standard error. For a two-sample t-test, this is \\[\\begin{equation} t = \\frac{\\bar{y}_2 - \\bar{y}_1}{SE_{\\bar{y}_2 - \\bar{y}_1}} \\tag{6.1} \\end{equation}\\] The numerator is the effect while the denominator is the precision of the estimate. Like many test statistics, a t-value is a signal-to-noise ratio – the effect is the signal and the SE of the difference is the noise. A t distribution looks like a standard, normal distribution, except the tails are heavy, meaning there are more large-ish values than the normal. Like the standard normal distribution, large t-values are unlikely under the null and, therefore, a large t has a low probability – or p-value – under the null. Looking at the equation for the two-sample t-test above, it is easy to see that three features of an experiment are associated with large t and small p-values: 1) big effect size (the numerator of the equation), 2) small sample standard deviations (which results in small standard errors of the difference, the denominator of equation (6.1), and 3) large sample size (which results in small standard errors of the difference). As a quick-and-dirty generalization, absolute t-values greater than 3 are uncommon if the null is true. The p-value for a t-test comes from comparing the observed t to a null t distribution and “counting” the values that are more extreme than the observed t. The p-value is the relative frequency of these more extreme values (relative to the total number of t-values in the distribution). I have “counting” in quotes because nothing is really counted – there are an infinite number of t-values in the t-distribution. Instead, the t-distribution function is integrated to compute the fraction of the total area under the curve with t-values more extreme than the observed value. In a two-tailed test, this fraction includes both tails (positive t-values more positive than \\(|t|_{observed}\\) and negative t-values more negative than \\(-|t|_{observed}\\). Let’s repeat the simulation of a null distribution of mean differences above but add the computation of the t-value for each replicate comparison in order to generate a null distribution of t-values. Importantly, I’ve also changed bits of the code to more properly think about what a computed t-value is. These changes are: I want to think of the first sample as being assigned to “WT” and the second sample as being assigned to “KO”. But, the KO sample is drawn from the same distribution (the same hat of numbers) as the WT sample – this guarantees that there is no difference in expected mean. I want to think of the observed variances of the WT and KO samples as sampled variances from this fake distribution. Therefore, I give the variance of the fake distribution the average of the observed WT and KO samples. The true (population) standard deviation (\\(\\sigma\\)) of the simulated data, then, is the square root of this averaged variance. I show the script, but don’t just cut and paste the code. Spend time thinking about what the each line does. Explore it by copying parts and pasting into console. seed &lt;- 1 n_rep &lt;- 10^5 # number of iterations mu &lt;- mean(fig_2i[treatment == &quot;ASK1F/F&quot;, liver_tg]) sd_control &lt;- sd(fig_2i[treatment == &quot;ASK1F/F&quot;, liver_tg]) sd_knockout &lt;- sd(fig_2i[treatment == &quot;ASK1Δadipo&quot;, liver_tg]) sigma &lt;- sqrt((sd_control^2 + sd_knockout^2)/2) n &lt;- nrow(fig_2i[treatment == &quot;ASK1F/F&quot;,]) treatment &lt;- rep(c(&quot;WT&quot;, &quot;KO&quot;), each = n) %&gt;% factor(levels = c(&quot;WT&quot;, &quot;KO&quot;)) # need this for for-loop t_null &lt;- numeric(n) t_null_manual &lt;- numeric(n) for(rep in 1:n_rep){ wt_sample &lt;- rnorm(n, mean = mu, sd = sigma) ko_sample &lt;- rnorm(n, mean = mu, sd = sigma) # way no.1 - compute the t-tests using the linear model y &lt;- c(wt_sample, ko_sample) m1 &lt;- lm(y ~ treatment) t_null[rep] &lt;- coef(summary(m1))[&quot;treatmentKO&quot;, &quot;t value&quot;] # way no. 2 - compute the t-tests manually! # check to make sure these are the same as t_null !!! diff &lt;- mean(ko_sample) - mean(wt_sample) se_diff &lt;- sqrt(sd(ko_sample)^2/n + sd(wt_sample)^2/n) t_null_manual[rep] &lt;- diff/se_diff } # plot the null distribution of t-values qplot(t_null) Figure 6.3: Null distribution of t-values. The simulation generated 10,000 t-tests with a true null. Now let’s use this null distribution of t-values to compute a p-value # what is the p-value? # the p-value is the number of t-values in t_null_2 that are as large # or larger than the observed t. Large, negative t-values # are as unlikely under the null as large, positive t-values. # To account for this, we want to use absolute values in our counts # this is a &quot;two-tail test&quot; # first assign the observed t-value t_obs &lt;- fig_2i_m1_coef[&quot;treatmentASK1Δadipo&quot;, &quot;t value&quot;] # now count the number of t-values in t_dis as big or bigger than this # include the observed value as one of these (so add 1 to the count) count &lt;- sum(abs(t_null) &gt;= abs(t_obs)) # the p-value is the frequency of t_dis &gt;= t_obs, so divide # count by the total number of t-values in the distribution. # Again add one since the observed value counts as a sample (p_ASK1Δadipo &lt;- count/(n_rep)) ## [1] 0.01173 Hey that looks pretty good! Compare this to the p-value in the coefficient table above. A p-value can be computed by counting the number of simulated t-values, including the observed value, that are equal to or more extreme (in either the positive or negative direction) than the observed t. Including the observed t, there are 1173 values that are more extreme than that observed. An approximate measure of p is this count divided by 100,001 (why is 1 added to the denominator?), which is 0.01173. This simulation-based p-value is very (very!) close to that computed from the observed t-test. 6.4 P-values from the perspective of permutation A very intuitive way to think about p-values as a frequency is random permutation. A permutation is a re-arrangement of items. If there is an effect of ASK1 deletion on liver TG, then the arrangement of the values in the treatment column matters. If there is no effect of ASK1 deletion on liver TG, then the arrangement of the values in the treatment column does not matter. Think about the structure of the liver TG data: there are two columns, treatment, which contains the assigned treatment, and liver_tg. The values in the treatment column were randomly assigned prior to the start of the experiment. If there is a negative effect of ASK1 deletion on liver TG, then assginment matters – the values in the liver_tg column for the ASK1Δadipo rows will be smaller than, on average, the values in the ASK1F/F rows. That is, a specific value of aliver_tg is what it is because of the value of treatement in the same row. Assignment to ASK1F/F or ASK1Δadipo changes the expected value of liver_tg. But, if there were no true effect, then assignment to ASK1F/F or ASK1Δadipo does not change the expected value of liver_tg. The expected value of every cell in the liver_tg column would be the same regardless of what is in the treatment column. In our thought experiment, let’s leave the values in the treatment column be, and just randomly re-arrange or permute the values in the liver_tg column. What is the new expected diference in liver TG between the rows assigned to ASK1F/F and the rows assigned to ASK1Δadipo? The expected difference is Zero. Because the liver_tg values were randomly re-arranged, they cannot be caused by treatment assignment. A permutation is a random re-arrangement of values in a column. Consider the many thousands of permutations of the values in the liver_tg column. A difference in means can be computed from each of these permuations and a distribution of differences can be generated. Is the observed difference extreme relative to the other values in this distribution? This is a permutation test – it compares an observed statistic to a distribution of the statistic computed over many thousands of permutations. Let’s create a script for a permutation test set.seed(1) n_iter &lt;- 5000 # number of random permutations y &lt;- fig_2i[, liver_tg] x &lt;- fig_2i[, treatment] d_dist_perm &lt;- numeric(n_iter) for(iter in 1:n_iter){ xbar1 &lt;- mean(y[x == &quot;ASK1F/F&quot;]) xbar2 &lt;- mean(y[x == &quot;ASK1Δadipo&quot;]) d_dist_perm[iter] &lt;- xbar2 - xbar1 # permute y y &lt;- sample(y, replace=FALSE) # note that, when i=1, the first &quot;permutation&quot; is the original arrangement } qplot(d_dist_perm) From this distribution of distances generated by random permuation of the response, we can compute a permutation p-value. {rpvalue-d-dist-perm-p } (p_permute &lt;- sum(abs(d_dist_perm) &gt;= abs(d_dist_perm[1]))/n_iter) 6.5 Parametric vs. non-parametric statistics A statistic such as the difference in mean liver TG between ASK1Δadipo and ASK1F/F groups does not have “a” p-value. A p-value is the probability of observing an event given a model of how the event was generated. For the p-value in the coefficient table above, the event is sampling a t-value from a modeled t distribution that is as or more extreme than the observed t-value. The model generating the null distribution of t-values includes random sampling from a distribution that is defined by specific parameters (in this case, a mean and a variance), these parameters define the location and shape of the distribution of values that could be sampled. A p-value computed from a distribution that is defined by a set of parameters is a parametric p-value. For the p-value computed using the permutation test, the event is the probability of of computing a difference of means from a randomly permuted set of \\(Y\\) as or more extreme than the observed difference of means. The distribution of differences from the permutated \\(Y\\) data sets was not generated by any of the known distributions (normal, Poisson, binomial, etc.) given a specific value of parameters. Consequently, the permutation p-value is non-parametric. The validity of all p-values depends on a set of model assumptions, which differ from model to model. The permutation p-value has fewer assumptions than a parametric p-value because no distribution is assumed (the permutation p-value is distribution-free). 6.6 frequentist probability and the interpretation of p-values 6.6.1 Background There are at least three different meanings of probability. subjective probability is the probability that an individual assigns to an event based on prior knowledge and the kinds of information considered reliable evidence. For example, if I asked a sample of students, what is the probability that a 30c homeopathic medicine could clear a Streptococcus infection from your respiratory system, their answers would differ because of variation in their knowledge of basic science, including chemistry and physics, their knowledge of what homeopathic medicines are, and how they weight different kinds of evidence. classical probability is simply one divided by the number of possible unique events. For example, with a six-sided die, there are six possible unique events. The probability of rolling a 2 is \\(\\frac{1}{6}\\) and the probability of rolling an odd number is \\(\\frac{1}{2}\\). frequentist probability is based on the concept of long run frequency. If I roll a die 10 times, the frequency of rolling a 2 will be approximately \\(\\frac{1}{6}\\). If I roll the die 100 times, the frequency of rolling a two will be closer to \\(\\frac{1}{6}\\). If I roll the die 1000 times, the frequency of rolling the die will be even closer to \\(\\frac{1}{6}\\). So the frequentist definition is the expected frequency given an infinite number of rolls. For events with continous outcomes, a frequentist probability is the long run frquency of observing an outcome equal to or more extreme that that observed. 6.6.2 This book covers frequentist approaches to statistical modeling and when a probability arises, such as the p-value of a test statistic, this will be a frequentist probability. When we do a t-test, we get a p-value. There are several ways to think about this probability. The most compact way is \\(P(data | null)\\), which is literally read as the probability of the data given the null (or “conditional” on the null), but is really short for the probability of the data, or something more extreme than the data, given that the null hypothesis is true. The “probability of the data” is kinda vague. More specifically, we mean the probability of some statistic about the data such as the difference in means between group A and group B or the t-value associated with this difference. So, a bit more formally, the probability returned in a t-test is \\(\\mathrm{prob}(t \\ge t_{obs} | H_0)\\). This is the long run frequency of observing a t-value as big or bigger than the observed t-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a t-value as big or bigger than the observed t-value” and “null is true”. A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a t-value. Unfortunately, google sheets doesn’t have a t-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now ``fill right’’ or copy and paste these functions into 999 new columns. You now have 1000 t tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected t-value will also be zero (why?) and the distribution of these 1000 t-values will look normal but the tails are a little fuller. This row of t-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a t-value in your head, say 0.72 (remember that t-values will largely range from about -3 to +3 although the theoretical range is \\(-\\infty\\) to \\(+\\infty\\). What is the probability of observing a t of 0.72 or bigger if the null is true? Look at the row of t-values! Count the number of \\(t \\ge 0.72\\) and then divide by the total number of t-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and t-values). Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact t is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding t as big, or bigger, than our observed t. Second, the t-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the \\(t \\ge 0.72\\) and the \\(t \\le -0.72\\) and then add these two counts to compute the frequency of as extreme or more extreme values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count \\(t \\ge 0.72\\) but take advantage of the beautiful mathematical properties of the theoretical t distribution, which allows us to compute the frequentist probability (expected long range frequency) given the t-value and the degrees of freedom using the t-distribution. Now what do I mean with the phrase “null is true”? Most people equate “null is true” with ``no difference in means’’ but the phrase entails much more than this. Effectively, the phrase means that the p-value is based on modeling the real data with a theoretical sample in which all the points were randomly sampled from the same distribution and that the assignment of the individual points to treatment was random. This model means the theoretical sample has three properties: First, random assignment to treatment after sampling from the same distribution means that the expected means are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment after sampling from the same distribution also means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. Here is what is super important about this: if we get a really low p-value, any one of these consequences may be untrue about our data, for example it could be that the true means of the two treatment groups really are different, or it could mean it is the variances that differ between the two groups, or it could mean that the data (or technically, the errors) are not independent of each other. This is why we need certain assumptions to make a p-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low p-value is evidence of unequal means. 6.6.3 Two interpretations of the p-value Since we want to be working scientists who want to use p-values as a tool, we need to know how to interpret (or use) the p-value to make reasonable inferences and how to avoid mis-interpreting the p-value and making unreasonable or even incorrect inferences. Two different interpretations of the p-value arose during the development of frequentist statistics. Ronald Fisher (who developed the bulk of the framework of frequentist statistics) thought of the p-value as a quantitative measure of evidence against the null hypothesis. Jerzy Neyman and Egon Pearson (Neyman-Pearson) thought of the p-value as a qualitative, threshold metric used for decision making – to act as if there is an effect. Modern researchers in biology typically use an interpretation that is an odd hybrid of the two, which often leads to illogical inference. Regardless, understanding the distinction between Fisher and Neyman-Pearson will inform how we write up our results in a manuscript. Fisher was working in the context of an agricultural experiments, the goal of which was to discover better agricultural practices – how do the yields in these five varieties of crop differ under this agricultural practice? Fisher thought of p as evidence against the null; the smaller the p, the stronger the evidence that the mean of the two sampling distributions differ given all model assumptions are true. Fisher never thought of a single experiment as definitive. Any decision following an experiment is only partly informed by the p-value and Fisher offered no formal rule about what p-value lies on the threshold of this decision. Neyman-Pearson thought of p as the necessary and sufficient information to make a decision between accepting the null (or at least not rejecting the null) or rejecting the null and accepting an alternative hypothesis. This decision balances two sorts of errors: Type I (false positives), which they called \\(\\alpha\\), and Type II (false negatives), which they called \\(\\beta\\). A false positive occurs when the null is rejected (because \\(p &lt; \\alpha\\)) but there is no effect of treatment (the null is true). A false negative occurs when the test fails to reject the null (because \\(p &gt; \\alpha\\)) but there actually is an effect (the null is false). \\(\\alpha\\) is set by the experimenter and is the long-term frequency (or “rate”) of false positives when the null is true that the experimenters are willing to accept. After setting \\(\\alpha\\), the experimenter designs the experiment to achieve an acceptable rate of \\(\\beta\\). Since \\(\\beta\\) is the false negative rate, \\(1-\\beta\\) is the rate of not making a false negative error. Or, stated without the double negative, \\(1-\\beta\\) is the rate of rejecting the null (“finding an effect”) when there really is an effect. This is called the power of the experiment. An experiment with high power will have a low probability of a Type II error. An experiment with low power will have a high probability of a Type II error. Power is partly determined by sample size, the bigger the sample the smaller the p-value, all other things equal (think about why in the context of the formula for the t-value). Power is a function of error variance, both the natural variance and the component added because of measurement error (think about why in the context of the formula for the t-value). Power is also a function of \\(\\alpha\\). If we set a low \\(\\alpha\\) (say, \\(\\alpha=0.01\\)), the test is conservative. We are more likely to fail to reject the null even if the null is false. A researcher can increase power by increasing sample size, using clever strategies to reduce measurement error, or increasing alpha. An experimenter sets \\(\\alpha\\), computes the sample size needed to achieve a certain level of power (\\(1-\\beta\\)), and then does the experiment. A thoughtful researcher will set \\(\\alpha\\) after considering and weighing the pros and cons of different levels of \\(\\alpha\\). If false positives have costly consequences (expense, time, deleterious side-effects), then set \\(\\alpha\\) to a low value, such as 0.01 or 0.001. For example, if an initial screen has identified a previously unknown candidate that potentially functions in the focal system of the researcher, then a researcher might decide to set a low \\(\\alpha\\) (0.001) in the initial tests of this candidate to avoid devoting time, personnel, and expense to chasing a phantom (a false-positive candidate). If false positives have trivial consequences, then set \\(\\alpha\\) to a high value, such as 0.05, or 0.1, or even 0.2. For example, if the initial tests of a candidate in a functional system are cheap and fast to construct, then a researcher might choose to set a high \\(\\alpha\\) for the screen that identifies candidates. False positive candidates don’t cost the lab much effort to identify them as false, but missing positive candidates because of a small \\(\\alpha\\) (which results in low power) at the screen stage costs the researcher the discovery of a potentially exciting component of the functional system. In Fisher’s interpretation, there is no \\(\\alpha\\), no \\(\\beta\\), no alternative hypothesis, and no sharp decision rule. Instead, in Fisher, p is a continuous measure of evidence against the null and its value is interpreted subjectively by an informed and knowledgeable expert using additional information to make decisions. Neyman-Pearson rejected Fisher’s conception of p as evidence against the null arguing that a single experimental p-value is too noisy without embedding it into a more formal system of of decision making that maintains long-term type I error rates at \\(\\alpha\\), given a certain power. In Neyman-Pearson, p is compared to a threshold, \\(\\alpha\\) and this alone makes the decision. In Neyman-Pearson, p is not treated as continuous information. \\(p=0.00000001\\) is no more evidence to use to reject the null than \\(p=0.049\\). 6.6.4 NHST Most biology researchers today interpret p using a combination of Fisher and Neyman-Pearson concepts in what has become known as Null Hypothesis Significance Testing (NHST). Nearly all papers in biology either explicitly state something like “P values &lt; 0.05 were considered to be statistically significant” or implicitly use 0.05 as the “level of significance” (\\(\\alpha\\)). Comparing a p-value to a pre-defined \\(\\alpha\\) is Neyman-Pearson. Unlike Neyman-Pearson, there is little evidence that researchers are thoughtfully considering the level of \\(\\alpha\\) for each experiment. Instead, researchers mindlessly choose \\(\\alpha=0.05\\) because this is what everyone else uses. Unlike Neyman-Pearson, but somewhat in the spirit of Fisher, researchers, journals, and textbooks, advocate polychotomizing a statistically significant p into “significance bins” – three asterisks for \\(p &lt; 0.001\\), two asterisks for \\(0.001 &lt; p &lt; 0.01\\), and one asterisk for \\(0.01 &lt; p &lt; 0.05\\)). This is not Neyman-Pearson. Again, Neyman-Pearson developed a system to control the long-run frequency of Type I error, which is controlled by a strict use of \\(\\alpha\\). If an observed p-value is in the *** bin or the * bin is meaningless in a system using Neyman-Pearson. There is only “accept” (\\(p \\ge \\alpha\\)) or “reject” (\\(p &lt; \\alpha\\)). Many researchers report exact p-values when \\(p &lt; 0.05\\) but “n.s.” (not significant) when \\(p &gt; 0.05\\). Reporting exact p-values is Fisher. Reporting n.s. is Neyman-Pearson. Many researchers further polychomotomize the p-value space just above 0.05 by using language such as “marginally significant”. 6.7 Some major misconceptions of the p-value Setting the type I error rate \\(\\alpha\\) to 0.05 is so pervasive that I’m going to simply use “0.05” instead of “alpha” in discussing misconceptions. 6.7.1 Misconception: \\(p &gt; 0.05\\) means there is no effect of treatment Many researchers believe that if \\(p &gt; 0.05\\) then “there is no effect.” A frequentist hypothesis test cannot show that an effect doesn’t exist, only that the null has a low probablity of producing a test statistic as extreme or more extreme than the observed effect. Even if there is a true effect of treatment, a high p-value can occur because of a low signal:noise ratio, where the signal is the true effect size (the magnitude of the true difference in response) and the noise is the combination of intrinsic (biological) and extrinsic (experimental) error. a small sample size, where small is relative to the sample size necessary for high power. The statement “There is no effect of knockout on glucose tolerance” is not a valid conclusion of a frequentist hypothesis test. The similar statement “We found no effect of knockout on glucose tolerance” is misleading because a frequentist hypothesis test can neither find an effect nor find no effect. 6.7.2 Misconception: a p-value is repeatable Many researchers believe that a p-value is a precise measure – that if the experiment were replicated, a similar p would result. This belief requires at least two misconceptions. First, if the null were true, then any p-value is equally likely. \\(p=0.00137\\) is just as likely as \\(p=0.492\\). In other words, if the null were true, the p-value is not replicable at all! Second, the p-value is highly dependent on the sample, and can be highly variable among replications, but there is no true p-value, so there can be no estimate or standard error. Let’s explore these. 6.7.2.1 The incredible inconsistency of the p-value How replicable is the conclusion of an experiment if the p-value for a t-test is 0.03? If our conclusion is based on \\(p &lt; 0.05\\), then the conclusion is not very replicable. The simulation below shows the results of 15 replicates of an experiment with true power of 40%. There are five “significant” results (one less than expected) but several replicates have very high p-values. Figure 6.4: Variability of p-values when the power is 0.4 6.7.2.2 What is the distribution of p-values under the null? I often ask students, “if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what is the most likely p-value?”. A common answer (although answers are uncommon) is \\(p = 0.5\\). Sometimes I rephrase the question, if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what do you think the distribution of p-values would look like?” The typical answer to this is a the distribtion will look like a normal curve with the peak at 0.5, (presumably the tails abruptly stop at 0 and 1). 6.7.3 Misconception: 0.05 is the lifetime rate of false discoveries An important and widespread misconception is that if a researcher consistently uses \\(\\alpha=0.05\\), then the frequency of incorrectly concluding an effect exists, or “discovering” an effect, over the lifetime of the researcher, will be 5%. This is incorrect. \\(\\alpha\\) is the rate of false positives in the subset of tests in which the null hypothesis is true. \\(\\alpha\\) is the Type I error rate. Our mental conception of “lifetime rate of false discoveries” is the False Discovery Rate, and is the frequency of false positives divided by the frequency of positives (the sum of false and true positives). To pump or intution about the differences between the Type I error rate and the False Discovery Rate, imagine we test 1000 null hypotheses over a lifetime 60% are true nulls, this means there are 600 true nulls and 400 true effects alpha is 5%. This means we expect to find \\(p \\le 0.05\\) 30 times (\\(0.05 \\times 600\\)) when the null is true power is 25%. This means we expect to find \\(p \\le 0.05\\) 100 times (\\(0.25 \\times 400\\)) when the null is false We have made \\(30 + 100=130\\) “discoveries” (all experiments with \\(p \\le 0.05\\)), but 30 of the 130, or 23%, are “false discoveries”. This is the false discovery rate. Think about this. If the null is never true, you cannot have a false discovery – every \\(p \\le 0.05\\) is a true discovery (the false discovery rate is 0%). And if the null is always true, every \\(p &lt; 0.05\\) is a false discovery (the false discovery rate is 100%). 6.7.4 Misconception: a low p-value indicates an important effect Many researchers write results as if they believe that a small p-value means the effect is big or important. This may misconception may arise because of the ubiquitous use of “significant” to indicate a small p-value and “very” or “extremely” or “wicked” significant to indicate a really small p-value. Regardless, this is a misconception. A small p-value will usually result when there is high power (but can occur even if power is low) and power is a function of effect size, variability (the standard deviation), and sample size. A small p could result from a large effect size but can also result with a small effect size if the sample size is big enough. This is easy to simulate (see script below). Let’s model the effect of the genotype of a gene on height set.seed(1) rho &lt;- 0.5 n &lt;- 10^4 genotype &lt;- c(&quot;+/+&quot;, &quot;+/-&quot;, &quot;-/-&quot;) Sigma &lt;- diag(2) Sigma[1,2] &lt;- Sigma[2,1] &lt;- rho X &lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma) colnames(X) &lt;- c(&quot;X1&quot;, &quot;X2&quot;) beta &lt;- c(0.05, 0.05) y &lt;- X%*%beta + rnorm(n) fit &lt;- lm(y ~ X) coefficients(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01 ## XX1 0.044304824 0.01154709 3.8368830 1.253725e-04 ## XX2 0.048228101 0.01170855 4.1190490 3.835033e-05 6.7.5 Misconception: a low p-value indicates high model fit or high predictive capacity On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point averages.” This is incorrect. A p-value is not a measure of the predictive capability of a model because the p-value is a function of the signal, noise (unmodeled error), and sample size while predictive ability is a function of just the signal:noise ratio. If the signal:noise ratio is tiny, the predictive ability is small but the p-value can be tiny if the sample size is large. This is easy to simulate (see script below). The whole-model p-value is exceptionally small (0.00001002) but the relative predictive ability, measured by the \\(R^2\\), is near zero (0.002). set.seed(1) rho &lt;- 0.5 n &lt;- 10^4 Sigma &lt;- diag(2) Sigma[1,2] &lt;- Sigma[2,1] &lt;- rho X &lt;- rmvnorm(n, mean=c(0,0), sigma=Sigma) colnames(X) &lt;- c(&quot;X1&quot;, &quot;X2&quot;) beta &lt;- c(0.05, -0.05) y &lt;- X%*%beta + rnorm(n) fit &lt;- lm(y ~ X) summary(fit) ## ## Call: ## lm(formula = y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6449 -0.6857 0.0148 0.6756 3.6510 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.007473 0.010079 0.741 0.458466 ## XX1 0.044305 0.011547 3.837 0.000125 *** ## XX2 -0.051772 0.011709 -4.422 9.9e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.008 on 9997 degrees of freedom ## Multiple R-squared: 0.0023, Adjusted R-squared: 0.002101 ## F-statistic: 11.52 on 2 and 9997 DF, p-value: 1.002e-05 6.8 What the p-value does not mean p is not the probability of the null being true. More formally, this probability is \\(Prob(null | data)\\) but our p-value is \\(P(data | null)\\). These are not the same. \\(P(null | data)\\) is the probability of the null being true given the data. \\(P(data | null)\\) is the probability of our data, or something more extreme than our data, conditional on a true null. \\(1-p\\) is not the probability of the alternative p is not a measure of effect size. p in one experiment is not the same level of evidence against the null as in another experiment p is not a great indicator of which is more likely, H0 or H1. If one treatment level has \\(p &lt; 0.05\\) and another treatment level has \\(p &gt; 0.05\\), this is not evidence that the treatment levels have different effects on the outcome. 6.9 On using p-values in experimental biology The flow of experiments in the article GDF15 mediates the effects of metformin on body weight and energy balance is typical of that in many experimental biology studies. (Figure 1) A cited observational studying showing an association between metformin and increased blood levels of the peptide hormone GDF15 in humans led to experiments to confirm this association in humans and mice. The positive results inferred from \\(p &lt; 0.05\\) led to follow-up experiments in Figures 2, 3, and 4. (Figure 2) Experiments with GDF15 knockout and GFRAL (the GDF15 receptor) knockout to probe if GDF15 is necessary for the metformin-associated weight loss, food intake reduction, and energy expenditure increase. The positive results inferred from \\(p &lt; 0.05\\) led to the conclusion that GDF15 signaling is a mediator of metformin-induced weight loss via GDF15 signaling of both food intake and energy expenditure. (Figure 3) Experiments with GDF15 knockout and GFRAL (the GDF15 receptor) knockout to probe if GDF15 is necessary for metformin-associated regulation of glucose homeostasis. The negative results inferred from \\(p &gt; 0.05\\) led to the conclusion that GDF15 signaling is a not a mediator of metformin-induced decreases in fasting glucose and insulin. (Figure 4) Experiments to probe which tissues respond to metformin by upregulating GDF15 expression. The positive results inferred from \\(p &lt; 0.05\\) led to follup up, confirmatory experiments using alternative methods and tissue sources. The positive results inferred from \\(p &lt; 0.05\\) in these original and follow up experiments led to the conclusion that metformin upregulates GDF15 expression in the small intestine, colon, rectum, and kidney. The researchers are using the p-values as a tool not just to draw conclusions about effects and lack of effects but, importantly to identify follow-up experiments. And positive results (\\(p &lt; 0.05\\)) in this publication will motivate this research group and others to execute follow-up experiments in future studies. Here, and in effectively all other experimental biology papers, p-values are not used in a Neyman-Pearson framework despite the ubiquitous statement “we consider \\(p &lt; 0.05\\) to be significant” in the methods. Instead, smaller p-values seem to give the researchers greater confidence in the conclusion. And, the decision strategy seems to be, if the p-value is at least close to 0.05 and we expect a treatment effect given available information, any p value close to but not less than 0.05 is good enough to act as if there is an effect. abandon p-values and emphasize effect sizes and uncertainty. Problem is biological consequences of effect size is unknown and some assays use units in which effect sizes are meaningless. abandon p-values in favor or model selection or model averaging abandon frequentist statistics in favor of Bayesian methods. Not going to happen. Decisions still have to be made. abandon decisions based on p-values for decisions based on Bayes factors abandon “signficance”. Worry is that researchers will then claim effects when p = 0.1 or whatev. Who cares? cost probably more to optimistic p-values due to pseudoreplication or not controlling for FDR or not sufficient replication, so continue as is but do better statistics. 6.10 Better reproducibility true randomization and selection, blind scoring knowledge of statistics helps with experimental design use best practice statistics especially accounting for pseudoreplication and blocking combine experiments properly experiments used to make decisions to move forward should 1) replicate experiments and 2) use conservative p-values (ideally results shouldn’t need a formal test) xxx finish section 6.11 Multiple testing and controlling for false positives Bench biologists compute a bunch of p-values in every paper. For example, in a 2 x 2 factorial experiment (e.g. WT-Chow, WT-HFD, KO-Chow, KO-HFD), there are six differences between means (pairwise comparisons) plus an interaction effect. Researchers typically compute p-values for multiple measured outcomes per experiment. If there are 5 outcomes from a 2 x 2 factorial experiment, researchers might report 35 p-values. The number of potential p-values, can rise very quickly above this if one or more of the variables are measured at multiple time points, such as weekly body weight measurements over 12 weeks, or multiple glucose measurements over the 120 minute duration of a glucose tolerance test. And this is just Figure 1! If these p-values are used to claim an effect (a discovery) or used to pursue a line of follow-up experiments, there will be multiple false positives. Here, I’ll define a false positive as either False positives lead to non-reproducible research. Researchers want to limit false positives because it is costly in both time and money to pursue lines of research based on a mistaken model of how something works. Exciting, positive results from one study spawn follow-up experiments not just by the original research team but also by other research groups. So, researchers want other researchers to limit false positives. And of course, funding agencies want researchers to limit false positives. 6.11.1 Controlling the family-wise error rate when all tests are testing the same hypothesis 6.12 Recommendations Simply report the exact p-value, along with a CI of the estimate. P-values are noisy, there is little reason to report more than two significant digits (report “\\(p = 0.011\\)” not “\\(p = 0.0108\\)”) although some journals recommend more than two significant digits. For high p-values, report “\\(p = 0.23\\)” not “\\(p = n.s.\\)”. For small p-values, there is little reason to report more than one significant digit (report “\\(p = 0.0002\\)” not “\\(p = 0.00018\\)”). For really small p-values, there is little reason to report the exact p-value (report “\\(p &lt; 0.0001\\)” and not “\\(p = 2.365E-11\\)”). Recognize that “really small” is entirely arbitrary. Rafael Irizarry suggested that p-values less than something like the probability of being killed by lightning strike should be reported as “\\(p &lt; m\\)”, where \\(m\\) is the probability of being killed by lightning strike4. According Google University, this is 0.00000142 in one year or 0.00033 in one lifetime. This text will use \\(p &lt; ls\\)” for p-values less than 0.0001 – the lifetime probability of being killed by lightning strike in someone that spends too much time indoors analyzing data. If \\(p &lt; 0.05\\) (or some other \\(\\alpha\\)) do not report this as “significant” – in fact, avoid the word “significant”. In the english language, “significant” implies big or important. Small p-values can result even with trivially small effects if \\(n\\) is big or sample variation is small. The phrase “ASK1 knockout had a significant effect on reducing liver TG (\\(p = 0.011\\))” is potentially misleading, if we interpret “significant” to mean “having a large effect on the regulation of liver TG”, wrong, if we interpret “significant” to mean “there is an ASK1 knockout effect”. A low p-value is evidence that the effect of ASK1 knockout is not zero, but I would wager that knocking out any gene expressed in white adipose cells will have some effect (however small) on liver TG. If a decision needs to be made (“do we devote time, expense, and personel to pursue this further?”), then a p-value is a useful tool. If p is smaller than say 0.001, this is pretty good evidence that the data is not a fluke of sampling, as long as we are justifiably confident in all the assumptions that went into computing this p-value. A replicate experiment with a small p-value is better evidence. If p is closer to 0.01 or 0.05, this is only weak evidence of a fluke because of the sampling variability of p. A replicate experiment with a small p-value is much better evidence. 6.12.1 Primary sources for recommendations Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. “Q: Why do so many colleges and grad schools teach p = 0.05? A: Because that’s still what the scientific community and journal editors use. Q: Why do so many people still use p = 0.05? A: Because that’s what they were taught in college or grad school.” – ASA Statement on Statistical Significance and P-Values “We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences.” – Abandon Statistical Significance “We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “\\(p&lt;0.05\\),” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.” – Moving to a World Beyond “p &lt; 0.05” “We agree, and call for the entire concept of statistical significance to be abandoned.”–Scientists rise up against statistical significance 6.13 Problems Problem 1 – simulate the distribution of p under the null. There are many ways to do this but a straightforard approach is to Create a \\(2n \\times m\\) matrix of random normal deviates with mean 0 and sd 1 Do a t-test on each column, with the first \\(n\\) values assigned to one group and the remaining \\(n\\) values assigned to the second group. Save the p-value from each. Plot a histogram of the p-values. What is the distribution? What is the most likely value of p? Problem 2 – simulate power. Again, many ways to do this but following up on Problem 1. 1. Create a \\(2n \\times m\\) matrix of random normal deviates with mean 0 and sd 1 2. Add an effect to the first \\(n\\) values of each column. Things to think about a. what is a good effect size to add? The effect/sd ratio, known as Cohen’s d, is a relative (or standardized) measure of effect size. Cohen suggest 0.2, 0.5, and 0.8 as small, medium, and large standardized effects. b. should the same effect be added to each individual? Yes! It is the random component that captures the individual variation in the response. 3. Do a t-test on each column of the matrix, using the first \\(n\\) values in group 1 and the remaining \\(n\\) values in group 2. Save the p-values for each. 4. Compute the power, the relative frequency \\(p \\le 0.05\\). 5. Repeat with different values of \\(n\\), effect size, and sd, but only vary one at a time. How does power vary with these three parameters? Amrhein, V., Trafimow, D. and Greenland, S., 2019. Inferential statistics as descriptive statistics: There is no replication crisis if we don’t expect replication. The American Statistician, 73(sup1), pp.262-270.↩︎ https://twitter.com/rafalab/status/1310610623898808320↩︎ "],["errors-in-inference.html", "Chapter 7 Errors in inference 7.1 Classical NHST concepts of wrong 7.2 A non-Neyman-Pearson concept of power", " Chapter 7 Errors in inference 7.1 Classical NHST concepts of wrong As described in chapter (p-values), two types of error occur in classical Neyman-Pearson hypothesis testing, and in the NHST version that dominates modern practice. Type I error occurs when the null hypothesis is true but the p-value of the test is less than \\(\\alpha\\). This is a false positive, where a positive is a test that rejects the null. Type II error occurs when the null hypothesis is false but the p-value of the test is greater than \\(\\alpha\\). This is a false negative, where a negative is a test that accepts (or fails to reject) the null. Power is not an error but the frequency of true, positive tests (or the frequency of avoiding Type II error). \\(\\alpha\\) is not an error but the rate of Type I error that a researcher is willing to accept. Ideally, a researcher sets \\(\\alpha\\) based on an evaluation of the pros and cons of Type I and Type II error for the specific experiment. In practice, researchers follow the completely arbitary practice of setting \\(\\alpha = 0.05\\). Why should a researcher care about \\(\\alpha\\) and power? Typically, most researchers don’t give \\(\\alpha\\) much thought. And power is considered only in the context of calculating a sample size for an experiment for a grant proposal. But researchers should care about rates of Type I error and power because these (and similar concepts) can help guide decisions about which model to fit to a specific dataset. 7.1.1 Type I error In classical Neyman-Pearson hypothesis testing, an important property of a hypothesis test is the size of a test, which may include an entire procedure that culminates in a hypothesis test. “Size” is a weird name for the probability of rejecting the null when the null is true. Size is not \\(\\alpha\\). \\(\\alpha\\) is the nominal value – size is the actual value under a specific parameterization of the model. It would probably come as a surprise to most researchers to learn that the size of some common tests used with data that look like the researcher’s data is not 0.05. “used with data that look like the researcher’s data” is important here – a t-test doesn’t have one size. With data that conform to the assumptions (independence, homogeneity, normality), the size of a t-test is \\(\\alpha\\). But with any violation, especially when the sample size differs between groups, the size of the t-test can move away from \\(\\alpha\\). A test that has a size that is less than \\(\\alpha\\) is “conservative” (fewer nulls are rejected than we think, so the status quo is more often maintained). A test that has a size that is greater than \\(\\alpha\\) is “anti-conservative”, or “liberal” (more nulls are rejected than we think, so the status quo is less often maintained). More conservative tests reduce power. More liberal tests artificially increase power and increase our rate of false rejection, which can mean “false discovery” if p-values are used as the arbiter of discovery. 7.1.1.1 Size example 1: the size of a t-test vs. a permutation test, when the data meet the assumptions set.seed(1) n &lt;- 10 n_iter &lt;- 10000 p_t &lt;- numeric(n_iter) p_perm &lt;- numeric(n_iter) treatment &lt;- rep(c(&quot;cn&quot;, &quot;tr&quot;), each = n) for(iter in 1:n_iter){ sample_1 &lt;- rnorm(n, mean = 10, sd = 1) sample_2 &lt;- rnorm(n, mean = 10, sd = 1) y &lt;- c(sample_1, sample_2) m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace p_t[iter] &lt;- coef(summary(m1))[&quot;treatmenttr&quot;, &quot;Pr(&gt;|t|)&quot;] m2 &lt;- lmp(y ~ treatment, perm = &quot;Prob&quot;, settings = FALSE) p_perm[iter] &lt;- coef(summary(m2))[&quot;treatment1&quot;, &quot;Pr(Prob)&quot;] } size_t &lt;- sum(p_t &lt; 0.05)/n_iter size_perm &lt;- sum(p_perm &lt; 0.05)/n_iter size_table &lt;- data.table(Method = c(&quot;lm&quot;, &quot;perm&quot;), Size = c(size_t, size_perm)) knitr::kable(size_table, digits = 4) Method Size lm 0.0489 perm 0.0488 7.1.1.2 Size example 2: the size of a t-test vs. a permutation test, when the data have a right skewed distribution set.seed(1) n &lt;- 10 n_iter &lt;- 10000 p_t &lt;- numeric(n_iter) p_perm &lt;- numeric(n_iter) treatment &lt;- rep(c(&quot;cn&quot;, &quot;tr&quot;), each = n) for(iter in 1:n_iter){ # qplot(rnegbin(n = 10^4, mu = 100, theta = 1)) sample_1 &lt;- rnegbin(n, mu = 100, theta = 1) sample_2 &lt;- rnegbin(n, mu = 100, theta = 1) y &lt;- c(sample_1, sample_2) # qplot(x=treatment, y = y) m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace p_t[iter] &lt;- coef(summary(m1))[&quot;treatmenttr&quot;, &quot;Pr(&gt;|t|)&quot;] m2 &lt;- lmp(y ~ treatment, perm = &quot;Prob&quot;, settings = FALSE) p_perm[iter] &lt;- coef(summary(m2))[&quot;treatment1&quot;, &quot;Pr(Prob)&quot;] } size_t &lt;- sum(p_t &lt; 0.05)/n_iter size_perm &lt;- sum(p_perm &lt; 0.05)/n_iter size_table &lt;- data.table(Method = c(&quot;lm&quot;, &quot;perm&quot;), Size = c(size_t, size_perm)) knitr::kable(size_table, digits = 4) Method Size lm 0.0438 perm 0.0504 7.1.1.3 Size example 3: the size of a t-test vs. a permutation test, when the data have heterogenous variance and the sample size is unequal set.seed(1) n1 &lt;- 10 n2 &lt;- n1/2 n_iter &lt;- 10000 p_t &lt;- numeric(n_iter) p_perm &lt;- numeric(n_iter) treatment &lt;- rep(c(&quot;cn&quot;, &quot;tr&quot;), times = c(n1, n2)) for(iter in 1:n_iter){ # qplot(rnegbin(n = 10^4, mu = 100, theta = 1)) sample_1 &lt;- rnorm(n1, mean = 10, sd = 0.5) sample_2 &lt;- rnorm(n2, mean = 10, sd = 1) y &lt;- c(sample_1, sample_2) # qplot(x=treatment, y = y) m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace p_t[iter] &lt;- coef(summary(m1))[&quot;treatmenttr&quot;, &quot;Pr(&gt;|t|)&quot;] m2 &lt;- lmp(y ~ treatment, perm = &quot;Prob&quot;, settings = FALSE) p_perm[iter] &lt;- coef(summary(m2))[&quot;treatment1&quot;, &quot;Pr(Prob)&quot;] } size_t &lt;- sum(p_t &lt; 0.05)/n_iter size_perm &lt;- sum(p_perm &lt; 0.05)/n_iter size_table &lt;- data.table(Method = c(&quot;lm&quot;, &quot;perm&quot;), Size = c(size_t, size_perm)) knitr::kable(size_table, digits = 4) Method Size lm 0.1150 perm 0.1211 7.1.2 Power In classical Neyman-Pearson hypothesis testing, an important property of a hypothesis test is the power of a test. “Power” is the probability of rejecting the null when the null is false. A common way to think about power is, power is a test’s ability to “detect” an effect if it exists. This makes sense using Neyman-Pearson but not Fisher (Using Fisher, a p-value is not a detector of an effect – a reasoning brain is). Using Fisher, we could say that power is the sensitivity of a test (it takes less sample to provide the same signal). 7.1.2.1 Power example 1: the power of a t-test vs. a permutation test, when the data meet the assumptions set.seed(1) n &lt;- 10 n_iter &lt;- 10000 p_t &lt;- numeric(n_iter) p_perm &lt;- numeric(n_iter) treatment &lt;- rep(c(&quot;cn&quot;, &quot;tr&quot;), each = n) for(iter in 1:n_iter){ sample_1 &lt;- rnorm(n, mean = 10, sd = 1) sample_2 &lt;- rnorm(n, mean = 11, sd = 1) y &lt;- c(sample_1, sample_2) m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace p_t[iter] &lt;- coef(summary(m1))[&quot;treatmenttr&quot;, &quot;Pr(&gt;|t|)&quot;] m2 &lt;- lmp(y ~ treatment, perm = &quot;Prob&quot;, settings = FALSE) p_perm[iter] &lt;- coef(summary(m2))[&quot;treatment1&quot;, &quot;Pr(Prob)&quot;] } power_t &lt;- sum(p_t &lt; 0.05)/n_iter power_perm &lt;- sum(p_perm &lt; 0.05)/n_iter power_table_normal &lt;- data.table(Method = c(&quot;lm&quot;, &quot;perm&quot;), Power = c(power_t, power_perm)) knitr::kable(power_table_normal, digits = 3) Method Power lm 0.554 perm 0.554 7.1.2.2 Power example 2: the power of a t-test vs. a permutation test, when the data look like typical count data set.seed(1) n &lt;- 10 n_iter &lt;- 10000 p_t &lt;- numeric(n_iter) p_perm &lt;- numeric(n_iter) treatment &lt;- rep(c(&quot;cn&quot;, &quot;tr&quot;), each = n) for(iter in 1:n_iter){ # qplot(rnegbin(n = 10^4, mu = 100, theta = 1)) sample_1 &lt;- rnegbin(n, mu = 100, theta = 1) sample_2 &lt;- rnegbin(n, mu = 300, theta = 1) y &lt;- c(sample_1, sample_2) # qplot(x=treatment, y = y) m1 &lt;- lm(y ~ treatment) # no data statement necessary because both variables in workspace p_t[iter] &lt;- coef(summary(m1))[&quot;treatmenttr&quot;, &quot;Pr(&gt;|t|)&quot;] m2 &lt;- lmp(y ~ treatment, perm = &quot;Prob&quot;, settings = FALSE) p_perm[iter] &lt;- coef(summary(m2))[&quot;treatment1&quot;, &quot;Pr(Prob)&quot;] } power_t &lt;- sum(p_t &lt; 0.05)/n_iter power_perm &lt;- sum(p_perm &lt; 0.05)/n_iter power_table_count &lt;- data.table(Method = c(&quot;lm&quot;, &quot;perm&quot;), Power = c(power_t, power_perm)) knitr::kable(power_table_count, digits = 3) Method Power lm 0.516 perm 0.586 7.2 A non-Neyman-Pearson concept of power Size and power are concepts specific to the Neyman-Pearson hypothesis testing framework. Size and power also have limited (or no) use in a research program in which the null hypothesis is never (or rarely) strictly true. That said, the concept of size and power are useful. For example, what if we framed power as the distribution of p-values instead of the frequency of p-values less than \\(\\alpha\\). Table ?? shows the p-value at the 10th, 25th, 50th, 75th, and 90th percentile of the set of p-values computed in Power Example 2 above (count data). The nth percentile is the value in an ordered set of numbers in which n % are less than the value and 100 - n% are greater than the value. The 50th percentile is the median. The table shows that at all percentiles except the 90th, the permutation p-value is smaller than the t-test p-value. And, importantly, the value at 75% for both is ~ 0.12. This means that for experiments that generate data something like the fake data generated in Power Example 2, the permutation test is more sensistive to the incompatibility between the null model and the data than the t-test, except in the random samples when both methods fail. quantile_list &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9) percentiles_t &lt;- quantile(p_t, quantile_list) percentiles_perm &lt;- quantile(p_perm, quantile_list) alt_power_table &lt;- data.table(method = c(&quot;t-test&quot;, &quot;permutation&quot;), (rbind(percentiles_t, percentiles_perm))) knitr::kable(alt_power_table, digits = c(1, 4, 3, 3, 2, 2)) method 10% 25% 50% 75% 90% t-test 0.0047 0.016 0.047 0.12 0.27 permutation 0.0012 0.007 0.031 0.12 0.32 7.2.1 Estimation error 7.2.2 Coverage This text advocates reporting a confidence interval with each reported effect size. An important property of an estimator is coverage probability, often shortened to “coverage”. 7.2.3 Type S error Instead of framing the “size” concept as the rate of Type I error, what if we framed this as the rate that an estimate is in the correct direction (meaning, the sign of an effect is the same as the true value). And, 7.2.4 Type M error "],["part-iv-introduction-to-linear-models.html", "Part IV: Introduction to Linear Models", " Part IV: Introduction to Linear Models "],["intro-linear-models.html", "Chapter 8 An introduction to linear models 8.1 Two specifications of a linear model 8.2 A linear model can be fit to data with continuous, discrete, or categorical \\(X\\) variables 8.3 Statistical models are used for prediction, explanation, and description 8.4 What is the interpretation of a regression coefficient? 8.5 What do we call the \\(X\\) and \\(Y\\) variables? 8.6 Modeling strategy 8.7 Predictions from the model 8.8 Inference from the model 8.9 “linear model,”regression model”, or “statistical model”?", " Chapter 8 An introduction to linear models All students are familiar with the idea of a linear model from learning the equation of a line, which is \\[\\begin{equation} Y = mX + b \\tag{8.1} \\end{equation}\\] where \\(m\\) is the slope of the line and \\(b\\) is the \\(Y\\)-intercept. It is useful to think of equation (8.1) as a function that maps values of \\(X\\) to values of \\(Y\\). Using this function, if we input some value of \\(X\\), we always get the same value of Y as the output. A linear model is a function, like that in equation (8.1), that is fit to a set of data, often to model a process that generated the data or something like the data. The line in Figure 8.1A is just that, a line, but the line in Figure 8.1B is a linear model fit to the data in Figure 8.1B. Figure 8.1: A line vs. a linear model. (A) the line \\(y=-3.48X + 105.7\\) is drawn. (B) A linear model fit to the data. The model coefficients are numerically equal to the slope and intercept of the line in A. 8.1 Two specifications of a linear model 8.1.1 The “error draw” specification In introductory textbooks, a linear model is typically specified using an error-draw scheme. \\[\\begin{align} Y &amp;= \\beta_0 + \\beta_1 X + \\varepsilon\\\\ \\varepsilon &amp;\\sim N(0, \\sigma^2) \\tag{8.2} \\end{align}\\] The first line of this specification has two components: the linear predictor \\(Y = \\beta_0 + \\beta_1 X\\) and the error \\(\\varepsilon\\). The linear predictor component looks like the equation for a line except that 1) \\(\\beta_0\\) is used for the intercept and \\(\\beta_1\\) for the slope and 2) the intercept term precedes the slope term. This re-labeling and re-arrangement make the notation for a linear model more flexible for more complicated linear models. For example \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) is a model where \\(Y\\) is a function of two \\(X\\) variables. The linear predictor is the deterministic or systematic part of the specification. As with the equation for a line, the linear predictor component of a linear model is a function that maps a specific value of \\(X\\) to a unique value of \\(Y\\). This mapped value is the expected value, or expectation, given a specific input value of \\(X\\). The expectation is often written as \\(\\mathrm{E}[Y|X]\\), which is read as “the expected value of \\(Y\\) given \\(X\\)”, where “given X” means a specific value of X. This text will often use the word conditional in place of “given”. For example, I would read \\(\\mathrm{E}[Y|X]\\) as “the expected value of \\(Y\\) conditional on \\(X\\)”. It is important to recognize that \\(\\mathrm{E}[Y|X]\\) is a conditional mean – it is the mean value of \\(Y\\) when we observe that \\(X\\) has some specific value \\(x\\) (that is \\(X = x\\)). The second line of the specification (8.2) is read as “epsilon is distributed as Normal with mean zero and variance sigma squared”. This line explicitly specifies the distribution of the error component of line 1. The error component of a linear model is a random “draw” from a normal distribution with mean zero and variance \\(\\sigma^2\\). The second line shows that the error component of the first line is stochastic. Using the error-model specification, we can think of any measurement of \\(Y\\) as an expected value plus some random value sampled from a normal distribution with a specified variance. Because the stochastic part of this specification draws an “error” from a population, I refer to this as the error-draw specification of the linear model. 8.1.2 The “conditional draw” specification A second way of specifying a linear model is using a conditional-draw scheme. \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2)\\\\ \\mathrm{E}(Y|X) &amp;= \\mu\\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_i \\tag{8.3} \\end{align}\\] The first line states that the response variable \\(Y\\) is a random variable independently drawn from a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). This first line is the stochastic part of the statistical model. The second line simply states that \\(\\mu\\) (the greek letter “mu”) from the first line is the conditional mean (or expectation). The third line is the liner predictor, which states how \\(\\mu_i\\) is generated given that \\(X=x_i\\). Again, the linear predictor is the systematic (or deterministic) part of the statistical model. It is systematic because the same value of \\(x_i\\) will always generate the same \\(\\mu_i\\). With the conditional-draw specification, we can think of a measurement (\\(y_i\\)) as a random draw from the specified distribution. Because it is \\(Y\\) and not some “error” that is drawn from the specified distribution, I refer to this as the conditional-draw specification of the linear model. 8.1.3 Comparing the error-draw and conditional-draw ways of specifying the linear model These two ways of specifying the model encourage slightly different ways of thinking about how the data (the response varible \\(Y\\)) were generated. The error-draw specification “generates” data by 1) constructing what \\(y_i\\) “should be” given \\(x_i\\) (this is the conditional expection), then 2) adding some error \\(e_i\\) drawn from a normal distribution with mean zero and some specified variance. The conditional-draw specification “generates” data by 1) constructing what \\(y_i\\) “should be” given \\(x_i\\), then 2) drawing a random variable from some specified distribution whose mean is this expectation. This random draw is not “error” but the measured value \\(y_i\\). For the error draw generation, we need only one hat of random numbers, but for the conditional draw generation, we need a hat for each value of \\(x_i\\). Here is a short script that generates data by implementing both the error-draw and conditional-draw specifications. See if you can follow the logic of the code and match it to the meaning of these two ways of specifying a linear model. n &lt;- 5 b_0 &lt;- 10.0 b_1 &lt;- 1.2 sigma &lt;- 0.4 x &lt;- 1:n y_expected &lt;- b_0 + b_1*x # error-draw. Note that the n draws are all from the same distribution set.seed(1) y_error_draw &lt;- y_expected + rnorm(n, mean = 0, sd = sigma) # conditional-draw. Note that the n draws are each from a different # distribution because each has a different mean. set.seed(1) y_conditional_draw &lt;- rnorm(n, mean = y_expected, sd = sigma) data.table(X = x, &quot;Y (error draw)&quot; = y_error_draw, &quot;Y (conditional draw)&quot; = y_conditional_draw) ## X Y (error draw) Y (conditional draw) ## 1: 1 10.94942 10.94942 ## 2: 2 12.47346 12.47346 ## 3: 3 13.26575 13.26575 ## 4: 4 15.43811 15.43811 ## 5: 5 16.13180 16.13180 What’s that code? rnorm() is a pseudorandom number generator that simulates random draws from a normal distribution with the specified mean and variance. The algorithm to generate the numbers is entirely deterministic – the numbers are not truly random but are “pseudorandom”. The list of numbers returned closely approximates a set of true, random numbers. The sequence of numbers returned is determined by the “seed”, which can be set with the set.seed() function (R will use an internal seed if not set by the user). The error-draw specification is not very useful for thinking about data generation for data analyzed by generalized linear models, which are models that allow one to specify distribution families other than Normal (such as the binomial, Poisson, and Gamma families). In fact, thinking about a model as a predictor plus error can lead to the misconception that, in a generalized linear model, the error (or residuals from the fit) has a distribution from the non-Normal distribution modeled. This cannot be true because the distributions modeled using generalized linear models (other than the Normal) do not have negative values (some residuals must have negative values since the mean of the residuals is zero). Introductory biostatistics textbooks typically only introduce the error-draw specification because introductory textbooks recommend data transformation or non-parametric tests if the data are not approximately normal. This is unfortunate because generalized linear models are extremely useful for real biological data. Although a linear model (or statistical model more generally) is a model of a data-generating process, linear models are not typically used to actually generate any data. Instead, when we use a linear model to understand something about a real dataset, we think of our data as one realization of a process that generates data like ours. A linear model is a model of that process. That said, it is incredibly useful to use linear models to create fake datasets for at least two reasons: to probe our understanding of statistical modeling generally and, more specifically, to check that a model actually creates data like that in the real dataset that we are analyzing. 8.1.4 ANOVA notation of a linear model Many textbooks treat ANOVA differently from regression and express a linear model as an ANOVA model (and generally do not use the phrase “linear model”). ANOVA models are all variations of \\[\\begin{equation} y_{ij} = \\mu + \\tau_{i} + \\varepsilon_{ij} \\tag{8.4} \\end{equation}\\] Unlike the error and conditional draw specifications above, the ANOVA model doesn’t have a linear predictor in the form of a regression equation (or the equation for a line) – that is, there are neither \\(X\\) variables nor coefficients (\\(\\beta\\)). Instead, the ANOVA model is made up of a linear combination of means and deviations from means. In (8.4), \\(\\mu\\) is the grand mean (the mean of the means of the groups), \\(\\tau_i\\) is the deviation of the mean of group \\(i\\) from the grand mean (these are the effects), and \\(\\varepsilon_{ij}\\) is the deviation (or error) of individual \\(j\\) from the mean of group \\(i\\). Traditional ANOVA computes effects and the statistics for inference by computing means and deviations from means. Modern linear models compute effects and the statistics for inference by solving for the coefficients of a regression model. 8.2 A linear model can be fit to data with continuous, discrete, or categorical \\(X\\) variables In the linear model fit to the data in Figure 8.1B, the \\(X\\) variable is continuous, which can take any real number between the minimum \\(X\\) and maximum \\(X\\) in the data. For biological data, most variables that are continuous are positive, real numbers (a zero is not physically possible but could be recorded in the data if the true value is less than the minimum measurable amount). One exception is a composition (the fraction of a total), which can be zero. Negative values can occur with variables in which negative represent a direction (work, electrical potential) or a rate. Discrete variables are numeric but limited to certain real numbers. Most biological variables that are discrete are counts, and can be zero, but not negative. Categorical variables are non-numeric descriptions of a measure. Many of the categorical variables in this text will be the experimentally controlled treatment variable of interest (the variable \\(treatment\\) containing the values “wild type” and “knockout”) but some are measured covariates (the variable \\(sex\\) containing the values “female” and “male”). 8.2.1 Fitting linear models to experimental data in which the \\(X\\) variable is continuous or discrete A linear model fit to data with a numeric (continous or discrete) \\(X\\) is classical regression and the result is typically communicated by a regression line. The experiment introduced in Chapter ?? [Linear models with a single, continuous X] is a good example. In this experiment, the researchers designed an experiment to measure the effect of warming on the timing of photosynthetic activity. Temperature was experimentally controlled at one of five settings (0, 2.25, 4.5, 6.75, or 9 °C above ambient temperature) within twelve, large enclosures. The response variable in the illustrated example is Autumn “green-down”, which is the day of year (DOY) of the transition to loss of photosynthesis. The intercept and slope parameters of the regression line (Figure 8.2) are the coefficients of the linear model. The slope (4.98 days per 1 °C added warmth) estimates the effect of warming on green-down DOY. What is not often appreciated at the introductory biostatistics level is that the slope is a difference in conditional means. Any point on a regression line is the expected value of \\(Y\\) at a specified value of \\(X\\), that is, the conditional mean \\(\\mathrm{E}(Y|X)\\). The slope is the difference in expected values for a pair of points that differ in \\(X\\) by one unit. \\[\\begin{equation} b_1 = \\mathrm{E}(Y|X=x+1) - \\mathrm{E}(Y|X=x+1) \\end{equation}\\] I show this in Figure 8.2 using the points on the regression line at \\(x = 5\\) and \\(x = 6\\). Thinking about a regression coefficient as a difference in conditional means is especially useful for understanding the coefficients of a categorical \\(X\\) variable, as described below. Figure 8.2: Illustration of the slope in a linear model with a numeric X. The slope (the coefficient of X) is the difference in expected value for any two X that are one unit apart. This is illustrated for the points on the line at x = 5 and x = 6. 8.2.2 Fitting linear models to experimental data in which the \\(X\\) variable is categorical Linear models can be fit to experimental data in which the \\(X\\) variable is categorical – this is the focus of this text! For the model fit to the data in Figure 8.1B, the coefficient of \\(X\\) is the slope of the line. Perhaps surprisingly, 1) we can fit a model like equation (8.2) to data in which the \\(X\\) variable is categorical and 2) the coefficient of \\(X\\) is a slope. How is this possible? The slope of a line is \\(\\frac{y_2 - y_1}{x_2 - x_1}\\) where \\((x_1, y_1)\\) and \\((x_2, y_2)\\) are the graph coordinates of any two points on the line. What is the denominator of the slope function \\((x_2 - x_1)\\) when \\(X\\) is categorical? The solution to using a linear model with categorical \\(X\\) is to recode the factor levels into numbers. An example of this was outlined in Chapter ?? (Analyzing experimental data with a linear model). The value of \\(X\\) for individual mouse i is a number that indicates the treatment assignment – a value of 0 is given to mice with a functional ASK1 gene and a value of 1 is given to mice with a knocked out gene. The regression line goes through the two group means (Figure 8.3). With the (0, 1) coding, \\(\\overline{x}_{ASK1Δadipo} - \\overline{x}_{ASK1F/F} = 1\\), so the denominator of the slope is equal to one and the slope is simply equal to the numerator \\(\\overline{y}_{ASK1Δadipo} - \\overline{y}_{ASK1F/F}\\). The coefficient (which is a slope!) is the difference in conditional means. Figure 8.3: Illustration of the slope in a linear model with categorical X. The slope (the coefficient of X) is the difference in conditional means. 8.3 Statistical models are used for prediction, explanation, and description Researchers typically use statistical models to understand relationships between one or more \\(Y\\) variables and one or more \\(X\\) variables. These relationships include Descriptive modeling. Sometimes a researcher merely wants to describe the relationship between \\(Y\\) and a set of \\(X\\) variables, perhaps to discover patterns. For example, the arrival of a spring migrant bird (\\(Y\\)) as a function of sex (\\(X_1\\)) and age (\\(X_2\\)) might show that males and younger individuals arrive earlier. Importantly, if another \\(X\\) variable is added to the model (or one dropped), the coefficients, and therefore, the precise description, will change. That is, the interpretation of a coefficient as a descriptor is conditional on the other covariates (\\(X\\) variables) in the model. In a descriptive model, there is no implication of causal effects and the goal is not prediction. Nevertheless, it is very hard for humans to discuss a descriptive model without using causal language, which probably means that it is hard for us to think of these models as mere description. Like natural history, descriptive models are useful as patterns in want of an explanation, using more explicit causal models including experiments. Predictive modeling. Predictive modeling is very common in applied research. For example, fisheries researchers might model the relationship between population density and habitat variables to predict which subset of ponds in a region are most suitable for brook trout (Salvelinus fontinalis) reintroduction. The goal is to build a model with minimal prediction error, which is the error between predicted and actual values for a future sample. In predictive modeling, the \\(X\\) (“predictor”) variables are largely instrumental – how these are related to \\(Y\\) is not a goal of the modeling, although sometimes an investigator may be interested in the relative importance among the \\(X\\) for predicting \\(Y\\) (for example, collecting the data may be time consuming, or expensive, or enviromentally destructive, so know which subset of \\(X\\) are most important for predicting \\(Y\\) is a useful strategy). Explanatory (causal) modeling. Very often, researchers are explicitly interested in how the \\(X\\) variables are causally related to \\(Y\\). The fisheries researchers that want to reintroduce trout may want to develop and manage a set of ponds to maintain healthy trout populations. This active management requires intervention to change habitat traits in a direction, and with a magnitude, to cause the desired response. This model is predictive – a specific change in \\(X\\) predicts a specific response in \\(Y\\) – because the coefficients of the model provide knowledge on how the system functions – how changes in the inputs cause change in the output. Causal interpretation of model coefficients requires a set of strong assumptions about the \\(X\\) variables in the model. These assumptions are typically met in experimental designs but not observational designs. With observational designs, biologists are often not very explicit about which of these is the goal of the modeling and use a combination of descriptive, predictive, and causal language to describe and discuss results. Many papers read as if the researchers intend explanatory inference but because of norms within the biology community, mask this intention with “predictive” language. Here, I advocate embracing explicit, explanatory modeling by being very transparent about the model’s goal and assumptions. 8.4 What is the interpretation of a regression coefficient? A regression coefficient is the difference in \\(Y\\) that we expect to see if we see a one unit difference in X, but we see no difference in any other covariate (the other X). 8.5 What do we call the \\(X\\) and \\(Y\\) variables? The inputs to a linear model (the \\(X\\) variables) have many names. In this text, the \\(X\\) variables are typically treatment variables – this term makes sense only for categorical variables and is often used for variables that are a factor containing the treatment assignment (for example “control” and “knockout”) factor variables (or simply, factors) – again, this term makes sense only for categorical variables covariates – this term is usually used for the non-focal \\(X\\) variables in a statistical model. A linear model is a regression model and in regression modeling, the \\(X\\) variables are typically called independent variables (often shortened to IV) – “independent” in the sense that in a statistical model at least, the \\(X\\) are not a function of \\(Y\\). predictor variables (or simply, “predictors”) – this makes the most sense in prediction models. explanatory variables – this term is usually applied in observational designs and is best used if the explicit goal is causal modeling. In this text, the output of a linear model (the \\(Y\\) variable or variables if the model is multivariate) will most often be calle either of response variable (or simply, “response”) outcome variable (or simply, “outcome”) These terms have a causal connotation in everyday english. These terms are often used in regression modeling with observational data, even if the model is not explicitly causal. On other term, common in introductory textbooks, is dependent variable – “dependent” in the sense that in a statistical model at least, the \\(Y\\) is a function of the \\(X\\). 8.6 Modeling strategy A “best practice” sequence of steps used throughout this text to analyze experimental data is examine the data using exploratory plots to examine individual points and identify outliers that are likely due to data transcription errors or measurement blunders examine outlier points that are biologically plausible, but raise ref flags about undue influe on fit models. This information is used to inform the researcher on the strategy to handle outliers in the statistical analysis, including algorithms for excluding data or implementation of robust methods. provide useful information for initial model filtering (narrowing the list of potential models that are relevant to the question and data). Statistical modeling includes a diverse array of models, yet almost all methods used by researchers in biology, and all models in this book, are generalizations of the linear model specified in (8.3). For some experiments, there may be multiple models that are relevant to the question and data. Model checking (step 3) can help decide which model to ultimately use. fit the model, in order to estimate the model parameters and the uncertainty in these estimates. check the model, which means to use a series of diagnostic plots and computations of model output to check that the fit model reasonably approximates the data. If the diagnostic plots suggest a poor approximation, then choose a different model and go back to step 2. inference from the model, which means to use the fit parameters to learn, with uncertainty, about the system, or to predict future observations, with uncertainty. plot the model, which means to plot the data, which may be adjusted, and the estimated parameters (or other results dervived from the estimates) with their uncertainty. Note that step 1 (exploratory plots) is not data mining, or exploring the data for patterns to test. 8.7 Predictions from the model For the linear model specified in Model (8.2), the fit model is \\[\\begin{equation} y_i = b_0 + b_1 x_i + e_i \\tag{8.5} \\end{equation}\\] where \\(b_0\\) and \\(b_1\\) are the coefficients of the fit model and the \\(e_i\\) are the residuals of the fit model. We can use the coefficients and residuals to recover the \\(y_i\\), although this would rarely be done. More commonly, we could use the coefficients to calculate conditional means (the mean conditional on a specified value of \\(X\\)). \\[\\begin{equation} \\hat{y}_i = b_0 + b_1 x_i \\tag{8.6} \\end{equation}\\] The conditional means are typically called fitted values, if the \\(X\\) are the \\(X\\) used to fit the model, or predicted values, if the \\(X\\) are new. “Predicted values” is often shortened to “the prediction”. 8.8 Inference from the model If our goal is inference, we want to use the fit parameters to learn, with uncertainty, about the system. Using equation (8.5), the coefficients \\(b_0\\) and \\(b_1\\) are point estimates of the true, generating parameters \\(\\beta_0\\) and \\(\\beta_1\\), the \\(e_i\\) are estimates of \\(\\varepsilon_i\\) (the true, biological “noise”), and \\(\\frac{\\sum{e_i^2}}{N-2}\\) is an estimate of the true, population variance \\(\\sigma^2\\) (this will be covered more in chapter xxx but you may recognize that \\(\\frac{\\sum{e_i^2}}{N-2}\\) is the formula for a variance). And, using equation (8.6), \\(\\hat{y}_i\\) is the point estimate of the parameter \\(\\mu_i\\) (the true mean conditional on \\(X=x_i\\)). Throughout this text, Greek letters refer to a theoretical parameter and Roman letters refer to point estimates. Our uncertainty in the estimates of the parameters due to sampling is the standard error of the estimate. It is routine to report standard errors of means and coefficients of the model. While a standard error of the estimate of \\(\\sigma\\) is available, this is effectively never reported, at least in the experimental biology literature, presumably because the variance is thought of as a nuisance parameter (noise) and not something worthy of study. This is a pity. Certainly treatments can effect the variance in addition to the mean. Parametric inference assumes that the response is drawn from some probability distribution (Normal, or Poisson, or Bernouli, etc.). Throughout this text, I emphasize reporting and interpreting point estimates and interval estimates of the point estimate. A confidence interval is a type of interval estimate. A confidence interval of a parameter is a measure of the uncertainty in the estimate. A 95% confidence interval has a 95% probability (in the sense of long-run frequency) of containing the parameter. This probability is a property of the population of intervals that could be computed using the same sampling and measuring procedure. It is not correct, without further assumptions, to state that there is a 95% probability that the parameter lies within the interval. Perhaps a more useful interpretation is that the interval is a compatability interval in that it contains the range of estimates that are compatible with the data, in the sense that a \\(t\\)-test would not reject the null hypothesis of a difference between the estimate and any value within the interval (this interpretation does not imply anything about the true value). Another kind of inference is a significance test, which is the computation of the probability of “seeing the data” or something more extreme than the data, given a specified null hypothesis. This probability is the p-value, which can be reported with the point estimate and confidence interval. There are some reasonable arguments made by very influential statisticians that p-values are not useful and lead researchers into a quagmire of misconceptions that impede good science. Nevertheless, the current methodology in most fields of Biology have developed in a way to become completely dependent on p-values. I think at this point, a p-value can be a useful, if imperfect tool in inference, and will show how to compute p-values throughout this text. Somewhat related to a significance test is a hypothesis test, or a Null-Hypothesis Signficance Test (NHST), in which the \\(p\\)-value from a significance test is compared to a pre-specified error rate called \\(\\alpha\\). Hypothesis testing was developed as a formal means of decision making but this is rarely the use of NHST in experimental biology. For almost all applications of p-values that I see in the literature that I read in ecology, evolution, physiology, and wet-bench biology, comparing a \\(p\\)-value to \\(\\alpha\\) adds no value to the communication of the results. 8.8.1 Assumptions for inference with a statistical model The data were generated by a process that is “linear in the parameters”, which means that the different components of the model are added together. This additive part of the model containing the parameters is the linear predictor in specifications (8.2) and (8.3) above. For example, a cubic polynomial model \\[\\begin{equation} \\mathrm{E}(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 \\end{equation}\\] is a linear model, even though the function is non-linear, because the different components are added. Because a linear predictor is additive, it can be compactly defined using matrix algebra \\[\\begin{equation} \\mathrm{E}(Y|X) = \\mathbf{X}\\boldsymbol{\\beta} \\end{equation}\\] where \\(mathbf{X}\\) is the model matrix and \\(\\boldsymbol{\\beta}\\) is the vector of parameters. We discuss these more in chapter xxx. A Generalized Linear Model (GLM) has the form \\(g(\\mu_i) = \\eta_i\\) where \\(\\eta\\) (the Greek letter “eta”) is the linear predictor \\[\\begin{equation} \\eta = \\mathbf{X}\\boldsymbol{\\beta} \\end{equation}\\] GLMs are extensions of linear models. There are non-linear models that are not linear in the parameters, that is, the predictor is not a simple dot product of the model matrix and a vector of parameters. For example, the Michaelis-Menten model is a non-linear model \\[\\begin{equation} \\mathrm{E}(Y|X) = \\frac{\\beta_1 X}{\\beta_2 + X} \\end{equation}\\] that is non-linear in the parameters because the parts are not added together. This text covers linear models and generalized linear models, but not non-linear models that are also non-linear in the parameters. The draws from the probability distribution are independent. Independence implies uncorrelated \\(Y\\) conditional on the \\(X\\), that is, for any two \\(Y\\) with the same value of \\(X\\), we cannot predict the value of one given the value of the other. For example, in the ASK1 data above, “uncorrelated” implies that we cannot predict the glucose level of one mouse within a specific treatment combination given the glucose level of another mouse in that combination. For linear models, this assumption is often stated as “independent errors” (the \\(\\varepsilon\\) in model (8.2)) instead of independent observations. There are lots of reasons that conditional responses might be correlated. In the mouse example, correlation within treatment group could arise if subsets of mice in a treatment group are siblings or are housed in the same cage. More generally, if there are measures both within and among experimental units (field sites or humans or rats) then we’d expect the measures within the same unit to err from the model in the same direction. Multiple measures within experimental units (a site or individual) creates “clustered” observations. Lack of independence or clustered observations can be modeled using models with random effects. These models go by many names including linear mixed models (common in Ecology), hierarchical models, multilevel models, and random effects models. A linear mixed model is a variation of model (8.2). This text introduces linear mixed models in chapter xxx. Measures that are taken from sites that are closer together or measures taken closer in time or measures from more closely related biological species will tend to have more similar values than measures taken from sites that are further apart or from times that are further apart or from species that are less closely related. Space and time and phylogeny create spatial and temporal and phylogenetic autocorrelation. Correlated error due to space or time or phylogeny can be modeled with Generalized Least Squares (GLS) models. A GLS model is a variation of model (8.2). 8.8.2 Specific assumptions for inference with a linear model Constant variance or homoskedasticity. The most common way of thinking about this is the error term \\(\\varepsilon\\) has constant variance, which is a short way of saying that random draws of \\(\\varepsilon\\) in model (8.2) are all from the same (or identical) distribution. This is explicitly stated in the second line of model specification (8.2). If we were to think about this using model specification (8.3), then homoskedasticity means that \\(\\sigma\\) in \\(N(\\mu, \\sigma)\\) is constant for all observations (or that the conditional probability distributions are identical, where conditional would mean adjusted for \\(\\mu\\)) Many biological processes generate data in which the error is a function of the mean. For example, measures of biological variables that grow, such as lengths of body parts or population size, have variances that “grow” with the mean. Or, measures of counts, such as the number of cells damaged by toxin, the number of eggs in a nest, or the number of mRNA transcripts per cell have variances that are a function of the mean. Heteroskedastic error can be modeled with Generalized Least Squares, a generalization of the linear model, and with Generalized Linear Models (GLM), which are “extensions” of the classical linear model. Normal or Gaussian probability distribution. As above, the most common way of thinking about this is the error term \\(\\varepsilon\\) is Normal. Using model specification (8.3), we’d say the conditional probablity distribution of the response is normal. A normal probability distribution implies that 1) the response is continuous and 2) the conditional probability is symmetric around \\(mu_i\\). If the conditional probability distribution has a long left or right tail it is skewed left or right. Counts (number of cells, number of eggs, number of mRNA transcripts) and binary responses (sucessful escape or sucessful infestation of host) are not continuous and often often have asymmetric probablity distributions that are skewed to the right and while sometimes both can be reasonably modeled using a linear model they are more often modeled using generalized linear models, which, again, is an extension of the linear model in equation (8.3). A classical linear model is a specific case of a GLM. A common misconception is that inference from a linear model assumes that the raw response variable is normally distributed. Both the error-draw and conditional-draw specifications of a linear model show precisely why this conception is wrong. Model (??) states explicitly that it is the error that has the normal distribution – the distribution of \\(Y\\) is a mix of the distribution of \\(X\\) and the error. Model (8.3) states that the conditional outcome has a normal distribution, that is, the distribution after adjusting for variation in \\(X\\). 8.9 “linear model,”regression model”, or “statistical model”? Statistical modeling terminology can be confusing. The \\(X\\) variables in a statistical model may be quantitative (continuous or integers) or categorical (names or qualitative amounts) or some mix of the two. Linear models with all quantitative independent variables are often called “regression models.” Linear models with all categorical independent variables are often called “ANOVA models.” Linear models with a mix of quantitative and categorical variables are often called “ANCOVA models” if the focus is on one of the categorical \\(X\\) or “regression models” if there tend to be many independent variables. This confusion partly results from the history of the development of regression for the analysis of observational data and ANOVA for the analysis of experimental data. The math underneath classical regression (without categorical variables) is the linear model. The math underneath classical ANOVA is the computation of sums of squared deviations from a group mean, or “sums of squares”. The basic output from a regression is a table of coefficients with standard errors. The basic ouput from ANOVA is an ANOVA table, containing the sums of squares along with mean-squares, F-ratios, and p-values. Because of these historical differences in usage, underlying math, and output, many textbooks in biostatistics are organized around regression “vs.” ANOVA, presenting regression as if it is “for” observational studies and ANOVA as if it is “for” experiments. It has been recognized for many decades that experiments can be analyzed using the technique of classical regression if the categorical variables are coded as numbers (again, this will be explained later) and that both regression and ANOVA are variations of a more general, linear model. Despite this, the “regression vs. ANOVA” way-of-thinking dominates the teaching of biostatistics. To avoid misconceptions that arise from thinking of statistical analysis as “regression vs. ANOVA”, I will use the term “linear model” as the general, umbrella term to cover everything in this book. By linear model, I mean any model that is linear in the parameters, including classical regression models, marginal models, linear mixed models, and generalized linear models. To avoid repetition, I’ll also use “statistical model”. "],["regression.html", "Chapter 9 Linear models with a single, continuous X (“regression”) 9.1 A linear model with a single, continuous X is classical “regression” 9.2 Working in R 9.3 Hidden code 9.4 Try it 9.5 Intuition pumps", " Chapter 9 Linear models with a single, continuous X (“regression”) 9.1 A linear model with a single, continuous X is classical “regression” 9.1.1 Analysis of “green-down” data To introduce some principles of modeling with a single continuous \\(X\\) variable, I’ll use a dataset from Richardson, A.D., Hufkens, K., Milliman, T. et al. Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures. Nature 560, 368–371 (2018). Source data The data are from a long-term experiment on the effects of warming and CO2 on a high-carbon northern temperate peatland and is the focal dataset of the study. The experiment involves 10 large, temperature and CO2 controlled enclosures. CO2 is set to 400 ppm in five enclosures and 900 ppm in five enclosures. Temperature of the five enclosures within each CO2 level is set to 0, 2.25, 4.5, 6.75, or 9 °C above ambient temperature. The multiple temperature levels is a regression design, which allows a researcher to measure non-linear effects. Read more about the experimental design and the beautiful implementation. The question pursued is in this study is, what is the causal effect of warming on the timing (or phenology) of the transition into photosynthetic activity (“green-up”) in the spring and of the transition out of photosynthetic activity (“green-down”) in the fall? The researchers measured these transition dates, or Day of Year (DOY), using foliage color. Here, we focus on the transition out of photosynthesis or “green-down” DOY. Import the data Examine the data gg1 &lt;- qplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth(method = &quot;lm&quot;) gg2 &lt;- qplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2)) gg3 &lt;- qplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth() plot_grid(gg1, gg2, gg3, ncol=3) No plot shows any obvious outlier that might be due to measurement blunders or curation error. The linear regression in the left-most plot clearly shows that a linear response is sufficient to capture the effect of temperature on day of green-down. choose a model. Because the \\(X\\) variable (\\(temperature\\)) was experimentally set to five levels, the data could reasonably be modeled using either a linear model with a categorical \\(X\\) or a linear model with a continuous \\(X\\). The advantage of modeling \\(temperature\\) as a continuous variable is that there is only one effect, the slope of the regression line. If modeled as a categorical factor with five levels, there are, at a minimum, four interesting effects (the difference in means between each non-reference level and reference (temperature = 0) level). Also, for inference, modeling \\(temperature\\) as a continuous variable increases power for hypothesis tests. fit the model # Step 1: fit the model m1 &lt;- lm(transition_date ~ temperature, data = fig2c) check the model # check normality assumption set.seed(1) qqPlot(m1, id=FALSE) The Q-Q plot indicates the distribution of residuals is well within that expected for a normal sample and there is no cause for concern with inference. # check homogeneity assumption spreadLevelPlot(m1, id=FALSE) ## ## Suggested power transformation: 0.6721303 The spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is no cause for concern with inference. inference from the model m1_coeff &lt;- summary(m1) %&gt;% coef() m1_confint &lt;- confint(m1) m1_coeff &lt;- cbind(m1_coeff, m1_confint) m1_coeff ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 289.458750 3.0593949 94.613071 1.738650e-13 282.403773 296.513728 ## temperature 4.982745 0.5541962 8.990941 1.866888e-05 3.704767 6.260724 The effect of added temperature on the day of green-down is 4.98 d per 1 °C (95% CI: 3.7, 6.3; p &lt; 0.001). plot the model Figure 9.1: Modification of the published Figure 2c showing the experimental effect of warming on the date of autumn green-down (the transition to fall foliage color) in a mixed shrub community. The bottom panel is a scatterplot. The regression line shows the expected value of Y (transition day of year) given a value of X (added temperature). The slope of the regression line is the estimate of the effect. The estimate and 95% confidence interval of the estimate are given in the top panel. Report the results The modeled effect of added temperature is Slope: 4.98 (3.7, 6.26) d per 1 °C (9.1). 9.1.2 Learning from the green-down example Figure 9.1 is a scatterplot with the green-down DOY for the mixed-shrub community on the \\(Y\\) axis and added temperature on the \\(X\\) axis. The line through the data is a regression line, which is the expected value of Y (green-down DOY) given a specific value of X (added temperature). The slope of the line is the effect of added temperature on timing of green-down. The intercept of the regression line is the value of the response (day of green-down) when \\(X\\) is equal to zero. Very often, this value is not of interest although the value should be reported to allow predictions from the model. Also very often, the value of the intercept is not meaningful because a value of \\(X = 0\\) is far outside the range of measured \\(X\\), or the value is absurd because it is impossible (for example, if investigating the effect of body weight on metabolic rate, the value \\(weight = 0\\) is impossible). The intercept and slope are the coefficients of the model fit to the data, which is \\[\\begin{equation} day_i = b_0 + b_1 temperature_i + e_i \\tag{9.1} \\end{equation}\\] where day is the day of green-down, temperature is the added temperature, and i refers (or “indexes”) the ith enclosure. This model completely reconstructs the day of green-down for all ten enclosures. For example, the day of green-down for enclosure 8 is \\[\\begin{equation} 332 = 289.458750 + 4.982745 \\times 6.73 + 9.00737 \\end{equation}\\] The coefficients in the model are estimates of the parameters of the generating model fit to the data \\[\\begin{align} day &amp;= \\beta_0 + \\beta_1 temperature + \\varepsilon\\\\ \\varepsilon &amp;\\sim N(0, \\sigma^2) \\tag{9.2} \\end{align}\\] A generating model of the data is used to make inference, for example, a measure of uncertainty in a prediction in the timing of green-down with future warming, or a measure of uncertainty about the effect of temperature on green-down. 9.1.3 Using a regression model for “explanation” – causal models In this text, “explanatory” means “causal” and a goal of explanatory modeling is the estimate of causal effects using a causal interpretation of the linear model (=regression) coefficients. “What what? I learned in my stats 101 course that we cannot interpret regression coefficients causally”. Statisticians (and statistics textbooks) have been quite rigid that a regression coefficient has a descriptive (or “observational”, see below) interpretation but not a causal interpretation. At the same time, statisticians (and statistics textbooks) do not seem to have any issue with interpeting the modeled effects from an experiment causally, since this is how they are interpreted. But if the modeled effects are simply coefficients from a linear (= regression) model, then this historical practice is muddled. Part of this muddled history arises from the use of “regression” for models fit to observational data with one or more continuous \\(X\\) variables and the use of “ANOVA” for models fit to experimental data with one or more categorical \\(X\\). This separation seems to have blinded statisticians from working on the formal probabilistic statements underlying causal interpretations of effect estimates in ANOVA and the synthesis of these statements with the probabilistic statements in regression modeling. Two major approaches to developing formal, probabilistic statements of causal modeling in statistics are the Rubin causal model and the do-operator of Pearl. Despite the gigantic progress in these approaches, little to none of this has found its way into biostatistics textbooks. 9.1.3.1 What a regression coefficient means Description: when we saw a one unit larger value in X, we saw, on average, a difference of b in Y. Prediction: when we see a one unit larger value in X, we expect to see a difference of b in Y. Explanation (causation): when we set X to have one unit larger value, we expect Y to change by b. A linear (“regression”) model coefficient, such as the coefficient for temperature, \\(\\beta_1\\), has three interpretations, an observational interpretation, a predictive interpretation, and a causal interpretation. This text is about causal interpretations, but to explain what is meant by this, I need to clarify the differences between causal and observational interpretations. To clarify the differences, it’s useful to remember that an expected value from a regression model is a conditional mean \\[\\begin{equation} \\textrm{E}[day|temperature] = \\beta_0 + \\beta_1 temperature \\tag{9.3} \\end{equation}\\] In words “the expected value of day conditional on temperature is beta-knot plus beta-one times temperature”. An expected value is a long run average – if we had an infinite number of enclosures with \\(temperature=x\\) (where \\(x\\) is a specific value of added temperature, say 2.5 °C), the average \\(day\\) of these enclosures is \\(\\beta_0 + \\beta_1 x\\). The parameter \\(\\beta_1\\) is a difference in conditional means. \\[\\begin{equation} \\beta_1 = \\textrm{E}[day|temperature = x+1] - \\textrm{E}[day|temperature = x] \\tag{9.4} \\end{equation}\\] In words, “beta-one is the expected value of day of green-down when the temperature equals x + 1 minus the expected value of day of green-down when the temperature equals x.” A very short way to state this is “beta-one is a difference in conditional means”. tl;dr. Note that the “+ 1” in this definition is mere convenience. Since the slope of a line is \\(\\frac{y_2 - y_1}{x_2 - x_1}\\), where (\\(x_1\\), \\(y_1\\)) and (\\(x_2\\), \\(y_2\\)) are the coordinates of any two points on the line, it is convenient to choose two points that differ in \\(X\\) by one unit, which makes the fraction equal to the numerator only. The numerator is a difference in conditional means. It is also why the units of a regression coefficient are “per unit of \\(X\\) even if defined as a difference in two \\(Y\\) values. The difference between observational and causal interpretations of \\(\\beta_1\\) depends on the “event” conditioned on in \\(\\textrm{E}[day|temperature]\\). Let’s start with the causal interpretation, which is how we should think about the regression coefficients in the green-down experiment. 9.1.3.2 Causal interpretation – conditional on “doing” X = x In the causal interpretation of regression, \\(\\textrm{E}[day|temperature]\\) is conditioned on “doing” a real or hypothetical intervention in the system where we set the value of \\(temperature\\) to a specific value \\(x\\) (“\\(temperature=x\\)), while keeping everything else about the system the same. This can be stated explicitly as \\(\\textrm{E}[day|\\;do\\;temperature = x]\\). Using the do-operator, we can interpret \\(\\beta_1\\) as an effect coefficient. \\[\\begin{equation} \\beta_1 = \\textrm{E}[day|\\;do\\;temperature = x+1] - \\textrm{E}[day|\\;do\\;temperature = x] \\end{equation}\\] In words, “beta-one is the expected value of day of green-down were we to set the temperature to x + 1, minus the expected value of day of green-down were we to set the temperature to x.” tl;dr. In the green-down experiment, the researchers didn’t set the temperature in the intervened enclosures to the ambient temperature + 1 but to ambient + 2.25, ambient + 4.5, ambient + 6.75, and ambient + 9.0. Again (see tl;dr above), the + 1 is mere convenience in the definition. 9.1.3.3 Observational interpretation – conditional on “seeing” X = x. In the observational interpretation of regression, \\(\\textrm{E}[day|temperature]\\) is conditioned on sampling data and “seeing” a value of \\(temperature\\). We can state this explicitly as \\(\\textrm{E}[day|\\;see\\;temperature]\\). From this, we can interpret \\(\\beta_1\\) as an observational coefficient \\[\\begin{equation} \\beta_1 = \\textrm{E}[day|\\;see\\;temperature = x+1] - \\textrm{E}[day|\\;see \\;temperature = x] \\end{equation}\\] In words, “beta-one is the expected value of day of green-down when see that temperature equals x + 1 minus the expected value of day of green-down when we see that temperature equals x.” To understand what I mean by “observational”, let’s imagine that the green-down data do not come from an experiment in which the researchers intervened and set the added temperature to a specifc value but from ten sites that naturally vary in mean annual temperature. Or from a single site with 10 years of data, with some years warmer and some years colder. Data from this kind of study is observational – the researcher didn’t intervene and set the \\(X\\) values but merely observed the \\(X\\) values. If we sample (or “see”) a site with a mean annual temperature that is 2.5 °C above a reference value, the expected day of green-down is \\(\\textrm{E}[day|temperature = 2.5 °C]\\). That is, values near \\(\\textrm{E}[day|temperature = 2.5 °C]\\) are more probable than values away from \\(\\textrm{E}[day|temperature = 2.5 °C]\\). Or, if the only information that we have about this site is a mean annual temperature that is 2.5 °C above some reference, then our best prediction of the day of green-down would be \\(\\textrm{E}[day|temperature = 2.5 °C]\\). We do not claim that the 4.98 day delay in green-down is caused by the warmer temperature, only that this is the expected delay relative to the reference having seen the data. The seeing interpretation of a regression coefficient is descriptive– it is a description of a mathematical relationship. In this interpretation, the coefficient is not causal in the sense of what the expected response in \\(Y\\) would be were we to intervene in the system and change \\(X\\) from \\(x\\) to \\(x+1\\). 9.1.3.4 Omitted variable bias What is the consequence of interpreting a regression coefficient causally instead of observationally? Figure 9.2: a Directed Acyclic (or causal) Graph of a hypothetical world where the day of green-down is caused by two, correlated environmental variables, temperature and moisture, and to a noise factor (U) that represents an unspecified set of additional variables that are not correlated to either temperature or moisture. Let’s expand the thought experiment where we have an observational data set of green down dates. In this thought experiment, only two variables systematically affect green-down DOY. The first is the temperature that the plants experience; the effect of \\(temperature\\) is \\(\\beta_1\\). The second is the soil moisture that the plants experience; the effect of \\(moisture\\) is \\(\\beta_2\\). \\(temperature\\) and \\(moisture\\) are correlated with a value \\(\\rho\\). This causal model is graphically represented by the causal graph above. Lets fit two linear models. \\[\\begin{align} (\\mathcal{M}_1)\\; day &amp;= b_0 + b_1 \\; temperature + b_2 \\; moisture + \\varepsilon\\\\ (\\mathcal{M}_2)\\; day &amp;= b_0 + b_1 \\; temperature + \\varepsilon \\end{align}\\] \\(\\mathcal{M}_1\\) the linear model including both \\(temperature\\) and \\(moisture\\) as \\(X\\) variables and \\(\\mathcal{M}_2\\) the linear model including only \\(temperature\\) as an \\(X\\) variable. Given the true causal model above, \\(b_1\\) is an unbiased estimate of the true causal effect of temperature \\(\\beta_1\\) in \\(\\mathcal{M}_1\\) because the expected value of \\(b_1\\) is \\(\\beta_1\\). By contrast, \\(b_1\\) is a biased estimate of the true causal effect of temperature \\(\\beta_1\\) in \\(\\mathcal{M}_2\\). The expected value of \\(b_1\\) in \\(\\mathcal{M}_2\\) is the true, causal effect plus a bias term. \\[\\begin{equation} \\mathrm{E}(b_1|\\mathcal{M}_1) = \\beta + \\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}} \\end{equation}\\] This bias (\\(\\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}}\\)) is omitted variable bias. The omitted variable \\(moisture\\) is an unmeasured, confounding variable. A variable \\(X_2\\) is a confounder for variable \\(X_1\\) if \\(X_2\\) has both a correlation with \\(X_1\\) and a path to the response \\(Y\\) that is not through \\(X_1\\). With ommitted variable bias, as we sample more and more data, our estimate of the effect doesn’t converge on the truth but the truth plus some bias. 9.1.3.5 Causal modeling with experimental versus observational data Causal interpretation requires conditioning on “doing X=x”. For the green-down data, “doing X=x” is achieved by the random treatment assignment of the enclosures. How does random treatment assignment achieve this? Look again at Figure 9.2. If the values of \\(treatment\\) are randomly assigned, then, by definition, the expected value of the correlation between \\(treatment\\) and \\(moisture\\) is zero. In fact, the expected value of the correlation between \\(treatment\\) and any measurable variable at the study site is zero. Given, this, the expected value of the regression coefficient for \\(temperature\\) (\\(b_1\\)) is \\(\\beta\\) because \\(\\rho=0\\). That is, the estimate of the true effect is unbiased. It doesn’t matter if \\(moisture\\), or any other variable, is excluded from the model. (That said, we may want to include moisture or other variables in the model to increase precision of the causal effect. This is addressed in the chapter “Models with Covariates”). This is what an experiment does and why experiments are used to answer causal questions. Can we use observational data for causal modeling? Yes, but the methods for this are outside of the scope of this text. The mathematical foundation for this is known as path analysis, which was developed by geneticist and evolutionary biologist Sewell Wright (I include this because this work has inspired much of how I think about biology and because he is both my academic grandfather and great-grandfather). Causal analysis of observational data in biology is rare outside of ecology and epidemiology. Good starting points for the modern development of causal analysis are Hernán MA and Robins JM (2020) and Burgess et al. (2018). A gentle introduction is Pearl and Mackenzie (2018). 9.1.4 Using a regression model for prediction – prediction models The researchers in the green-down study also presented estimates of the effect of temperature on green-down using two observational datasets. Let’s use the one in Extended Data Figure 3d to explore using a regression model for prediction. The data are taken from measurements of the day of green-down over an 86 year period at a single site. The response variable is \\(green\\_down\\_anomaly\\) (the difference between the day of green down for the year and the mean of these days over the 86 years). The predictor variable is \\(autumn\\_temp\\_anomaly\\) (the difference between the mean temperature over the year and the mean of these means). The plot in figure ?? shows the regression line of a linear model fit to the data. Two statistics are given in the plot. The p-value of the slope (the coefficient \\(b_1\\) of \\(autumn\\_temp\\_anomaly\\)) The \\(R^2\\) of the model fit. In addition, two intervals are shown. 95% confidence interval of the expected values in blue. The width of this band is a measure of the precision of the estimates of expected values. 95% prediction interval of the predictions in gray. The width of this band is a measure of how well the model predicts. It is important that researchers knows what each of these statistics and bands are, when to compute them and when to ignore them them, and what to say about each when communicating the results. 9.1.4.1 95% CI and p-value Both the 95% confidence interval of the expected values and the p-value are a function of the standard error of the slope coefficient \\(b_1\\) and so are complimentary statistics. The p-value is the probability of sampling the null that results in a \\(t\\)-value as or more extreme than the observed t for the coefficient \\(b_1\\). This null includes the hypothesis \\(\\beta_1 = 0\\). The 95% confidence interval of the expected values is the band containing expected values (mean \\(green\\_down\\_anomaly\\) conditional on \\(autumn\\_temp\\_anomaly\\)) that are compatible with the data. There are a couple of useful ways of thinking about this band. The band captures an infinite number of regression lines that are compatible with the data. Some are more steep and predict smaller expected values at the low end of \\(autumn\\_temp\\_anomaly\\) and higher expected values at the high end of \\(autumn\\_temp\\_anomaly\\). Others are less steep and predict higher expected values at the low end of \\(autumn\\_temp\\_anomaly\\) and lower expected values at the high end of \\(autumn\\_temp\\_anomaly\\). The band captures the 95% CI of the conditional mean at every value of \\(X\\). Consider the 95% CI of the conditional mean at the mean value of \\(X\\). As we move away from this mean value (lower to higher \\(X\\)), the 95% CI of the conditional mean increases. A 95% CI and p-value are useful statistics to report if the purpose is causal modeling, as in the example above using the experimental green-down data (where the 95% CI was not presented as a confidence band of the expected values but a CI of the estimate of the effect of added temperature). A 95% CI and p-value are also useful statistics to report if the purpose is descriptive modeling, simply wanting to know how the conditional mean of the response is related to an \\(X\\) variable. 9.1.4.2 95% prediction interval and \\(R^2\\) Both the \\(R^2\\) and the 95% prediction interval are a function of the population variability of \\(green\\_down\\_anomaly\\) conditional on \\(autumn\\_temp\\_anomaly\\) (the spread of points along a vertical axis about the regression line) and are complimentary statistics. Briefly, the \\(R^2\\) is a measure of the fraction of the variance in the response that is accounted by the model (some sources say “explained by the model” but this is an odd use of “explain”). It is a number between 0 and 1 and is a measure of “predictability” if the goal is prediction modeling. The 95% prediction interval will contain a new observation 95% of the time. It provides bounds on a prediction – given a new observation, there is a 95% probability that the interval at \\(x_{new}\\) will contain \\(y_{new}\\). To understand \\(R^2\\) and the 95% prediction interval a bit better, let’s back up. \\[\\begin{equation} green\\_down\\_anomaly_i = b_0 + b_1 autumn\\_temp\\_anomaly_i + e_i (\\#eq:doy_fit) \\end{equation}\\] Model @ref(eq:eq:doy_fit) recovers the measured value of \\(green\\_down\\_anomaly\\) for any year, given the \\(autumn\\_temp\\_anomaly\\) for the year. The equation includes the linear predictor (\\(b_0 + b_1 autumn\\_temp\\_anomaly_i\\)) and the residual from the predictor (\\(e_i\\)). The predictor part of @ref(eq:doy_fit) is used to compute \\(\\hat{y}\\) (“y hat”). \\[\\begin{equation} \\hat{y}_i = b_0 + b_1 autumn\\_temp\\_anomaly_i (\\#eq:doy_hat) \\end{equation}\\] The \\(\\hat{y}\\) are fitted values, if the values are computed from the data that were used for the fit, or predicted values (or simply the prediction), if the values are computed from values of the predictor variables outside the set used to fit the model. For the purpose of plotting, generally, with models with categorical factors as \\(X\\), I prefer either modeled values or conditional means to fitted values. 9.1.4.3 \\(R^2\\) A good model accounts for a sizable fraction of the total variance in the response. This fraction is the \\(R^2\\) value, which is given in summary(m1) and accessed directly with summary(m1)$r.squared ## [1] 0.1305754 \\(R^2\\) is the fraction of the total variance of \\(Y\\) that is generated by the linear predictor. \\[\\begin{equation} R^2 = \\frac{\\mathrm{VAR}(\\hat{y})}{\\mathrm{VAR}(y)} \\end{equation}\\] yhat &lt;- fitted(m1) y &lt;- efig_3d[, green_down_anomaly] var(yhat)/var(y) ## [1] 0.1305754 \\(R^2\\) will vary from zero (the model accounts for nothing) to one (the model accounts for everything). \\(R^2\\) is often described as the fraction of total variation explained by the model” but the usage of “explain” is observational and not causal. Because of the ambiguous usage of “explain” in statistics, I prefer to avoid the word. It can be useful sometimes to think of \\(R^2\\) in terms of residual error, which is the variance of the residuals from the model. The larger the residual error, the smaller the \\(R^2\\), or \\[\\begin{equation} R^2 = 1 - \\frac{\\mathrm{VAR}(e)}{\\mathrm{VAR}(y)} \\end{equation}\\] e &lt;- residuals(m1) y &lt;- efig_3d[, green_down_anomaly] 1 - var(e)/var(y) ## [1] 0.1305754 The smaller the residuals, the higher the \\(R^2\\) and the closer the predicted values are to the measured values. The sum of the model variance and residual variance equals the total variance and, consequently, \\(R^2\\) is a signal to signal + noise ratio. The noise in \\(R^2\\) is the sampling variance of the individual measures. The noise in a t-value is the sampling variance of the parameter (for m1, this is the sampling variance of \\(b_1\\)). This is an important distinction because it means that t and \\(R^2\\) are not related 1:1. In a simple model with only a single \\(X\\), one might expect \\(R^2\\) to be big if the p-value for the slope is tiny, but this isn’t necessarily true because of the different meaning of noise in each. A study with a very large sample size \\(n\\) can have a tiny p-value and a small \\(R^2\\). A p-value is not a good indicator of predictability. \\(R^2\\) is. This is explored more below. 9.1.4.4 Prediction interval A good prediction model has high predictability, meaning the range of predicted values is narrow. A 95% CI is a reasonable range to communicate. For any decision making using prediction, it is better to look at numbers than a band on a plot. Autumn Temp Anomaly (°C) Expected (days) 2.5% (days) 97.5% (days) 0.5 0.8 -8.7 10.3 1.0 1.6 -7.9 11.1 1.5 2.4 -7.2 12.0 2.0 3.2 -6.4 12.8 2.5 4.0 -5.7 13.7 Given these data and the fit model, if we see a 2 °C increase in mean fall temperature, we expect the autumn green-down to extend 3.2 days. This expectation is an average. We’d expect 95% of actual values to range between -6.4 days and 12.8 days. This is a lot of variability. Any prediction has a reasonable probability of being over a week early or late. This may seem surprising given the p-value of the fit, which is 0.0006. But the p-value is not a reliable indicator of predictability. It is a statistic related to the blue band, not the gray band. To understand this prediction interval a bit more, and why a p-value is not a good indicator of predictability, it’s useful to understand what makes a prediction interval “wide”. The width of the prediction interval is a function of two kinds of variability The variance of the expected value at a specific value of \\(X\\). This is the square of the standard error of \\(b_1\\). The blue band is communicating the CI based on this variance. The p-value is related to the wideth of this band. The variance of \\(Y\\) at a specific value of \\(X\\). This variance is \\(\\sigma^2\\). It is useful for learning to think about the equation for the estimate of this variance. \\[\\begin{equation} \\hat\\sigma^2 = \\frac{\\sum{e_i^2}}{N-2} \\end{equation}\\] Again, \\(e_i\\) is the residual for the ith case. The denominator (\\(N-2\\)) is the degrees of freedom of the model. Computing \\(\\hat\\sigma^2\\) manually helps to insure that we understand what is going on. summary(m1)$sigma # R&#39;s calculation of sigma hat ## [1] 4.736643 df &lt;- summary(m1)$df[2] # R&#39;s calculation of df. Check that this is n-2! sqrt(sum(e^2)/df) ## [1] 4.736643 Remember that an assumption of the linear models that we are working with at this point is, this variance is constant for all values of \\(X\\), so we have a single \\(\\sigma\\). Later, we will cover linear models that model heterogeneity in this variance. \\(\\sigma\\) is a function of variability in the population – it is the population standard deviation conditional on \\(X\\). Importantly, predictability is a function of both these components of variability. As a consequence, it is \\(R^2\\), and not the p-value, that is the indicator of predictability. In the observational green-down data, even if we had thousands of years of data, we would still have a pretty low \\(R^2\\) because of the population variability of \\(green\\_down\\_anomaly\\) at a given \\(autumn\\_temp\\_anomaly\\). 9.1.4.5 Median Absolute Error and Root Mean Square Error are absolute measures of predictability A p-value is not at all a good guide to predictability. \\(R^2\\) is proportional to predictability but is not really useful in any absolute sense. If we want to predict the effect of warming on the day of green-down, I would like to have a measure of predictability in the units of green-down, which is days. The prediction interval gives this for any value of \\(X\\). But what about an overall measure of predictability? Three overall measures of predictability are \\(\\hat{\\sigma}\\), the estimate of \\(\\sigma\\). This is the standard deviation of the sample conditional on \\(X\\). \\(RMSE\\), the root mean squared error. This is \\[\\begin{align} SSE &amp;= \\sum{(y_i - \\hat{y}_i)^2}\\\\ MSE &amp;= \\frac{SSE}{N}\\\\ RMSE &amp;= \\sqrt{MSE} \\end{align}\\] \\(SSE\\) (“sum of squared error”) is the sum of the squared residuals (\\(y_i - \\hat{y}_i\\)) of the model. \\(MSE\\) (“mean squared error”) is the mean of the squared errors. \\(RMSE\\) is the square root of the mean squared error. \\(RMSE\\) is almost equal to \\(\\hat{\\sigma}\\). The difference is the denominator, which is \\(N\\) in the computation of \\(RMSE\\) and \\(df\\) (the degrees of freedom of the model, which is \\(N\\) minus the number of fit parameters) in the computation of \\(\\hat{\\sigma}\\). \\(MAE\\), the mean absolute error. This is \\[\\begin{equation} MAE = \\frac{1}{N}\\sum{|y_i - \\hat{y}_i|} \\end{equation}\\] sigma RMSE MAE 4.74 4.68 3.58 If the goal of an analysis is prediction, one of these statistics should be reported. For the model fit to the observational green-down data in Extended Figure 3d, these three statistics are given in Table ?? above (to two decimal places simply to show numerical difference between \\(\\hat{\\sigma}\\) and \\(RMSE\\)). All of these are measures of an “average” prediction error in the units of the response. The average error is either 4.7 or 3.6 days, depending on which statistic we report. Why the difference? Both \\(\\hat{\\sigma}\\) and \\(RMSE\\) are functions of squared error and so big differences between predicted and actual value are emphasized. If an error of 6 days is more than twice as bad than an error of 3 days, report \\(RMSE\\). Why \\(RMSE\\) and not \\(\\hat{\\sigma}\\)? Simply because researchers using prediction models are more familiar with \\(RMSE\\). If an error of 6 days is not more than twice as bad than an error of 3 days, report \\(MAE\\). 9.1.4.6 Prediction modeling is more sophisticated than presented here For data where the response is a non-linear function of the predictor, or for data with many predictor variables, researchers will often build a model using a model selection method. Stepwise regression is a classical model selection method that is commonly taught in intro biostatistics and commonly used by researchers. Stepwise regression as a method of model selection has many well-known problems and should be avoided. Some excellent books that are useful for building models and model selection are The Elements of Statistical Learning Regression and Other Stories Regression Modeling Strategies 9.1.5 Using a regression model for creating a new response variable – comparing slopes of longitudinal data In the study in this example, the researchers compared the growth rate of tumors in mice fed normal chow versus mice with a methionine restricted diet. Growth rate wasn’t actually compared. Instead, the researchers used a t-test to compare the size of the tumor measured at six different days. A problem with multiple t-tests for this dataset is that the errors (residuals from the model) on one day are correlated with the errors from another day because of the repeated measures on each mouse. This correlation will inflate Type I error rate. Instead of six t-tests, a better strategy for these data is to use a regression to calculate a tumor growth rate for each mouse. There are sixteen mice so this is sixteen fit models. Here I use a “for loop” to fit the model to the data from a single mouse and use the slope (\\(b_1\\)) as the estimate of the tumor growth rate for that mouse. Use a for-loop to estimate growth rate for each mouse. In each pass through the loop the subset of fig1f (the data in long format) belonging to mouse i is created the linear model volume ~ day is fit to the subset the coefficient of day (the slope, \\(b_1\\)) is inserted in mouse i’s row in the column “growth” in the data.table “fig1f_wide”. At the end of the loop, we have the data.table fig1f_wide which has one row for each mouse, a column for the treatment factor (diet) and a column called “growth” containing each mouse’s growth rate. There are also columns of tumor volume for each mouse on each day but these are ignored. N &lt;- nrow(fig1f_wide) id_list &lt;- fig1f_wide[, id] for(i in 1:N){ mouse_i_data &lt;- fig1f[id == id_list[i]] # subset fit &lt;- lm(volume ~ day, data = mouse_i_data) fig1f_wide[id == id_list[i], growth := coef(fit)[2]] } # View(fig1f_wide) # qplot(x = treatment, y = growth, data = fig1f_wide) # qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_smooth(aes(group = id), method = &quot;lm&quot;, se = FALSE) Step 3. fit the model m1 &lt;- lm(growth ~ treatment, data = fig1f_wide) Step 5. inference from the model m1_coef &lt;- summary(m1) %&gt;% coef m1_ci &lt;- confint(m1) (m1_coef_table &lt;- cbind(m1_coef, m1_ci)) ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 52.63406 3.102096 16.967258 9.865155e-11 45.98072 59.28739 ## treatmentMR -23.57616 4.387026 -5.374065 9.809457e-05 -32.98540 -14.16693 Step 6. plot the model 9.1.6 Using a regression model for for calibration 9.2 Working in R 9.2.1 Fitting the linear model A linear model is fit with the lm function, which is very flexible and will be a workhorse in much of this text. m1 &lt;- lm(transition_date ~ temperature, data = fig2c) m1 is an lm model object that contains many different kinds information, such as the model coefficients. coef(m1) ## (Intercept) temperature ## 289.458750 4.982745 We’ll return to others, but first, let’s explore some of the flexibility of the lm function. Two arguments were sent to the function the model formula transition_date ~ temperature with the form Y ~ X, where Y and X are names of columns in the data. The model formula itself can be assigned to a variable, which is useful when building functions. An example coef_table &lt;- function(x, y, data){ m1_form &lt;- formula(paste(y, &quot;~&quot;, x)) m1 &lt;- lm(m1_form, data = data) return(coef(summary(m1))) } coef_table(x = &quot;temperature&quot;, y = &quot;transition_date&quot;, data = fig2c) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 289.458750 3.0593949 94.613071 1.738650e-13 ## temperature 4.982745 0.5541962 8.990941 1.866888e-05 Both Y and X can also be column names embedded within a function, for example m2 &lt;- lm(log(transition_date) ~ temperature, data = fig2c) coef(m2) ## (Intercept) temperature ## 5.6690276 0.0160509 or m3 &lt;- lm(scale(transition_date) ~ scale(temperature), data = fig2c) coef(m3) ## (Intercept) scale(temperature) ## 4.171921e-16 9.539117e-01 The data frame (remember that a data.table is a data frame) containing the columns with the variable names in the model formula. A data argument is not necessary but it is usually the better way (an exception is when a researcher wants to create a matrix of Y variables or to construct their own model matrix) type ?lm into the console to see other arguments of the lm function. x &lt;- fig2[, temperature] y &lt;- fig2[, transition_date] m4 &lt;- lm(y ~ x) coef(m4) ## (Intercept) x ## 204.8866185 0.4324755 9.2.2 Getting to know the linear model: the summary function The lm function returns an lm object, which we’ve assigned to the name m1. m1 contains lots of information about our fit of the linear model to the data. Most of the information that we want for most purposes can be retrieved with the summary function, which is a general-purpose R command the works with many R objects. summary(m1) ## ## Call: ## lm(formula = transition_date ~ temperature, data = fig2c) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5062 -3.8536 0.6645 2.7537 9.0074 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 289.4588 3.0594 94.613 1.74e-13 *** ## temperature 4.9827 0.5542 8.991 1.87e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.443 on 8 degrees of freedom ## Multiple R-squared: 0.9099, Adjusted R-squared: 0.8987 ## F-statistic: 80.84 on 1 and 8 DF, p-value: 1.867e-05 What is here: Call. This is the model that was fit Residuals. This is a summary of the distribution of the residuals. From this one can get a sense of the distribution (for inference, the model assumes a normal distribution with mean zero). More useful ways to examine this distribution will be introduced later in this chapter. Coefficients table. This contains the linear model coefficients and their standard error and associated \\(t\\) and \\(p\\) values. The column of values under “Estimate” are the coefficients of the fitted model (equation (9.1)). Here, 289.4587503 is the intercept (\\(b_0\\)) and 4.9827453 is the effect of \\(temperature\\) (\\(b_1\\)). The column of values under “Std. Error” are the standard errors of the coefficients. The column of values under “t value” are the t-statistics for each coefficient. A t-value is a signal to noise ratio. The coefficient \\(b_1\\) is the “signal” and the SE is the noise. Get used to thinking about this ratio. A t-value greater than about 3 indicates a “pretty good” signal relative to noise, while one much below than 2 is not. The column of values under “Pr(&gt;|t|)” is the p-value, which is the t-test of the estimate. What is the p-value a test of? The p-value tests the hypothesis “how probable are the data, or more extreme than than the data, if the true parameter is zero?”. Formally \\(p = \\mathrm{freq(t&#39; \\ge t|H_o)}\\), where \\(t&#39;\\) is the hypothetical t-value, t is the observed \\(t\\)-value, and \\(H_o\\) is the null hypothesis. We will return to p-values in Chapter xxx. Signif. codes. Significance codes are extremely common in the wet bench experimental biology literature but do not have much to recommend. I’ll return to these in the p-values chapter. Beneath the Signif. codes are some model statistics which are useful Residual standard error This is \\(\\sqrt{\\sum{e_i^2}/(n-2)}\\), where \\(e_i\\) are the residuals in the fitted model. “degrees of freedom” is the number of \\(e_i\\) that are “allowed to vary” after fitting the parameters, so is the total sample size (\\(n\\)) minus the number of parameters in the model. The fit model has two fit parameters (\\(b_0\\) and \\(b_1\\) so the df is \\(n-2\\). Note that this is the denominator in the residual standard error equation. Multiple R-squared. This is an important but imperfect summary measure of the whole model that effectively measures how much of the total variance in the response variable “is explained by” the model. Its value lies between zero and 1. It’s a good measure to report in a manuscript, especially for observational data. F-statistic and p-value. These are statistics for the whole model (not the individual coefficients) and I just don’t find these very useful. 9.2.3 Inference – the coefficient table There are different ways of # step by step m1_summary &lt;- summary(m1) # get summary m1_coef_p1 &lt;- coef(m1_summary) # extract coefficient table m1_coef_p2 &lt;- confint(m1) # get CIs m1_coef &lt;- cbind(m1_coef_p1, m1_coef_p2) # column bind the two # this can be shortened (but still readable) using m1_coef &lt;- cbind(coef(summary(m1)), confint(m1)) m1_coef ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 289.458750 3.0593949 94.613071 1.738650e-13 282.403773 296.513728 ## temperature 4.982745 0.5541962 8.990941 1.866888e-05 3.704767 6.260724 Note that the p-value for the coefficient for temperature is very small and we could conclude that the data are not compatible with a model of no temperature effect on day of green-down. But did we need a formal hypothesis test for this? We haven’t learned much if we have only learned that the slope is “not likely to be exactly zero” (Temperature effects everything in biology). A far more important question is not if there is a relationship between temperature and day of green-down, but what is the sign and magnitude of the effect and what is the uncertainty in our estimate of this effect. For this, we we want the coefficient and its SE or confidence interval, both of which are in this table. Remember, our working definition of a confidence interval: A confidence interval contains the range of parameter values that are compatible with the data, in the sense that a \\(t\\)-test would not reject the null hypothesis of a difference between the estimate and any value within the interval A more textbook way of defining a CI is: A 95% CI of a parameter has a 95% probability of including the true value of the parameter. It does not mean that there is a 95% probability that the true value lies in the interval. This is a subtle but important difference. Here is a way of thinking about the proper meaning of the textbook definition: we don’t know the true value of \\(\\beta_1\\) but we can 1) repeat the experiment or sampling, 2) re-estimate \\(\\beta_1\\), and 3) re-compute a 95% CI. If we do 1-3 many times, 95% of the CIs will include \\(\\beta_1\\) within the interval. Confidence intervals are often interpreted like \\(p\\)-values. That is, the researcher looks to see if the CI overlaps with zero and if it does, concludes there is “no effect”. First, this conclusion is not correct – the inability to find sufficient evidence for an effect does not mean there is no effect, it simply means there is insufficient evidence to conclude there is an effect! Second, what we want to use the CI for is to guide us about how big or small the effect might reasonably be, given the data. Again, A CI is a measure of parameter values that are “compatible” with the data. If our biological interpretations at the small-end and at the big-end of the interval’s range radically differ, then we don’t have enough precision in our analysis to reach an unambiguous conclusion. 9.2.4 How good is our model? – Model checking There are two, quite different senses of what is meant by a good model. How good is the model at predicting? Or, how much of the variance in the response (the stuff to be “explained”) is accounted for by the model? This was described above. How well do the data look like a sample from the modeled distribution? If not well, we should consider alternative models. This is model checking. For inference, a good model generates data that look like the real data. If this is true, our fit model will have well-behaved residuals. There are several aspects of “well-behaved” and each is checked with a diagnostic plot. This model checking is covered in more detail in chapter xxx. Inference from model (9.2) assumes the data were sampled from a normal distribution. To check this, use a quantile-quantile or Q-Q plot. The qqPlot function from the car package generates a more useful plot than that from Base R. set.seed(1) qqPlot(m1, id=FALSE) Approximately normal residuals will track the solid line and stay largely within the boundaries marked by the dashed lines. The residuals from m1 fit to the green-down data track the solid line and remain within the dashed lines, indicating adequate model fit. Note that a formal test of normality is often recommended. Formal tests do not add value above a diagnostic check. Robustness of inference (for example, a p-value) is a function of the type and degree of “non-normalness”, not of a p-value. For a small sample, there is not much power to test for normality, so samples from non-normal distributions pass the test (\\(p &gt; 0.05\\)) and are deemed “normal”. For large samples, samples from distributions that deviate slightly from normal fail the test (\\(p &lt; 0.05\\)) and are deemed “not normal”. Inference with many non-normal samples with large \\(n\\) are very robust (meaning infernece is not likely to fool you with randomness). Inference from model (9.2) assumes homogeneity of the response conditional on \\(X\\). For a continuous \\(X\\), this means the residuals should have approximately equal variance at low, mid, and high values of \\(X\\) (and everywhere in between). One can visually inspect the spread of points in the \\(Y\\) direction across the groups for categorical \\(X\\) or along the X-axis for continuous \\(X\\). A useful method for checking how residual variance changes (usually increases) with the conditional mean of \\(Y\\) is a spread-location plot. The spreadLevelPlot(m1) function from the car package is useful. spreadLevelPlot(m1) ## ## Suggested power transformation: 0.6721303 The dashed blue line shows linear trends while the magenta line shows non-linear trends. For the green-down data, the dashed line is very flat while the magenta-line shows what looks like random fluctations. Taken together, the two lines indicate adequate model fit. 9.2.5 Plotting models with continuous X 9.2.5.1 Quick plot qplot from the ggplot package is useful for initial examinations of the data qplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth() Some variations of this quick plot to explore include use geom_smooth(method = \"lm\") 9.2.5.2 ggpubr plots ggpubr is a package with functions that automates the construction of publication-ready ggplots. ggscatter can generate a publishable plot with little effort. ggscatter(data = fig2c, x = &quot;temperature&quot;, y = &quot;transition_date&quot;, add = &quot;reg.line&quot;, xlab = &quot;Added Temperature (°C)&quot;, ylab = &quot;Day of Green-down (DOY)&quot;) It take only a little more effort to add useful modifications. ggscatter(data = fig2c, x = &quot;temperature&quot;, y = &quot;transition_date&quot;, color = &quot;black&quot;, fill = &quot;red&quot;, shape = 21, size = 3, add = &quot;reg.line&quot;, add.params = list(color = &quot;steelblue&quot;, fill = &quot;lightgray&quot;), conf.int = TRUE, xlab = &quot;Added Temperature (°C)&quot;, ylab = &quot;Day of Green-down (DOY)&quot;) + stat_regline_equation(size = 4, label.y = 340) + stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = &quot;~`,`~&quot;)), size = 4, label.y = 335) + NULL Notes The interval shown is the 95% confidence interval of the expected values, which is what we want to communicate with the experimental green-down data. Were we to want to use this as a prediction model, we would want the 95% prediction interval. I’m not sure that ggpubr can plot a prediction interval. To see how I plotted the prediction interval in Figure ?? above, see the Hidden Code section below. The arguments of the ggscatter function are typed in explicitly (x = \"temperarture\" and not just \"temperature\"). Each argument starts on a new line to increase readability. The + after the ggscatter function adds additional layers to the plot. Each additional component is started on a new line to increase readability. The first line of the stat_cor function (everything within aes()) plots the \\(R^2\\) instead of the correlation coefficient \\(r\\). Copy and pasting this whole line just works. Comment out the line of the ggplot script starting with stat_cor and re-run (comment out by inserting a # at the front of the line. A consistent way to do this is to triple-click the line to highlight the line and then type command-shift-c on Mac OS). The script runs without error because NULL has been added as the final plot component. Adding “NULL” is a useful trick. ggscatter(data = efig_3d, x = &quot;autumn_temp_anomaly&quot;, y = &quot;green_down_anomaly&quot;, color = &quot;black&quot;, fill = &quot;red&quot;, shape = 21, size = 3, add = &quot;reg.line&quot;, add.params = list(color = &quot;steelblue&quot;, fill = &quot;lightgray&quot;), conf.int = TRUE, xlab = &quot;Added Temperature (°C)&quot;, ylab = &quot;Day of Green-down (DOY)&quot;) 9.2.5.3 ggplots Use ggplot from the ggplot2 package for full control of plots. See the Hidden Code below for how I generated the plots in Figure 9.1 above. 9.2.6 Creating a table of predicted values and 95% prediction intervals efig_3d is the data.table created from importing the data in Extended Data Figure 3d above. The fit model is efig3d_m1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d) The predict(efig3d_m1) function is used to compute either fitted or predicted values, and either the confidence or prediction interval of these values. Unless specified by newdata, the default x-values used to generate the y in predict(efig3d_m1) are the x-values in the data used to fit the model. The returned values are the expected value for each \\(x_i\\) in the data. The argument newdata passes a data.frame (remember a data.table is a data.frame!) with new x-values. Since these x-values were not in the data used to fit the model, the returned \\(y_{hat}\\) are predicted values. The range of x-values in the data is range(efig_3d[,autumn_temp_anomaly]) ## [1] -2.673793 2.568045 Let’s get predicted values for a value of \\(autumn\\_temp\\_anomaly = 2\\). predict(efig3d_m1, newdata = data.table(autumn_temp_anomaly = 2)) ## 1 ## 3.192646 And predicted values across the range of measured values new_dt &lt;- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2)) predict(efig3d_m1, newdata = new_dt) ## 1 2 3 4 ## -3.192646 -1.596323 1.596323 3.192646 Add 95% confidence intervals (this could be used to create the band for plotting) predict(efig3d_m1, newdata = new_dt, interval = &quot;confidence&quot;, se.fit = TRUE)$fit ## fit lwr upr ## 1 -3.192646 -5.2485713 -1.1367215 ## 2 -1.596323 -2.9492686 -0.2433778 ## 3 1.596323 0.2433778 2.9492686 ## 4 3.192646 1.1367215 5.2485713 Change to the 95% prediction intervals predict(efig3d_m1, newdata = new_dt, interval = &quot;prediction&quot;, se.fit = TRUE)$fit ## fit lwr upr ## 1 -3.192646 -12.833739 6.448446 ## 2 -1.596323 -11.112325 7.919679 ## 3 1.596323 -7.919679 11.112325 ## 4 3.192646 -6.448446 12.833739 Put this all together in a pretty table. I’ve used knitr’s kable function but there are table packages in R that allow extensive control of the output. efig3d_m1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d) new_dt &lt;- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2)) prediction_table &lt;- predict(efig3d_m1, newdata = new_dt, interval = &quot;prediction&quot;, se.fit = TRUE)$fit prediction_table &lt;- data.table(new_dt, prediction_table) pretty_names &lt;- c(&quot;Autumn Temp Anomaly&quot;, &quot;Estimate&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) setnames(prediction_table, old = colnames(prediction_table), new = pretty_names) knitr::kable(prediction_table, digits = c(1, 1, 1, 1)) Autumn Temp Anomaly Estimate 2.5% 97.5% -2 -3.2 -12.8 6.4 -1 -1.6 -11.1 7.9 1 1.6 -7.9 11.1 2 3.2 -6.4 12.8 9.3 Hidden code 9.3.1 Import and plot of fig2c (ecosystem warming experimental) data Import data_from &lt;- &quot;Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures&quot; file_name &lt;- &quot;41586_2018_399_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig2 &lt;- read_excel(file_path) %&gt;% # import clean_names() %&gt;% # clean the column names data.table() # convert to data.table # View(fig2) fig2c &lt;- fig2[panel == &quot;2c&quot;,] # View(fig2c) Creating the response plot (the bottom component) m1_b &lt;- coef(m1) m1_ci &lt;- confint(m1) m1_b0_text &lt;- paste0(&quot;Intercept: &quot;, round(m1_b[1],1), &quot; (&quot;, round(m1_ci[1,1],1), &quot;, &quot;, round(m1_ci[1,2],1), &quot;) d&quot;) m1_b1_text &lt;- paste0(&quot;Slope: &quot;, round(m1_b[2],2), &quot; (&quot;, round(m1_ci[2,1],2), &quot;, &quot;, round(m1_ci[2,2],2), &quot;) d per 1 °C&quot;) # regression line m1_x &lt;- min(fig2c[, temperature]) m1_xend &lt;- max(fig2c[, temperature]) m1_y &lt;- m1_b[1] + m1_b[2]*m1_x m1_yend &lt;- m1_b[1] + m1_b[2]*m1_xend fig2c_gg_response &lt;- ggplot(data = fig2c, aes(x = temperature, y = transition_date)) + # regression line first, to not block point geom_segment(x = m1_x, y = m1_y, xend = m1_xend, yend = m1_yend) + # create black edge to points # geom_point(size = 4, # color = &quot;black&quot;) + geom_point(size = 3, color = pal_okabe_ito[1]) + scale_x_continuous(breaks = c(0, 2.25, 4.5, 6.75, 9)) + xlab(&quot;Plot temperature (ΔT, °C)&quot;) + ylab(&quot;Autumn green-down (DOY)&quot;) + theme_pubr() + NULL # fig2c_gg_response Creating the effects plot (the top component) m1_coeff_dt &lt;- data.table(term = row.names(m1_coeff), data.table(m1_coeff))[2,] %&gt;% clean_names() m1_coeff_dt[ , p_pretty := pvalString(pr_t)] min_bound &lt;- min(m1_coeff_dt[, x2_5_percent]) max_bound &lt;- min(m1_coeff_dt[, x97_5_percent]) y_lo &lt;- min(min_bound+min_bound*0.2, -max_bound) y_hi &lt;- max(max_bound + max_bound*0.2, -min_bound) y_lims &lt;- c(y_lo, y_hi) fig2c_gg_effect &lt;- ggplot(data=m1_coeff_dt, aes(x = term, y = estimate)) + # confidence level of effect geom_errorbar(aes(ymin=x2_5_percent, ymax=x97_5_percent), width=0, color=&quot;black&quot;) + # estimate of effect geom_point(size = 3) + # zero effect geom_hline(yintercept=0, linetype = 2) + # p-value annotate(geom = &quot;text&quot;, label = m1_coeff_dt$p_pretty, x = 1, y = 7.5) + # aesthetics scale_y_continuous(position=&quot;right&quot;) + scale_x_discrete(labels = &quot;Temperature\\neffect&quot;) + ylab(&quot;Effects (day/°C)&quot;) + coord_flip(ylim = y_lims) + theme_pubr() + theme(axis.title.y = element_blank()) + NULL #fig2c_gg_effect Combining the response and effects plots into single plot fig3d_fig &lt;- plot_grid(fig2c_gg_effect, fig2c_gg_response, nrow=2, align = &quot;v&quot;, axis = &quot;lr&quot;, rel_heights = c(0.4,1)) fig3d_fig 9.3.2 Import and plot efig_3d (Ecosysem warming observational) data Import data_from &lt;- &quot;Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures&quot; file_name &lt;- &quot;41586_2018_399_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) efig_3d &lt;- read_excel(file_path, range = &quot;J2:K87&quot;, col_names = FALSE) %&gt;% # header causing issues data.table() # convert to data.table setnames(efig_3d, old = colnames(efig_3d), new = c(&quot;autumn_temp_anomaly&quot;, &quot;green_down_anomaly&quot;)) # View(efig_3d) Plot m1 &lt;- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d) # get x for drawing slope minx &lt;- min(efig_3d[,autumn_temp_anomaly]) maxx &lt;- max(efig_3d[,autumn_temp_anomaly]) new_x &lt;- seq(minx, maxx, length.out = 20) new_data &lt;- data.table(autumn_temp_anomaly = new_x) new_data[, yhat := predict(m1, newdata = new_data)] new_data[, conf_lwr := predict(m1, se.fit = TRUE, interval = &quot;confidence&quot;, newdata = new_data)$fit[, &quot;lwr&quot;]] new_data[, conf_upr := predict(m1, se.fit = TRUE, interval = &quot;confidence&quot;, newdata = new_data)$fit[, &quot;upr&quot;]] new_data[, pred_lwr := predict(m1, se.fit = TRUE, interval = &quot;prediction&quot;, newdata = new_data)$fit[, &quot;lwr&quot;]] new_data[, pred_upr := predict(m1, se.fit = TRUE, interval = &quot;prediction&quot;, newdata = new_data)$fit[, &quot;upr&quot;]] gg &lt;- ggscatter(data = efig_3d, x = &quot;autumn_temp_anomaly&quot;, y = &quot;green_down_anomaly&quot;, color = &quot;black&quot;, shape = 21, size = 3, add = &quot;reg.line&quot;, add.params = list(color = &quot;steelblue&quot;, fill = &quot;lightgray&quot;), xlab = &quot;Temperature Anomaly (°C)&quot;, ylab = &quot;Day of Green-down Anomaly (DOY)&quot;) + geom_ribbon(data = new_data, aes(ymin = pred_lwr, ymax = pred_upr, y = yhat, fill = &quot;band&quot;), fill = &quot;gray&quot;, alpha = 0.3) + geom_ribbon(data = new_data, aes(ymin = conf_lwr, ymax = conf_upr, y = yhat, fill = &quot;band&quot;), alpha = 0.3) + stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = &quot;~`,`~&quot;)), size = 4, label.y = 10) + scale_fill_manual(values = pal_okabe_ito) + theme(legend.position=&quot;none&quot;) + NULL gg 9.3.3 Import and plot of fig1f (methionine restriction) data Import data_from &lt;- &quot;Dietary methionine influences therapy in mouse cancer models and alters human metabolism&quot; file_name &lt;- &quot;41586_2019_1437_MOESM2_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig1f_wide &lt;- read_excel(file_path, sheet = &quot;f&quot;, range = &quot;B7:R12&quot;, col_names = FALSE) %&gt;% data.table() # convert to data.table setnames(fig1f_wide, old = colnames(fig1f_wide), new = c(&quot;day&quot;, paste0(&quot;Cn_&quot;, 1:8), paste0(&quot;MR_&quot;, 1:8))) fig1f_wide &lt;- transpose(fig1f_wide, make.names = 1, keep.names = &quot;id&quot;) fig1f_wide[, treatment := factor(substr(id, 1, 2))] days &lt;- c(21, 25, 28, 30, 34, 39) fig1f &lt;- melt(fig1f_wide, id.vars &lt;- c(&quot;treatment&quot;, &quot;id&quot;), measure.vars &lt;- as.character(days), variable.name = &quot;day&quot;, value.name = &quot;volume&quot;) fig1f[, day := as.numeric(as.character(day))] # View(fig1f) # qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_line(aes(group = id)) Creating the response plot (the bottom component) fig1f_gg_response &lt;- ggplot(data = fig1f, aes(x = day, y = volume, color = treatment)) + geom_point() + geom_smooth(aes(group = id), method = &quot;lm&quot;, se = FALSE) + xlab(&quot;Day&quot;) + ylab(expression(Tumor~Volume~(mm^3))) + scale_color_manual(values = pal_okabe_ito) + theme_pubr() + theme( legend.position = c(.15, .98), legend.justification = c(&quot;right&quot;, &quot;top&quot;), legend.box.just = &quot;right&quot;, legend.margin = margin(6, 6, 6, 6), legend.title = element_blank() ) + NULL # fig1f_gg_response Creating the effects plot (the top component) m1_coeff_dt &lt;- data.table(term = row.names(m1_coef_table), data.table(m1_coef_table))[2,] %&gt;% clean_names() m1_coeff_dt[ , p_pretty := pvalString(pr_t)] min_bound &lt;- min(m1_coeff_dt[, x2_5_percent]) max_bound &lt;- min(m1_coeff_dt[, x97_5_percent]) y_lo &lt;- min(min_bound+min_bound*0.2, -max_bound) y_hi &lt;- max(max_bound + max_bound*0.2, -min_bound) y_lims &lt;- c(y_lo, y_hi) fig1f_gg_effect &lt;- ggplot(data=m1_coeff_dt, aes(x = term, y = estimate)) + # confidence level of effect geom_errorbar(aes(ymin=x2_5_percent, ymax=x97_5_percent), width=0, color=&quot;black&quot;) + # estimate of effect geom_point(size = 3) + # zero effect geom_hline(yintercept=0, linetype = 2) + # p-value annotate(geom = &quot;text&quot;, label = m1_coeff_dt$p_pretty, x = 1, y = 7.5) + # aesthetics scale_y_continuous(position=&quot;right&quot;) + scale_x_discrete(labels = &quot;MR\\neffect&quot;) + ylab(expression(Growth~(mm^3/day))) + coord_flip(ylim = y_lims) + theme_pubr() + theme(axis.title.y = element_blank()) + NULL #fig1f_gg_effect Combining the response and effects plots into single plot fig1f_gg &lt;- plot_grid(fig1f_gg_effect, fig1f_gg_response, nrow=2, align = &quot;v&quot;, axis = &quot;lr&quot;, rel_heights = c(0.4,1)) fig1f_gg 9.4 Try it 9.4.1 A prediction model from the literature The data come from the top, middle plot of Figure 1e of Parker, B.L., Calkin, A.C., Seldin, M.M., Keating, M.F., Tarling, E.J., Yang, P., Moody, S.C., Liu, Y., Zerenturk, E.J., Needham, E.J. and Miller, M.L., 2019. An integrative systems genetic analysis of mammalian lipid metabolism. Nature, 567(7747), pp.187-193. Public source Source data The researchers built prediction models from a hybrid mouse diversity panel (HMDP) to predict liver lipid levels from measured plasma lipid levels in mice and in humans. The value of the predictor (\\(X\\)) variable for an individual is not a measured value of a single plasma lipid but the predicted value, or score, from the prediction model based on an entire panel of lipid measurements in that individual. The \\(Y\\) variable for the individual is the total measured level across a family of lipids (ceramides, triacylglycerols, diacylglycerols) in the liver of the individual. The question is, how well does the prediction score predict the actual liver level? Use the data for TG (triacylglycerols) (the top, middle plot of Figure 1e). The column D is the “score” from the prediction model using plasma lipid levels. This is the \\(X\\) variable (the column header is “fitted.total.Cer”). The column E is the total measured liver TG, so is the \\(Y\\) variable. Fit the linear model y ~ x. Create a publication-quality plot of liver TG (Y-axis) against score (X-axis) – what the researchers labeled “fitted.total.Cer”. Include \\(R^2\\) on the plot. Advanced – add a 95% prediction interval to the plot (the template code for this is in the Hidden Code section for efig3d) Create a table of expected liver TG and 95% prediction interval of liver TG of the score values (13.5, 14, 14.5, 15, 15.5, 16). Comment on the predictability of liver TG using the plasma scores. ` 9.5 Intuition pumps 9.5.1 Correlation and $R^2 In the code below change the value of n to explore effect on the variability of the estimate. Look at this variability, and the magnitude of the SE, and the magnitude of the p-value. change the value of beta_1 to explore effect on what different slopes and correlations look like. Notice that if the variance is fixed (as in this simulation) the expected slope and expected correlation are equal. (Because I’ve fixed the variance in this simple simulation, this code will fail if abs(beta_1) &gt;= 1). n &lt;- 100 # choose between 3 and 10^5 beta_1 &lt;- 0.6 # choose a value between -0.99 and 0.99 x &lt;- rnorm(n) y &lt;- beta_1*x + sqrt(1-beta_1^2)*rnorm(n) m1 &lt;- lm(y ~ x) slope &lt;- paste(&quot;b_1: &quot;, round(coef(m1)[2], 3)) se &lt;- paste(&quot;SE: &quot;, round(coef(summary(m1))[2,2], 3)) r &lt;- paste(&quot;r: &quot;, round(cor(x,y), 3)) r2 &lt;- paste(&quot;R^2: &quot;, round(summary(m1)$r.squared, 3)) ggscatter(data = data.table(x=x, y=y), x = &quot;x&quot;, y = &quot;y&quot;, add = &quot;reg.line&quot;) + ggtitle(label = paste(slope,se,r,r2,sep=&quot;; &quot;)) "],["oneway.html", "Chapter 10 Linear models with a single, categorical X (“t-tests” and “ANOVA”) 10.1 A linear model with a single factor (categorical Xvariable) estimates the effects of the levels of factor on the response 10.2 Working in R 10.3 Hidden Code", " Chapter 10 Linear models with a single, categorical X (“t-tests” and “ANOVA”) In the traditional hypothesis-testing strategy, researchers use a t-test if the factor variable has only two groups, or an ANOVA followed by post-hoc tests if the factor variable has more than two groups. In the linear-modeling strategy, we fit the same model, regardless of the number of groups. 10.1 A linear model with a single factor (categorical Xvariable) estimates the effects of the levels of factor on the response To introduce a linear model with a single factor (categorical \\(X\\) variable), I’ll use data from a set of experiments designed to measure the effect of the lipid 12,13-diHOME on brown adipose tissue (BAT) thermoregulation and the mechanism of this effect. Lynes, M.D., Leiria, L.O., Lundh, M., Bartelt, A., Shamsi, F., Huang, T.L., Takahashi, H., Hirshman, M.F., Schlein, C., Lee, A. and Baer, L.A., 2017. The cold-induced lipokine 12, 13-diHOME promotes fatty acid transport into brown adipose tissue. Nature medicine, 23(5), pp.631-637. Public source Data source Download the source data files and move to a new folder named “The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue”. Cold temperature and the neurotransmitter/hormone norepinephrine are known to stimulate increased thermogenesis in BAT cells. In this project, the researchers probed the question “what is the pathway that mediates the effect of cold-exposure on BAT thermogenesis?”. In the “discovery” component of this project, the researchers measured plasma levels of 88 lipids with known signaling properties in humans exposed to one hour of both normal (20 °C) and cold temperature (14 °C) temperature. Of the 88 lipids, 12,13-diHOME had the largest response to the cold treatment. The researchers followed this up with experiments on mice. 10.1.1 Example 1 (fig3d) – two treatment levels (“groups”) Let’s start with the experiment in Figure 3d, which was designed to measure the effect of 12,13-diHOME on plasma triglyceride level. If 12,13-diHOME stimulates BAT activity, then levels in the 12,13-diHOME mice should be less than levels in the control mice. 10.1.1.1 Step 1 – Understand the experiment design and the focal comparisons Design: single, categorical X Response variable: \\(\\texttt{serum_tg}\\), A measure of serum triglycerides (mg/dl). \\(\\texttt{serum_tg}\\) is a continuous variable. Factor variable: \\(\\texttt{treatment}\\), with levels: “Vehicle” – injected with saline; the negative control giving the expected response given handling and injection, but no 12,13-diHOME “12,13-diHOME” Contrasts of interest 12,13-diHOME - Vehicle. Estimates the effect of 12,13-diHOME treatment. This is the focal contrast (and the only contrast). 10.1.1.2 Step 2 – import Open the data and, if necessary, wrangle into an analyzable format. The script to import these data is in the section Hidden code below. 10.1.1.3 Step 3 – inspect the data The second step is to examine the data to get a sense of sample size and balance check for biologically implausible outliers that suggest measurement failure, or transcription error (from a notebook, not in a cell) assess outliers for outlier strategy or robust analysis assess reasonable distributions and models for analysis. ggstripchart(data = fig3d, x = &quot;treatment&quot;, y = &quot;serum_tg&quot;, add = c(&quot;mean_sd&quot;) ) There are no obviously implausible data points. A normal distribution model is a good, reasonable start. This can be checked more thoroughly after fitting the model. 10.1.1.4 Step 4 – fit the model fig3d_m1 &lt;- lm(serum_tg ~ treatment, data = fig3d) 10.1.1.5 Step 5 – check the model set.seed(1) # qqPlot(fig3d_m1, id=FALSE) # spreadLevelPlot(fig3d_m1, id=FALSE) ggcheck_the_model(fig3d_m1) The Q-Q plot indicates that the distribution of residuals is well within that expected for a normal sample and there is no cause for concern with inference. The spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is no cause for concern with inference. Write something like this in your .Rmd file following the model check code chunk: “The residuals are well within the range expected from sampling from a Normal distribution. The heterogeneity of the residuals is well within the range expected from sampling from a single distribution.” 10.1.1.6 Step 6 – inference 10.1.1.6.1 coefficient table fig3d_m1_coef &lt;- cbind(coef(summary(fig3d_m1)), confint(fig3d_m1)) kable(fig3d_m1_coef, digits = c(1,2,1,4,1,1)) %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 42.6 1.67 25.6 0.0000 38.9 46.3 treatment12,13-diHOME -7.2 2.36 -3.0 0.0125 -12.4 -1.9 10.1.1.6.2 emmeans table fig3d_m1_emm &lt;- emmeans(fig3d_m1, specs = &quot;treatment&quot;) kable(fig3d_m1_emm, digits = c(1,1,2,0,1,1)) %&gt;% kable_styling() treatment emmean SE df lower.CL upper.CL Vehicle 42.6 1.67 10 38.9 46.3 12,13-diHOME 35.5 1.67 10 31.7 39.2 10.1.1.6.3 contrasts table fig3d_m1_pairs &lt;- contrast(fig3d_m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) kable(fig3d_m1_pairs, digits = c(1,1,1,0,1,1,2,4)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value (12,13-diHOME) - Vehicle -7.2 2.4 10 -12.4 -1.9 -3.04 0.0125 10.1.1.7 Step 6 – plot the model The norm in bench biology research is a response plot. gg1 &lt;- ggplot_the_response( fig3d_m1, fig3d_m1_emm, fig3d_m1_pairs, legend_position = &quot;none&quot;, y_label = &quot;Serum TG (µg/dL)&quot;, palette = pal_okabe_ito_blue ) gg1 If the researchers want to explitly communicate more about the treatment effect, then they should “plot the model”. gg2 &lt;- ggplot_the_model( fig3d_m1, fig3d_m1_emm, fig3d_m1_pairs, legend_position = &quot;none&quot;, y_label = &quot;Serum TG (µg/dL)&quot;, effect_label = &quot;Effects (µg/dL)&quot;, palette = pal_okabe_ito_blue, rel_heights = c(0.5,1) ) gg2 10.1.1.8 Step 7 – report the model results Different ways of reporting the results in increasing order of not making claims that are not evidenced by the statistical analysis, “12,13-diHOME reduced serum TG” “12,13-diHOME reduced serum TG (Estimate = -7.17 µg/dL; 95% CI: -12.4, -1.9; p = 0.012)” “The estimated effect of 12,13-diHOME on serum TG is -7.17 µg/dL (95% CI: -12.4, -1.9, \\(p = 0.012\\)).” Don’t do this “12,13-diHOME significantly reduced serum TG (\\(p = 0.012\\))” Why this is problematic: Significance applies to a p-value and not the effect. In English usage, “significant” means “large” or “important” and the p-value is not good evidence for either the size of an effect or the importance of an effect (see the p-value chapter). We interpret the size of effect from the estimated effect size and CI and the importance of an effect from knowledge of the physiological consequences of TG reduction over the range of the CI. 10.1.2 Understanding the analysis with two treatment levels The variable \\(\\texttt{treatment}\\) in the Figure 3d mouse experiment, is a single, categorical \\(X\\) variable. In a linear model, categorical variables are called factors. \\(\\texttt{treatment}\\) can take two different values, “Vehicle” and “12,13-diHOME”. The different values in a factor are the factor levels (or just “levels”). “Levels” is a strange usage of this word; a less formal name for levels is “groups”. In a Nominal categorical factor, the levels have no units and are unordered, even if the variable is based on a numeric measurement. For example, I might design an experiment in which mice are randomly assigned to one of three treatments: one hour at 14 °C, one hour at 18 °C, or one hour at 26 °C. If I model this treatment as a nominal categorical factor, then I simply have three levels. While I would certainly choose to arrange these levels in a meaningful way in a plot, for the analysis itself, these levels have no units and there is no order. Ordinal categorical factors have levels that are ordered but there is no information on relative distance. The treatment at 18 °C is not more similar to 14 °C than to 26 °C. Nominal categorical factors is the default in R and how all factors are analyzed in this text. 10.1.2.1 Linear models are regression models The linear model fit to the serum TG data is \\[\\begin{align} serum\\_tg &amp;= treatment + \\varepsilon\\\\ \\varepsilon &amp;\\sim N(0, \\sigma^2) \\tag{10.1} \\end{align}\\] This notation is potentially confusing because the variable \\(\\texttt{treatment}\\) is a factor containing the words “Vehicle” and “12,13-diHOME” and not numbers. The linear model in (10.1) can be specified using notation for a regression model using \\[\\begin{align} serum\\_tg &amp;= \\beta_0 + \\beta_1 treatment_{12,13-diHOME} + \\varepsilon\\\\ \\varepsilon &amp;\\sim N(0, \\sigma^2) \\tag{10.2} \\end{align}\\] Model (10.2) is a regression model where \\(treatment_{12,13-diHOME}\\) is not the variable \\(\\texttt{treatment}\\), containing the words “Vehicle” or “12,13-diHOME” but a numeric variable that indicates membership in the group “12,13-diHOME”. This variable contains the number 1 if the mouse belongs to “12,13-diHOME” and the number 0 if the mouse doesn’t belong to “12,13-diHOME”. \\(treatment_{12,13-diHOME}\\) is known as an indicator variable because it indicates group membership. There are several ways of coding indicator variables and the way described here is called dummy or treatment coding. Dummy-coded indicator variables are sometimes called dummy variables. The lm function creates indicator variables under the table, in something called the model matrix. X &lt;- model.matrix(~ treatment, data = fig3d) N &lt;- nrow(X) X[1:N,] ## (Intercept) treatment12,13-diHOME ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 0 ## 6 1 0 ## 7 1 1 ## 8 1 1 ## 9 1 1 ## 10 1 1 ## 11 1 1 ## 12 1 1 The columns of the model matrix are the names of the model terms in the fit model. R names dummy variables by combining the names of the factor and the name of the level within the factor. So the \\(X\\) variable that R creates in the model matrix for the fit linear model in model (10.2) is \\(treatment12,13-diHOME\\). You can see these names as terms in the coefficient table of the fit model. There are alternatives to dummy coding for creating indicator variables. Dummy coding is the default in R and it makes sense when thinking about experimental data with an obvious control level. I also like the interpretation of a “interaction effect” using Dummy coding. The classical coding for ANOVA is deviation effect coding, which creates coefficients that are deviations from the grand mean. In contrast to R, Deviation coding is the default in many statistical software packages including SAS, SPSS, and JMP. The method of coding can make a difference in an ANOVA table. Watch out for this – I’ve found several published papers where the researchers used the default dummy coding but interpreted the ANOVA table as if they had used deviation coding. This is both getting ahead of ourselves and somewhat moot, because I don’t advocate reporting ANOVA tables. Recall from stats 101 that the slope of \\(X\\) in the model \\(Y = b_0 + b_1 X\\) is \\(b_1 = \\frac{\\textrm{COV}(X,Y)}{\\textrm{VAR}(X)}\\). This can be generalized using the equation \\[\\begin{equation} \\mathbf{b} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\end{equation}\\] where \\(\\mathbf{X}\\) is the model matrix containing a column for an intercept, columns for all indicator variables, and columns for all numeric covariates. \\(\\mathbf{b}\\) is a vector containing the model coefficients, including the intercept in the first element. The first part of the RHS (\\(\\mathbf{X}^\\top \\mathbf{X}\\)) is a matrix of the “sums of squares and cross-products” of the columns of \\(\\mathbf{X}\\). Dividing each element of this matrix by \\(N-1\\) gives us the covariance matrix of the \\(\\mathbf{X}\\), which contains the variances of the \\(X\\) columns along the diagonal, so this component has the role of the denominator in the stats 101 equation. Matrix algebra doesn’t do division, so the inverse of this matrix is multiplied by the second part. The second part or the RHS (\\(\\mathbf{X}^\\top \\mathbf{y}\\)) is a vector containing the cross-products of each column of \\(\\mathbf{X}\\) with \\(\\mathbf{y}\\). Dividing each element of this vector by \\(N-1\\) gives us the covariances of each \\(X\\) with \\(y\\), so this component has the role of the numerator in the stats 101 equation. Self-learning. lm fits the model y ~ X where X is the model matrix. Fit the model using the standard formula and the model using the model matrix. The coefficient table should be the same. m1 &lt;- lm(serum_tg ~ treatment, data = fig3d) X &lt;- model.matrix(~ treatment, data = fig3d) m2 &lt;- lm(serum_tg ~ X, data = fig3d) coef(summary(m1)) coef(summary(m2)) 10.1.2.2 The Estimates in the coefficient table are estimates of the parameters of the linear model fit to the data. Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 42.6 1.67 25.6 0.0000 38.9 46.3 treatment12,13-diHOME -7.2 2.36 -3.0 0.0125 -12.4 -1.9 The row names of the coefficient table are the column names of the model matrix. These are the model terms. There are two terms (two rows) because there are two parameters in the regression model (10.2). The values in the column \\(\\texttt{Estimate}\\) in the coefficient table are the estimates of the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\). These estimates are the coefficients of the fit model, \\(b_0\\) and \\(b_1\\). 10.1.2.3 The coefficients of a linear model using dummy coding have a useful interpretation Table 10.1: Understanding model coefficients of a linear model with a single treatment variable with two groups. The means in the interpretation column are conditional means. coefficient parameter model term interpretation \\(b_0\\) \\(\\beta_0\\) (Intercept) \\(\\overline{Vehicle}\\) \\(b_1\\) \\(\\beta_1\\) treatment12,13-diHOME \\(\\overline{12,13\\;diHOME} - \\overline{Vehicle}\\) It is important to understand the interpretation of the coefficients of the fit linear model (10.1) (Table 10.1). The coefficient \\(b_0\\) is the is the conditional mean of the response for the reference level, which is “Vehicle”. Remember that a conditional mean is the mean of a group that all have the same value for one or more \\(X\\) variables. The coefficient \\(b_1\\) is the difference between the conditional means of the “12,13-diHOME” level and the reference (“Vehicle”) level: \\[\\begin{equation} \\mathrm{E}[serum\\_tg|treatment = \\texttt{&quot;12,13-diHOME&quot;}] - \\mathrm{E}[serum\\_tg|treatment = \\texttt{&quot;Vehicle&quot;}] \\end{equation}\\] Because there are no additional covariates in model, this difference is equal to the difference between the sample means \\(\\bar{Y}_{12,13-diHOME} - \\bar{Y}_{Vehicle}\\). The direction of this difference is important – it is non-reference level minus the reference level. The estimate \\(b_1\\) is the effect that we are interested in. Specifically, it is the measured effect of 12,13-diHOME on serum TG. When we inject 12,13-diHOME, we find the mean serum TG decreased by -7.2 µg/dL relative to the mean serum TG in the mice that were injected with saline. Importantly, the reference level is not a property of an experiment but is set by whomever is analyzing the data. Since the non-reference estimates are differences in means, it often makes sense to set the “control” treatment level as the reference level. Many beginners mistakenly memorize the coefficient \\(b_1\\) to equal the mean of the non-reference group (“12,13-diHOME”). Don’t do this. In a regression model, only \\(b_0\\) is a mean. The coefficient \\(b_1\\) in model (10.2) is a difference in means. Figure 10.1: What the coefficients of a linear model with a single categorical X mean. The means of the two treatment levels for the serum TG data are shown with the large, filled circles and the dashed lines. The intercept (\\(b_0\\)) is the mean of the reference treatment level (“Vehicle”). The coefficient \\(b_1\\) is the difference between the treatment level’s mean and the reference mean. As with a linear model with a continuous \\(X\\), the coefficient \\(b_1\\) is an effect. A geometric interpretation of the coefficients is illustrated in Figure 10.1. \\(b_0\\) is the conditional mean of the reference level (“Vehicle”) and is an estimate of \\(\\beta_0\\), the true, conditional mean of the population. \\(b_1\\) is the difference in the conditional means of the first non-reference level (“12,13-diHOME”) and the reference level (“Vehicle”) and is an estimate of \\(\\beta_1\\), the true difference in the conditional means of the population with and without the treatment 12,13-diHOME. tl;dr. What is a population? In the experimental biology examples in this text, we might consider the population as a very idealized, infinitely large set of mice, or fish, or fruit flies, or communities from which our sample is a reasonably representative subset. For the experiments in the 12,13-diHOME study, the population might be conceived of as the hypothetical, infinitely large set of 12-week-old, male, C57BL/6J mice, raised in the mouse facility at Joslin Diabetes Center. An even more abstract way to way to think about what the population could be is the infinitely large set of values that could generated by the linear model. 10.1.2.4 Better know the coefficient table fig3d_m1_coef %&gt;% kable(digits = c(1,2,1,4,1,1)) %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 42.6 1.67 25.6 0.0000 38.9 46.3 treatment12,13-diHOME -7.2 2.36 -3.0 0.0125 -12.4 -1.9 The \\(\\texttt{(Intercept)}\\) row contains the statistics for \\(b_0\\) (the estimate of \\(\\beta_0\\)). Remember that \\(b_0\\) is the conditional mean of the reference treatment (“Vehicle”). The \\(\\texttt{treatment12,13-diHOME}\\) row contains the statistics for \\(b_1\\) (the estimate of \\(\\beta_1\\)). Remember that \\(b_1\\) is the difference in conditional means of the groups “12,13-diHOME” and “Vehicle”. The column \\(\\texttt{Estimate}\\) contains the model coefficients, which are estimates of the parameters. The column \\(\\texttt{Std. Error}\\) contains the model SEs of the coefficients. The SE of \\(\\texttt{(Intercept)}\\) is a standard error of a mean (SEM). The SE of \\(\\texttt{treatment12,13-diHOME}\\) is a standard error of a difference (SED). The column \\(\\texttt{t value}\\) contains the test statistic of the coefficients. This value is the ratio \\(\\frac{Estimate}{SE}\\). For this model, we are only interested in the test statistic for \\(b_1\\). Effectively, we will never be interested in the test statistic for \\(b_0\\) because the mean of a group will never be zero. The column \\(\\texttt{Pr(&gt;|t|)}\\) contains the p-values for the test statistic of the coefficients. For this model, and all models in this text, we are only interested in the p-value for the non-intercept coefficients. The columns \\(\\texttt{2.5 %}\\) and \\(\\texttt{97.5 %}\\) contain the lower and upper limits of the 95% confidence interval of the estimate. 10.1.2.5 The emmeans table is a table of modeled means and inferential statistics (#tab:fig3d_m1_emm_kable)Estimated marginal means table for model fig3d_m1. treatment emmean SE df lower.CL upper.CL Vehicle 42.6 1.67 10 38.9 46.3 12,13-diHOME 35.5 1.67 10 31.7 39.2 The table of marginal means for the model fit to the fig3d serum TG data (Model (10.1)) is given in Table @ref(tab:fig3d_m1_emm_kable). The table of marginal means gives the **modeled* mean, standard error and confidence interval for all specified groups. There is no test-statistic with a p-value because there is no significance test. In this text, I’ll refer to this table as the “emmeans table”, since it is the output from the emmeans function (“em” is the abbreviation for “estimated marginal”). I’ll use “modeled means” to refer to the means themselves as these are the estimate of means from a fit linear model. A marginal mean is the mean over a set of conditional means. For example, if a treatment factor has three levels, the conditional means are the means for each level and the marginal mean is the mean of the three means. Or, if the conditional means are the expected values given a continous covariate, the marginal mean is the expected value at the mean of covariate. The specified emmeans table of the fig3d data is not too exciting because it simply contains the conditional means – the values are not marginalized over any \\(X\\). Because the emmeans table contains different sorts of means (conditional, marginal, adjusted), this text will generally refer to the means in this table as “modeled means”. Table 10.2: The emmeans table contains modeled means, SE, and CIs. treatment emmean SE df lower.CL upper.CL Vehicle 42.62004 1.66723 10 38.90523 46.33485 12,13-diHOME 35.45233 1.66723 10 31.73752 39.16714 Table 10.3: A summary table contains sampled means, SE, and CIs. treatment mean SE lower.CL upper.CL Vehicle 42.62004 1.77325 38.66899 46.57109 12,13-diHOME 35.45233 1.55398 31.98984 38.91482 The means in the emmeans table are modeled means. Here, and for many linear models, these will be equal to the sampled means. This will not be the case in models with one or more continuous covariates, or if marginal means are explicitly specified. Unlike the modeled means, the modeled standard errors and confidence intervals will, effectively, never equal sample standard errors and confidence intervals. In many models, plots using sample statistics can lead to very deceiving inference of differences between groups. This is why this text advocates plotting the model – using modeled means and confidence intervals in plots. It is exceptionally important to understand the difference between the means, SEs, and CIs in the emmeans table and the statistics of the same name in a summary table of the data. The statistics in a summary table are sampled means, SEs, and CIs – these statistics are computed for the group using only the data in the group. To understand modeled SEs and CIs, recall that the standard error of a sample mean is \\(\\frac{s}{\\sqrt{n}}\\), where \\(s\\) is the sample standard deviation and \\(n\\) is the sample size in the group. The computation of the SE in the emmeans table uses the same equation, except the numerator is not the sample standard deviation of the group but the model standard deviation, which is an estimate of the true standard deviation \\(\\sigma\\). As with the sample SE, the denominator for the modeled SE is the sample size \\(n\\) for the group. Since the numerator of the modeled SE is the same for all groups, the modeled SE will be the same in all groups that have the same sample size, as seen in the marginal means table for the model fit to the Figure 3d data. This is not true for sampled SEs, since sampled standard deviations will always differ. It may seem odd to use a common standard deviation in the computation of the modeled SEs. It is not. Remember that an assumption of the linear model is homogeneity of variances – that all residuals \\(e_i\\) are drawn from the same distribution (\\(N(0, \\sigma^2)\\)) (a “single hat”) regardless of group. The model standard deviation \\(\\hat{\\sigma}\\) is the estimate of the square root of the variance of this distribution. Given this interpretation, it is useful to think of each sample standard deviation as an estimate of \\(\\sigma\\) (the linear model assumes that all differences among the sample standard deviations are due entirely to sampling). The model standard deviation is a more precise estimate of \\(\\sigma\\) since it is computed from a larger sample (all \\(N\\) residuals). The model standard deviation is called the “pooled” standard deviation in the ANOVA literature and is computed as a sample-size weighted average of the sample standard deviations. The modeled standard error of the mean uses the estimate of \\(\\sigma\\) from the fit model. This estimate is \\[\\begin{equation} \\hat{\\sigma} = \\sqrt{\\frac{\\sum{(y_i - \\hat{y}_i)^2}}{df}} \\end{equation}\\] Create a code chunk that computes this. Recall that \\((y_i - \\hat{y}_i)\\) is the set of residuals from the model, which can be extracted using residuals(fit) where “fit” is the fit model object. \\(df\\) is the model degrees of freedom, which is \\(N-k\\), where \\(N\\) is the total sample size and \\(k\\) is the number of parameters that are fit. This makes sense – for the sample variance there is one parameter that is fit, the mean of the group. In model fig3d_m1, there are two parameters that are fit, the intercept and the coefficient of treatment12,13-diHOME. 10.1.2.6 Estimates of the effects are in the contrasts table contrast estimate SE df lower.CL upper.CL t.ratio p.value (12,13-diHOME) - Vehicle -7.2 2.36 10 -12.4 -1.9 -3 0.012 This table is important for reporting treatment effects and CIs and for plotting the model. A contrast is a difference in means. With only two treatment levels, the table of contrasts doesn’t give any more information than the coefficient table – the single contrast is the coefficient \\(b_1\\) in the coefficient table. Nevertheless, I advocate computing this table to stay consistent and because the script (or function) to plot the model uses this table and not the coefficient table. The value in the column \\(\\texttt{Estimate}\\) is the mean of the non-reference group (“12,13-diHOME”) minus the mean of the reference group (“Vehicle”). The value in the “SE” column is the standard error of the difference (SED), specifically the difference in the estimate column. This SE is computed using the model standard deviation \\(\\sigma\\). The values in the “lower.CL” and “upper.CL” columns are the bounds of the 95% confidence interval of the estimate. Remember (from Chapter ??) to think of this interval as containing potential values of the true parameter (the true difference in means between the two groups) that are reasonably compatible with the data. Don’t think of the interval as having 95% probability of containing the true effect. Remember that a confidence interval applies to the procedure and not a parameter – 95% of the CIs from hypothetical, replicate experiments that meet all the assumptions used to compute the CI will include the true effect. The columns “t.ratio” and “p.value” contains the t and p values of the significance (not hypothesis!) test of the estimate. The t-statistic is the ratio of the estimate to the SE of the estimate (use the console to confirm this given the values in the table). It is a signal (the estimate) to noise (SE of the estimate) ratio. The p-value is the probability of sampling from normal distribution with the observed standard deviation, randomly assigning the sampled values to either “Vehicle” or “12,13-diHOME”, fitting the linear model, and observing a t-value as or more extreme than the observed t. A very small p-value is consistent with the experiment “not sampling from distributions with the same mean” – meaning that adding a treatment affects the mean of the distribution. This is the logic used to infer a treatment effect. Unfortunately, it is also consistent with the experiment not approximating other conditions of the model, including non-random assignment, non-independence, non-normal conditional responses, and variance heterogeneity. It is up to the rigorous researcher to be sure that these other model conditions are approximated or “good enough” to use the p-value to infer a treatment effect on the mean. 10.1.2.7 t and p from the contrasts table – when there are only two levels in \\(X\\) – are the same as t and p from a t-test Compare coefficient table: m1 &lt;- lm(serum_tg ~ treatment, data = fig3d) coef(summary(m1)) %&gt;% kable() %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 42.62004 1.667226 25.563447 0.000000 treatment12,13-diHOME -7.16771 2.357813 -3.039982 0.012463 contrast table: emmeans(m1, specs = &quot;treatment&quot;) %&gt;% contrast(method = &quot;revpairwise&quot;) %&gt;% kable() %&gt;% kable_styling() contrast estimate SE df t.ratio p.value (12,13-diHOME) - Vehicle -7.16771 2.357813 10 -3.039982 0.012463 t-test: m2 &lt;- t.test(fig3d[treatment == &quot;12,13-diHOME&quot;, serum_tg], fig3d[treatment == &quot;Vehicle&quot;, serum_tg], var.equal = TRUE) glance(m2) %&gt;% # glance is from the broom package kable() %&gt;% kable_styling() estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative -7.16771 35.45233 42.62004 -3.039982 0.012463 10 -12.42125 -1.914175 Two Sample t-test two.sided Notes The default t.test in R is the Welch t-test for heterogenous variance. To compute the Student t-test, use var.equal = TRUE. The “statistic” in the t-test output contains the t-value of the t-test. It is precisely the same as the t-statistic in the coefficient table and the contrast table. The p-values in all three tables are precisely the same. The t and p values for the t-test are the same as those for the linear model, because the t-test is a specific case of the linear model. Reasons to abandon classic t-tests and learn the linear modeling strategy include A linear modeling strategy encourages researchers to think about the effect and uncertainty in the effect and not just a p-value. The linear model is nearly infinitely flexible and expandible while the t-test is limited to a few variations. There is rarely a reason to ever use the t.test() function. Throw the function away. Ignore web pages that teach you to use it. The t-test is easy to learn, which encourages its overuse. If your only tool is a t-test, every problem looks like a comparison between two-means. 10.1.3 Example 2 – three treatment levels (“groups”) 10.1.3.1 Understand the experiment design The data come from the experiment reported in Figure 2a of the 12,13-diHOME article described above. This experiment was designed to probe the hypothesis that 12,13-diHOME is a mediator of known stimulators of increased BAT activity (exposure to cold temperature and sympathetic nervous system activation). Mice were assigned to control (30 °C), one-hour exposure to 4 °C, or 30 minute norepinephrine (NE) treatment level (NE is the neurotransmitter of the sympathetic neurons targeting peripheral tissues). design: single, categorical X with three levels. response variable: \\(\\texttt{diHOME}\\), the serum concentration of 12,13-diHOME. a continuous variable. factor variable: \\(\\texttt{treatment}\\), with levels: “Control” – the negative control. We expect diHOME to be low relative to the two treated conditions. “Cold” – focal treatment 1. Given the working model of 12,13-diHome as a mediator between stimulation and BAT, this response should be relatively high compared to Control. In the archived data, this group is “1 hour cold”. “NE” – focal treatment 2. Given the working model of 12,13-diHome as a mediator between stimulation and BAT, this response should be relatively high compared to Control. In the archived data, this group is “30 min NE”. planned contrasts Cold - Control – If diHOME is a mediator of cold, then difference should be positive. NE - Control – If diHOME is a mediator of NE, then difference should be positive. The contrast Cold - NE is not of interest. 10.1.3.2 fit the model fig2a_m1 &lt;- lm(diHOME ~ treatment, data = fig2a) 10.1.3.3 check the model set.seed(1) ggcheck_the_model(fig2a_m1) The Q-Q plot indicates that the distribution of residuals is within that expected for a normal sample. The spread-location plot shows no conspicuous trend in how the spread changes with the conditonal mean. There is little cause for concern with inference from a linear model. Write something like this in your .Rmd file following the model check code chunk: “The residuals are within the range expected from sampling from a Normal distribution. The heterogeneity of the residuals is well within the range expected from sampling from a single distribution.” 10.1.3.4 Inference from the model 10.1.3.4.1 coefficient table fig2a_m1_coef &lt;- cbind(coef(summary(fig2a_m1)), confint(fig2a_m1)) fig2a_m1_coef %&gt;% kable(digits = c(1,2,1,4,1,1)) %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 12.0 3.08 3.9 0.0016 5.4 18.6 treatmentCold 7.1 4.57 1.6 0.1405 -2.7 16.9 treatmentNE 14.8 4.36 3.4 0.0044 5.4 24.1 10.1.3.4.2 emmeans table fig2a_m1_emm &lt;- emmeans(fig2a_m1, specs = &quot;treatment&quot;) fig2a_m1_emm %&gt;% kable(digits = c(1,1,2,0,1,1)) %&gt;% kable_styling() treatment emmean SE df lower.CL upper.CL Control 12.0 3.08 14 5.4 18.6 Cold 19.2 3.38 14 11.9 26.4 NE 26.8 3.08 14 20.2 33.4 10.1.3.4.3 contrasts table fig2a_m1_planned &lt;- contrast(fig2a_m1_emm, method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;, level = 0.95) %&gt;% summary(infer = TRUE) fig2a_m1_planned %&gt;% kable(digits = c(1,1,1,0,1,1,2,4)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Cold - Control 7.1 4.6 14 -2.7 16.9 1.56 0.1405 NE - Control 14.8 4.4 14 5.4 24.1 3.40 0.0044 10.1.3.5 plot the model ggplot_the_model( fig2a_m1, fig2a_m1_emm, fig2a_m1_planned, legend_position = &quot;none&quot;, y_label = &quot;12,13-diHOME (pmol/mL)&quot;, effect_label = &quot;Effects (pmol/mL)&quot;, palette = pal_okabe_ito_blue, rel_heights = c(0.5,1) ) 10.1.3.6 Report the model results Different ways of reporting the results in increasing order of making claims that are evidenced by the statistical analysis, “NE exposure increased 12,13-diHOME but the effect of cold exposure on 12,13-diHOME is not clear”. “The estimated effect of NE exposure is consistent with a NE-induced increase in 12,13-diHOME while the estimated effect of cold exposure is less clear”. “The estimated effect of NE exposure is consistent with a NE-induced increase in 12,13-diHOME (Estimate = 14.8 pmol/mL; 95% CI: 5.4, 24.1; \\(p = 0.004\\)) while the estimated effect of cold exposure is less clear (Estimate = 7.1 pmol/mL; 95% CI: -2.7, 16.9; \\(p = 0.14\\))”. The first statement makes the definitive claim that NE causes the increase but there is not unreasonable probability of this magnitude of effect occurring by random sampling. In addition the experiment could be infected by experiment implementation decisions that make this p-value unreliable. The second statement makes a tentative claim. The third statement adds the statistics that provide the evidence for the tentative claim. The statistics could be moved to some combination of the figure, the figure caption, and a supplement. Don’t do this NE exposures significantly increased 12,13-diHOME (\\(p = 0.004\\))” There is no effect of cold exposure on 12,13-diHOME (\\(p = 0.14\\))” Why the first statement is problematic: Significance applies to a p-value and not the effect. In English usage, “significant” means “large” or “important” and the p-value is not good evidence for either the size of an effect or the importance of an effect (see the p-value chapter). We interpret the size of effect from the estimated effect size and CI and the importance of an effect from knowledge of the physiological consequences of TG reduction over the range of the CI. Why the second statement is problematic: p &gt; 0.05, or a high p-value more generally, is not evidence of no effect because a p-value (or 1 - p) does not give the probability that a treatment effect is zero. One could use an equivalence test to give [the probability that an effect is less than some physiologically meaningful magnitude][https://journals.sagepub.com/doi/abs/10.1177/1948550617697177] 10.1.4 Understanding the analysis with three (or more) treatment levels 10.1.4.1 Better know the coefficient table The fit regression model for the data in Figure 2a is \\[\\begin{equation} diHOME_i = b_0 + b_1 treatment_{Cold,i} + b_2 treatment_{NE,i} + e_i \\tag{10.3} \\end{equation}\\] The coefficients of the model are in the \\(\\texttt{Estimate}\\) column of the coefficient table. Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 12.0 3.1 3.9 0 5.4 18.6 treatmentCold 7.1 4.6 1.6 0 -2.7 16.9 treatmentNE 14.8 4.4 3.4 0 5.4 24.1 The \\(\\texttt{(Intercept)}\\) row contains the statistics for \\(b_0\\) (the estimate of \\(\\beta_0\\)). Here, \\(b_0\\) is the mean of the reference group, which is “Control”. The \\(\\texttt{treatmentCold}\\) row contains the statistics for \\(b_1\\) (the estimate of \\(\\beta_1\\)). Here, \\(b_1\\) is the difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{&quot;Cold&quot;}] - \\mathrm{E}[diHOME|treatment = \\texttt{&quot;Control&quot;}]\\). This difference in conditional means is equal to the difference in the sample means of the two groups for this model because there are no additional covariates in the model. The \\(\\texttt{treatmentNE}\\) row contains the statistics for \\(b_2\\) (the estimate of \\(\\beta_2\\)). Here, \\(b_2\\) is the difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{&quot;NE&quot;}] - \\mathrm{E}[diHOME|treatment = \\texttt{&quot;Control&quot;}]\\). Do not make the mistake in thinking that the value in \\(\\texttt{Estimate}\\) is the mean of the “NE” group. The number of non-intercept coefficients generalizes to any number of levels of the factor variable. If there are \\(k\\) levels of the factor, there are \\(k-1\\) indicator variables, each with its own coefficient (\\(b_1\\) through \\(b_{k-1}\\)) estimating the effect of that treatment level relative to the control (if using dummy coding). Again – Do not make the mistake in thinking that the values in \\(\\texttt{Estimate}\\) for the \\(\\texttt{treatmentCold}\\) and \\(\\texttt{treatmentNE}\\) rows are the means of the “Cold” and “NE” groups. These coefficients are differences in means. And, to emphasize further understanding of these coefficients, both \\(b_1\\) and \\(b_2\\) are “slopes”. Don’t visualize this as a single line from the control mean through both non-control means. Slopes is plural – there are two regression lines. \\(b_1\\) is the slope of the line from the control mean to the “Cold” mean while \\(b_2\\) is the slope of the line from the control mean to the “NE” mean. The numerator of each slope is the difference between that group’s mean and the control mean. The denominator of each slope is 1 (because each has the value 1 when the row is assigned to that group). Two understand the names of the model terms, it’s useful to recall the order of the factor levels of \\(\\texttt{treatment}\\), which is levels(fig2a$treatment) ## [1] &quot;Control&quot; &quot;Cold&quot; &quot;NE&quot; Given this ordering, the lm function creates a regression model with an intercept column for the “Control” group (because the first group in the list is the reference level), an indicator variable for the “Cold” group called treatmentCold, and an indicator variable for the “NE” group called treatmentNE. We can see these model names by peeking at the model matrix of the fit model fig2a_m1_X &lt;- model.matrix(fig2a_m1) head(fig2a_m1_X) ## (Intercept) treatmentCold treatmentNE ## 1 1 0 0 ## 2 1 1 0 ## 3 1 0 1 ## 4 1 0 0 ## 5 1 1 0 ## 6 1 0 1 The column \\(\\texttt{treatmentCold}\\) is a dummy-coded indicator variable containing the number 1, if the individual is in the “Cold” group, or the number 0, otherwise. The column \\(\\texttt{treatmentNE}\\) is a dummy-coded indicator variable containing the number 1, if the individual is in the “NE” group, or the number 0, otherwise. The model coefficients, parameters, model term, and interpretation are summarized in the following table. Table 10.4: Understanding model coefficients of a linear model with a single treatment variable with three groups. The means in the interpreation column are conditional means. coefficient parameter model term interpretation \\(b_0\\) \\(\\beta_0\\) (Intercept) \\(\\overline{Control}\\) \\(b_1\\) \\(\\beta_1\\) treatmentCold \\(\\overline{1\\;hour\\;cold} - \\overline{Control}\\) \\(b_2\\) \\(\\beta_2\\) treatmentNE \\(\\overline{30\\;min\\;NE} - \\overline{Control}\\) 10.1.4.2 The emmeans table treatment emmean SE df lower.CL upper.CL Control 12.0 3.08 14 5.4 18.6 Cold 19.2 3.38 14 11.9 26.4 NE 26.8 3.08 14 20.2 33.4 This table is important for reporting means and CIs and for plotting the model. As in example 1, the modeled means in the column “emmean” are the sample means of each group (what you would compute if you simply computed the mean for that group). Again, this is true for this model, but is not generally true. And again, as in example 1, the SE for each mean is not the sample SE but the modeled SE – the numerator is the estimate of \\(\\sigma\\) from the fit model, which includes residuals from all groups combined. These are the SEs that you should report because it is these SEs that are used to compute the p-value and CI that you report, that is, they tell the same “story”. The SE for the “Cold” group is a bit higher because the sample size \\(n\\) for this group is smaller by 1. 10.1.4.3 The contrasts table contrast estimate SE df lower.CL upper.CL t.ratio p.value Cold - Control 7.1 4.57 14 -2.7 16.9 1.6 0.1405 NE - Control 14.8 4.36 14 5.4 24.1 3.4 0.0044 This table is important for reporting treatment effects and CIs and for plotting the model. A contrast is a difference in means. The contrast table here has no more information than is in the coefficient table, but that is not generally true for models with treatment factors with more than two groups. In “Working in R” below, I show how to compute a contrast table with all pairwise comparisons (contrasts between all possible pairings of the groups) The column \\(\\texttt{Contrast}\\) contains the names of the contrasts. Note that the name gives the direction of the difference. The values in the column \\(\\texttt{estimate}\\) are the contrasts. These are the differences in the conditional means of the groups identified in the \\(\\texttt{Contrast}\\) column. These are the effects that we are interested in. The value in the “SE” column is the standard error of the difference (SED) of each contrast. This SE is computed using the model standard deviation \\(\\sigma\\). The values in the “lower.CL” and “upper.CL” columns are the bounds of the 95% confidence interval of the estimate. Remember (from Chapter ??) to think of this interval as containing potential values of the true parameter (the true difference in means between the two groups) that are reasonably compatible with the data. Don’t think of the interval as having 95% probability of containing the true effect. Remember that a confidence interval applies to the procedure and not a parameter – 95% of the CIs from hypothetical, replicate experiments that meet all the assumptions used to compute the CI will include the true effect. The columns “t.ratio” and “p.value” contains the t and p values of the significance (not hypothesis!) test of the estimate. The t-statistic is the ratio of the estimate to the SE of the estimate (use the console to confirm this given the values in the table). It is a signal (the estimate) to noise (SE of the estimate) ratio. The p-value is the probability of sampling from normal distribution with the observed standard deviation, randomly assigning the sampled values to the three groups (using the original sample sizes for each), fitting the linear model, and observing a t-value as or more extreme than the observed t. A very small p-value is consistent with the experiment “not sampling from distributions with the same mean” – meaning that adding a treatment affects the mean of the distribution. This is the logic used to infer a treatment effect. Unfortunately, it is also consistent with the experiment not approximating other conditions of the model, including non-random assignment, non-independence, non-normal conditional responses, and variance heterogeneity. It is up to the rigorous researcher to be sure that these other model conditions are approximated or “good enough” to use the p-value to infer a treatment effect on the mean. 10.1.4.4 t and p from the contrasts table – when there are more than two levels in \\(X\\) – are not the same as those from pairwise t-tests among pairs of groups fig2a_m1_pairs &lt;- contrast(fig2a_m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) The chunk above computes a contrast table that includes comparisons of all pairs of groups in the factor \\(\\texttt{treatment}\\) (this adds a 3rd comparison to the contrast table of planned comparisons above). The t-tests for the contrasts are derived from a single fit linear model. In contrast to the analysis in the chunk above, researchers commonly fit separate t-tests for each pair of treatment levels. # classic t-test test1 &lt;- t.test(fig2a[treatment == &quot;Cold&quot;, diHOME], fig2a[treatment == &quot;Control&quot;, diHOME], var.equal = TRUE) test2 &lt;- t.test(fig2a[treatment == &quot;NE&quot;, diHOME], fig2a[treatment == &quot;Control&quot;, diHOME], var.equal = TRUE) test3 &lt;- t.test(fig2a[treatment == &quot;NE&quot;, diHOME], fig2a[treatment == &quot;Cold&quot;, diHOME], var.equal = TRUE) Notes Again, the default t.test in R is the Welch t-test for heterogenous variance. To compute the Student t-test, use var.equal = TRUE To see the full t.test output, type “test1” into the console. Compare the t and p values from the three independent tests with the t and p-values from the single linear model. contrast t (lm) p (lm) t (t-test) p (t-test) Cold - Control 1.562324 0.1405278 2.415122 0.0389207 NE - Cold 1.674696 0.1161773 1.380666 0.2007006 NE - Control 3.395015 0.0043559 3.238158 0.0088971 The t and p-values computed from three separate tests differ from the t and p-values computed from the single linear model shown in the contrasts table above. The values differ because the SE in the denominators used to compute the \\(t\\)-values differ. The linear model uses the same value of \\(\\sigma\\) to compute the SED (the denominator of t) for all three t-tests in the contrast table. Each separate t-test uses a different value of \\(\\sigma\\) to compute the SED. Which is correct? Neither – they simply make different assumptions about the data generating model. Most importantly, never do both methods, look at the p-values, and then convince yourself that the method with the p-values that match your hypothesis is the correct method. Human brains are very, very good at doing this. This is called p-hacking. When you p-hack, the interpretation of the p-value is effectively meaningless. P-hacking leads to irreproducible science. In general, using the linear model is a better practice than the separate t-tests. The reason is the homogeneity of variance assumption. If we assume homogeneity of variances, then we can think of the sample standard deviation of all three groups as an estimate of \\(\\sigma\\). In the linear model, we use three groups to estimate \\(\\sigma\\) but in each separate t-test, we use only two groups. Consequently, our estimate of \\(\\sigma\\) in the linear model is more precise than that in the t-tests. While the difference can be large with any individual data set (it’s pretty big with the fig2a data), the long-run advantage of using the linear model instead of separate t-tests is pretty small, especially with only three groups (the precision increases with more groups). We can drop the homogeneity of variance assumption with either the linear model or the three, separate t-tests. This is outlined below in “Heterogeneity of variance”. In this case, the t and p-values for the three comparisons are the same. Still, the linear model (that models heterogenity) is better practice than the separate t-tests because the linear model is much more flexible and expandable. 10.1.4.5 The contrasts table – when there are more than two levels in \\(X\\) – has multiple p-values. How to handle this “multiple testing” is highly controversial Multiple testing is the practice of adjusting p-values and confidence intervals to account for the expected increase in the frequency of Type I error in a batch, or family, of tests. Multiple testing is a concept that exists because of Neyman-Pearson hypothesis testing strategy. If multiple tests are used to answer the same question then, these are in the same family. Issues surrounding multiple testing are fleshed out in more detail in Chapter xxx “Best Practices”. Computing adjusted values is covered below in the “Working in R” section. 10.2 Working in R 10.2.1 Fit the model m1 &lt;- lm(diHOME ~ treatment, data = fig2a) As described in Fitting the linear model, diHOME ~ treatment is the model formula. All functions that will be used to fit models in this text will use a model formula. Many base R and package functions that are meant to be either “easy” or follow an ANOVA strategy use lists of dependent and independent variables instead of a model formula. We won’t use these because they are not consistent with a linear modeling way of thinking about data analysis. \\(\\texttt{treatment}\\) was specifically coded as a factor variable in the import and wrangle chunk and R will automatically create the correct indicator variables. If categorical variables on the RHS of the formula have not be converted to factors by the user, then R will treat character variables as factors and create the indicator variables. If the categorical variable is numeric (for example, the variable time might have levels 1, 2, 3 instead of “1 hour”, “2 hours”, “three hours”), R will treat the variable as numeric and not create the indicator variables. A user could use the formula y ~ factor(time) to force R to create the indicator variable. I prefer to explicitly create a version of the variable that is recoded as factor using something like dt[, time_fac := factor(time, levels = c(\"1\", \"2\", \"3\"))]. 10.2.2 Controlling the output in tables using the coefficient table as an example m1_coef &lt;- cbind(coef(summary(m1)), confint(m1)) m1_coef ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 12.023075 3.081337 3.901902 0.001595771 5.414264 18.63189 ## treatmentCold 7.140386 4.570362 1.562324 0.140527829 -2.662066 16.94284 ## treatmentNE 14.794354 4.357669 3.395015 0.004355868 5.448083 24.14063 The standard print of the m1_coef object is too long for the display and often wraps when printed on the display, which can make it hard to read. I use knitr::kable to print the table with fewer decimal places and kableExtra::kable_styling to make it a little prettier. # the row names are not part of the m1_coef object # so there is no digit designation for this column m1_coef %&gt;% # pipe the m1_coef object to kable kable(digits = c(2,3,3,5,2,2)) %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 12.02 3.081 3.902 0.00160 5.41 18.63 treatmentCold 7.14 4.570 1.562 0.14053 -2.66 16.94 treatmentNE 14.79 4.358 3.395 0.00436 5.45 24.14 # explore other styles in the kableExtra package 10.2.3 Using the emmeans function m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_emm ## treatment emmean SE df lower.CL upper.CL ## Control 12.0 3.08 14 5.41 18.6 ## Cold 19.2 3.38 14 11.92 26.4 ## NE 26.8 3.08 14 20.21 33.4 ## ## Confidence level used: 0.95 Notes Note that printing the emmeans object displays useful information. Here, this information includes the confidence level used. If the object is printed using kable (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost. emmeans computes the modeled means of all combinations of the levels of the factor variables specified in specs. If there are two factor variables in the model, and both are passed to specs, then the modeled means of all combinations of the levels of the two variables are computed. If only one factor variable is passed, then the marginal means (averaged over all levels of the missing factor) are computed. This will become more clear in the chapter “Models for two (or more) categorical X variables”. If there are continuous covariates in the model, the modeled means are computed at the average values of these covariates. These covariates do not need to be passed to the specs argument. You can pass numeric and integer covariates to specs to control the value of the covariates used compute the modeled means. This is outlined in Adding covariates to a linear model 10.2.4 Using the contrast function m1_planned &lt;- contrast(m1_emm, method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;, level = 0.95) %&gt;% summary(infer = TRUE) m1_planned ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## Cold - Control 7.14 4.57 14 -2.66 16.9 1.562 0.1405 ## NE - Control 14.79 4.36 14 5.45 24.1 3.395 0.0044 ## ## Confidence level used: 0.95 Notes Note that printing the contrast object displays useful information, including the confidence level used and the method of adjustment for multiple tests. If the object is printed using kable() %&gt;% kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost. The method argument is used to control the set of contrasts that are computed. See below. The adjust argument controls if and how to adjust for multiple tests. Each method has a default adjustment method. See below. The level argument controls the percentile boundaries of the confidence interval. The default is 0.95. Including this argument with this value makes this level transparent. 10.2.4.1 method argument The method = argument is used to control the set of contrasts that are computed. Type help(\"contrast-methods\") into the console to see the list of available methods. Also, read the comparisons and contrasts vignette for more on emmeans::contrast() method = \"trt.vs.ctrl\" computes all non-reference minus reference contrasts. This method was used in the “Inference” section because it gives the two contrasts of the planned comparisons identified in the “understand the experimental design” step. The default adjustment for multiple tests is “dunnettx”, which is Dunnett’s test. “none” was specified in the inference section because both comparisons were planned. m1_tukey &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;tukey&quot;, level = 0.95) %&gt;% summary(infer = TRUE) m1_tukey ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## Cold - Control 7.14 4.57 14 -4.82 19.1 1.562 0.2936 ## NE - Control 14.79 4.36 14 3.39 26.2 3.395 0.0114 ## NE - Cold 7.65 4.57 14 -4.31 19.6 1.675 0.2490 ## ## Confidence level used: 0.95 ## Conf-level adjustment: tukey method for comparing a family of 3 estimates ## P value adjustment: tukey method for comparing a family of 3 estimates Both method = \"pairwise\" and method = \"revpairwise\" compute all pairwise comparisons. I prefer “revpairwise” because the contrasts that include the reference are in the direction non-reference minus reference. The default adjustment for multiple tests is “tukey”, which is the Tukey’s HSD method. 10.2.4.2 Adjustment for multiple tests See the Section 15.8.3 in the chapter “Linear models with two categorical \\(X\\) – Factorial linear models” for a more thorough explanation of the p-value adjustment for multiple tests that arise if an experiment has more than two treatment groups. Here, I just outline choices available in the emmeans::contrast function. “none” – no adjustment “dunnettx” – Dunnett’s test is a method used when comparing all treatments to a single control. “tukey” – Tukey’s HSD method is a method used to compare all pairwise comparisons. “bonferroni” – Bonferroni is a general purpose method to compare any set of multiple tests. The test is conservative. A better method is “holm” “holm” – Holm-Bonferroni is a general purpose method like the Bonferroni but is more powerful. “fdr” – controls the false discovery rate not the Type I error rate for a family of tests. One might use this in an exploratory experiment. “mvt” – based on the multivariate t distribution and using covariance structure of the variables. 10.2.4.3 Planned comparisons using custom contrasts With any but purely exploratory experiment, we have certain contrasts that test the focal hypotheses. We care little to none about other contrasts. We can limit the contrasts computed using emmeans::contrast() by passing custom contrasts to method. create a set of vectors with as many elements as there are rows in the emmeans table. Name each vector using the (or a) group name for the row in the emmeans table. For the vector created for row j, set the value of the jth element to 1 and set all other values to zero. create a list of contrasts, with each set in the list the difference of two of the named vectors from step 1. # m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table cn &lt;- c(1,0,0) # control is in row 1 cold &lt;- c(0,1,0) # cold in in row 2 ne &lt;- c(0,0,1) # ne is in row 3 # contrasts are the difference in the vectors created above # the focal contrasts are in the understand the experimental # design section # 1. (cold - cn) # 2. (ne - cn) planned_contrasts &lt;- list( &quot;Cold - Cn&quot; = c(cold - cn), &quot;NE - Cn&quot; = c(ne - cn) ) m1_planned &lt;- contrast(m1_emm, method = planned_contrasts, adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) m1_planned %&gt;% kable(digits = c(1,1,2,0,1,1,1,4)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Cold - Cn 7.1 4.57 14 -2.7 16.9 1.6 0.1405 NE - Cn 14.8 4.36 14 5.4 24.1 3.4 0.0044 10.2.5 How to generate ANOVA tables ANOVA is Analysis of Variance. Researchers frequently use the term “ANOVA” as the name for an analysis of an experiment with single-factor with more than two groups. However, ANOVA is a general method of inference for complex experimental designs. ANOVA models and regression models are different ways of expressing the same underlying linear model. Table 10.5: ANOVA table for the Figure 2a (example 2) data. Df Sum Sq Mean Sq F value Pr(&gt;F) treatment 2 656.9 328.4 5.765 0.015 Residuals 14 797.5 57.0 Notes Do not confuse the statistics in an ANOVA table with those in a coefficient table. The ANOVA table 10.5 has two rows. The first row contains the statistics for the factor \\(\\texttt{treatment}\\). The statistics address the null hypothesis “there is no effect of treatment – all groups have the same population mean”. This is a p value about the factor as a whole and not the individual comparisons of different groups within the factor. The second row contains the statistics for the error – the residuals of the model. \\(\\texttt{F value}\\) is the test statistic. It is a ratio of variances, which is why this analysis is called Analysis of Variance. The numerator variance is the \\(\\texttt{Mean Sq}\\) of \\(\\texttt{treatment}\\). The numerator variance is the value in $ of \\(\\texttt{treatment}\\). The denominator variance is the value in $ of \\(\\texttt{Residuals}\\). \\(\\texttt{Mean Sq}\\) contains the mean square for each term (row) in the table. The mean square is a variance. Remember that the numerator of a variance is a sum of squared differences between observed and mean values. The numerator of \\(\\texttt{Mean Sq}\\) is the value in \\(\\texttt{Sum Sq}\\) from the same row. And, remember that the denominator of a variance is a degree of freedom. The denominator of \\(\\texttt{Mean Sq}\\) is the value in \\(\\texttt{DF}\\) from the same row. An ANOVA table for a single factor with more than two groups has a single p-value for the treatment term. The single p-value is the probability of sampling a value of F as large or larger than the observed F under the null (no true effects of either treatment and all specifications of the generating model are true). There is not much we can do with this number - we want to estimate the effect sizes and their uncertainty and we don’t get this from an ANOVA table. Many textbooks, websites, and colleagues suggest to 1) fit the ANOVA, 2) check the F, and, if \\(F &lt; 0.05\\), 3) do “tests after an ANOVA”. These tests after an ANOVA are the planned comparisons and post-hoc tests described above using the linear model. In classical ANOVA, the initial computation of the cell means (means of treatment combinations) and sums of squares was a logical first step to the decomposition of these sums of squares to compute the contrasts. With modern linear models using regression, the ANOVA first step is unnecessary and not recommended. If your PI, manager, thesis committee, or journal editor insists that you do ANOVA, and you cannot convince them otherwise, here is how to generate an ANOVA table in R. Note that even though we are generating that table, the computation of the contrast table and all inference from that is not part of the ANOVA. 10.2.5.1 The afex aov_4 function The package afex was developed to make it much easier for researchers to generate ANOVA tables that look like those from other statistics packages including SAS, SPSS, JMP, and Graphpad Prism. # .I is a data.table function that returns the row number fig2a[, fake_id := paste(&quot;mouse&quot;, .I)] m1_aov4 &lt;- aov_4(diHOME ~ treatment + (1|fake_id), data = fig2a) anova(m1_aov4) ## Anova Table (Type 3 tests) ## ## Response: diHOME ## num Df den Df MSE F ges Pr(&gt;F) ## treatment 2 14 56.968 5.7651 0.45163 0.01491 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notes The afex package has three function names for generating the same ANOVA table and statistics – here I’m using aov_4 because this functions uses a linear model formula argument (specifically, that used in the lme4 package), which is consistent with the rest of this text. The formula includes the addition of a random factor ((1|id)) even though there really is no random factor in this model. See Section 12.1 below for a brief explanation of a random factor. The random factor (the factor variable “id” created in the line before the fit model line) identifies the individual mouse from which the response variable was measured. Because the response was only measured once on each individual mouse, “id” is not really a random factor but the addition of this in the model formula is necessary for the aov_4 function to work. It is easy to get an ANOVA table that you don’t want in R. If you want an ANOVA table that matches one from Graphpad Prism or JMP or similar software, the best practice is using the ANOVA functions from the afex package. What do I mean by “an ANOVA table that you don’t want”? With factorial ANOVA with unbalanced data, there are three ways to compute the sums of squares for the different terms of the ANOVA table. SAS termed these Type I, II, and III sums of squares and these names have stuck. Following SAS, almost all statistics packages use Type III as the default (or only) method for computing ANOVA tables. R uses Type I as the default. There are very good arguments for using Type II. This distinction is moot for single factor ANOVA or multi-factor ANOVA for balanced designs but is not moot for unbalanced multi-factor ANOVA or any ANOVA with covariates. If you want an ANOVA table from R to match what would be generated by Graphpad Prism or JMP (Type III), then the afex package is the best practice. 10.2.5.2 The car Anova function The car package has the extremely useful Anova function although using it is a bit like doing brain surgery having only watched a youtube video. type3 &lt;- list(treatment = contr.sum) m1_type3 &lt;- lm(diHOME ~ treatment, data = fig2a, contrasts = type3) Anova(m1_type3, type=&quot;3&quot;) ## Anova Table (Type III tests) ## ## Response: diHOME ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 6308.4 1 110.7355 4.95e-08 *** ## treatment 656.9 2 5.7651 0.01491 * ## Residuals 797.5 14 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notes car::Anova has arguments for reporting the Type III sum of squares. Again, this is not relevant to a single factor ANOVA with no covariates but to avoid catastrophic code in the future, its good to know about best practices now, so I’m pre-peating what is written in Section 15.11.4.2. Background: The default model matrix in the lm function uses dummy (or treatment) coding. For a Type 3 SS ANOVA (the kind that matches that in Graphpad Prism or JMP), we need to tell lm to use sum (or deviation) coding. The best practice method for changing the contrasts in the model matrix is using the contrasts argument within the lm function, as in the code above to fit m1_type3. This is the safest practice because this sets the contrasts only for this specific fit. The coefficients of m1_type3 will be different from m1. The intercept will be the grand mean and the coefficients of the non-reference levels (the effects) will be their deviations from the grand mean. I don’t find this definition of “effects” very useful for most experiments in biology. The contrasts (differences in the means among pairs of groups) in the contrast table will be the same, regardless of the contrast coding. Danger!. Many online sites suggest this bit of code before a Type III ANOVA using car::Anova: options(contrasts = c(\"contr.sum\", \"contr.poly\") If you’re reading this book, you almost certainly don’t want to do this because this code resets how R computes coefficients of linear models and SS of ANOVA tables. This will effect all future analysis until the contrasts are set to something else or a new R session is started. base R aov and anova m1_aov &lt;- aov(diHOME ~ treatment, data = fig2a) summary(m1_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 656.9 328.4 5.765 0.0149 * ## Residuals 14 797.5 57.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness # same as m1 in the Example 2 section m1 &lt;- lm(diHOME ~ treatment, data = fig2a) anova(m1) ## Analysis of Variance Table ## ## Response: diHOME ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment 2 656.85 328.43 5.7651 0.01491 * ## Residuals 14 797.55 56.97 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notes Many introduction to statistics textbooks and websites use the base R aov function. I don’t find this function useful given the afex package functions. The base R anova is useful if you know what you are doing with it. 10.3 Hidden Code 10.3.1 Importing and wrangling the fig3d data for example 1 data_folder &lt;- &quot;data&quot; data_from &lt;- &quot;The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue&quot; # need data_folder and data_from from earlier chunk file_name &lt;- &quot;41591_2017_BFnm4297_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # ignore the column with animal ID. Based on methods, I am inferring # that the six mice in vehicle group *are different* from the # six mice in the 1213 group. col_names_3d &lt;- c(&quot;Vehicle&quot;, &quot;1213&quot;) treatment_levels &lt;- c(&quot;Vehicle&quot;, &quot;12,13-diHOME&quot;) fig3d &lt;- read_excel(file_path, sheet = &quot;Figure 3d&quot;, range = &quot;B3:C9&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% melt(measure.vars = col_names_3d, variable.name = &quot;treatment&quot;, value.name = &quot;serum_tg&quot;) # change group name of &quot;1213&quot; fig3d[treatment == &quot;1213&quot;, treatment := &quot;12,13-diHOME&quot;] # make treatment a factor with the order in &quot;treatment_levels&quot; fig3d[, treatment := factor(treatment, treatment_levels)] #View(fig3d) 10.3.2 Importing and wrangling the fig2a data for example 2 # need data_folder and data_from from earlier chunk file_name &lt;- &quot;41591_2017_BFnm4297_MOESM2_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # assuming mice are independent and not same mouse used for all three treatment melt_col_names &lt;- paste(&quot;Animal&quot;, 1:6) fig2a &lt;- read_excel(file_path, sheet = &quot;Fig 2a&quot;, range = &quot;A3:G6&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% melt(measure.vars = melt_col_names, variable.name = &quot;id&quot;, value.name = &quot;diHOME&quot;) # cannot start a variable with number setnames(fig2a, old = colnames(fig2a)[1], new = &quot;treatment&quot;) treatment_order &lt;- c(&quot;Control&quot;, &quot;Cold&quot;, &quot;NE&quot;) fig2a[, treatment := factor(treatment, treatment_order)] # order levels #View(fig2a) "],["model-checking.html", "Chapter 11 Model Checking 11.1 All statistical analyses should be followed by model checking 11.2 Linear model assumptions 11.3 Diagnostic plots use the residuals from the model fit 11.4 Using R 11.5 Hidden Code", " Chapter 11 Model Checking 11.1 All statistical analyses should be followed by model checking We us a linear model to infer effects or predict future outcomes. Our inference is uncertain. Given the model assumptions, we can quantify this uncertainty with standard errors, and from these standard errors we can compute confidence intervals and p-values. It is good practice to use a series of diagnostic plots, diagnostic statistics, and simulation to check how well the data approximate the fit model and model assumptions. Model checking is used to both check our subjective confidence in the modeled estimates and uncertainty and to provide empirical evidence for subjective decision making in the analysis workflow. NHST blues – Researchers are often encouraged by textbooks, colleagues, or the literature to test the assumptions of a t-test or ANOVA with formal hypothesis tests of distributions such as a Shapiro-Wilks test of normality or a Levine test of homogeneity. In this strategy, an alternative to the t-test/ANOVA is used if the distribution test’s p-value is less than some cut-off (such as 0.05). Common alternatives include 1) transformations of the response to either make it more normal or the variances more homogenous, 2) implementation of alternative tests such as a Mann-Whitney-Wilcoxon (MWW) test for non-normal data or a Welch t-test/ANOVA for heterogenous variances. The logic of a test of normality or homogeneity before a t-test/ANOVA isn’t consistent with frequentist thinking because the failure to reject a null hypothesis does not mean the null hypothesis is true. We shouldn’t conclude that a sample is “normal” or that the variances are “homogenous” because a distributional test’s p-value &gt; 0.05. But, maybe we should of the distributional pre-test as an “objective” model check? The logic of this objective decision rule suffers from several issues. First, the subsequent p-value of the ttest/ANOVA test is not valid because this p-value is the long-run frequency of a test-statistic as large or larger than the observed statistic conditional on the null – not conditional on the subset of nulls with \\(p &gt; 0.05\\) in the distribution test. Second, real data are only approximately normal; with small \\(n\\), it will be hard to reject the null of a normal distribution because of low power, but, as \\(n\\) increses, a normality test will reject any real dataset. Third, and most importantly, our analysis should follow the logic of our goals. If our goal is the estimation of effects, we cannot get meaningful estimates from a non-parametric test (with a few exceptions) or a transformed response, as these methods are entirely about computing a “correct” p-value. Good alternatives to classic non-parametric tests and transformations are bootstrap estimates of confidence limits, permutation tests, and generalized linear models. 11.2 Linear model assumptions To facilitate explanation of assumptions of the linear model and extensions of the linear model, I will use both the error-draw and conditional-draw specifications of a linear model with a single \\(X\\) variable. error draw: \\[\\begin{align} y &amp;= \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\\\ \\varepsilon_i &amp;\\sim N(0, \\sigma^2) \\tag{11.1} \\end{align}\\] conditional draw: \\[\\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2)\\\\ \\mathrm{E}(Y|X=x_i) &amp;= \\mu_i\\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 x_i \\tag{11.2} \\end{align}\\] This model generates random data using the set of rules specified in the model equations. To quantify uncertainty in our estimated parameters, including standard errors, confidence intervals, and p-values, we make the assumption that the data from the experiment is a random sample generated using these rules. The two rules specified in the model above (Model (??)) are The systematic component of data generation is \\(\\beta_0 + \\beta_1 X\\). More generally, all linear models in this text specify systematic components that are linear in the parameters. Perhaps a better name for this is “additive in the parameters”. Additive (or linear) simply means that we can add up the products of a parameter and an \\(X\\) variable to get the conditional expectation \\(\\mathrm{E}(Y|X)\\). For observational inference, the rule \\(\\mathrm{E}(Y| see\\;X=x_i) = \\mu_i\\) is sufficient. For causal inference with data from an experiment, or with observational data and a well defined causal diagram, we would need to modify this to \\(\\mathrm{E}(Y|do\\; X=x_i) = \\mu_i\\). The stochastic component of data generation is “IID Normal”, where IID is independent and identically distributed and Normal refers to the Normal (or Gaussian) distribution. The IID assumption is common to all linear models. Again, for the purpose of this text, I define a “linear model” very broadly as a model that is linear in the parameters. This includes many extensions of the classical linear model including generalized least squares linear models, linear mixed models, generalized additive models, and generalized linear models. Parametric inference form all linear models requires the specification of a distribution family to sample. Families that we will use in this book include Normal, gamma, binomial, poisson, and negative binomial. This text will also cover distribution free methods of quantifying uncertainty using the bootstrap and permutation, which do not specify a sampling distribution family. 11.2.1 A bit about IID Independent means that the random draw for one case cannot be predicted from the random draw of any other case. A lack of independence creates correlated error. There are lots or reasons that errors might be correlated. If individuals are measured both within and among cages, or tanks, or plots, or field sites, then we’d expect the measures within the same unit (cage, tank, plot, site) to err from the model in the same direction because of environmental features shared by individuals within the unit but not by individuals in other units. Multiple measures within experimental units create “clusters” of error. Lack of independence or clustered error can be modeled using generalized least squares (GLS) models that directly model the structure of the error and with random effects models. Random effects models go by many names including linear mixed models (common in Ecology), hierarchical models, and multilevel models. Both GLS and random effects models are variations of linear models. tl;dr – Measures taken within the same individual over time (repeated measures) are correlated and are common in all areas of biology. In ecology and evolutionary studies, measures that are taken from sites that are closer together or measures taken closer in time or measures from more closely related biological species will tend to have more similar error than measures taken from sites that are further apart or from times that are further apart or from species that are less closely related. Space and time and phylogeny create spatial, temporal, and phylogenetic autocorrelation. Identical means that all random draws at a given value of \\(X\\) are from the same distribution. Using the error-draw specification of the model above, this can be restated as, the error-draw (\\(\\varepsilon_i\\)) for every \\(i\\) is from the same distribution \\(N(0, \\sigma^2\\)). Using the conditional-draw specification, this can be restated as, the random-draw \\(y_i\\) for every \\(i\\) with the same expected value \\(\\mu = \\mu_i\\) is from the same distribution \\(N(\\mu_i, \\sigma^2\\)). Understand the importance of this. Parametric inference using this model assumes that the sampling variance of \\(\\mu\\) at a single value of \\(X\\) is the same for all values of \\(X\\). If \\(X\\) is continuous, this means the spread of the points around the regression line is the same at all values of \\(X\\) in the data. If \\(X\\) is categorical, this means the spread of the points around the mean of a group is the same for all groups. A consequence of “identical”, then, for all classical linear models, is the assumption of homogeneity (or homoskedasticity) of variance. If the sampling variance differs among the \\(X\\), then the variances are heterogenous or heteroskedastic. Experimental treatments can affect the variance of the response in addition to the mean of the response. Heterogenous variance can be modeled using Generalized Least Squares (GLS) linear models. Many natural biological processes generate data in which the error is a function of the mean. For example, measures of biological variables that grow, such as size of body parts, have variances that “grow” with the mean. Or, measures of counts, such as the number of cells damaged by toxin, the number of eggs in a nest, or the number of mRNA transcripts per cell have variances that are a function of the mean. Both growth and count measures can sometimes be reasonably modeled using a linear model but more often, they are better modeled using a Generalized Linear Model (GLM). 11.3 Diagnostic plots use the residuals from the model fit 11.3.1 Residuals A residual of a statistical model is \\(y_i - \\hat{y}_i\\). Remember that \\(\\hat{y}_i\\) is the predicted value of \\(Y\\) when \\(X\\) has the value \\(x_i\\) (compactly written as \\(X=x_i\\)). And remember that \\(\\hat{y}_i\\) is the estimate of \\(\\mu_i\\). For linear models (but not generalized linear models), the residuals of the fit model are estimates of the \\(\\varepsilon\\) in equation (11.1). This is not true for generalized linear models because GLMs are not specified using (11.1). Alert A common misconception is that inference from a linear model assumes that the response (the measured \\(Y\\)) is IID Normal. This is wrong. Either specification of the linear model shows precisely why this conception is wrong. Model (11.1) explicitly shows that it is the error that has the normal distribution – the distribution of \\(Y\\) is a mix of the distribution of \\(X\\) and that of the error. A more general way of thinking about the assumed distribution uses the specification in model (11.2), which shows that it is the conditional response that is assumed to be IID normal. Remember, a conditional response (\\(y_i\\)) is a random draw from the infinite set of responses at a given value of \\(X\\). Let’s look at the distribution of residuals versus the distribution of responses for a hypothetical experiment with a single, categorical \\(X\\) variable (the experimental factor) with two levels (“Cn” for control and “Tr” for treatment). The true parameters are \\(\\beta_0 = 10\\) (the true mean for the control group, or \\(\\mu_{0}\\)), \\(\\beta_1=4\\) (the difference between the true mean for the treatment minus the true mean for the control, or \\(\\mu_1 - \\mu_0\\)), and \\(\\sigma = 2\\) (the error standard deviation). (#fig:model-check-histogram, model-check-residuals1)Histogram of the (A) response, showing with modes near the true means of each group and (B) residuals, with a mode for both groups at zero. The plot above shows a histogram of the response (A) and residuals (B). In the plot of the response, the mode (the highest bar, or bin with the most cases) includes true mean for each group. And, as expected given \\(\\beta_1=4\\), the modes of the two groups are 4 units apart. It should be easy to see from this plot that the response does not have a normal distribution. Instead, it is distincly bimodal. But the distribution of the response within each level looks like these are drawn from a normal distribution – and it should. In the plot of the residuals, the values of both groups are shifted so that the mean of each group is at zero. The consequence of the shift is that the combined set of residuals does look like it is drawn from a Normal distribution. The two plots suggest two different approaches for model checking. First, we could examine the responses within each level of the experimental factor. Or, second, we could examine the residuals of the fit model, ignoring that the residuals come from multiple groups. The first is inefficient because it requires as many checks as there are levels in the factor. The second requires a single check. Alert Some textbooks that recommend formal hypothesis tests of normality recommend the inefficient, multiple testing on each group separately. This isn’t wrong, it’s just more work than it needs to be and also suffers from “multiple testing”. 11.3.2 A Normal Q-Q plot is used to check for characteristic departures from Normality A Normal Q-Q plot is a scatterplot of sample quantiles on the y axis. The sample quantiles is the vector of \\(N\\) residuals in rank order, from smallest (most negative) to largest (most positive). Sometimes this vector is standardized by dividing the residual by the standard deviation of the residuals (doing this makes no difference to the interpretation of the Q-Q plot). standard normal quantiles on the x axis. This is the vector of standard, Normal quantiles given \\(N\\) elements in the vector. “Standard Normal” means a normal distribution with mean zero and standard deviation (\\(\\sigma\\)) one. A Normal quantile is the expected deviation given a probability. For example, if the probability is 0.025, the Normal quantile is -1.959964. Check your understanding: 2.5% of the values in a Normal distribution with mean 0 and standard deviation one are more negative than -1.959964. The Normal quantiles of a Normal Q-Q plot are computed for the set of \\(N\\) values that evenly split the probability span from 0 to 1. For \\(N=20\\), this would be p &lt;- data.frame(quantile = qnorm(ppoints(1:20))) row.names(p) &lt;- ppoints(1:20) p ## quantile ## 0.025 -1.95996398 ## 0.075 -1.43953147 ## 0.125 -1.15034938 ## 0.175 -0.93458929 ## 0.225 -0.75541503 ## 0.275 -0.59776013 ## 0.325 -0.45376219 ## 0.375 -0.31863936 ## 0.425 -0.18911843 ## 0.475 -0.06270678 ## 0.525 0.06270678 ## 0.575 0.18911843 ## 0.625 0.31863936 ## 0.675 0.45376219 ## 0.725 0.59776013 ## 0.775 0.75541503 ## 0.825 0.93458929 ## 0.875 1.15034938 ## 0.925 1.43953147 ## 0.975 1.95996398 A Normal Q-Q plot is not a test if the data are Normal. Instead, a Normal Q-Q plot is used to check for characteristic departures from Normality that are signatures of certain well-known distribution families. A researcher can look at a QQ-plot and reason that a departure is small and choose to fit a classic linear model using the Normal distribution. Or, a researcher can look at a QQ-plot and reason that the departure is large enough to fit a generalized linear model with a specific distribution family. Stats 101 A quantile is the value of a distribution that is greater than \\(p\\) percent of the values in the distribution. The 2.5% quantile of a uniform distribution from 0 to 1 is 0.025. The 2.5% quantile of a standard normal distribution is -1.96 (remember that 95% of the values in a standard normal distribution are between -1.96 and 1.96). The 50% quantile of a uniform distribution is 0.5 and the 50% quantile of a standard normal distribution is 0.0 (this is the median of the distribtion – 50% of the values are smaller and 50% of the values are larger). Stats 201 A Q-Q plot more generally is a scatter plot of two vectors of quantiles either of which can come from a sample or a theoretical distribution. In the GLM chapter, the text will introduce Q-Q plots of residual quantiles transformed to have an expected uniform distribution. These are plotted against theoretical uniform quantiles from 0 to 1. Intuition Pump – Let’s construct a Normal Q-Q plot. A quantile (or percentile) of a vector of numbers is the value of the point at a specified percentage rank. The median is the 50% quantile. The 95% confidence intervals are at the 2.5% and 97.5% quantiles. In a Normal Q-Q plot, we want to plot the quantiles of the residuals against a set of theoretical quantiles. To get the observed quantiles, rank the residuals of the fit linear model from most negative to most positive – these are your quantiles! For example, if you have \\(n=145\\) residuals, then the 73rd point is the 50% quantile. A theoretical quantile from the normal distribution can be constructed using the qnorm function which returns the normal quantiles for a specified vector of percents. Alternatively, one could randomly sample \\(n\\) points using rnorm. These of course will be sampled quantiles so will only approximate the expected theoretical quantiles, but I add this here because we use this method below. Now simply plot the observed against theoretical quantiles. Often, the standardized quantiles are plotted. A standardized variable has a mean of zero and a standard deviation of one and is computed by 1) centering the vector at zero by subtracting the mean from every value, and 2) dividing each value by the standard deviation of the vector. Recognize that because a standard deviation is a function of deviations from the mean, it doesn’t matter which of these operations is done first. A standardized theoretical quantile is specified by qnorm(p, mean = 0, sd = 1), which is the default. Code for this would look something like this 11.3.2.1 Normal QQ-plot of the fake data generated above If the sampled distribution approximates a sample from a normal distribution, the scatter should fall along a line from the bottom, left to the top, right of the plot. The interpretation of a normal Q-Q plot is enhanced with a line of “expected values” of the sample quantiles if the sample residuals are drawn from a normal distribution. The closer the sample quantiles are to the line, the more closely the residuals approximate the expectation from a normal distribution. Because of sampling, the sampled values always deviate from the line, especially at the ends. The shaded gray area in the Q-Q plot in Figure ?? are the 95% confidence bands of the quantiles. A pattern of observed quantiles with some individual points outside of these boundaries indicates a sample that would be unusual if sampled from a Normal distribution. Biological datasets frequently have departures on a Normal Q-Q plot that are characteristic of specific distribution families, including lognormal, binomial, poisson, negative binomial, gamma, and beta. It is useful to learn how to read a Normal Q-Q plot to help guide how to model your data. What about the intepretation of the Q-Q plot in Figure ??? At the small end of the distribution (bottom-left), the sample values are a bit more negative than expected, which means the left tail is a bit extended. At the large end (upper-right), the sample values are, a bit less positive than expected, which means the right tail is a bit shortened. This is a departure in the direction of a left skewed distribution. Should we fit a different model given these deviations? To guide us, we compare the quantiles to the 95% confidence band of the quantiles. Clearly the observed quantiles are within the range of quantiles that we’d expect if sampling from a Normal distribution. 11.3.3 Mapping QQ-plot departures from Normality Let’s look at simulated samples drawn from non-normal distributions to identify their characteristic deviations. Each set of plots below shows (left panel) A histogram of 10,000 random draws from the non-Normal distribution (blue). This histogram is superimposed over that of 10,000 random draws from a Normal distribution (orange) with the same mean and variance as that of the non-normal distribution. (middle panel) Box plots and strip chart of a random subset (\\(N=1000\\)) of data in the left panel. (right panel) Normal Q-Q plot of the non_Normal data only. Skewed-Right Q-Q The Normal Q-Q plot of a sample from a right-skewed distribution is characterized by sample quantiles at the high (right) end being more positive than the expected Normal quantiles. Often, the quantiles at the low (left) end are also less negative than the expected normal quantiles. The consequence is a concave up pattern. The histograms in the left panel explain this pattern. The right tail of the skewed-right distribution extends further than the right tail of the Normal. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the upper quantiles of the skewed-right distribution will be larger than the matching quantile of the Normal distribution. For example, the 99,990th quantile for the skewed-right distribution will be much more positive than the 99,990th quantile for the Normal distribution. The opposite occurs at the left tail, which extends further in the negative direction in the Normal than the skewed-right distribution. The middle panel compares a boxplot and stripchart of samples from the two distributions to show what researchers should look for in their own publication-ready plots as well as the published plots of colleagues. The skewed-right plot exhibits several hallmarks of a skewed-right distribution including 1) a median line (the horizontal line within the box) that is closer to the 25th percentile line (the lower end of the box) than to the 75th percentile line (the upper end of the box), 2) a longer upper than lower whisker (the vertical lines extending out of the box), 3) more outliers above the upper whisker than below the lower whisker, and 4) a lengthened, upward smear of the scatter of points at the high end of the values, relative to the more compact smear at the low end of the values. Skewed-Left Q-Q The Normal Q-Q plot of a sample from a left-skewed distribution is characterized by sample quantiles at the low (left) end being more negative than the expected Normal quantiles. Often, the quantiles at the high (right) end are also less positive than the expected normal quantiles. The consequence is a concave down pattern. The histograms in the left panel explain this pattern. The left tail of the skewed-left distribution extends further than the left tail of the Normal. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the lower quantiles of the skewed-left distribution will be more negative than the matching quantile of the Normal distribution. For example, the 10th quantile for the skewed-left distribution will be much more negative than the 10th quantile for the Normal distribution. The opposite occurs at the right tail, which extends further in the positive direction in the Normal than the skewed-left distribution. The skewed-left plot in the middle panel highlights several hallmarks of a skewed-left distribution including 1) a median line (the horizontal line within the box) that is closer to the 75th percentile line (the lower end of the box) than to the 25th percentile line (the upper end of the box), 2) a longer lower than upper whisker (the vertical lines extending out of the box), 3) more outliers below the lower whisker than above the upper whisker, and 4) a lengthened, downward smear of the scatter of points at the low end of the values, relative to the more compact smear at the upper end of the values. Heavy Tail Q-Q The Normal Q-Q plot of a sample from a heavy-tail distribution is characterized by sample quantiles at the low (left) end being more negative than the expected Normal quantiles and quantiles at the high (right) end that are more positive than the expected normal quantiles. The histograms in the left panel explain this pattern. At each tail, the heavy-tail distribution has more density – there are more values far from the mean – compared to the Normal distribution. This is the origin of “heavy tail”. It is easy to see from this that, if we rank the values of each distribution from small to large (these are the quantiles), the lower quantiles of the heavy tail distribution will be more negative than the matching quantile of the Normal distribution. For example, the 10th quantile for the heavy-tail distribution will be much more negative than the 10th quantile for the Normal distribution. Likewise, the upper quantiles of the heavy tail distribution will be more positive than the matching quantile of the Normal distribution. For example, the 99,990th quantile for the heavy-tail distribution will be much more positive than the 99,990th quantile for the Normal distribution. The heavy-tail plot in the middle panel shows more boxplot outliers than in the Normal plot. This would be hard to recognize in a plot of real data. 11.3.3.1 Mapping characteristic departures on a Q-Q plot to specific distributions Continuous response variables of length, area, weight, or duration will often look like samples from a continous probability distribution that is right-skewed, such as the lognormal or gamma distributions. Count response variables will frequently look like samples from a discrete probability distribution that is right-skewed, such as the poisson, quasi-poisson, or negative binomial distributions. Proportion (fraction of a whole) response variables will frequently look like samples from a continuous probability distribution bounded by 0 and 1, such as the beta distribution. Samples from a beta distribution can be left skewed, if the mean is near 1, right-skewed, if the mean is near zero, or symmetrical, if the mean is near 0.5. 11.3.3.2 Pump your intuition – confidence bands of a Q-Q plot In introducing the confidence bands of the Q-Q plot above, I stated “A pattern of observed quantiles with some individual points outside of these boundaries indicates a sample that would be unusual if sampled from a Normal distribution.” Let’s use a parametric bootstrap to explore this. Sample \\(n\\) values from a Normal distribution Compute the sample quantiles by re-ordering the residuals of the sampled values from the sampled mean, from most negative to most positive. Plot the quantiles against Normal quantiles for \\(n\\) points. Repeat steps 1-3 \\(n\\_iter\\) times, superimposing the new sample quantiles over all previous sample quantiles. This creates a band of all sample quantiles over \\(n\\_iter\\) iterations of sampling \\(n\\) values from a Normal distribution. At each value of the Normal quantile, compute the 95 percentile range of the sampled quantiles. Draw a ribbon inside these boundaries. n_iter &lt;- 1000 n &lt;- 20 normal_qq &lt;- ppoints(n) %&gt;% qnorm() sample_qq &lt;- numeric(n_iter*n) inc &lt;- 1:n for(iter in 1:n_iter){ y &lt;- rnorm(n) y_res &lt;- y - mean(y) sample_qq[inc] &lt;- y_res[order(y_res)] inc &lt;- inc + n } qq_data &lt;- data.table(normal_qq = normal_qq, sample_qq = sample_qq) qq_ci &lt;- qq_data[, .(median = median(sample_qq), lower = quantile(sample_qq, 0.025), upper = quantile(sample_qq, 0.975)), by = normal_qq] ggplot(data = qq_data, aes(x = normal_qq, y = sample_qq)) + geom_point(alpha = 0.2) + geom_ribbon(data = qq_ci, aes(ymin = lower, ymax = upper, y = median, fill = &quot;band&quot;), fill = pal_okabe_ito[1], alpha = 0.3) + xlab(&quot;Normal Quantile&quot;) + ylab(&quot;Sample Quantile&quot;) + theme_grid() + NULL 11.3.4 Model checking homoskedasticity 11.4 Using R 11.5 Hidden Code Source: Wellenstein, M.D., Coffelt, S.B., Duits, D.E., van Miltenburg, M.H., Slagter, M., de Rink, I., Henneman, L., Kas, S.M., Prekovic, S., Hau, C.S. and Vrijland, K., 2019. Loss of p53 triggers WNT-dependent systemic inflammation to drive breast cancer metastasis. Nature, 572(7770), pp.538-542. Public source Data source data_from &lt;- &quot;Loss of p53 triggers WNT-dependent systemic inflammation to drive breast cancer metastasis&quot; file_name &lt;- &quot;41586_2019_1450_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) treatment_levels &lt;- c(&quot;Trp53+/+&quot;, &quot;Trp53-/-&quot;) fig1f &lt;- read_excel(file_path, sheet = &quot;Fig. 1f&quot;, range = &quot;A2:B23&quot;) %&gt;% data.table() %&gt;% melt(measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;il1beta&quot;) fig1f[, treatment := factor(treatment, treatment_levels)] # be careful of the missing data. This can create mismatch between id and residual unless specified in lm # head(fig1f) Fit a linear model m1 &lt;- lm(il1beta ~ treatment, data = fig1f) 11.5.1 Normal Q-Q plots car::qqPlot has several important arguments to control the type of Q-Q plot. The function uses base graphics instead of ggplot2. Typically, these plots would not be published other than possibly a supplement. Q-Q Plots and Worm Plots from Scratch is a good source of some of the arguments in qqPlot. Three important arguments are: simulate. If passing a lm object, then the default confidence band is generated by a parametric bootstrap (simulate = TRUE). This band will differ somewhat each time you replot unless you set the seed with set.seed. Setting the argument simulate = FALSE returns the parametric band. line. If passing a lm object, then the default line is a fit from a robust regression (line = \"robust\"). Setting the argument line = \"quartiles\" fits a line throught the 25th and 75th percentile (or “quartiles”) quantiles. id. The default identifies the index of the two points with the most extreme quartiles. Set to FALSE to hide. The robust line is more sensitive to departures from Normality than the quartiles line. # defaults: robust line with bootstrap CI set.seed(1) qqPlot(m1, id = FALSE) # classic: standard line with parametric CI qqPlot(m1, line = &quot;quartiles&quot;, simulate = FALSE, id = FALSE) ggpubr::ggqqplot generates a pretty, ggplot2 based Normal Q-Q plot, using the standard method for computing the line and confidence band. m1_residuals &lt;- data.table(m1_residuals = residuals(m1)) m1_residuals[, studentized := m1_residuals/sd(m1_residuals)] ggqqplot(data = m1_residuals, x = &quot;studentized&quot;) "],["violations.html", "Chapter 12 Violations of independence, homogeneity, or Normality 12.1 Lack of independence 12.2 Heterogeneity of variances 12.3 The conditional response isn’t Normal 12.4 Hidden Code", " Chapter 12 Violations of independence, homogeneity, or Normality 12.1 Lack of independence 12.1.1 Example 1 (exp1b) – a paired t-test is a special case of a linear mixed model Lynes, M.D., Leiria, L.O., Lundh, M., Bartelt, A., Shamsi, F., Huang, T.L., Takahashi, H., Hirshman, M.F., Schlein, C., Lee, A. and Baer, L.A., 2017. The cold-induced lipokine 12, 13-diHOME promotes fatty acid transport into brown adipose tissue. Nature medicine, 23(5), pp.631-637. Public source Data source The data are from the experiment for Figure 1b of the 12,13-diHOME article from the previous chapter. Response variable – \\(\\texttt{diHOME}\\): concentration (pmol per ml) of the putative lipokine 12,13 di-HOME. This is a continuous variable. Factor variable – \\(\\texttt{treatment}\\), with levels: “saline” and “cold”. Coded as a factor. Blocking variable – id: identification code of the individual human. Design – Blocked. Plasma concentrations of 12,13-diHOME were measured two times in each individual, after injection with saline and after a 1 hour cold exposure. The consequence of the two measures in each human is correlated error. In blocked designs like this, the correlated error arises because we expect measures within a block (the individual for the exp1b experiment) to be more similar to each other than to measures among blocks. A blocked design is a powerful experimental method to reduce uncertainty in estimates of differences among means – and, consequently, the power of a statistical test. The paired t-test exploits this design while the classic Student’s t-test ignores it. Multiple response measures per individual violate the independent sampling assumption for inference. If there is a lack of independence between the two groups, a “which test?” strategy points to a paired t-test in place of Student’s t-test. A paired t-test is a special case of a linear mixed model. Linear mixed models are discussed more in the chapter Models with random factors – Blocking and pseudoreplication. A good way to think about the model generating the data is \\[ \\begin{equation} \\texttt{diHome} = (\\beta_0 + \\gamma_{0_j}) + \\beta_1 (\\texttt{treatment_cold}) + \\varepsilon \\end{equation} \\] \\(\\beta_0\\) is the expected value of \\(\\texttt{diHome}\\) in humans given a saline treatment. \\(gamma_{0_j}\\) is the effect of being human j on the expected value of \\(\\texttt{diHome}\\). \\(\\gamma_{0_j}\\) is a kind of random effect. \\(\\beta_0 + \\gamma_{0_j}\\) is the expected value of \\(\\texttt{diHome}\\) in human j given a saline treatment. It is the random intercept for human \\(j\\). \\(\\beta_0 + \\gamma_{0_j} + \\beta_1\\) is the expected value of \\(\\texttt{diHome}\\) in human j given a cold treatment. Importantly, if the random effect (\\(\\gamma_{0_j}\\)) isn’t modeled, then variation in this component is picked up by the model error \\(e\\). Consequently, the error component is correlated because the subsets of \\(e\\) from the same individual are expected to have more similar values to each other than to that from other individuals. Correlated error is discussed more in the chapter Models with random factors – Blocking and pseudoreplication. Importantly, if the random effect isn’t modeled, the increased variance of \\(e\\) decreases precision of the estimate. Consequently, CIs are wider and significance tests have less power. 12.1.1.1 Fit the model # fit the model exp1b_m1 &lt;- lmer(diHOME ~ treatment + (1|id), data = exp1b) The function lmer() from the lme4 package allows the addition of random factors to the model formula. Otherwise, the function works like the lm function. The added random factor is \\(\\texttt{id}\\), which contains the id of the human. \\(\\texttt{id}\\) didn’t appear from nowhere, it is a column in the data.table exp1b. Random factors are added embedded within parentheses. A random intercept is added using the formula (1|factor) where \\(\\texttt{factor}\\) is the names of the column of the data containing the random factor. \\(\\texttt{id}\\) is a random factor. \\(\\texttt{treatment}\\) is a fixed factor. Models with both fixed and random factors go by many names. In this text, I use “linear mixed model”. 12.1.1.2 Inference Inference using the linear model with added random factor: # estimated marginal means table exp1b_m1_emm &lt;- emmeans(exp1b_m1, specs = &quot;treatment&quot;) # contrasts table exp1b_m1_pairs &lt;- contrast(exp1b_m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) exp1b_m1_pairs %&gt;% kable(digits = c(1,1,2,1,1,1,5,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value cold - saline 0.2 0.05 8 0.1 0.4 4.27223 0.00272 Inference using paired t-test: exp1b_ttest &lt;- t.test( x = exp1b[treatment == &quot;cold&quot;, diHOME], y = exp1b[treatment == &quot;saline&quot;, diHOME], paired = TRUE ) exp1b_ttest ## ## Paired t-test ## ## data: exp1b[treatment == &quot;cold&quot;, diHOME] and exp1b[treatment == &quot;saline&quot;, diHOME] ## t = 4.2722, df = 8, p-value = 0.002716 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 0.1078778 0.3609173 ## sample estimates: ## mean difference ## 0.2343975 Notes The t and p values from the linear model and from the t-tests are the same because the paired t-test is a special case of the linear model. 12.1.2 Example 2 (diHOME exp2a) – A repeated measures ANOVA is a special case of a linear mixed model The structure of the Figure 2A experiment from the chapter Linear models with a single, categorical X is ambiguous. It is not clear from the archived data if the measures of 12,13 diHome were on separate mice within each of the three treatments (“Control”, “1 hour cold”, “30 min NE”) or if the same mice were used in each set. If each mouse was measured three times, I assume the researchers would have stated this, which they didn’t. But, to outline modeling correlated data when we have more than two groups, let’s say each mouse in the Figure 2A experiment was subjected to all three treatments. We have three measures per mouse and the response measure violates the independence assumption. The linear model (in regression notation) is \\[ \\begin{equation} \\texttt{diHome} = (\\beta_0 + \\gamma_{0_j}) + \\beta_1 \\texttt{treatment}_\\texttt{cold} + \\beta_2 \\texttt{treatment}_\\texttt{NE} + \\varepsilon \\end{equation} \\] 12.1.2.1 Fit the model # fit the model exp2a_m1 &lt;- lmer(diHOME ~ treatment + (1|id), data = exp2a) Notes If a treatment factor has more than two groups, nothing special has to be done relative to a model with a treatment factor with only two groups. 12.1.2.2 Inference Inference using the linear model with added random factor: # estimated marginal means table exp2a_m1_emm &lt;- emmeans(exp2a_m1, specs = &quot;treatment&quot;) # contrasts table exp2a_m1_planned &lt;- contrast(exp2a_m1_emm, method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) exp2a_m1_planned %&gt;% kable(digits = c(1,1,2,1,1,1,5,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value 1 hour cold - Control 8.1 3.88 9.4 -0.6 16.8 2.08124 0.06589 30 min NE - Control 14.8 3.62 9.0 6.6 23.0 4.08718 0.00271 12.1.2.3 repeated measures ANOVA In some fields, this analysis would be called repeated measures ANOVA and in others it might be called a two-way Model II (mixed effect) ANOVA with one fixed and one random factor. exp2a_m2 &lt;- aov_4(diHOME ~ treatment + (treatment|id), data = exp2a) ## Warning: Missing values for following ID(s): ## Animal 6 ## Removing those cases from the analysis. Notes The measure of \\(\\texttt{diHOME}\\) for animal_6 for the 1 hour cold treatment is missing. To use the exp2a data in repeated measures ANOVA, the “Control” and “30 min NE” values for animal_6 have to be excluded. This exclusion isn’t necessary for the linear mixed model, which is one of the advantages of a linear mixed model over repeated measures ANOVA. The contrasts from the model are exp2a_m1_planned &lt;- emmeans(exp2a_m2, specs = &quot;treatment&quot;) %&gt;% contrast(method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) exp2a_m1_planned %&gt;% kable(digits = c(1,1,2,1,1,1,5,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value X1.hour.cold - Control 7.3 3.32 4 -1.9 16.5 2.19728 0.09293 X30.min.NE - Control 11.3 2.27 4 5.0 17.6 4.97783 0.00761 Lets remove animal 6 and rerun the linear mixed model exp2a_m1 exp2a_complete &lt;- exp2a[id != &quot;Animal 6&quot;] exp2a_complete_m1 &lt;- lmer(diHOME ~ treatment + (1|id), data = exp2a_complete) exp2a_m1_complete_planned &lt;- emmeans(exp2a_complete_m1, specs = &quot;treatment&quot;) %&gt;% contrast(method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) exp2a_m1_complete_planned %&gt;% kable(digits = c(1,1,2,1,1,1,5,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value 1 hour cold - Control 7.3 2.83 8 0.8 13.8 2.57671 0.03278 30 min NE - Control 11.3 2.83 8 4.8 17.9 3.99395 0.00398 Notes - Inference is the same. 12.1.2.4 Inferences from the linear mixed model and paired t-tests are not the same when there are more than two groups Inference using two paired t-tests: exp2a_ttest_cold &lt;- t.test( x = exp2a[treatment == &quot;1 hour cold&quot;, diHOME], y = exp2a[treatment == &quot;Control&quot;, diHOME], paired = TRUE ) exp2a_ttest_cold ## ## Paired t-test ## ## data: exp2a[treatment == &quot;1 hour cold&quot;, diHOME] and exp2a[treatment == &quot;Control&quot;, diHOME] ## t = 2.1973, df = 4, p-value = 0.09293 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## -1.925189 16.533005 ## sample estimates: ## mean difference ## 7.303908 exp2a_ttest_ne &lt;- t.test( x = exp2a[treatment == &quot;30 min NE&quot;, diHOME], y = exp2a[treatment == &quot;Control&quot;, diHOME], paired = TRUE ) exp2a_ttest_ne ## ## Paired t-test ## ## data: exp2a[treatment == &quot;30 min NE&quot;, diHOME] and exp2a[treatment == &quot;Control&quot;, diHOME] ## t = 3.7564, df = 5, p-value = 0.01321 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 4.670273 24.918436 ## sample estimates: ## mean difference ## 14.79435 Notes important – the t-test for the comparison of “1 hour cold” and “Control” excluded mouse 6 because of a missing “1 hour cold” measure. The linear mixed model was fit to all data, including the “Control” and “30 min NE” measures. The consequence is more precision in the linear mixed model. The paired t-test t and p values are not equal to those from the linear mixed model. This is partly because of the missing data (note 1). Even if there were no missing data, these wouldn’t be equal because the linear mixed model uses an estimate of \\(\\sigma\\) computed from the single model with all three groups to compute the standard errors. See the comparison of t-tests and the contrast table from a linear model in Section ??. The results of the separate, paired t-tests and the linear mixed model differ in a way that might affect our inference about the system. Which is correct? Neither – they simply make different assumptions about the data generating model. The linear model strategy has more power and precision but this advantage is small. The best reason to use linear models instead of separate t-tests is learning how to use linear models, and their extensions, gives you phenomenal cosmic power. Do not compute both separate paired t-tests and the linear models and then convince yourself that that the assumption of the method with the p-value that matches your hypothesis is correct. See the p-hacking discussion above. 12.2 Heterogeneity of variances Heterogeneity of variance among treatment groups is a problem for inference, especially if the sample size is unequal among groups (statisticians tend to agree that heterogeneity is much more problematic than a non-normal response). 12.2.0.1 A Welch t-test is a special case of a linear model for heterogeneity A “which test?” strategy points to a Welch’s t-test in place of Student’s t-test if there is heterogeneity of variances between treatment groups. A Welch t-test is infrequent in the experimental biology literature, perhaps because it is poorly known and it doesn’t occur to researchers to use a test that models heterogeneity of variances. heterogeneity often arises in right-skewed data, which is often analyzed with a non-parametric test like the Mann-Whitney U test. The Welch t-test is a special case of a linear model that explicitly models the within-group variance using generalized least squares (GLS). The 95% CI of a mean differences and p-values from the fit gls linear model and from Welch’s t-test are the same. Advantages of using a linear modeling strategy is that a researcher uses the model to estimate effects (difference in means) and measures of uncertainty in the effects (standard errors or confidence intervals of the difference). Advantages of specifically using a GLS linear model is that it is easily expanded to analyze more complex designs including 1) more than one factor, 2) added covariates, 3) correlated residuals due to non-independence. Some statisticians argue that researchers should always use a Welch t-test instead of Student’s t-test. Given this logic, researchers should consider using GLS linear models for more complex experimental designs (added covariates, factorial) in place of classical ANCOVA and two-way ANOVA. Modeling variance heterogeneity is the focus of Chapter 19 so the account here is brief. Heterogeneity can be modeled using a generalized least squares linear model with the gls function. The weights argument is used to model the variances using each group’s sample variance. In this example, I use the data from the Figure 1b experiment, which can be compared to the analysis of the same data in Example 2 above. 12.2.0.2 Fit the model # gls fails with missing data subdata &lt;- exp2a[is.na(diHOME) == FALSE,] # omit rows with missing data exp2a_m3 &lt;- gls(diHOME ~ treatment, data = subdata, weights = varIdent(form = ~ 1 | treatment)) The model exp2a_m3 uses variance computed in each group separately as the estimate of \\(\\sigma\\) for that group. The coefficient table of the GLS model is 12.2.0.3 Inference from the linear model exp2a_m3_coef &lt;- cbind(coef(summary(exp2a_m3)), confint(exp2a_m3)) exp2a_m3_coef %&gt;% kable(digits = c(4,4,4,6,4,4)) %&gt;% kable_styling() Value Std.Error t-value p-value 2.5 % 97.5 % (Intercept) 12.0231 1.2001 10.0181 0.000000 9.6708 14.3753 treatment1 hour cold 7.1404 3.1635 2.2571 0.040505 0.9401 13.3407 treatment30 min NE 14.7944 4.5688 3.2382 0.005951 5.8398 23.7490 Notes Important for reporting CIs and p-values. Unlike the linear model modeling homogenous variance, the CIs and p-values for the coefficients of \\(\\texttt{treatment1 hour cold}\\) and \\(\\texttt{treatment30 min NE}\\) are not the same as the p-values of these equivalent contrasts in the contrasts table (see below). The reason is, the computation of the CI and p-values in the two tables use two different degrees of freedom. Report the CI and p-values from the contrast table using the Satterthwaite df. The modeled means and contrasts are computed as above for the lm object exp2a_m3_emm &lt;- emmeans(exp2a_m3, specs=&quot;treatment&quot;) exp2a_m3_emm ## treatment emmean SE df lower.CL upper.CL ## Control 12.0 1.20 5 8.94 15.1 ## 1 hour cold 19.2 2.93 4 11.04 27.3 ## 30 min NE 26.8 4.41 5 15.49 38.1 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 Notes The SE of the means in this table are modeled SEs but are equal to the sample SE of the means, because this was specified in the GLS model. exp2a_m3_pairs &lt;- contrast(exp2a_m3_emm, method = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) exp2a_m3_pairs %&gt;% kable(digits = c(1,4,4,1,4,4,4,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value 1 hour cold - Control 7.1404 3.1635 5.3 -0.8393 15.1201 2.2571 0.07026 30 min NE - Control 14.7944 4.5688 5.7 3.4897 26.0990 3.2382 0.01889 30 min NE - 1 hour cold 7.6540 5.2915 8.4 -4.4596 19.7675 1.4465 0.18452 Notes Compare the statistics for “1 hour cold - Control” and “30 min NE - Control” to the coefficients for \\(\\texttt{treatment1 hour cold}\\) and \\(\\texttt{treatment30 min NE}\\) in the coefficient table. The estimates, SE, and t are the same but the CIs and p values differ. The contrast function is using a different method (“satterthwaite”) for computing the degrees of freedom and this results in a different value of the t-distribution tail area used to compute the CI and p value. 12.2.0.4 Inference using Welch t-test test1 &lt;- t.test(exp2a[treatment == &quot;1 hour cold&quot;, diHOME], exp2a[treatment == &quot;Control&quot;, diHOME], var.equal = FALSE) test2 &lt;- t.test(exp2a[treatment == &quot;30 min NE&quot;, diHOME], exp2a[treatment == &quot;Control&quot;, diHOME], var.equal = FALSE) test3 &lt;- t.test(exp2a[treatment == &quot;30 min NE&quot;, diHOME], exp2a[treatment == &quot;1 hour cold&quot;, diHOME], var.equal = FALSE) Notes the default t.test is the Welch t-test. However, I’ve included var.equal = FALSE so the method is transparent. 12.2.0.5 Compare inference from the linear model and Welch t-tests Compare the contrast p values to the three Welch t-tests of all pairs of treatment levels in the exp2a experiment. contrast t (gls lm) p (gls lm) t (welch t) p (welch t) 1 hour cold - Control 2.257139 0.0702604 2.257139 0.0702622 30 min NE - 1 hour cold 1.446454 0.1845159 1.446454 0.1845155 30 min NE - Control 3.238157 0.0188899 3.238158 0.0188904 The t and p-values computed from the GLS linear model and from the three, pairwise Welch t-tests are the same (to about the 6th decimal place). They are the same because each is estimating \\(\\sigma^2\\) separately for each group and not as the pooled (among two groups for t-test or three groups for the linear model) estimate and because they use the same degrees of freedom to compute the p-value. Let’s summarize these comparisons Inference from a linear model using homogenous variance (the lm function) and from a Student’s t-test are the same if there are only two levels in the treatment variable. Inference from a linear model using homogenous variance (the lm function) and from the series of pairwise, Student’s t-tests differ when there are more than two levels in the treatment variable. Inference from a GLS linear model using heterogenous variance (the gls function) and from a Welch t-test are the same regardless of the number of levels in the treatment variable. Even though the linear model that models heterogeneity and the Welch t-test produce the same results, researchers should use the linear model because A linear modeling strategy encourages researchers to think about the effect and uncertainty in the effect and not just a p-value. The linear model is nearly infinitely flexible and expandible while the t-test has extremely limited flexibility (The Welch t-test is one way to expand the classical, Student’s t-test). 12.2.1 When groups of the focal test have &gt;&gt; variance 12.3 The conditional response isn’t Normal It is common to hear or read from a statistitican that of all the issues with inference from a classical linear model (including t-tests and ANOVA), non-normal “data” is the least worrisome – we should be much more concerned about lack of independence and heterogeneity. The theoretical basis for this sentiment is the central limit theorem. In short, if we repeatedly 1) sample a non-normal distribution and 2) compute the sample mean, the distribution of these resampled means is approximately normal (the distribution converges to normal as the sample size increases). Because inferential statistics using a classical linear model assume a normal distribution of the means, inference from classical linear models are increasingly robust to non-normal distributions as the sample size increases. The main issue for applied statistics in experimental biology is the small sample sizes. Empirical results from simulations support the theory. Actual type I error using classical linear models with non-normal data tends to be close to the nominal (say, 0.05) level. That said, generalized linear models that explicitly model the distribution have more power than classical linear models and this is a pretty good reason to use GLMs. Confidence intervals computed using the normal distribution on non-normal data can be awkward, for example, the CIs might include impossible values such as negative counts or percent of cells expressing a certain marker above 100 %. The normality assumption of the classical linear model does not apply to the distribution of the response (Y) variable and certainly not to the distribution of the treatment (X) variable but to the residuals of the fit model (actually, it applies to the distribution of the means, but if the sampling distribution is normal then the means are normal). An equivalent way to think about this is that the normality assumption applies to the distribution of the response if all values of \\(X\\) in the sample are the same – this is known as the conditional response. Contrary to common practice by experimental biologists, and the advice of some textbooks, there is no good reason to use the p-value from a test of normality as a decision tool to use either a t-test or a non-parametric test like the Mann-Whitney-Wilcoxon. A test of normality for small samples will frequently result in \\(p &gt; 0.05\\) (“normal”), even for fake data sampled from a nonnormal distribution, while a test with a large sample size will frequently result in \\(p &lt; 0.05\\) (“not-normal”) even if a histogram of the residuals appears normal. A Normal Q-Q plot (Section 11.3.2) is a useful tool for the decision to use some alternative to inference using a Normal assumption. If a Q-Q plot indicates that the residuals are far from expected for a normal distribution, then the researcher has several alternatives, the best choice of which partially depends on the goals of the analysis. Some alternative to a linear model assuming normal distribution include: Count data – count data tend to be right skewed and groups with higher mean counts tend to have larger variance. Generalized linear models using either a negative binomial or quasi-poisson distribution family are a good alternative to consider. Bootstrap confidence intervals and permutation p-values are resampling techniques. Bootstrap confidence intervals are a good alternative if the sample size is much larger (say, &gt; 40) than typical in experimental biology. Permutation p-values are a good alternative regardless of sample size. A GLS linear model to account for any heterogeneity in variance that typically accompanies non-normal data. This doesn’t address the non-normal distribution but, again, heterogeneity is typically a much larger problem than non-normality. Log transformation of a count response is controversial at best. log transformations can be pretty good at making the response look more like it was sampled from a normal distribution (and can make variances more similar). But…If the counts include zero, researchers have to add a kludge factor (typically equal to 1) to all counts prior to transformation. The value of the kludge factor is arbitrary and matters (adding .1 gives different results than adding 1). A log transformation (or any transformation) raises interpretation problems. For the log transformation, the effect is the difference between the means of the log-transformed counts. I don’t know what the magnitude of an effect on a log scale means biologically. If we backtransform this, the effect is the ratio of the geometric means of the two groups. A ratio effect is easy enough to interpret but do we really want to model the geometric and not the arithmetic means? Finally, log transformation does not specifically address either the shape of the distribution or the heterogeneity that often comes with the non-normal shape. A generalized linear model can specifically model both the shape and the variance. Classic non-parametric tests such as Mann-Whitney-Wilcoxon were invented when computers were not fast enough to perform permutation tests and before the development of Generalized Linear Models (and other modern methods). Since at least 1990, there are better alternatives to non-parametric tests. Fraction (proportion) data – For example, the number of cells expressing a certain marker relative to all counted cells in the sample. Fraction data have hard bounds at 0 and 1, or at 0 and 100 if converted to a percent. Fraction data tend to be right skewed if the mean is closer to zero and left skewed if the mean is closer to the upper bound. Generalized linear model using a binomial distribution family with a logit link (“logistic regression”) is a good alternative to consider. This model is used for Bernouili (success/fail) responses (for example the subject “lived” or “died”), where success is assigned the value 1 and fail is assigned the value 0. Using this model with proportion data is equivalent to assigning a 0 (“does not express marker”) or 1 (“expresses marker”) to all cells in the count. Bootstrap confidence intervals and permutation p-values are resampling techniques. Bootstrap confidence intervals are a good alternative if the sample size is much larger (say, &gt; 40) than typical in experimental biology. Permutation p-values are a good alternative regardless of sample size. A GLS linear model to account for any heterogeneity in variance that typically accompanies non-normal data. This doesn’t address the non-normal distribution but, again, heterogeneity is typically a much larger problem than non-normality. arcsin transformation. See [The arcsine is asinine: the analysis of proportions in ecology] (https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0340.1) Classic non-parametric tests such as Mann-Whitney-Wilcoxon were invented when computers were not fast enough to perform permutation tests and before the development of Generalized Linear Models (and other modern methods). Since at least 1990, there are better alternatives to non-parametric tests. Generalized linear models that explicitly model the distribution of the response is covered in more detail in the chapter Linear models for count data – Generalized Linear Models I. 12.3.1 Example 1 (fig6f) – Linear models for non-normal count data Source article: Exercise reduces inflammatory cell production and cardiovascular inflammation via instruction of hematopoietic progenitor cells Data source Public source 12.3.1.1 Fit a Linear Model – Inference assuming Normal conditional response fit the model m1 &lt;- lm(neutrophil_count ~ treatment, data = fig6f) check the model ggcheck_the_model(m1) The left plot shows classic right skew. The right plot shows evidence for positive relationship between mean and variance. This suggest we fit a negative binomial or quasi-poisson model. inference m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;trt.vs.ctrl&quot;) %&gt;% summary(infer = TRUE) 12.3.1.2 Fit a Generalized Linear Model – inference assuming negative binomial conditional response Generalized linear models for count data are covered more thoroughly in the chapter Linear models for count data – Generalized Linear Models I. fit the model m2 &lt;- glmmTMB(neutrophil_count ~ treatment, data = fig6f, family = nbinom2(link = &quot;log&quot;)) Notes The function glmmTMB allows fitting from multiple distribution families and works like the lm function with a few tweaks, including an argument for the distribution family. inference m2_coef &lt;- coef(summary(m2))$cond m2_coef %&gt;% kable(digits = c(2,3,1,5)) %&gt;% kable_styling() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 15.11 0.085 177.9 0.00000 treatmentExercise -0.53 0.137 -3.8 0.00013 Notes The coef function for a glmmTMB object is a list with several elements. the element \\(\\texttt{\\$cond}\\)” returns the coefficient table. \\(b_0\\) (\\(\\texttt{(intercept)}\\)) is the natural log of the mean of the reference (“sedentary”) group. \\(b_1\\) (\\(\\texttt{(intercept)}\\)) is the difference of the natural log of the mean of exercise group mean and the natural log of the mean of the sedentary group mean. The coefficient values are on a link scale, which is the scale of the transformation of the group means in the GLM fit. The link transformation for this fit was log (the natural log transformation). In general, values on the response scale are more interpretable. \\(exp(b_0)\\) is the modeled mean of the reference group on the response scale (because the model had no covariates, it is the sample mean of the reference group). \\(exp(b_1)\\) is the ratio of the exercise to sedentary means. Remember: \\(exp(log(a) - log(b)) = a/b\\) m2_emm &lt;- emmeans(m2, specs = &quot;treatment&quot;, type = &quot;response&quot;) m2_pairs &lt;- contrast(m2_emm, method = &quot;trt.vs.ctrl&quot;, type = &quot;response&quot;) %&gt;% summary(infer = TRUE) Notes The argument type = \"response\" tells emmeans and contrast to return the estimates on the response (not link) scale. See the notes for the coefficient table above for a brief explanation of the link and response scale. For more detailed explanation, see the Chapter Linear models for count data – Generalized Linear Models I 12.3.1.3 Compare inference from the LM and GLM models gg1 &lt;- ggplot_the_model( m1, m1_emm, m1_pairs, legend_position = &quot;none&quot;, y_label = &quot;Neutrophil Count (/mL)&quot;, effect_label = &quot;Effect (/mL)&quot;, palette = pal_okabe_ito_blue, rel_heights = c(0.75,1) ) gg2 &lt;- ggplot_the_model( m2, m2_emm, m2_pairs, legend_position = &quot;none&quot;, y_label = &quot;Neutrophil Count (/mL)&quot;, effect_label = &quot;Relative Effect&quot;, palette = pal_okabe_ito_blue, rel_heights = c(0.75,1), effect_x_lim = c(0.35, 1.25) ) plot_grid(gg1, gg2, nrow=2, labels = &quot;AUTO&quot;) Notes Generalized linear models are covered more thoroughly in Chapter 18. The effect estimated from the GLM is not a difference but the ratio of the exercise group mean to the sedentary group mean. Both differences and ratios indicate how much bigger (or smaller) the mean of one group is compared to another but in a linear model “how much bigger” is a difference while in the glm here “how much bigger” is a multiple (or “times”) – the mean of the exercise group is .59X the mean of the sedentary group. The CIs for the means and for the effect are asymmetric in the GLM. The CIs from a 12.3.2 My data aren’t normal, what is the best practice? The major advantage of using a GLM for count data instead of the classical linear model is the increased power of a significance test and increased precision of estimates. And, CIs will reflect the asymmetry in the uncertainty, unlike the CIs of a classical linear model. However, researchers should gaurd against the slight increase in Type I error of a GLM. Figure 12.1: (A) histogram of distribution used for simulation of non-normal count response (sampled from negative binomial distribution with mu and theta equal to the observed values in Fig. 6f). (B) An example of a sampled fake data set. Table 12.1: Table of Type I error and Power computed from simulations of non-normal count data. The model columns are linear model, generalized least squares, GLM negative binomial, GLM quasipoisson, permutation, and Mann-Whitney-Wilcoxon. The values in the model columns are the frequency of simulated experiments with p sim normal type n theta lm gls glm_nb glm_qp lmp mww 1 TRUE type 1 10 0.05 0.050 0.076 0.052 0.053 0.046 2 FALSE type 1 10 6.6 0.04 0.041 0.075 0.044 0.046 0.039 3 FALSE type 1 10 1 0.04 0.034 0.075 0.046 0.046 0.041 4 FALSE type 1 12, 8 6.6 0.05 0.049 0.080 0.048 0.050 0.046 5 TRUE power 10 0.57 0.567 0.653 0.572 0.571 0.528 6 FALSE power 10 6.6 0.49 0.476 0.590 0.494 0.499 0.440 7 FALSE power 10 1 0.10 0.082 0.171 0.113 0.114 0.088 8 FALSE power 12, 8 6.6 0.44 0.501 0.578 0.452 0.444 0.438 9 FALSE power 12, 8 1, 6.6 0.03 0.082 0.144 0.054 0.041 0.045 10 FALSE type 1 12, 8 6.6, 1 0.13 0.103 0.139 0.116 0.132 0.144 Table 12.1 summarizes the results of a simulation to compare the performance of alternatives to a t-test. The simulated data were modeled to look like the Figure 6f (Figure 12.1). The methods are the classical linear model (lm) (equivalent to the t-test), GLS linear model (gls), negative binomial GLM (glm-nb), quasi-poisson GLM (glm-qp), permutation test (lmp), and Mann-Whitney-Wilcoxon test. Performance is based on the the p-values from the test. The frequency of \\(p &lt; 0.05\\) is given in the column for each method. If the simulation was run with no treatment effect, this frequency is the simulated Type I error, given the nominal error of \\(\\alpha = 0.05\\). Values less then 0.05 are conservative and values greater than 0.05 are liberal. If the simulation was run with a treatment effect, the frequency is the simulated power of the test. Over the limited parameter space of the non-normal distribution (the shape of the count distribution including the relationship between mean and variance, the sample size, the effect size), the negative binomial GLM has substantially higher power than the classical linear model, although this comes at some cost to inflated Type I error. Even with a Normal simulated distribution, the negative binomial GLM has higher power than the classical linear model, but again, at some cost to Type I error. In the simulations with a negative binomial distributed response, the quasipoisson GLM has consistently higher power than the classical linear model and excellent Type I error control. The performance of the permutation test is very similar to that of the quasipoisson GLM. In the simulations with a negative binomial distributed response, the GLS linear model has about the same power as the classical linear model. Perhaps surprisingly, the Mann-Whitney-Wilcoxon fails to outperform the classical linear model and even underperforms the classical linear in some parts of the parameter space. 12.4 Hidden Code 12.4.1 Importing and wrangling the exp1b data data_from &lt;- &quot;The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue&quot; file_name &lt;- &quot;41591_2017_BFnm4297_MOESM1_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp1b &lt;- read_excel(file_path, sheet = &quot;Fig 1 b thur c&quot;, range = &quot;A1:C20&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% clean_names() %&gt;% na.omit() # get rid of blank row setnames(exp1b, old = names(exp1b), new = c(&quot;sample&quot;, &quot;diHOME&quot;, &quot;bat_activity&quot;)) exp1b[, id := substr(sample, 1, 6)] exp1b[, treatment := ifelse(substr(sample, 8,8) == &quot;C&quot;, &quot;cold&quot;, &quot;saline&quot;)] exp1b[, treatment := factor(treatment, levels = c(&quot;saline&quot;, &quot;cold&quot;))] #View(exp1b) 12.4.2 Importing and wrangling the exp2a data data_from &lt;- &quot;The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue&quot; file_name &lt;- &quot;41591_2017_BFnm4297_MOESM2_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # assuming mice are independent and not same mouse used for all three treatment melt_col_names &lt;- paste(&quot;Animal&quot;, 1:6) exp2a &lt;- read_excel(file_path, sheet = &quot;Fig 2a&quot;, range = &quot;A3:G6&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% melt(measure.vars = melt_col_names, variable.name = &quot;id&quot;, value.name = &quot;diHOME&quot;) # cannot start a variable with number setnames(exp2a, old = colnames(exp2a)[1], new = &quot;treatment&quot;) treatment_order &lt;- c(&quot;Control&quot;, &quot;1 hour cold&quot;, &quot;30 min NE&quot;) exp2a[, treatment := factor(treatment, treatment_order)] # order levels #View(exp2a) 12.4.3 Importing and wrangling the fig6f data # need data_folder from earlier chunk data_from &lt;- &quot;Exercise reduces inflammatory cell production and cardiovascular inflammation via instruction of hematopoietic progenitor cells&quot; file_name &lt;- &quot;41591_2019_633_MOESM8_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # assuming mice are independent and not same mouse used for all three treatment melt_col_names &lt;- c(&quot;Sedentary&quot;, &quot;Exercise&quot;) fig6f &lt;- read_excel(file_path, sheet = &quot;Figure 6f&quot;, range = &quot;A7:B29&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% melt(measure.vars = melt_col_names, variable.name = &quot;treatment&quot;, value.name = &quot;neutrophils&quot;) %&gt;% na.omit() # danger! treatment_levels &lt;- melt_col_names fig6f[, treatment := factor(treatment, levels = treatment_levels)] # neutrophils is count/10^6 fig6f[, neutrophil_count := round(neutrophils*10^6, 0)] fig6f[1, neutrophil_count] ## [1] 4936007 #View(fig6f) "],["issues.html", "Chapter 13 Issues in inference 13.1 Replicated experiments – include \\(\\texttt{Experiment}\\) as a random factor (better than one-way ANOVA of means) 13.2 Comparing change from baseline (pre-post) 13.3 Longitudinal designs with more than one-post baseline measure 13.4 Normalization – the analysis of ratios 13.5 Don’t do this stuff 13.6 A difference in significance is not necessarily significant 13.7 Researcher degrees of freedom 13.8 Hidden code", " Chapter 13 Issues in inference 13.1 Replicated experiments – include \\(\\texttt{Experiment}\\) as a random factor (better than one-way ANOVA of means) Replicated experiments are the norm in bench biology. Researchers most commonly 1) analyze the mean response from each replicate, 2) pool all the data, ignoring that they come from multiple, independent experiments, or 3) analyze a “representive” experiment. The best practice for analyzing data from multiple, independent experiments, one that is almost never used outside of certain subfields where pipelines for this alternative exist, is a linear mixed model with the experiment id added as a random factor. A mixed-effect or repeated measures ANOVA is equivalent to one version of this kind of linear model. The analysis of data from pooled independent experiments with a linear mixed model is a best practice because it increases precision and power relative to a t-test/ANOVA of experiment means and avoids pseudoreplication. It’s important to understand the assumptions of the different models for analyzing replicated experiments. In a study with multiple, replicated experiments, the replications are “independent” – solutions are re-made, instruments have drifted and are re-calibrated, the grad student doing the work has lived another day. Each experiment has a unique set of factors that contribute to the error variance of the response variable. All measures within an experiment share the component of the error variance unique to that experiment and, as a consequence, the error (residuals) within an experiment are more similar to each other than they are to the residuals between experiments. This is a violation of the independent sampling assumption of the linear model and the lack of independence creates correlated error. The consequences of this on the three analyses are: Linear model/t-test/ANOVA of pooled data ignoring experiment. This model assumes perfect replication between experiments, that essentially, the world was exactly the same for each. If this assumption is true, each replicate within each experiment is an independent data point. A study with n = 5 mice per treatment replicated five times has n = 25. If there are two treatment levels, the t-test will have 48 df. But, if the measures within an experiment share a common source contributing to the variance of the response within that experiment (the temperature in the room was 1°C cooler causing a machine to give a slightly different reading), there are not 25 independent measures because the five measures within each experiment are not independendent. The true df will be less than 48, how much less depends on the magnitude of the correlated error resulting from the non-independence. Pooling data across experiments with correlated error creates an incorrectly small standard error of the mean, resulting in incorrectly narrow confidence intervals and incorrectly small p-values. Pooling data inflates false discovery. Linear model/t-test/ANOVA of experiment means. Analysis of the means violates the independence assumption because the means of the treatment levels within an independent experiment are expected to share common sources of measure variability of that experiment. Instead of inflating false discovery, this correlated error generally works against discovery. The correlated error among the means can be modeled to increase precision and gain power. Linear model of the experiment means with \\(\\texttt{experiment_id}\\) added as a random factor to account for correlated error due to the unique sources of variance within each experiment. This model is equivalent to a mixed effect ANOVA of the full data set. Reproducibility blues. Researchers are improving best practices by archiving data on public servers. Nevertheless, it is extremely rare to find the full data for many experiments. For analyses of the means from multiple experiments, researchers almost always archive the means – the data are means pooled – and not the full set of measurements from each experiment. Or, the full data are archived but the experiment id is not included – the data are complete pooled. There is no way to truly replicate a result if only the means are archived (were the means computed correctly?). And, there is no way to re-analyze the data with alternative models that require the full data with the experiment identified. 13.1.1 Multiple experiments Example 1 (wound healing Exp4d) Article source: Distinct inflammatory and wound healing responses to complex caudal fin injuries of larval zebrafish data source The experiment was designed to estimate the effect of a STAT3(https://en.wikipedia.org/wiki/STAT3){target=“_blank”} knockout on the number of cells expressing the cytoskeletal protein vimentin in the wounded tissue. Following Fisher, we can be more confident of an effect if we consistently get small p-values from replicate experiments. ids &lt;- levels(exp4d[, experiment_id]) n_exp &lt;- length(ids) p &lt;- list() for(exp_i in ids){ exp4d_m1 &lt;- lm(vimentin_cells ~ genotype, data = exp4d[experiment_id == exp_i,]) # subset exp4d_m1_test &lt;- coef(summary(exp4d_m1)) p[[exp_i]] &lt;- exp4d_m1_test[&quot;genotypeSTAT3_ko&quot;, &quot;Pr(&gt;|t|)&quot;] } data.table( experiment = ids, p = unlist(p) ) %&gt;% kable(digits = 3, caption = &quot;P-values from the three independent experients of the exp4d data.&quot;) %&gt;% kable_styling() Table 13.1: P-values from the three independent experients of the exp4d data. experiment p Exp1 0.023 Exp2 0.022 Exp3 0.017 There are methods for combining p-values but we do not need to do this because we have the raw data. 13.1.2 Models for combining replicated experiments A linear mixed model for combining replicated experiments is most easily fit using the aov_4 function from the afex package. The aov_4 function fits a mixed-effects ANOVA model, which is a factorial ANOVA that includes both fixed and random factors. In the Experiment 4d data, the variable \\(\\texttt{genotype}\\) is a fixed factor and the variable \\(\\texttt{experiment_id}\\) is a random factor. More information on fixed and random factors is given in the Linear Mixed Model chapter. The mixed-effect ANOVA models fit by aov_4 are not linear mixed models but inference is equivalent in balanced designs – I give more detail below. The aov_4 function uses a linear mixed model formula interface and is a useful substitute for actually fitting the equivalent linear mixed model because fitting a linear mixed model can be sensitive to the data and sometimes fails, even if the mixed-effect ANOVA model “works”. fit the model exp4d_m1 &lt;- aov_4(vimentin_cells ~ genotype + (genotype | experiment_id), data = exp4d) inference from the model exp4d_m1_emm &lt;- emmeans(exp4d_m1, specs = &quot;genotype&quot;) exp4d_m1_pairs &lt;- contrast(exp4d_m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) exp4d_m1_pairs %&gt;% kable(digits = c(1,2,3,1,2,2,1,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value STAT3_ko - STAT3_wt -3.5 0.048 2 -3.7 -3.29 -72.5 0.00019 plot the model # wrangling to get this to work # ggplot_the_response doesn&#39;t like the fit object from afex. That&#39;s okay, # we use it only to get the individual points exp4d_m0 &lt;- lm(vimentin_cells ~ genotype, data = exp4d) exp4d_m0 &lt;- lm(vimentin_cells ~ genotype * experiment_id, data = exp4d) exp4d_m0_emm &lt;- emmeans(exp4d_m0, specs = c(&quot;genotype&quot;, &quot;experiment_id&quot;)) ggplot_the_response(fit = exp4d_m0, fit_emm = exp4d_m0_emm, fit_pairs = exp4d_m1_pairs, y_label = &quot;vimGFP+ cell count&quot;, palette = pal_okabe_ito_blue) 13.1.3 Understanding Model exp4d_m1 The univariate and multivariate models fit by AOV_4 gives researchers an easy tool in R for replicating ANOVA results from other software, such as Graphpad Prism or JMP. But it is really the contrasts that we want, not the ANOVA table, and we can get these contrasts by passing the aov_4 object to emmeans. The formula in Model exp4d_m1 is misleading. The formula specifies a linear mixed model with a random intercept and a random slope (see Models with random factors – Blocking and pseudoreplication but this is not the model that is fit. The aov_4 function actually fits two models. A linear model of the aggregated data with adjusted degrees of freedom (the “aov” or univariate model). “Aggregated” data are the means of each treatment level within each experiment (the means-pooled data). A multivariate linear model (the multivariate model) of the aggregated data. A multivariate model has multiple Y variables. For the exp4d data, each experiment (containing the means for each treatments) is a seperate \\(Y\\) variable. The default output is the univariate model but the researcher can choose the multivariate model. exp4d_m1_pairs_multi &lt;- emmeans(exp4d_m1, specs = &quot;genotype&quot;, model = &quot;multivariate&quot;) %&gt;% contrast(method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) Table 13.2: Contrasts from the univariate and multivarate models fit to the exp4d data. contrast estimate SE df lower.CL upper.CL t.ratio p.value univariate model (m1) STAT3_ko - STAT3_wt -3.5 0.048 2 -3.7 -3.29 -72.52 0.00019 multivarite model (m2) STAT3_ko - STAT3_wt -3.5 0.048 2 -3.7 -3.29 -72.52 0.00019 The univariate and multivariate models are the same when there is only a single factor with two treatment levels, as with these data. Otherwise the two differ. See Section 16.7.10 in the Linear Mixed Models chapter. The univariate model is equivalent to the linear mixed model exp4d_m2 (see next section) of the aggregated data if there are no missing data. If there are the same number of replicates within all treatment by experiment combinations, then the univariate model is equivalent to the linear mixed model exp4d_m3 of the full (not aggregated) data. 13.1.4 The univariate model is equivalent to a linear mixed model of the aggregated data (Model exp4d_m2) aggregate the data # compute the experiment means exp4d_means &lt;- exp4d[, .(vimentin_cells = mean(vimentin_cells)), by = .(genotype, experiment_id)] fit the model exp4d_m2 &lt;- lmer(vimentin_cells ~ genotype + (1 | experiment_id), data = exp4d_means) Table 13.3: Contrasts from the univariate model fit by AOV_4 (exp4d_m1) and the linear mixed model of the aggregated data (exp4d_m2) contrast estimate SE df lower.CL upper.CL t.ratio p.value univariate model (exp4d_m1) STAT3_ko - STAT3_wt -3.5 0.048 2 -3.7 -3.29 -72.52 0.00019 linear mixed model (exp4d_m2) STAT3_ko - STAT3_wt -3.5 0.048 2 -3.7 -3.29 -72.52 0.00019 Notes The univariate model is equivalent to Model exp4d_m2 only if there is no missing data (for example, one treatment has no data in experiment 2). 2.The factor \\(\\texttt{experiment_id}\\) is added as a random intercept using the formula notation (1 | experiment_id). This random intercept models common variation shared within experiments. Random intercepts were introduced in Example 1 (exp1b) – a paired t-test is a special case of a linear mixed model in the Violations chapter and are described in more detail in Example 1 – A random intercepts and slopes explainer (demo1) in the Linear Mixed Model chapter. 13.1.5 A linear mixed model of the full data exp4d_m3 &lt;- lmer(vimentin_cells ~ genotype + (1|experiment_id) + (1|experiment_id:genotype), data = exp4d) Notes Model exp4d_m3 is equivalent to the univariate mixed-effect ANOVA (exp4d_m1) and the linear mixed model of the aggregated data (exp4d_m2) if there are the same number of subsamples in every genotype by experiment combination. Model exp4d_m3 is a linear mixed model with the random factor added as two random intercepts. The (1|experiment_id) intercept models common variation shared within experiments. The (1|experiment_id:genotype) is an interaction intercept that models common variation shared within experiment by genotype combinations. The function lmer returns the “boundary (singular) fit” message, which is a warning to researchers to use caution if using the model for inference. This message will not be rare when the the number of experiments is small (2-4). Note 5 raises the question, why bother with the linear mixed model of the full data when the aov_4 function always “works” and is the same when the design is balanced? The short answer is, just use the aov_4 model with the default (univariate model) outputs. The long answer is, read chapter Models with random factors – linear mixed models. 13.1.6 Analysis of the experiment means has less precision and power Compare inference from the mixed-ANOVA exp4d_m1 or the linear mixed model exp4d_m2 (these are the same) to a linear model of the experiment means (equivalent to a t-test/ANOVA) exp4d_m4 &lt;- lm(vimentin_cells ~ genotype, data = exp4d_means) Table 13.4: Contrasts from the univariate model fit by AOV_4 (exp4d_m1) and the linear model of the experiment means (exp4d_m4) contrast estimate SE df lower.CL upper.CL t.ratio p.value univariate model (exp4d_m1) STAT3_ko - STAT3_wt -3.5 0.048 2 -3.70 -3.29 -72.52 0.00019 linear model (exp4d_m4) STAT3_ko - STAT3_wt -3.5 0.486 4 -4.84 -2.15 -7.20 0.00197 Notes Model exp4d_m4 violates the independence assumption because residuals within an experiment are expected to be more similar. Analyzing the experiment means does not fix this. With the aggregated data, this correlated error masks the true effect. The SE for the estimate of the \\(\\texttt{genotype}\\) effect is much smaller for the mixed ANOVA/random intercept model than for the fixed effect model of experiment means. The degrees of freedom for the fixed effect model is slighly larger than that for the mixed model exp4d_m1. Degrees of freedom is a measure of independent variation – the fixed effect model assumes that every measure is independent. They aren’t. The inflated degrees of freedom is pseudoreplication. Despite the (slightly) inflated degrees of freedom of the fixed effects model, the mixed model has much more narrow CIs and smaller p-value. This is because of item 2. The increased precision and power of the mixed model relative to the fixed effect model of the experiment means is a general result and not one specific to this example. 13.1.7 Don’t do this – a t-test/fixed-effect ANOVA of the full data exp4d_m5 is a fixed effect model (there are no added random factors). exp4d_m5 &lt;- lm(vimentin_cells ~ genotype, data = exp4d) (#tab:issues-exp4d_m5-show)Contrasts from the univariate model fit by AOV_4 (exp4d_m1) and the linear model of the full data (exp4d_m5) contrast estimate SE df lower.CL upper.CL t.ratio p.value univariate model (exp4d_m1) STAT3_ko - STAT3_wt -3.50 0.048 2 -3.70 -3.29 -72.52 0.00019 linear model (exp4d_m5) STAT3_ko - STAT3_wt -3.49 0.789 58 -5.07 -1.91 -4.42 0.00004 Notes Model exp4d_m4 violates the independence assumption because residuals within an experiment are expected to be more similar. The SE for the estimate of the \\(\\texttt{genotype}\\) effect is much smaller for the mixed ANOVA/random intercept model than for the fixed effect model but the degrees of freedom for the fixed effect model is much greater than that for the mixed model exp4d_m1. Degrees of freedom is a measure of independent variation – the fixed effect model assumes that every measure is independent. They aren’t. The inflated degrees of freedom is pseudoreplication. The consequence of the pseudoreplication is a too-small p-value. Pseudoreplication leads to more false discovery. 13.2 Comparing change from baseline (pre-post) Figure 13.1: Serum DPP4 levels at baseline and 3 months in two groups. (A) The actual experiment: the two treatment levels are randomly allocated to treatment at baseline. We are interested in the effect of DMAB at 3 months and use the baseline measure to increase precision. (B) A thought experiment: the two treatment levels are different groups at baseline. We give DMAB to both groups at baseline to estimate the difference in response. In a longitudinal experiment, the response variable is measured on the same individuals both before (baseline) and after (post-baseline) some condition is applied. Experiments in which only one post-baseline measure is taken are known as pre-post experiments. For simplicity, I call the baseline measure \\(\\texttt{pre}\\) and the post-baseline measure \\(\\texttt{post}\\). Researchers often analyze pre-post data by comparing the change score (\\(\\texttt{post} - \\texttt{pre}\\)) between the groups using a \\(t\\)-test or one-way ANOVA (or a non-parametric equivalent such as Mann-Whitney-Wilcoxon). The linear model form of the t-test is \\[ \\texttt{post} - \\texttt{pre} = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\varepsilon \\] This is the change score model. A similar analysis is a t-test or ANOVA using the percent change from baseline as the response: \\[ \\frac{\\texttt{post} - \\texttt{pre}}{\\texttt{pre}} \\times 100 = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\varepsilon \\] The best practice for how to estimate the treatment effect depends very much on when the treatment is applied relative to the baseline (\\(\\texttt{pre}\\)) measure. Consider the two experiments in Figure 13.1 In 13.1A, the individuals are randomized into the “Placebo” and the “Denosumab” groups. Plasma DPP4 is measured at baseline, the treatment is applied, and, three months later, plasma DPP4 is re-measured. The treatment is “Denosumab” and we want to compare this to “Placebo”. The key feature of this design is the individuals are from the same initial group and have been treated the same up until the baseline measure. Consequently, the expected difference in means for any measure at baseline is zero. In 13.1B, the treatment of interest (knockout) was applied prior to baseline, DPP4 is measured at baseline without Denosumab and then both groups are given Denosumab and measured again three months later. The treatment is not Denosumab but “knockout” and we want to compare this to “wild type” in the two different conditions. The key here is that individuals are randomly sampled from two different groups (wild type and knockout). Consequently, we cannot expect the expected difference in means for any measure at baseline to be zero. The best practice for Design 2 is the change score model given above (or equivalents discussed below). The best practice for Design 1 is not the change score model but a linear model in which the baseline measure is added as a covariate. \\[ \\texttt{post} = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\beta_{2}(\\texttt{pre}) + \\varepsilon \\] This model is commonly known as the ANCOVA model (Analysis of Covariance) even if no ANOVA table is generated. The explanation for this best practice is given below, in the section regression to the mean. The ANCOVA linear model is common in clinical medicine and pharmacology, where researchers are frequently warned about regression to the mean from statisticians. By contrast, the ANCOVA linear model is rare in basic science experimental biology. The analysis of linear models with added covariates is the focus of the chapter Linear models with added covariates. Alert! If the individuals are sampled from the same population and treatment is randomized at baseline, do not test for a difference in means of the response variable at baseline and then use the ANCOVA linear model if \\(p &gt; 0.05\\) and the change score model \\(p &lt; 0.05\\). The best practice is only a function of the design, which leads to the expectation of the difference in means at baseline. If the individuals are sampled from the same population and treatment is randomized at baseline, the expected difference at baseline is zero. Use the ANCOVA linear model. What can go wrong if we use the change score model? Regression to the mean. If the individuals are sampled from the two populations because the treatment was applied prior to baseline, we cannot expect the difference at baseline to be zero (even if the treatment is magic). Use the change score model. What can go wrong if we use the ANCOVA linear model? Two things. First, the ANCOVA linear model computes a biased estimate of the true effect, meaning that as the sample size increases the estimate does not converge on the true value but the true value plus some bias. Second, the change score has more power than the ANCOVA linear model under the assumption of sampling from different populations at baseline. Alert! Researchers also use two-way ANOVA or repeated measures ANOVA to analyze data like these. Two-way ANOVA is invalid because the multiple measures on each individual violates the independence assumption. Repeated measures ANOVA will give equivalent results for a pre-post experiment but not for better practice methods that are available when there are more than one post-baseline measure. The better practice methods are linear models for correlated error, including GLS and linear mixed models. For a pre-post design, these models give equivalent results to the change score model but also allows the estimate of additional effects of interest (see below). For more detail on the analysis of pre-post experiments, see the Linear models for longitudinal experiments – I. pre-post designs chapter. 13.2.1 Pre-post example 1 (DPP4 fig4c) source: Identification of osteoclast-osteoblast coupling factors in humans reveals links between bone and energy metabolism The data in the left panel of Figure 13.1 are from an experiment to estimate the effect of Denosumab on the plasma levels of the enzyme DPP4 in humans. Denosumab is a monocolonal antibody that inhibits osteoclast maturation and survival. Osteoclasts secrete the enzyme DPP4. 13.2.1.1 For the ANCOVA linear model, the data need to be in wide format In the ANCOVA linear model, the baseline measure is added as a covariate and thought of as a separate variable and not a “response”. This makes sense – how could it be a “response” to treatment when the treatment hasn’t been applied? This means the baseline and post-baseline measures of DPP4 have to be in separate columns of the data (Table 13.5. Table 13.5: First six rows of the DPP4 data in wide format showing the baseline and post-baseline measures of DPP4 as separate variables. treatment DPP4_baseline DPP4_post id Denosumab 436 405 human_1 Denosumab 434 392 human_2 Denosumab 534 480 human_3 Denosumab 317 266 human_4 Denosumab 440 397 human_5 Denosumab 336 370 human_6 13.2.1.2 Fit the ANCOVA model m1 &lt;- lm(DPP4_post ~ treatment + DPP4_baseline, data = fig4c) 13.2.1.3 Inference The coefficient table m1_coef &lt;- cbind(coef(summary(m1)), confint(m1)) m1_coef %&gt;% kable(digits = c(1,2,2,4,1,1)) %&gt;% kable_styling() Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 70.4 24.85 2.83 0.0070 20.2 120.5 treatmentDenosumab -27.0 10.25 -2.63 0.0117 -47.7 -6.3 DPP4_baseline 0.8 0.06 14.08 0.0000 0.7 0.9 Notes Here, we care only about the \\(\\texttt{treatmentDenosumab}\\) row. This is the estimated effect of Denosumab on serum DPP4 adjusted for baseline (do not use the word “control” as the baseline values were not controlled in any manipulative sense). Chapter Linear models with added covariates explains the interpretation of these coefficients in more detail. The emmeans table m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_emm %&gt;% summary() %&gt;% kable(digits = c(1,2,2,0,1,1)) %&gt;% kable_styling() treatment emmean SE df lower.CL upper.CL Placebo 403.5 7.09 43 389.2 417.8 Denosumab 376.5 7.40 43 361.6 391.4 Notes The means are conditional on treatment and a value of \\(\\texttt{DPP4_baseline}\\). emmeans uses the grand mean of \\(\\texttt{DPP4_baseline}\\) to compute the conditional means. The conditional means are adjusted for baseline DPP4. Note that these means are not equal to the sample means. The contrast table m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) m1_pairs %&gt;% kable(digits = c(1,3,2,0,1,1,1,3)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Denosumab - Placebo -27.003 10.25 43 -47.7 -6.3 -2.6 0.012 Notes As in the coefficient table, the contrast is the estimated effect of Denosumab on serum DPP4. It is the difference in means adjusted (not controlled!) for baseline values. ggplot_the_model(m1, m1_emm, m1_pairs, y_label = &quot;Serum DPP4 (ng/mL)&quot;, effect_label = &quot;Effect (ng/mL)&quot;, palette = pal_okabe_ito_blue, legend_position = &quot;none&quot;) Figure 13.2: Estimated effect of Denosumab on serum DPP4 relative to placebo. Notes The treatment means in Figure 13.2 are conditional means adjusted for the baseline measure and are, therefore, not equal to the sample means. The estimated effect is the difference between the conditional means and not the sample means and the inferential statistics (CI, p-value) are based on this difference between the conditional and not the sample means. 13.2.2 What if the data in example 1 were from from an experiment where the treatment was applied prior to the baseline measure? 13.2.2.1 Fit the change score model m1 &lt;- lm(DPP4_post - DPP4_baseline ~ genotype, data = fig4c_fake) m1_emm &lt;- emmeans(m1, specs = &quot;genotype&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;) m1_pairs %&gt;% kable() %&gt;% kable_styling() contrast estimate SE df t.ratio p.value ko - wt -25.83333 11.53394 44 -2.239766 0.0302079 Notes The change score is created on the LHS of the model formula. Alternatively, the change score could be created as a variable in the fig4c_fake data.table using fig4c_fake[, change := DPP4_post - DPP4_baseline]. Making the change in the model formula shows the flexibility of the model formula method of fitting linear models in R. 13.2.2.2 Rethinking a change score as an interaction If the treatment is randomized at baseline, a researcher should focus on the effect of treatment (the difference between the post-basline measures), adjusted for the baseline measures. The addition of the baseline variable as a covariate increases the precision of the treatment effects and the power of the significance test. If the treatment is generated prior to baseline, a researcher should focus on the difference in the change from baseline to post-baseline, which is \\[ effect = (post_{ko} - pre_{ko}) - (post_{wt} - pre_{wt}) \\] This is the difference in change scores, which is a difference of differences. This difference of differences is the interaction effect between the genotype (“wt” or “ko”) and the time period of the measurement of DPP4 (baseline or post-baseline). The more usual way to estimate interaction effects is a linear model with two crossed factors, which is covered in more detail in the chapter # Linear models with two categorical X – Factorial designs (“two-way ANOVA”). The interaction effect (equal to the difference among the mean of the change scores) can be estimated with the model \\[ \\texttt{dpp4} = \\beta_{0} + \\beta_{1}(\\texttt{genotype}_{\\texttt{ko}}) + \\beta_{2}(\\texttt{time}_{\\texttt{post}}) + \\beta_{3}(\\texttt{genotype}_{\\texttt{ko}} \\times \\texttt{time}_{\\texttt{post}}) + \\epsilon \\] The R script for this looks like this m2_fixed &lt;- lm(dpp4 ~ genotype*time, data = fig4c_fake_long) This model has two factors (\\(\\texttt{genotype}\\) and \\(\\texttt{time}\\)), each with two levels. The two levels of \\(\\texttt{time}\\) are “pre” and “post”. \\(genotype_{ko}\\) is an indicator variable for “ko” and \\(time_{post}\\) is an indicator variable for “post” Don’t fit this model – the data violate the independence assumption! This violation arises because \\(\\texttt{dpp4}\\) is measured twice in each individual and both these measures are components of the response (both \\(\\texttt{dpp4}\\) measures are stacked into a single column). This violation doesn’t arise in the ANCOVA linear model because the baseline measures are a covariate and not a response (remember that the independence assumption only applies to the response variable). Alert! It is pretty common to see this model fit to pre-post and other longitudinal data. The consequence of the violation is invalid (too large) degrees of freedom for computing standard errors and p-values. This is a kind of pseudoreplication. To model the correlated error due to the two measures per individual, we use a linear mixed model using \\(\\texttt{id}\\) as the added random factor. Linear mixed models were introduced in the Violations of independence, homogeneity, or Normality chapter and are covered in more detail in the Models with random factors – Blocking and pseudoreplication chapter. m2 &lt;- lmer(dpp4 ~ genotype*time + (1|id), data = fig4c_fake_long) m2_emm &lt;- emmeans(m2, specs = c(&quot;genotype&quot;, &quot;time&quot;), lmer.df = &quot;Satterthwaite&quot;) m2_ixn &lt;- contrast(m2_emm, interaction = &quot;revpairwise&quot;) m2_ixn %&gt;% kable() %&gt;% kable_styling() genotype_revpairwise time_revpairwise estimate SE df t.ratio p.value ko - wt post - baseline -25.83333 11.53394 44 -2.239766 0.0302079 Notes This is the same result as that for the change score score model. The interaction effect is one of the coefficients in the model but to get the same CIs as those in the change score model, we need to use Satterthwaite’s formula for the degrees of freedom. We pass this to the emmeans function using lmer.df = \"Satterthwaite\" The interaction contrast is computed using the contrast function but using interaction = \"revpairwise\" instead of method = \"revpairwise\". 13.2.2.3 The linear mixed model estimates additional effects that we might want While the linear mixed model and change score model give the same result for the effect of treatment in response to the different conditions, the linear mixed model estimates additional effects that may be of interest. I use a real example (Example 2) to demonstrate this. 13.2.3 Pre-post example 2 (XX males fig1c) Source: AlSiraj, Y., Chen, X., Thatcher, S.E., Temel, R.E., Cai, L., Blalock, E., Katz, W., Ali, H.M., Petriello, M., Deng, P. and Morris, A.J., 2019. XX sex chromosome complement promotes atherosclerosis in mice. Nature communications, 10(1), pp.1-13. The experiments in this paper were designed to measure the independent effects of the sex chromosome complement (X or y) and gonads on phenotypic variables related to fat storage, fat metabolism, and cardiovascular disease. Response variable – \\(\\texttt{fat_mass}\\). Fat mass was measured in each mouse at baseline (exposed to the standard chow diet) and after one week on a western diet. Fixed factor – The design is two crossed factors (sex chromosome complement and gonad type) each with two levels but here I collapse the four treatment combinations into a single factor \\(\\texttt{treatment}\\) with four levels: “female_xx”, “female_xy”, “male_xx”, “male_xy”. Male and female are not the typical sex that is merely observed but are constructed by the presence or absence of SRY on an autosome using the Four Core Genotype mouse model. SRY determines the gonad that develops (ovary or testis). Females do not have the autosome with SRY. Males do. Similarly, the chromosome complement is not observed but manipulated. In “xx”, neither chromosome has SRY as the natural condition because there are two X chromosomes. In “xy”, SRY has been removed from the Y chromosome. Random factor \\(\\texttt{id}\\). The identification of the individual mouse. Planned comparisons “female_xy” - “female_xx” at baseline (chow diet) “male_xx” - “male_xy” at baseline (chow diet) “female_xy” - “female_xx” at one week (western diet) “male_xx” - “male_xy” at one week (western diet) the interaction contrast (3 - 1) which addresses, is the effect of the chromosome complement in females conditional on diet the interaction contrast (4 - 2) which addresses, is the effect of the chromosome complement in males conditional on diet 13.2.3.1 Fit the change score model The change score model only estimates planned comparisons 5 and 6. m1 &lt;- lm(week_1 - baseline ~ treatment, data = fig1c_wide) Inference from the change score model m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_planned &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) m1_planned[c(1, 6),] %&gt;% kable(digits = c(1,2,2,1,2,2,2,3)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value 1 female_xy - female_xx -0.54 0.48 16 -1.55 0.47 -1.13 0.277 6 male_xy - male_xx -1.00 0.48 16 -2.01 0.01 -2.09 0.053 13.2.3.2 Using the linear mixed model to compute all six planned comparisons m2 &lt;- lmer(fat_mass ~ treatment*time + (1|id), data = fig1c) m2_emm &lt;- emmeans(m2, specs = c(&quot;treatment&quot;, &quot;time&quot;), lmer.df = &quot;Satterthwaite&quot;) Notes important to add lmer.df = \"Satterthwaite\" argument interaction contrasts The interaction contrasts estimate planned comparisons 5 and 6. m2_ixn &lt;- contrast(m2_emm, interaction = c(&quot;revpairwise&quot;), by = NULL, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) m2_planned_ixn &lt;- m2_ixn[c(1,6), ] %&gt;% data.table() m2_planned_ixn %&gt;% kable(digits = c(1,1,2,2,1,2,2,2,3)) %&gt;% kable_styling() treatment_revpairwise time_revpairwise estimate SE df lower.CL upper.CL t.ratio p.value female_xy - female_xx week_1 - baseline -0.54 0.48 16 -1.55 0.47 -1.13 0.277 male_xy - male_xx week_1 - baseline -1.00 0.48 16 -2.01 0.01 -2.09 0.053 Notes These are same results as those using the change scores. Simple effects The simple effects estimate planned comparisons 1-4. # get simple effects from model m2_pairs &lt;- contrast(m2_emm, method = c(&quot;revpairwise&quot;), simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) # reduce to planned contrasts m2_planned_simple &lt;- m2_pairs[c(1,6,7,12),] %&gt;% data.table() # clarify contrast m2_planned_simple[, contrast := paste(time, contrast, sep = &quot;: &quot;)] # dump first two cols keep_cols &lt;- names(m2_planned_simple)[-(1:2)] m2_planned_simple &lt;- m2_planned_simple[, .SD, .SDcols = keep_cols] m2_planned_simple %&gt;% kable(digits = c(1,3,3,1,2,2,2,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value baseline: female_xy - female_xx -0.046 0.502 24.6 -1.08 0.99 -0.09 0.92766 baseline: male_xy - male_xx -1.422 0.502 24.6 -2.46 -0.39 -2.84 0.00902 week_1: female_xy - female_xx -0.582 0.502 24.6 -1.62 0.45 -1.16 0.25701 week_1: male_xy - male_xx -2.418 0.502 24.6 -3.45 -1.38 -4.82 0.00006 We can combine the six planned comparisons into a single table. # create contrast table for ixns m2_planned_ixn[, contrast := paste0(&quot;ixn: &quot;, treatment_revpairwise)] # dump first two cols keep_cols &lt;- names(m2_planned_ixn)[-(1:2)] m2_planned_ixn &lt;- m2_planned_ixn[, .SD, .SDcols = keep_cols] # row bind -- smart enought to recognize column order m2_planned &lt;- rbind(m2_planned_simple, m2_planned_ixn) m2_planned %&gt;% kable(digits = c(1,2,2,1,2,2,2,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value baseline: female_xy - female_xx -0.05 0.50 24.6 -1.08 0.99 -0.09 0.92766 baseline: male_xy - male_xx -1.42 0.50 24.6 -2.46 -0.39 -2.84 0.00902 week_1: female_xy - female_xx -0.58 0.50 24.6 -1.62 0.45 -1.16 0.25701 week_1: male_xy - male_xx -2.42 0.50 24.6 -3.45 -1.38 -4.82 0.00006 ixn: female_xy - female_xx -0.54 0.48 16.0 -1.55 0.47 -1.13 0.27697 ixn: male_xy - male_xx -1.00 0.48 16.0 -2.01 0.01 -2.09 0.05280 13.2.4 Regression to the mean Regression to the mean is the phenomenon that if an extreme value is sampled, the next sample will likely be less extreme – it is closer to the mean. This makes sense. If we randomly sample a single human male and that individual is 6’10” (about four standard deviations above the mean), the height of the next human male that we randomly sample will almost certainly be closer to the mean (about 5’10” in the united states). This phenomenon also applies to sampling a mean. If we randomly sample five human males and the mean height in the group is 5’6” (about 3 SEM below the mean), the mean height of the next sample of five human males that we measure will almost certainly be closer to the mean. And, this phenomenon applies to sampling a difference in means. If we randomly sample two groups of five human males and the difference between the mean heights is 5.7” (about 3 SED), then the difference in mean height in the next sample of two groups of human males that we measure will almost certainly be closer to zero. How does regression to the mean apply to the analysis of change scores in a pre-post experiment? Consider an experiment where the response is body weight in mice. In a pre-post experiment, mice are randomized to treatment group at baseline. Weight is measured at baseline and at post-baseline. We expect the difference in means at baseline to be zero. If there is no treatment effect, we expect the difference in means at post-baseline to be zero. Because of sampling error there is a difference in weight at baseline. Do we expect the mean difference to be the same post-baseline? No. Even if taken after only 1 hour, the post-baseline weight of each mouse would not equal the baseline weight because of variation in water intake and loss, food intake, fecal weight, and other variables that affect body weight (this is a within-mouse variance). There is a correlation between the pre and post measures – individual mice that weigh more than the mean at baseline will generally weigh more than the mean at post-baseline. But, the bigger this within-mouse variance (or, the longer the time difference between baseline and post-baseline), the smaller this correlation. The consequence is that all these uncontrolled variables contribute to the sampling variance of the means and the difference in means at baseline and post-baseline. If the difference is unusually large at baseline because of these uncontrolled factors contribution to within mouse variance, we expect the difference at post-baseline to be less large – there is regression to the mean (Figure 13.3A). This regression to the mean between the baseline and post-baseline measures will emerge as a treatment by time interaction, or, equivalently, a difference in the mean change score between treatments (Figure 13.3B). Figure 13.3: Regression to the mean. The individual values in (A) are fake data sampled from a normal distribution with true mean equal to 30 at baseline (gray line) or 35 at post-baseline(gray line). The unusually large different in means at baseline is an extreme event. Consequently, the difference at post-baseline is much smaller. This regression to the mean is easily visualized by the lines that converge at the post-baseline value. (B) The results of a linear model (or t-test) fit to the data in (A) using change scores as the response variable. The apparent treatment effect is due to regression to the mean. 13.3 Longitudinal designs with more than one-post baseline measure A rigorous analysis of longitudinal data typically requires sophisticated statistical models but for many purposes, longitudinal data can be analyzed with simple statistical models using summary statistics of the longitud data. Summary statistics include the slope of a line for a response that is fairly linear or the Area Under the Curve (AUC) for humped responses. 13.3.1 Area under the curve (AUC) An AUC (Area under the curve) is a common and simple summary statistic for analyzing data from a glucose tolerance test and many other longitudinal experiments. Here I use the AUC of glucose tolerance tests (GTT) as an example. 13.3.1.1 AUC and iAUC Let’s generate and plot fake GTT data for a single individual in order to clarify some AUC measurements and define new ones. fake_auc &lt;- data.table( time = c(0, 15, 30, 60, 120), glucose = c(116, 268, 242, 155, 121) ) fake_auc[, glucose_change := glucose - glucose[1]] Figure 13.4: Glucose tolerance curve measures for the fake GTT data. A. Glucose values for an individual, B. The glucose values shifted so the baseline value is zero. The post-baseline values are change scores (glucose - baseline), or change from baseline. The filled area is the AUC in (A) and the iAUC in (B). The dashed line is the mean glucose over the test period in (A) and the mean change from baseline over the test period in (B). AUC – The glucose tolerance curve for an individual is a connected set of straight lines that serves as a proxy for the continuous change of glucose over the period (Figure 13.4A). The AUC is the area under this set of straight lines (Figure 13.4A) and is conveniently computed as the sum of the areas of each of the connected trapezoids created by the connected lines. iAUC – the incremental AUC (iAUC) is the baseline-zeroed AUC. It can be visualized as the area under the connected lines that have been rigidly shifted down so that the baseline value is zero (Figure 13.4B). The iAUC is computed using the trapezoid rule after first subtracting the individual’s baseline value from all of the individual’s values (including the baseline). 13.3.1.2 Rethinking the iAUC as a change-score The baseline-zeroed values of glucose used to compute iAUC are change scores from the baseline measure. This makes the iAUC a change score – it is the AUC minus the area under the baseline. 13.3.1.3 Rethinking a t-test of the AUC as a t-test of the glucuose concentration averaged over the test period The glucose concentration averaged over the post-baseline period for an individual is \\(glucose_{gtt-post} = \\frac{AUC}{Period}\\). Importantly, a t-test of \\(glucose_{gtt-post}\\) is equivalent to a t-test of \\(AUC\\) because the mean glucose values are simply the AUC values times the same constant (the test period) for all individuals. 13.3.1.4 Rethinking a t-test of the iAUC as a t-test of the change from baseline (change score) averaged over the test period The change from baseline (change score) averaged over the test period for an individual is \\(glucose_{gtt-change} = \\frac{iAUC}{Period}\\). As above, a t-test of \\(glucose_{gtt-change}\\) is equivalent to a t-test of \\(iAUC\\) because the \\(glucose_{gtt-change}\\) values are simply the iAUC values times the same constant (the test period) for all individuals. 13.3.1.5 Rethinking AUC as a pre-post design. We can now rethink data used to construct an AUC as a pre-post design using the baseline value (\\(glucose_0\\)) as a measure of \\(pre\\), \\(glucose_{gtt-post}\\) as a measure of \\(post\\) and \\(glucose_{gtt-change}\\) as a measure of \\(post - pre\\) (note that \\(glucose_{gtt-change} = glucose_{gtt-post} - glucose_0\\)). And, we can use the principles outlined in Comparing change from baseline (pre-post) above to determine best practices. 13.3.1.6 Best practice strategies for analyzing AUC If the treatment was applied prior to the baseline measure, then use the change score model (Or use the linear mixed model if you want to estimate the effect at baseline). This is the most common kind of design in the experimental biology literature glucose_gtt_post - glucose_0 ~ treatment iauc ~ treatment. The t and p values are equivalent to those in 1a glucose ~ treatment*time + (1|id). This is a linear mixed model that allows the computation of both the effect of treatment at baseline and the effect of treatment on the change in the response to the condition (the interaction effect). This is not a LMM of the glucose values at all time periods but a pre-post LMM with the values of \\(\\texttt{glucose_0}\\) and \\(\\texttt{glucose_gtt_post}\\) stacked in the data column \\(\\texttt{glucose}\\). The t and p values for the interaction effect are equivalent to those in 1a and 1b. If treatment is randomized at baseline, use the ANCOVA linear model. glucose_post ~ treatment + glucose_0. auc ~ treatment + glucose_0. The t and p values are equivalent to those in 2a but the units of the response or the effect. 13.3.1.7 The difference in iAUC between groups is an interaction effect. This is an important recognition. If the treatment was not randomized at baseline, then the potential effects in a glucose tolerance test are: the effect of treatment at baseline, which is a measure of what is going on independent of added glucose. This is the baseline effect. the effect of glucose infusion, which is a measure of the physiological response during the absorptive state. This effect in each treatment level are the change scores. Researchers typically are not interested in this effect (we know glucose levels rise then fall). the difference in the change from baseline to the new condition (glucose infusion), which is equivalent to the difference in the change scores. This is the interaction effect. An interaction effect is evidence that the difference between treatment and control during the post-baseline (absorptive state) period is not more of the same difference occurring at baseline (fasted state) but something different (Figure 13.5). Figure 13.5: Effects in a glucose tolerance data in an experiment in which the treatment is not randomized at baseline. “something different is the interaction effect”. It is the difference in the change from baseline (or the difference in the change scores). iAUC is a change score (see Rethinking the iAUC as a change-score above). Recall (or re-read) from section 13.2.2.2 above that the difference in the mean change-score between two groups is the interaction effect of the linear model with two factors $} (“cn” and “tr”) and $} (“pre” and “post”) and their interaction. \\[ \\texttt{glucose} = \\beta_0 + \\beta_1 (\\texttt{treatment}_\\texttt{tr}) + \\beta_2 (\\texttt{time}_\\texttt{post}) + \\beta_3 (\\texttt{treatment}_\\texttt{tr} \\times \\texttt{time}_\\texttt{post}) + \\varepsilon \\] This means a t-test of iAUC is equivalent to thet-test of the interaction effect of treatment by time. This recognition adds an important perspective to the controversy of using iAUC in the analysis of glucose tolerance curves. iAUC is often used in place of AUC to “adjust” for baseline variation in glucose with the belief that this adjustment makes the AUC measure independent of (uncorrelated with) baseline glucose. As Allison et al. note, a change score doesn’t do this for us. The correct way to adjust for baseline variation is adding the baseline measure as a covariate in the linear model (Section 13.2 above). \\[ \\texttt{glucose_mean_post} = \\beta_0 + \\beta_1 (\\texttt{treatment}_\\texttt{tr}) + \\beta_2 (\\texttt{glucose_0}) + \\varepsilon \\] Nevertheless, in a pre-post design, if the treatment is applied prior to the baseline measure, it is the interaction effect that we want as the measure of the treatment effect and not the difference between post-baseline means conditional on (adjusted for) baseline. That is we want the change score model and not the ANCOVA linear model. 13.3.1.8 Issues in the analysis of designs where the treatment is applied prior to the baseline measure Almost all glucose tolerance tests in the experimental biology literature have designs where the treatment (genotype, diet, exercise) was applied prior to the baseline measure and we cannot expect the difference in means to be zero at baseline. In these, it is common, but far from standard, for researchers to analyze the data using t tests or post-hoc tests with the AUC adjusted for baseline as the response. This is equivalent to the recommended linear model in 1b. Two common alternative analyses with potential consequences that can severely mislead the researcher because of conflated effects are Separate t tests at each time point. t test/post-hoc tests of \\(\\texttt{auc}\\) (the standard AUC) The reason that these can mislead is because the results conflate the baseline effect and the interaction effect (Figure 13.5). Both the baseline effect and the interaction effect are of physiological interest. The difference in the mean of the AUC combines these two effects. The difference in the means at any post-baseline time point combines these two effects. A t-test of the AUC or separate t-tests at the post-baseline time points conflate these effects. The conflated results muddle the physiology. If a researcher wants to simply conclude “The knockout causes glucose intolerance” then the full AUC is okay but the researcher should recognize that this is the question they are trying to answer. But if a researcher is asking, “is the difference between treatment groups in the absorptive (post-baseline) state something different, or more of the same, as the difference between treatment groups in the fasted (baseline) state?”, then the researcher should avoid t-tests of the AUC or the separate t-tests at post-baseline times. Two common, alternative analyses that can mislead because of model assumptions that are more severely violated than best practice models are two-way ANOVA with treatment and time as the factors, followed by post-hoc tests. The advantage of this analysis is the ability to estimate the effect at baseline and the interaction effect. But, the correlated error due to the multiple measures on each individual violates the independence assumption. Inference from this model will generally be optimisitic – the CIs will be too narrow and the p-values too small. This is an example of pseudoreplication. The correlated error can be modeled with a GLS linear model or a Linear Mixed Model. repeated measures ANOVA with treatment and time as the factors, followed by post-hoc tests. Like the two-way ANOVA, a repeated measures ANOVA can be used to estimate both the baseline and interaction effects. Unlike the two-way ANOVA, a repeated measures ANOVA models the correlated error. But the model for the correlated error is too unrealistic. The better alternatives for modeling the correlated error are the GLS linear model or the Linear Mixed Model. 13.3.1.9 AUC Example 1 – Treatment applied prior to baseline Source: Innervation of thermogenic adipose tissue via a calsyntenin 3β–S100b axis Source data: Fig. 3f 13.3.1.9.1 An initial plot 13.3.1.9.2 Inference change-score linear model (model 1a), which estimates the interaction effect (the effect of treatment on the difference in the change from baseline to post-baseline). m1 &lt;- lm(glucose_gtt_post - glucose_0 ~ treatment, data = fig3f_wide) m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) m1_pairs %&gt;% kable(digits = 3) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value KO - WT 48.304 36.904 12 -32.104 128.711 1.309 0.215 # increased digits to compare with lmm below Notes The estimate is not the average difference over the period but the difference in the average change from baseline. The knockout has an average change from baseline that is 48.3 mg/dL larger than the average change from baseline of the wildtype. Over the period, blood glucose in the knockout is 48.3 mg/dL larger than the expected difference if the only mechanisms generating a difference post-baseline are the same as the mechanisms generating the differences at baseline. This is equivalent to a t test of the AUC adjusted for baseline ($) m1_t &lt;- t.test(iauc ~ treatment, data = fig3f_wide, var.equal = TRUE) %&gt;% tidy() m1_t[1, c(1, 4:8)] ## # A tibble: 1 × 6 ## estimate statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -5796. -1.31 0.215 12 -15445. 3852. Notes The estimate of the effect is the average change from baseline over the period times the period. Can anyone look at this number and claim with sincerity, wow that is huge!. The estimate from the change-score model, which is simply the difference in the average change from baseline over the period is a number that should be interpretable. The change-score model (or the analysis of the AUC adjusted for baseline) does not estimate the effect of treatment at baseline (the effect in the fasted state). Researchers probably want this. For this we need a linear model with correlated error such as a linear mixed model. Linear mixed model (model 1c), which estimates the interaction effect and the baseline effect. m2 &lt;- lmer(glucose ~ treatment*time + (1|id), data = fig3f_long) m2_emm &lt;- emmeans(m2, specs = c(&quot;treatment&quot;, &quot;time&quot;), lmer.df = &quot;Satterthwaite&quot;) m2_pairs &lt;- contrast(m2_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) m2_ixn &lt;- contrast(m2_emm, interaction = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) m2_pairs %&gt;% kable(digits = 3) %&gt;% kable_styling() time treatment contrast estimate SE df lower.CL upper.CL t.ratio p.value glucose_0 . KO - WT 111.571 37.454 18.976 33.172 189.970 2.979 0.008 glucose_gtt_post . KO - WT 159.875 37.454 18.976 81.476 238.274 4.269 0.000 . WT glucose_gtt_post - glucose_0 191.518 26.095 12.000 134.661 248.375 7.339 0.000 . KO glucose_gtt_post - glucose_0 239.821 26.095 12.000 182.965 296.678 9.190 0.000 m2_ixn %&gt;% kable(digits = 3) %&gt;% kable_styling() treatment_trt.vs.ctrl time_trt.vs.ctrl estimate SE df lower.CL upper.CL t.ratio p.value KO - WT glucose_gtt_post - glucose_0 48.304 36.904 12 -32.104 128.711 1.309 0.215 # increased digits to compare to change-score model above Notes The treatment effect at baseline is in the first row of the m2_pairs contrast table from the linear mixed model. The interaction effect (the effect of treatment on the change from baseline) is in the m2_ixn contrast table from the linear mixed model. The estimate, SE, confidence intervals, t-value, and p-value are the same as those from the change score model in m1_pairs. 13.3.1.9.3 A t test of the AUC or separate t-tests at each time point result in ambiguous inference t-test of the AUC m3_t &lt;- t.test(auc ~ treatment, data = fig3f_wide, var.equal = TRUE) %&gt;% tidy() m3_t[1, c(1, 4:8)] ## # A tibble: 1 × 6 ## estimate statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -19185 -3.52 0.00422 12 -31057. -7313. Separate t-tests at each time point m4_t0 &lt;- t.test(time_0 ~ treatment, data = fig3f_wide, var.equal = TRUE) m4_t30 &lt;- t.test(time_30 ~ treatment, data = fig3f_wide, var.equal = TRUE) m4_t60 &lt;- t.test(time_60 ~ treatment, data = fig3f_wide, var.equal = TRUE) m4_t90 &lt;- t.test(time_90 ~ treatment, data = fig3f_wide, var.equal = TRUE) m4_t120 &lt;- t.test(time_120 ~ treatment, data = fig3f_wide, var.equal = TRUE) m4_t &lt;- data.table( Time = times, p.value = c(m4_t0$p.value, m4_t30$p.value, m4_t60$p.value, m4_t90$p.value, m4_t120$p.value ) ) m4_t %&gt;% kable %&gt;% kable_styling() Time p.value 0 0.0014957 30 0.0006251 60 0.0096282 90 0.0137238 120 0.0263193 13.4 Normalization – the analysis of ratios 13.4.1 Kinds of ratios in experimental biology The ratio is a density (count per length/area/volume) or a rate (count/time). Example: number of marked cells per area of tissue. Best practice: GLM for count data with an offset in the model, where an offset is the denominator of the ratio. The ratio is relative to a standard (“normalized”). Example: expression of focal mRNA relative to expression of a standard mRNA that is thought not to be affected by treatment. Best practice: GLM for count data with an offset in the model, where an offset is the denominator of the ratio. The ratio is a proportion (or percent). Example: Number of marked cells per total number of cells. Best practice: GLM logistic. The ratio is relative to a whole and both the thing in the numerator and the thing in the denominator grow (allometric data). Example: adipose mass relative to total lean body mass. Best practice: ANCOVA linear model. Alert! – It has been known for more than 100 years, and repeatedly broadcasted, that inference from ratios of allometric data range from merely wrong (the inferred effect size is in the right direction, but wrong) to absurd (the direction of the inferred effect is opposite that of the true effect). 13.4.2 Example 1 – The ratio is a density (number of something per area) Researchers frequently count objects and compare the counts among treatments. A problem that often arises is that the counts are made in samples with different areas or volumes of tissue. As a consequence, the variation in treatment response is confounded with tissue size – samples with higher counts may have higher counts because of a different response to treatment, a larger amount of tissue, or some combination. The common practice in experimental biology is to adjust for tissue size variation by constructing the ratio \\(\\frac{count}{area}\\) and then testing for a difference in the ratio using either a linear model NHST (t-test/ANOVA) or a non-parametric NHST (Mann-Whitney-Wilcoxan). The issue is, the initial counts will have some kind of count distribution (Poisson or negative binomial) that can sometimes look like a sample from a normal distribution (see Introducing Generalized Linear Models using a count data example). The ratio will have some kind of ratio distribution that is hard to model correctly. A better practice is to model the count using a Generalized Linear Model (GLM) and an offset that adjusts for differences in the area of the sample. NHST of the ratios will perform okay in the sense of Type I error that is close to nominal but will have relatively low power compared to a generalized linear model with offset. If the researcher is interested in best practices including the reporting of uncertainty of estimated effects, a GLM will have more useful confidence intervals – for example CIs from linear model assuming Normal error can often include absurd values such as ratios less than zero. Source article (Fernández, Álvaro F., et al. “Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice.” Nature 558.7708 (2018): 136-140.)https://www.nature.com/articles/s41586-018-0162-7 Public source Source data for Fig. 3 The example here is from Fig 3b. Response variable – number of TUNEL+ cells measured in kidney tissue, where a positive marker indicates nuclear DNA damage. Background. The experiments in Figure 3 were designed to measure the effect of a knock-in mutation in the gene for the beclin 1 protein on autophagy and tissue health in the kidney and heart. The researchers were interested in autophagy because there is evidence in many non-mammalian model organisms that increased autophagy reduces age-related damage to tissues and increases health and lifespan. BCL2 is an autophagy inhibitor. Initial experiments showed that the knock-in mutation in beclin 1 inhibits BCL2. Inhibiting BCL2 with the knock-in mutation should increase autophagy and, as a consequence, reduce age-related tissue damage. The researchers measured Tunel+ cells in both 2 month old and 20 month old mice in order to look at age related effects. This design is factorial with two factors (\\(\\texttt{age}\\) and \\(\\texttt{genotype}\\). Because we haven’t covered factorial designs in this text, I limit the analysis to the 20 month old mice. Design - single factor \\(\\texttt{genotype}\\) with levels “WT” (wildtype) and “KI” (knock-in). The code for importing the exp3b data is in Section 13.8.6 below. 13.4.2.1 Fit the models exp3b_m1 &lt;- lm(count_per_area ~ genotype, data = exp3b) exp3b_m2 &lt;- glm.nb(positive_nuclei ~ genotype + offset(log(area_mm2)), data = exp3b) exp3b_m3 &lt;- wilcox.test(count_per_area ~ genotype, data = exp3b) 13.4.2.2 Check the models ggcheck_the_model(exp3b_m1) exp3b_m2_simulation &lt;- simulateResiduals(fittedModel = exp3b_m2, n = 250) plot(exp3b_m2_simulation, asFactor = FALSE) 13.4.2.3 Inference from the model exp3b_m2_coef &lt;- cbind(coef(summary(exp3b_m2)), confint(exp3b_m2)) Estimate Std. Error z value Pr(&gt;|z|) 2.5 % 97.5 % (Intercept) 0.79 0.295 2.7 0.007 0.26 1.43 genotypeKI -0.76 0.393 -1.9 0.053 -1.55 0.01 exp3b_m2_emm &lt;- emmeans(exp3b_m2, specs = c(&quot;genotype&quot;), type=&quot;response&quot;) genotype response SE df asymp.LCL asymp.UCL WT 56.8 16.74 Inf 31.9 101.22 KI 26.6 6.91 Inf 16.0 44.27 exp3b_m2_pairs &lt;- contrast(exp3b_m2_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) contrast ratio SE df asymp.LCL asymp.UCL null z.ratio p.value KI / WT 0.47 0.184 Inf 0.22 1.01 1 -1.93266 0.1 Notes 13.4.2.4 Plot the model The code for this plot is in Section 13.8.7 below. 13.4.3 Example 2 – The ratio is normalizing for size differences data_from &lt;- &quot;A big-data approach to understanding metabolic rate and response to obesity in laboratory mice&quot; file_name &lt;- &quot;mmpc_all_phases.csv&quot; file_path &lt;- here(data_folder, data_from, file_name) geometric_mean &lt;- function(x){ gm &lt;- exp(mean(log(x))) return(gm) } exp1 &lt;- fread(file_path) exp1 &lt;- exp1[acclimation == &quot;TRUE&quot;, ] exp1[, resid_mass := total_mass - fat_mass - lean_mass] exp1[, nonfat_mass := total_mass - fat_mass] size_cols &lt;- c(&quot;fat_mass&quot;, &quot;lean_mass&quot;, &quot;resid_mass&quot;) exp1[, size := apply(exp1[, .SD, .SDcols = size_cols], 1, geometric_mean)] size_cols &lt;- c(&quot;fat_mass&quot;, &quot;lean_mass&quot;, &quot;resid_mass&quot;, &quot;total_mass&quot;, &quot;size&quot;) cor(exp1[, .SD, .SDcols = size_cols]) ## fat_mass lean_mass resid_mass total_mass size ## fat_mass 1.0000000 0.6486331 -0.03980370 0.94096284 0.8192087 ## lean_mass 0.6486331 1.0000000 -0.41585035 0.78512117 0.4214359 ## resid_mass -0.0398037 -0.4158503 1.00000000 0.05819842 0.4595406 ## total_mass 0.9409628 0.7851212 0.05819842 1.00000000 0.8589070 ## size 0.8192087 0.4214359 0.45954060 0.85890700 1.0000000 13.5 Don’t do this stuff 13.5.1 Normalize the response so that all control values are equal to 1. In many experiments, the response variable is a measure with units that are not readily interpretable biologically. In these experiments, researchers often normalize the response by the mean of the reference group. \\[ y_i \\; \\mathrm{(normalized)} = \\frac{y_i}{\\overline{y}_{ref}} \\] The normalized response variable is a multiple of (or a fraction) of the mean response of the reference group. Less often, researchers normalize by the value of the reference within a batch – for example, the mean of the control group within each independent experiment – and then analyze the batch means with a t-test or ANOVA. If the goal of this by-experiment_id normalization is to adjust for the variance among the experiments, the proper way to do this is Linear mixed model for combining replicated experiments above. Regardless of the goal, don’t do this. There is no variance in the reference group as all n values are 1. A researcher is telling the t-test or ANOVA that there is no natural variability in the reference group (and the values were measured without measurement error). A classical (Student) t-test or ANOVA will have an incorrectly small error variance (\\(\\sigma\\)) because this zero-variance will go into the computation of the pooled (modeled) variance. The consequence is incorrectly small standard errors, confidence intervals, and p-values, and inflated false discovery. Let’s do this, to show why we don’t do this. All examples that I’ve found in the literature only archive the batch-normalized data and not the raw data so I’m using fake data for this. set.seed(9) n_exp &lt;- 8 # number of experiments beta_0 &lt;- 100 beta_1 &lt;- 1.5 # genotype effect as a multiple mu &lt;- c(beta_0, beta_0*beta_1) sigma &lt;- (beta_1 - 1)*beta_0 # iid error rho &lt;- 0.7 # correlated error Sigma &lt;- matrix(c(sigma^2, rho*sigma^2, rho*sigma^2, sigma^2), nrow = 2) fake_data &lt;- data.table(NULL) fake_data[, genotype := rep(c(&quot;wt&quot;, &quot;ko&quot;), each = n_exp)] fake_data[, genotype := factor(genotype, levels = c(&quot;wt&quot;, &quot;ko&quot;))] fake_data[, experiment_id := rep(paste0(&quot;exp_&quot;, 1:n_exp), 2)] # modeling the experiment means raw &lt;- rmvnorm(n_exp, mean = mu, sigma = Sigma) fake_data[, y := c(raw)] # mean-normalize by the mean of the experiment means of wt group fake_data[, y_correct := c(raw/mean(raw[,1]))] # id-normalize by experiment mean of wt group for each experiment_id fake_data[, y_wrong := c(raw/raw[,1])] The best practice is a linear mixed model using the mean-normalized response (the paired t-test is a specific case of this). m1 &lt;- lmer(y_correct ~ genotype + (1|experiment_id), data = fake_data) m1_emm &lt;- emmeans(m1, specs = &quot;genotype&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) The common practice is a linear model using the mean-normalized response (the Student t-test is a specific case of this). This model violates the independence assumption but this violation usually results in less precision and lower power. m2 &lt;- lm(y_correct ~ genotype, data = fake_data) m2_emm &lt;- emmeans(m2, specs = &quot;genotype&quot;) m2_pairs &lt;- contrast(m2_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) The incorrect practice is a linear model (or t-test or ANOVA) using the id-normalized response. This model violates heterogeneity and estimates the error variance using a group with zero variation. m3 &lt;- lm(y_wrong ~ genotype, data = fake_data) m3_emm &lt;- emmeans(m3, specs = &quot;genotype&quot;) m3_pairs &lt;- contrast(m3_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) A summary table of the three models … method contrast estimate SE df lower.CL upper.CL t.ratio p.value paired t ko - wt 0.206 0.09 7 0.00 0.41 2.42 0.0459 student t ko - wt 0.206 0.16 14 -0.13 0.54 1.33 0.2047 dont do this t ko - wt 0.318 0.13 14 0.03 0.60 2.40 0.0308 and a plot of the mean-normalized and id-normalized data and modeled means and CIs. gg1 &lt;- ggplot_the_response(m1, m1_emm, m1_pairs) gg2 &lt;- ggplot_the_response(m3, m3_emm, m3_pairs, dots = &quot;jitter&quot;) set.seed(1) plot_grid(gg1, gg2, ncol = 2, labels = &quot;AUTO&quot;) Figure 13.6: Plot of the fake normalized data and modeled means and CI from A) model m1, the linear mixed model with experiment_id as a random factor and B) model m3, the linear model of the response normalized to each experiment mean. Notes 13.6 A difference in significance is not necessarily significant Thermal stress induces glycolytic beige fat formation via a myogenic state Figure 2j 13.7 Researcher degrees of freedom 13.8 Hidden code 13.8.1 Import exp4d vimentin cell count data (replicate experiments example) data_from &lt;- &quot;Distinct inflammatory and wound healing responses to complex caudal fin injuries of larval zebrafish&quot; file_name &lt;- &quot;elife-45976-fig4-data2-v1.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp4d_wide &lt;- read_excel(file_path, sheet = &quot;Sheet1&quot;, range = &quot;A16:C49&quot;) %&gt;% data.table() exp4d_wide[, experiment_id := nafill(Replicate, type = &quot;locf&quot;)] exp4d_wide[, experiment_id := paste0(&quot;Exp&quot;, experiment_id)] old_cols &lt;- c(&quot;STAT3 +/+&quot;, &quot;STAT3 -/-&quot;) genotype_levels &lt;- c(&quot;STAT3_wt&quot;, &quot;STAT3_ko&quot;) setnames(exp4d_wide, old = old_cols, new = genotype_levels) exp4d &lt;- melt(exp4d_wide, id.vars = &quot;experiment_id&quot;, measure.vars = genotype_levels, variable.name = &quot;genotype&quot;, value.name = &quot;vimentin_cells&quot;) %&gt;% # cell count na.omit() exp4d[, genotype := factor(genotype, levels = genotype_levels)] 13.8.2 Import Fig4c data data_from &lt;- &quot;Identification of osteoclast-osteoblast coupling factors in humans reveals links between bone and energy metabolism&quot; file_name &lt;- &quot;41467_2019_14003_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig4c_type &lt;- c(&quot;text&quot;, rep(&quot;numeric&quot;, 8)) fig4c &lt;- read_excel(file_path, sheet = &quot;Fig4C and 4E&quot;, range = &quot;A3:I48&quot;, col_names = FALSE, col_types &lt;- fig4c_type) %&gt;% data.table() # DPP4 (ng/mL), GLP-1 (pM), Glucose (mmol/L), Insulin (uIU/mL) measures &lt;- c(&quot;DPP4&quot;, &quot;GLP1&quot;, &quot;Glucose&quot;, &quot;Insulin&quot;) # post is 3 months post treatment period &lt;- c(&quot;baseline&quot;, &quot;post&quot;) new_colnames &lt;- paste(rep(measures, 2), rep(period, each = 4), sep = &quot;_&quot;) old_colnames &lt;- colnames(fig4c) setnames(fig4c, old = old_colnames, new = c(&quot;treatment&quot;, new_colnames)) treatment_levels &lt;- c(&quot;Placebo&quot;, &quot;Denosumab&quot;) fig4c[, treatment := factor(treatment, levels = treatment_levels)] fig4c[, id := paste0(&quot;human_&quot;, .I)] # .I inserts row number #View(fig4c) fig4c_long &lt;- melt(fig4c, id.vars = c(&quot;id&quot;, &quot;treatment&quot;), measure.vars = c(&quot;DPP4_baseline&quot;, &quot;DPP4_post&quot;), variable.name = &quot;time&quot;, value.name = &quot;dpp4&quot;) fig4c_long[, time := substr(as.character(time), 6, nchar(as.character(time)))] fig4c_long[, time := factor(time, levels = c(&quot;baseline&quot;, &quot;post&quot;))] # View(fig4c_long) 13.8.3 XX males fig1c data_from &lt;- &quot;XX sex chromosome complement promotes atherosclerosis in mice&quot; file_name &lt;- &quot;41467_2019_10462_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig1c_1 &lt;- read_excel(file_path, sheet = &quot;Figure 1C&quot;, range = &quot;C3:G6&quot;, col_names = FALSE) %&gt;% data.table() setnames(fig1c_1, old = colnames(fig1c_1), new = paste(&quot;mouse&quot;, 1:5)) fig1c_1[, time := rep(c(&quot;baseline&quot;, &quot;week_1&quot;), each = 2)] fig1c_1[, sex := rep(c(&quot;female&quot;, &quot;male&quot;), 2)] fig1c_2 &lt;- read_excel(file_path, sheet = &quot;Figure 1C&quot;, range = &quot;I3:M6&quot;, col_names = FALSE) %&gt;% data.table() setnames(fig1c_2, old = colnames(fig1c_2), new = paste(&quot;mouse&quot;, 6:10)) fig1c_2[, time := rep(c(&quot;baseline&quot;, &quot;week_1&quot;), each = 2)] fig1c_2[, sex := rep(c(&quot;female&quot;, &quot;male&quot;), 2)] fig1c &lt;- rbind(data.table(chromosome = &quot;xx&quot;, melt(fig1c_1, id.vars = c(&quot;sex&quot;, &quot;time&quot;), variable.name = &quot;id&quot;, value.name = &quot;fat_mass&quot;)), data.table(chromosome = &quot;xy&quot;, melt(fig1c_2, id.vars = c(&quot;sex&quot;, &quot;time&quot;), variable.name = &quot;id&quot;, value.name = &quot;fat_mass&quot;))) # id is not unique fig1c[, id := paste(sex, id, sep=&quot;_&quot;)] # now it is fig1c[, treatment := paste(sex, chromosome, sep = &quot;_&quot;)] # unique(fig1c$treatment) # &quot;female_xx&quot; &quot;male_xx&quot; &quot;female_xy&quot; &quot;male_xy&quot; treatment_levels &lt;- c(&quot;female_xx&quot;, &quot;female_xy&quot;, &quot;male_xx&quot;, &quot;male_xy&quot;) fig1c[, treatment := factor(treatment, levels = treatment_levels)] fig1c_wide &lt;- dcast(fig1c, chromosome + sex + id + treatment ~ time, value.var = &quot;fat_mass&quot;) fig1c_wide[, percent_change := (week_1 - baseline)/baseline*100] 13.8.4 Generation of fake data to illustrate regression to the mean n_iter &lt;- 1000 n &lt;- 6 np1 &lt;- n + 1 N &lt;- n*2 mu &lt;- 30 beta_1 &lt;- 5 sigma_among &lt;- 2 sigma_within &lt;- sigma_among/2 rho &lt;- sigma_among/(sigma_among + sigma_within) max_baseline_d &lt;- -9999 time_levels &lt;- c(&quot;pre&quot;, &quot;post&quot;) treatment_levels &lt;- c(&quot;cn&quot;,&quot;tr&quot;) fake_data &lt;- data.table( treatment = rep(rep(treatment_levels, each = n), 2), time = rep(time_levels, each = n*2), id = rep(paste0(&quot;mouse_0&quot;, 1:N), 2) ) fake_data[, time := factor(time, levels = time_levels)] fake_data[, treatment := factor(treatment, levels = treatment_levels)] diff_pre &lt;- numeric(n_iter) diff_post &lt;- numeric(n_iter) ixn &lt;- numeric(n_iter) ixn_p &lt;- numeric(n_iter) p_ancova &lt;- numeric(n_iter) p_change &lt;- numeric(n_iter) seed_i &lt;- 0 for(i in 1:n_iter){ seed_i &lt;- seed_i + 1 set.seed(seed_i) # the contribution to variance due to individual differences mu_i &lt;- rnorm(N, mean = mu, sd = sigma_among) # the contribution to variance due to variation within individual mu_i_pre &lt;- rnorm(N, sd = sigma_within) mu_i_post &lt;- rnorm(N, sd = sigma_within) pre &lt;- mu_i + mu_i_pre post &lt;- mu_i + beta_1 + mu_i_post fake_data[, weight := c(pre, post)] fit &lt;- lm(weight ~ treatment*time, data = fake_data) diff_pre[i] &lt;- mean(pre[np1:N]) - mean(pre[1:n]) diff_post[i] &lt;- mean(post[np1:N]) - mean(post[1:n]) ixn[i] &lt;- coef(summary(fit))[4, &quot;Estimate&quot;] ixn_p[i] &lt;- coef(summary(fit))[4, &quot;Pr(&gt;|t|)&quot;] fake_data_wide &lt;- dcast(fake_data, id + treatment ~ time, value.var = &quot;weight&quot;) fake_data_wide[, change := post-pre] fake_data_wide[, percent := (post-pre)/pre*100] p_ancova[i] &lt;- coef(summary(lm(post ~ treatment + pre, data = fake_data_wide)))[2, &quot;Pr(&gt;|t|)&quot;] p_change[i] &lt;- coef(summary(lm(change ~ treatment, data = fake_data_wide)))[2, &quot;Pr(&gt;|t|)&quot;] } diverge_list &lt;- which(abs(diff_pre) &lt; 0.1 &amp; abs(diff_post) &gt; 1) converge_list &lt;- which(abs(diff_pre) &gt; 1 &amp; abs(diff_post) &lt; 0.1) p_list &lt;- which(p_ancova &gt; 0.1 &amp; p_change &lt; 0.01) keep &lt;- intersect(converge_list, p_list) max_seed_con &lt;- which(abs(ixn) == max(abs(ixn)[converge_list])) # convergence ixn set.seed(max_seed_con) mu_i &lt;- rnorm(N, mean = mu, sd = sigma_among) # the contribution to variance due to variation within individual mu_i_pre &lt;- rnorm(N, sd = sigma_within) mu_i_post &lt;- rnorm(N, sd = sigma_within) pre &lt;- mu_i + mu_i_pre post &lt;- mu_i + beta_1 + mu_i_post fake_data[, weight := c(pre, post)] fake_data_wide &lt;- dcast(fake_data, id + treatment ~ time, value.var = &quot;weight&quot;) fake_data_wide[, change := post-pre] fake_data_wide[, percent := (post-pre)/pre*100] # coef(summary(lm(post ~ treatment + pre, # data = fake_data_wide))) # coef(summary(lm(change ~ treatment, # data = fake_data_wide))) # coef(summary(lm(percent ~ treatment, # data = fake_data_wide))) # not much we can do about divergence so limit to convergence fake_means &lt;- fake_data[, .(weight = mean(weight)), by = c(&quot;time&quot;, &quot;treatment&quot;)] 13.8.5 Import fig3f data_from &lt;- &quot;Innervation of thermogenic adipose tissue via a calsyntenin 3β–S100b axis&quot; file_name &lt;- &quot;41586_2019_1156_MOESM5_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig3f_wide &lt;- read_excel(file_path, sheet = &quot;Figure 3f&quot;, range = &quot;H2:M15&quot;, col_names = FALSE) %&gt;% data.table() time_cols &lt;- c(0, 30, 60, 90, 120) setnames(fig3f_wide, old = names(fig3f_wide), new = c(&quot;id&quot;, paste0(&quot;time_&quot;, time_cols))) treatment_levels &lt;- c(&quot;WT&quot;, &quot;KO&quot;) fig3f_wide[, treatment := factor(substr(id, 1, 2), treatment_levels)] fig3f_wide[, glucose_0 := time_0] times &lt;- c(0, 30, 60, 90, 120) n_times &lt;- length(times) period_full &lt;- times[n_times] - times[1] period_post &lt;- times[n_times] - times[2] time_cols &lt;- paste0(&quot;time_&quot;, times) Y &lt;- fig3f_wide[, .SD, .SDcols = time_cols] %&gt;% as.matrix() # auc fig3f_wide[, auc := apply(Y, 1, trap.rule, x = times)] # mean response over full period fig3f_wide[, glucose_gtt_post := auc/period_full] # auc of the change. Equal to iauc of le Floch fig3f_wide[, iauc := apply(Y - Y[,1], 1, trap.rule, x = times)] # mean change over full period fig3f_wide[, glucose_gtt_change := iauc/period_full] # View(fig3f_wide) **convert fig3f to long format for LMM 13.8.6 Import exp3b data_from &lt;- &quot;Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice&quot; file_name &lt;- &quot;41586_2018_162_MOESM5_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp3b &lt;- read_excel(file_path, sheet = &quot;Fig. 3b&quot;, range = &quot;H4:K50&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% tidyr::fill(genotype) %&gt;% data.table() exp3b[, count_per_area := positive_nuclei/area_mm2] genotype_levels &lt;- c(&quot;WT&quot;, &quot;KI&quot;) exp3b[, genotype := factor(genotype, levels = genotype_levels)] exp3b[, age := &quot;Old&quot;] 13.8.7 Plot the model of exp3b (glm offset data) gg1 &lt;- ggplot_the_effects(exp3b_m2, exp3b_m2_pairs, effect_label = &quot;Effect Ratio&quot;) # gg1 xdat_old &lt;- expand.grid( area_mm2 = seq(min(exp3b[age == &quot;Old&quot;, area_mm2]), max(exp3b[age == &quot;Old&quot;, area_mm2]), length.out = 50), genotype = genotype_levels, age = c(&quot;Old&quot;) ) %&gt;% data.table() xdat_old[, y := predict(exp3b_m2, xdat_old, type = &quot;response&quot;)] gg2 &lt;- ggplot(data = exp3b[age == &quot;Old&quot;], aes(x = area_mm2, y = positive_nuclei, color = genotype)) + geom_point() + geom_path(data = xdat_old, aes(x = area_mm2, y = y, color = genotype)) + xlab(expression(paste(&quot;Area (&quot;, mm^2, &quot;)&quot;))) + ylab(&quot;TUNEL+ nuclei&quot;) + scale_color_manual(values = pal_okabe_ito_blue) + theme_pubr() + theme(legend.position = &quot;left&quot;) + NULL #gg2 plot_grid(gg1, gg2, nrow = 2, rel_heights = c(0.4,1), align = c(&quot;v&quot;), axis = &quot;lr&quot;) "],["part-v-more-than-one-x-multivariable-models.html", "Part V: More than one \\(X\\) – Multivariable Models", " Part V: More than one \\(X\\) – Multivariable Models "],["covariates.html", "Chapter 14 Linear models with added covariates (“ANCOVA”) 14.1 Adding covariates can increases the precision of the effect of interest 14.2 Understanding a linear model with an added covariate – heart necrosis data 14.3 Understanding interaction effects with covariates 14.4 Understanding ANCOVA tables 14.5 Working in R 14.6 Best practices 14.7 Best practices 2: Use a covariate instead of normalizing a response", " Chapter 14 Linear models with added covariates (“ANCOVA”) In its most general sense, Covariates are simply the \\(X\\) variables in a statistical model. With data from experiments, “covariates” more typically refers to continuous \\(X\\) variables that are added to a model to increase precision of the treatment effects. In observational designs, continuous and categorical covariates might be added to a model to 1) increase predictive ability, 2) because the researcher is interested in specific conditional effects, or 3) to eliminate confounding. These are discussed in later chapters. In this chapter, a single, continuous covariate is added to a linear model. Nothing is fundamentally different if the covariate added is categorical (\\(Sex\\) is a common categorical covariate) or if multiple covariates are added. 14.1 Adding covariates can increases the precision of the effect of interest I use fake data to introduce the concept of statistical elimination of a covariate in a statistical model. Here I am modeling the effect of a new drug on blood LDL-C levels. LDL is a kind of lipoprotein, which are particles in the blood that transport fats and cholesterol to and from different tissues. LDL-C is cholesterol associated with LDL particles. LDL-C is considered “bad cholesterol” because LDL is believed to transport cholesterol and other lipids to arterial walls, which is the basis for atherosclerosis. Thirty applied biostats students are recruited and are randomly assigned to either the “placebo” treatment level or “drug” treatment level. The response is blood LDL-C concentration. The drug manufacturer wants a measure of the effect of the new drug on ldlc. Figure 14.1: Effect of drug therapy on plasma LDL-C. The linear model fit to the simulated LDL-C data is \\[\\begin{equation} ldlc = \\beta_0 + \\beta_1 treatment_{drug} + \\varepsilon \\tag{14.1} \\end{equation}\\] where \\(treatment_{drug}\\) is the dummy variable, which is set to 0 when \\(treatment = \\mathrm{&#39;&#39;placebo&#39;&#39;}\\) and to 1 when \\(treatment = \\mathrm{&#39;&#39;drug&#39;&#39;}\\). The coefficient table is ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 106.045 1.070 99.111 0.000 103.854 108.237 ## treatmentdrug -1.554 1.513 -1.027 0.313 -4.654 1.546 The response-effect plot shows large overlap in the LDL-C response and treatment effect that is small relative to the noise. “No effect of the drug (\\(p = .31\\))” is an incorrect interpretation of the p-value of the significance test of the estimate of \\(\\beta_1\\). A better interpretation is, the estimated effect is -1.6 but everything from large, negative effects to moderate positive effects are consistent with the data. As expert biologists, we know that LDL-C is strongly correlated with age and there is a large range in age among the Applied Bistats students. If age contributes to a large fraction of the variance in LDL-C among applied biostats students, then age-related variance might mask the effect of the drug. Here is a plot of LDL-C vs. age, with treatment assignment color coded. Remember, these are the exact same values of LDL-C as in figure ?? above. Figure 14.2: Linear regression of \\(ldlc\\) on age fit to the fake LDL-C data. The points are color coded by treatment. The regression line is the fit of the linear model \\[\\begin{equation} ldlc = \\beta_0 + \\beta_1 age + \\varepsilon \\tag{14.2} \\end{equation}\\] The points are color-coded by treatment level but \\(treatment\\) is not in model (14.2). The color-coding makes it clear that most of the “placebo” data points are above the line, or have positive residuals from the model, while the “drug” data points are below the line, or have negative residuals from the model. That is, at any specific value of age, there is very small overlap of LDL-C values for drug and for placebo. What is happening? Age is contributing to the variance of LDL-C, and the noise in \\(\\varepsilon\\) in model (14.1), and this added noise makes decreases the precision in our estimate of the effect of the new drug relative to placebo. When we view the data as in Figure ??, age is masking the effect. If we could somehow measure the effect of the drug at a specific age, then we could get a more precise estimate of the effect. But how to do this? Here are three possible methods. The third is the only one you should use but the second is useful for understanding the third. We could just analyze a subset of the data, that is, only the cases in which the value of age is nearly equal. This throws away perfectly good data and, consequently, greatly reduces the sample size and thus precision to estimate the effect. We could first fit model (14.2) and then use the residuals of this fit as a new response variable to estimate the effect of drug treatment (this is what we did by eye in figure 14.2). step 1: \\[\\begin{equation} ldlc = \\beta_0 + \\beta_1 age + \\varepsilon \\end{equation}\\] step 2: \\[\\begin{equation} ldlc\\_residual = \\beta_0 + \\beta_1 treatment_{drug} + \\varepsilon \\tag{14.3} \\end{equation}\\] Here, I use this two-stage method because it is useful to introduce the concept of adjusting for a covariate in a linear model, where the covariate here is \\(age\\). But, in general, don’t do this – the method usually “works” pretty well if the mean of the covariate (the step 1 \\(X\\) variable) is nearly the same in both treatment levels but artifacts that lead to wrong inference are introduced if the mean of the covariate is far apart. Figure 14.3: Effect of drug therapy on plasma LDL-C using residuals. Don’t do this! Figure ??A is an effect-response plot of the effect of treatment on the LDL-C adjusted for age using the two-step method. Figure ??B is the original (not adjust) effect-response plot. The scale of both plots are the same. This means that the response axis has the same length in both plots and the effects axis has the same length in both plots. This makes comparing the two easy. Two patterns are conspicuous The age-adjusted means in Figure ??A are further apart (the difference is bigger) then the unadjusted means in Figure ??B. This is seen in both the response and the effects components of the plot. The spread of the residual LDL-C measures within each treatment level in Figure ??A is less than the spread of the raw LDL-C measures in Figure ??B. The confidence interval of the effect (difference in means) is smaller using the adjusted model (Figure ??A) than in the unadjusted model (Figure ??B) The p-value of the effect (difference in means) is smaller using the adjusted model (Figure ??A) than in the unadjusted model (Figure ??B) These patterns are quantified by the Estimates and SEs of the coefficient table. Again, I show both for comparison. Can you match comparisons 1-4 above with the statistic in the coefficient table? Coefficient table of two-step, “adjusted” model (adjusting for \\(age\\)): ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 1.901 0.556 3.422 0.002 0.763 3.040 ## treatmentdrug -3.803 0.786 -4.840 0.000 -5.412 -2.193 Coefficient table of unadjusted model (adjusting for \\(age\\)): ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 106.045 1.070 99.111 0.000 103.854 108.237 ## treatmentdrug -1.554 1.513 -1.027 0.313 -4.654 1.546 It is clear from the plots and the tables that this two-stage adjustment increases the precision of the estimates of the means and the differences in means by eliminating the contribution of Age to the variance in LDL-C. The best practice for adjusting for a covariate (or the statistical elimination of a covariate) is to simply add the covariate to the linear model. \\[\\begin{equation} ldlc = \\beta_0 + \\beta_1 age + \\beta_2 treatment_{drug} + \\varepsilon \\tag{14.4} \\end{equation}\\] ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 75.60 3.260 23.20 2.28e-19 68.900 82.30 ## age 1.26 0.133 9.47 4.46e-10 0.988 1.53 ## treatmentdrug -4.45 0.802 -5.55 6.94e-06 -6.090 -2.81 Again, compare the coefficient table from the model fit without the covariate. ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 106.00 1.07 99.10 3.36e-37 104.00 108.00 ## treatmentdrug -1.55 1.51 -1.03 3.13e-01 -4.65 1.55 The adjusted effect is larger (-4.5 vs. -1.6) The adjusted SE of the difference is smaller (0.8 vs. 1.5) The adjusted CIs are narrower (-6.1, -2.8 vs. -4.7, 1.6) The p-value of the adjusted difference is smaller (0.000007 vs. 0.31) A plot of the model is Figure 14.4: Effect of drug therapy, adjusted for age, on plasma LDL-C. 14.2 Understanding a linear model with an added covariate – heart necrosis data 1-Deoxydihydroceramide causes anoxic death by impairing chaperonin-mediated protein folding Source data In this article, the researchers are investigating the effects of specific sphingolipids on hypoxia (low O2) in the heart. This hypoxia results in necrosis (death) of heart tissue. The researchers are specifically looking at the sphingolipid 1-deoxydihydroceramide (DoxDHCer), which is derived from 1-deoxysphinganine6 (DoxSa). In the experiment for Figure 4h, the researchers measured the effect of three treatments on necrosis. Vehicle – there is no “cardioprotection” from the hypoxia-producing sphingolipids. This is the “control”. We expect more necrotic area in this group. 2 Myriocin – a drug that inhibits the enzyme that iniates sphingolipid production. This drug should provide protection from the hypoxia-producing sphingolipids. We expect less necrotic area (or, cardioprotection) in this group. Myriocin + DoxSa – DoxSa is the specific sphingolipid that the researchers believe cause the hypoxia/necrosis. The drug should inhibit the production of sphingolipids but the added DoxSa should reverses the protective effect of the drug. If there is a reversal of the protection, then this supports the hypothesis that DoxSa is the sphingolipid causing the hypoxia/necrosis. The response (\\(Y\\)) variable is \\(area\\_of\\_necrosis\\) – the measured “area” of the necrotic tissue (area is used here in the sense of “region” and not in the sense of length times width). The covariate is \\(area\\_at\\_risk\\) – the area of heart tissue that is susceptible to necrosis. 14.2.1 Fit the model The verbal model with added covariate is \\(area\\_of\\_necrosis \\sim area\\_at\\_risk + treatment\\;\\;\\;(\\mathcal{M}_1)\\). which I’ll name model \\(\\mathcal{M}_1\\). To understand the model coefficients, it helps to expand model \\(\\mathcal{M}_1\\) into the full linear model. \\[\\begin{equation} area\\_of\\_necrosis = \\beta_0 + \\beta_1 area\\_at\\_risk + \\beta_2 treatment_{myriosin} + \\beta_3 treatment_{Myriocin\\;+\\;DoxSa} + \\varepsilon \\tag{14.5} \\end{equation}\\] The model includes parameters for the effects of area at risk (\\(\\beta_1\\)), the myriosin treatment (\\(\\beta_2\\)) and the myriosin + DoxSa treatment (\\(\\beta_3\\)). I explain the interpretation of these effects in “Interpretation of the model coefficients” below. fig4h_m1 &lt;- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h) 14.2.2 Plot the model Figure 14.5: The effect of treatment on area of necrosis, adjusted for area of risk. 14.2.3 Interpretation of the model coefficients fig4h_m1_coef &lt;- cbind(coef(summary(fig4h_m1)), confint(fig4h_m1)) signif(fig4h_m1_coef, digits = 3) ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) -3.890 2.1200 -1.83 7.63e-02 -8.220 0.436 ## area_at_risk 0.453 0.0587 7.73 8.28e-09 0.334 0.573 ## treatmentMyriocin -3.110 1.2300 -2.53 1.64e-02 -5.620 -0.611 ## treatmentMyriocin + DoxSa -3.150 1.3700 -2.31 2.78e-02 -5.930 -0.367 The estimate in the “(Intercept)” row (\\(b_0\\)) is the expected value of the reference (here, this is the Vehicle group) when \\(area\\_at\\_risk=0\\). Using Figure 14.5, this can be visualized as the value of \\(Y\\) where the Vehicle line (the regression line for the reference group) crosses the y-axis (at \\(X = 0\\)). This is not a meaningful estimate since the area at risk in all hearts is above zero. The estimate in the “area_at_risk” row (\\(b_1\\)) is a common slope for all three regression lines. Some might refer to this slope as the “effect” of \\(area\\_at\\_risk\\) on \\(area\\_of\\_necrosis\\) but I recommend against using this causal language because the \\(area\\_at\\_risk\\) is not randomly assigned – its purpose in this model (but not necessarily all linear models with an added covariate) is not for interpretation but for improving inference in the expimental treatment factor. The estimate in the “treatmentMyriocin” row (\\(b_2\\)) is the effect of \\(Myriocin\\) on \\(area\\_of\\_necrosis\\) conditional on (or adjusted for) \\(area\\_at\\_risk\\). A frequent phrase is “the estimate controlling for area at risk” but avoid this because “control” implies experimenter intervention and this is not true for \\(area\\_at\\_risk\\). The value is the difference in the elevation of the regression line for the reference group and that for the Myriocin group. This difference is equal to the difference in conditional expectations at a specific value of the covariate. \\[\\begin{equation} b_2 = \\mathrm{E}(area\\_of\\_necrosis|treatment = &quot;Myriocin&quot;, X = x) - \\mathrm{E}(area\\_of\\_necrosis|treatment = &quot;Vehicle&quot;, X = x) \\end{equation}\\] The estimate in the “treatmentMyriocin + DoxSa” row (\\(b_3\\)) is the effect of \\(Myriocin + DoxSa\\) on \\(area\\_of\\_necrosis\\) conditional on (or adjusted for) \\(area\\_at\\_risk\\). It’s interpretation is similar to that for \\(b_2\\). How do we summarize these interpretations given the motivating hypothesis? The results do not support the hypothesis. If DoxSa is one of the sphingolipids inducing hypoxia/necrosis, then we’d expect the Myriocin + DoxSa line to be elevated above the Myriocin line. But the coefficient table “tests” this prediction only indirectly. The explicit statistic for this prediction is the (Myriocin + DoxSa) - Myriocin contrast in the contrast table below. 14.2.4 Everything adds up Remember that everything adds up in a linear model. A regression line is a line of expected values (the expectation of \\(Y\\) conditional on \\(X\\)). The point on the Vehicle line at \\(area\\_of\\_risk = 30\\) is \\(b_0 + b_1 \\cdot 30\\). The point on the “Myriocin” line at \\(area\\_of\\_risk = 30\\) is \\(b_0 + b_1 \\cdot 30 + b_2\\). And, the point on the “Myriocin + DoxSa” line at \\(area\\_of\\_risk = 30\\) is \\(b_0 + b_1 \\cdot 30 + b_3\\). Understanding how the components of the linear model add up gives you phenomenonal cosmic power in statistical analysis. 14.2.5 Interpretation of the estimated marginal means fig4h_m1_emm &lt;- emmeans(fig4h_m1, specs = &quot;treatment&quot;) fig4h_m1_emm ## treatment emmean SE df lower.CL upper.CL ## Vehicle 11.98 0.839 32 10.3 13.7 ## Myriocin 8.87 0.916 32 7.0 10.7 ## Myriocin + DoxSa 8.83 1.045 32 6.7 11.0 ## ## Confidence level used: 0.95 The values in the column “emmean” are the expected values of each group when (“conditional on”) \\(area\\_at\\_risk\\) is equal to the mean \\(area\\_at\\_risk\\). 14.2.6 Interpretation of the contrasts fig4h_m1_pairs &lt;- contrast(fig4h_m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) fig4h_m1_pairs ## contrast estimate SE df lower.CL upper.CL t.ratio ## Myriocin - Vehicle -3.1139 1.23 32 -5.62 -0.611 -2.534 ## (Myriocin + DoxSa) - Vehicle -3.1485 1.37 32 -5.93 -0.367 -2.306 ## (Myriocin + DoxSa) - Myriocin -0.0346 1.43 32 -2.95 2.881 -0.024 ## p.value ## 0.0164 ## 0.0278 ## 0.9809 ## ## Confidence level used: 0.95 The values in the column “estimate” are the differences between the estimated marginal means in the estimated marginal means table. The first two contrasts are equal to the coefficients \\(b_2\\) and \\(b_3\\) in the coefficient table. 14.2.7 Adding the covariate improves inference Compare the effects in Model \\(\\mathcal{M}_1\\) (with the added covariate) to the effects in a linear model without the added covariate. ## Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % ## (Intercept) 11.200 1.39 8.070 2.60e-09 8.38 14.000 ## treatmentMyriocin -3.660 2.04 -1.790 8.26e-02 -7.82 0.499 ## treatmentMyriocin + DoxSa 0.293 2.15 0.136 8.93e-01 -4.09 4.670 Inference from these two models is very different. From the model adjusting for area at risk (Model \\(\\mathcal{M}_1\\)), we would conclude “Unexpectedly, the estimated Myriocin + DoxSa effect (-3.2 mg, 95% CI: -5.9, -0.4) is effectively as big as the Myriocin alone effect.” By contrast, using the model without the covariate, we would conclude “Relative to the Myriocin effect, the estimated Myriocin + DoxSa effect is small (.29 mg), and there is large uncertainty in its direction and magnitude (95% CI: -4.1, 4.7).” (Note that the authors published a different conclusion to either of these. I cannot recover the results leading to their conclusion using the methods published by the authors.) 14.3 Understanding interaction effects with covariates 14.3.1 Fit the model When we add a continuous covariate to a model we sometimes want to model the interaction with the categorical factor variable. An interaction effect represents how the effect of one variable changes given the level of a second variable. The verbal model with an added interaction effect is \\(area\\_of\\_necrosis \\sim area\\_at\\_risk + treatment + area\\_at\\_risk \\times treatment\\;\\;\\;(\\mathcal{M}_2)\\) which I’ll refer to as Model \\(\\mathcal{M}_2\\). To understand the model coefficients, it helps to expand this into the full linear model. \\[\\begin{align} area\\_of\\_necrosis = \\beta_0 &amp;+ \\beta_1 area\\_at\\_risk + \\beta_2 treatment_{myriosin} + \\beta_3 treatment_{Myriocin\\;+\\;DoxSa}\\\\ &amp;+ \\beta_4 area\\_at\\_risk \\cdot treatment_{myriosin}\\\\ &amp;+ \\beta_5 area\\_at\\_risk \\cdot treatment_{Myriocin\\;+\\;DoxSa}\\\\ &amp;+ \\varepsilon \\tag{14.6} \\end{align}\\] In addition to the effects of the myriosin (\\(\\beta_2\\)) and the myriosin + DoxSa (\\(\\beta_3\\)) treatments of Model \\(\\mathcal{M}_1\\), Model \\(\\mathcal{M}_2\\) includes coefficients for two interaction effects, the interaction between myriosin and area at risk (\\(\\beta_4\\)) and the interaction between myriosin + DoxSa and area at risk (\\(\\beta_4\\)). The parameters \\(\\beta_1\\) and \\(\\beta_2\\) are additive effects because they are linear (additive) “in the \\(X\\)” variables. The parameters \\(\\beta_3\\) and \\(\\beta_4\\) are non-additive effects because they are not linear “in the \\(X\\)” variables – these effects are coefficients of an \\(X\\) that is the product of two variables. You can see this in the model equation but I explain this further below. For the goals of the researchers in the heart necrosis study, we specifically would not want to model the interaction. Nevertheless, we would want to plot the interaction effect during the “Step 2: examine the data” phase of analysis to be sure that an additive model is a reasonable model given the data. # see working in R to understand this model formula fig4h_m2 &lt;- lm(area_of_necrosis ~ area_at_risk*treatment, data = fig4h) 14.3.2 Plot the model with interaction effect Figure 14.6: The effect of treatment on area of necrosis, adjusted for area of risk. 14.3.3 Interpretation of the model coefficients fig4h_m2_coef &lt;- cbind(coef(summary(fig4h_m2)), confint(fig4h_m2)) round(fig4h_m2_coef[,c(1,2)], digits = 2) ## Estimate Std. Error ## (Intercept) -6.55 3.01 ## area_at_risk 0.53 0.09 ## treatmentMyriocin 4.90 4.90 ## treatmentMyriocin + DoxSa -1.17 5.28 ## area_at_risk:treatmentMyriocin -0.25 0.15 ## area_at_risk:treatmentMyriocin + DoxSa -0.06 0.14 The estimate in the “(Intercept)” row (\\(b_0\\)) is the expected value of the reference (here, this is the Vehicle group) when \\(area\\_at\\_risk=0\\). This is the same interpretation of the intercept in the additive model (Model \\(\\mathcal{M}_1\\)) but the value is different. This is because the slope coefficient (\\(b_1\\)) estimated in Model \\(\\mathcal{M}_1\\) is a common slope (to all three groups) estimated by pooling data from all three groups. The slope coefficient in the non-additive model (Model \\(\\mathcal{M}_2\\)) is computed from just the reference data. Because the slope differs between Model \\(\\mathcal{M}_1\\) and Model \\(\\mathcal{M}_2\\), the intercept differs (think about why they are necessarily related). As in Model \\(\\mathcal{M}_1\\), the value and inferential statistics are meaningful for this parameterization but there are slight modifications of the model that can make this meaningful. The estimate in the “area_at_risk” row (\\(b_1\\)) is the slope of the regression line for the reference (Vehicle) group, that is, the slope conditional on \\(treatment = \\mathrm{&#39;&#39;Vehicle&#39;&#39;}\\) The estimate in the “treatmentMyriocin” row (\\(b_2\\)) is the effect of \\(Myriocin\\) on \\(area\\_of\\_necrosis\\) conditional on \\(area\\_at\\_risk = 0\\). Note the difference in interpretation with \\(b_2\\) in Model \\(\\mathcal{M}_1\\). The difference is because of the interaction. The effect of \\(Myriocin\\) on \\(area\\_of\\_necrosis\\) is no longer constant for all values of \\(area\\_at\\_risk\\). This is easily seen in Figure 14.6, where the effect of Myriocin (the vertical distance between the Vehicle and Myriocin line) is very small at small values of area at risk but very large at large areas at risk. The estimate in the “treatmentMyriocin + DoxSa” row (\\(b_3\\)) is the effect of \\(Myriocin + DoxSa\\) on \\(area\\_of\\_necrosis\\) conditional on \\(area\\_at\\_risk = 0\\). It’s interpretation is similar to that for \\(b_2\\). The estimate in the “area_at_risk:treatmentMyriocin” row (\\(b_4\\)) is the interaction effect between area at risk and Myriocin. It’s value is the difference between the slope of the regression line through the Myriocin points in Figure 14.6 and the slope of the regression line through the reference (Vehicle) points in Figure 14.6. Consequently, the slope of the regression line through the Myriocin points is \\(b_1 + b_4\\). The estimate in the “area_at_risk:treatmentMyriocin + DoxSa” row (\\(b_5\\)) is the interaction effect between area at risk and Myriocin + DoxSa It’s value is the difference between the slope of the regression line through the Myriocin + DoxSa points in Figure 14.6 and the slope of the regression line through the reference (Vehicle) data in Figure 14.6. Consequently, the slope of the regression line through the Myriocin + DoxSa points is \\(b_1 + b_5\\). Interaction effects are differences in slopes (this is also true with interactions between two factor variables, even though we usually describe this as “differences in differences”) and these difference are easily seen by inspection of a plot similar to that in Figure 14.6. The bigger the interaction, the less parallel the regression line. 14.3.4 What is the effect of a treatment, if interactions are modeled? – it depends. A very useful way to remember what an interaction is is “it depends”. What is the effect of Myriocin on area at risk? With an interaction term in the linear model, the answer is “it depends”. It depends on the level of \\(area\\_at\\_risk\\). At small values of area at risk, the effect of Myriocin is very small. At large values of area at risk, the effect of Myriocin is large. Likewise, what is the effect of area at risk on necrosis? It depends (if interactions are modeled). It depends on the level of treatment, for example the effect in the Myriocin group is smaller (a smaller slope) than the effect in the Vehicle group. “It depends” is the hallmark of interactions – the effect of one variable on the outcome depends on the level of a second variable. 14.3.5 Which model do we use, \\(\\mathcal{M}_1\\) or \\(\\mathcal{M}_2\\)? The answer to this question depends partly on our goals. If we are explicitly interested in measuring an interaction effect (perhaps some theory predicts an positive interaction) then we would necesarily add interaction effects to the model. But if we are interested in estimating a treatment effect conditional on a covariate, then we don’t include the interaction. Remember that a purpose of initial explorations of data (“Step 2 – examine the data”) is to help decide which model to specify in the model formula. If an initial plot shows large interactions between treatment levels and the covariate, then adjusting for the covariate won’t work, or at least won’t work without additional complexity to the interpretation. “Large interactions” of course raises the question, how large is too larege to ignore interactions? Many textbooks in biostatistics recommend using an interaction p-value to make this decision. I disagree. There is no objective answer to the question. We lose information with all models – that is what we do in science. It is up to the researcher to be transparent with all decisions. If an additive (no interactions) model is used to condition on (adjust for) a covariate, the researcher should use the supplement to plot the model with interactions as evidence of why this decision is reasonable. 14.4 Understanding ANCOVA tables 14.5 Working in R 14.5.1 Importing the heart necrosis data data_folder &lt;- &quot;data&quot; data_from &lt;- &quot;1-Deoxydihydroceramide causes anoxic death by impairing chaperonin-mediated protein folding&quot; file_name &lt;- &quot;42255_2019_123_MOESM7_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) sheet_i &lt;- &quot;Figure 4h&quot; fig4h_1 &lt;- read_excel(file_path, sheet = sheet_i, range = &quot;A5:B19&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% data.table() fig4h_2 &lt;- read_excel(file_path, sheet = sheet_i, range = &quot;D5:E17&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% data.table() fig4h_3 &lt;- read_excel(file_path, sheet = sheet_i, range = &quot;G5:H15&quot;, col_names = TRUE) %&gt;% clean_names() %&gt;% data.table() fig4h &lt;- rbind(data.table(treatment = &quot;Vehicle&quot;, fig4h_1), data.table(treatment = &quot;Myriocin&quot;, fig4h_2), data.table(treatment = &quot;Myriocin + DoxSa&quot;, fig4h_3)) treatment_levels &lt;- c(&quot;Vehicle&quot;, &quot;Myriocin&quot;, &quot;Myriocin + DoxSa&quot;) fig4h[, treatment := factor(treatment, levels = treatment_levels)] 14.5.2 Fitting the model For an additive model, add the covariate to the model formula using the + operator. The order of the variables doesn’t matter (the right-hand-side of the model formula could be treatment + area_at_risk). m1 &lt;- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h) For an nonadditive model with interactions, add the covariate to the model formula using the * operator. ~ area_at_risk * treatment is a shortcut to the full model formula, which is 1 + area_at_risk + treatment + area_at_risk * treatment. R expands the short formula to the full formula automatically. Again, the order of the variables doesn’t matter (the right-hand-side of the model formula could be treatment * area_at_risk). m2 &lt;- lm(area_of_necrosis ~ area_at_risk * treatment, data = fig4h) 14.5.3 Using the emmeans function m1_emm &lt;- emmeans(m1, specs = c(&quot;treatment&quot;)) m1_emm ## treatment emmean SE df lower.CL upper.CL ## Vehicle 11.98 0.839 32 10.3 13.7 ## Myriocin 8.87 0.916 32 7.0 10.7 ## Myriocin + DoxSa 8.83 1.045 32 6.7 11.0 ## ## Confidence level used: 0.95 m1_emm &lt;- emmeans(m1, specs = c(&quot;area_at_risk&quot;, &quot;treatment&quot;)) m1_emm ## area_at_risk treatment emmean SE df lower.CL upper.CL ## 35 Vehicle 11.98 0.839 32 10.3 13.7 ## 35 Myriocin 8.87 0.916 32 7.0 10.7 ## 35 Myriocin + DoxSa 8.83 1.045 32 6.7 11.0 ## ## Confidence level used: 0.95 # recenter emms at the mean of the vehicle group vehicle_mean_x &lt;- mean(fig4h[treatment == &quot;Vehicle&quot;, area_at_risk]) m1_emm_2 &lt;- emmeans(m1, specs = c(&quot;area_at_risk&quot;,&quot;treatment&quot;), at = list(area_at_risk = vehicle_mean_x)) m1_emm_2 ## area_at_risk treatment emmean SE df lower.CL upper.CL ## 33.3 Vehicle 11.21 0.833 32 9.51 12.90 ## 33.3 Myriocin 8.09 0.903 32 6.25 9.93 ## 33.3 Myriocin + DoxSa 8.06 1.082 32 5.85 10.26 ## ## Confidence level used: 0.95 fig4h[, .(mean = mean(area_of_necrosis)), by = treatment] ## treatment mean ## 1: Vehicle 11.206580 ## 2: Myriocin 7.546081 ## 3: Myriocin + DoxSa 11.499319 14.5.4 ANCOVA tables If you are going to report statistics from an ANCOVA table, be sure that what you report is what you think you are reporting – it is very easy to get this wrong in R. One way to easily get a table using Type II or III sums of squares is to set the contrast option of the factor variable to contr.sum (this tells R to use deviation coding to form the model matrix for the linear model) and then use the function Anova from the car package (not base R anova). It is safest to set this with the lm function itself. The coefficients of the linear model will have a different interpretation than that given above. Either understand the new interpretation (not given here) or fit two models, one without the reset contrasts and one with the reset contrasts. The estimated marginal mean table and the contrast table are the same, regardless of how contrasts are set in the linear model. Many websites show code that changes the default coding in the R session. I strongly caution against this. If you don’t change it back, then you are likely to misinterpret coefficients in later analyses (see point 2). For additive models, use Type II sum of squares (Type III will give same results), Anova(m1_aov, type = 2) # be sure to use the correct model! ## Anova Table (Type II tests) ## ## Response: area_of_necrosis ## Sum Sq Df F value Pr(&gt;F) ## treatment 82.04 2 4.2195 0.02364 * ## area_at_risk 580.25 1 59.6874 8.28e-09 *** ## Residuals 311.09 32 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For non-additive models with interactions The statistics for the interaction effect are the same, regardless of the sum of squares used. The interpretation of the main effects is problematic, at best. This is discussed more in the ANOVA chapter. Some statisticians recommend using Type III sums of squares. The effect of $treatment* has a very specific interpretation with Type III – it is the effect when the covariate = 0. This can be biologically relevant if the covariate data is re-centered. # for coefficients m2 &lt;- lm(area_of_necrosis ~ treatment * area_at_risk, data = fig4h) # for anova m2_aov &lt;- lm(area_of_necrosis ~ treatment * area_at_risk, data = fig4h, contrasts = list(treatment = contr.sum)) Anova(m2_aov, type = 2) # be sure to use the correct model! ## Anova Table (Type II tests) ## ## Response: area_of_necrosis ## Sum Sq Df F value Pr(&gt;F) ## treatment 82.04 2 4.3369 0.02216 * ## area_at_risk 580.25 1 61.3485 9.685e-09 *** ## treatment:area_at_risk 27.34 2 1.4453 0.25163 ## Residuals 283.75 30 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Anova(m2_aov, type = 3) # be sure to use the correct model! ## Anova Table (Type III tests) ## ## Response: area_of_necrosis ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 56.06 1 5.9270 0.02107 * ## treatment 12.98 2 0.6863 0.51116 ## area_at_risk 491.92 1 52.0087 5.012e-08 *** ## treatment:area_at_risk 27.34 2 1.4453 0.25163 ## Residuals 283.75 30 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 14.5.5 Plotting the model 14.5.5.1 Using ggpubr ggpubr will only plot the model with interactions. ggscatter(data = fig4h, x = &quot;area_at_risk&quot;, y = &quot;area_of_necrosis&quot;, color = &quot;treatment&quot;, add = &quot;reg.line&quot;, ylab = &quot;Area of necrosis (mg)&quot;, xlab = &quot;Area at risk (mg)&quot;, palette = pal_okabe_ito ) 14.5.5.2 ggplot2 # pal_batlow &lt;- scico(n, alpha = NULL, begin = 0, end = 1, direction = 1, palette = &quot;batlow&quot;) m1 &lt;- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h) gg &lt;- ggplot(fig4h, aes(x = area_at_risk, y = area_of_necrosis, color = treatment)) + geom_point(aes(color=treatment)) + geom_smooth(method = &quot;lm&quot;, mapping = aes(y = predict(m1, fig4h))) + scale_color_manual(values = pal_okabe_ito, name = NULL) + # scale_color_manual(values = pal_batlow, # name = NULL) + ylab(&quot;Area of necrosis (mg)&quot;) + xlab(&quot;Area at risk (mg)&quot;) + theme_pander() + theme(legend.position=&quot;bottom&quot;) + NULL gg 14.5.5.3 A response-effects plot using ggplot2 m1 &lt;- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h) b &lt;- coef(m1) groups &lt;- levels(fig4h$treatment) line_data &lt;- fig4h[, .(min_x = min(area_at_risk), max_x = max(area_at_risk)), by = treatment] line_data[, dummy1 := ifelse(treatment == &quot;Myriocin&quot;, 1, 0)] line_data[, dummy2 := ifelse(treatment == &quot;Myriocin + DoxSa&quot;, 1, 0)] line_data[, y_min := b[1] + b[2]*min_x + b[3]*dummy1 + b[4]*dummy2] line_data[, y_max := b[1] + b[2]*max_x + b[3]*dummy1 + b[4]*dummy2] gg_response &lt;- ggplot(data = fig4h, aes(x = area_at_risk, y = area_of_necrosis, color = treatment)) + # points geom_point(size = 2) + geom_segment(data = line_data, aes(x = min_x, y = y_min, xend = max_x, yend = y_max)) + scale_color_manual(values = pal_okabe_ito, name = NULL) + ylab(&quot;Area of necrosis (mg)&quot;) + xlab(&quot;Area at risk (mg)&quot;) + theme_pubr() + theme(legend.position=&quot;bottom&quot;) + NULL #gg_response m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) %&gt;% data.table() # pvalString is from package lazyWeave m1_pairs[ , p_pretty := pvalString(p.value)] # also create a column with &quot;p-val: &quot; m1_pairs[ , pval_pretty := paste(&quot;p = &quot;, p_pretty)] contrast_order &lt;- m1_pairs[, contrast] m1_pairs[, contrast := factor(contrast, contrast_order)] gg_effect &lt;- ggplot(data = m1_pairs, aes(y = contrast, x = estimate)) + # confidence level of effect geom_errorbar(aes(xmin = lower.CL, xmax = upper.CL), width = 0, color = &quot;black&quot;) + # estimate of effect geom_point(size = 3) + # draw a line at effect = 0 geom_vline(xintercept = 0, linetype = 2) + # p-value. The y coordinates are set by eye annotate(geom = &quot;text&quot;, label = m1_pairs$pval_pretty, y = 1:3, x = 4.5) + # x-axis label and aesthetics xlab(&quot;Effect (mg)&quot;) + ylab(&quot;Contrast&quot;) + coord_cartesian(xlim = c(-8,5.5)) + scale_x_continuous(position=&quot;top&quot;) + theme_pubr() + # theme(axis.title.x = element_blank()) + NULL # gg_effect gg &lt;- plot_grid(gg_effect, gg_response, nrow=2, align = &quot;v&quot;, axis = &quot;rl&quot;, rel_heights = c(0.35, 1) ) gg 14.6 Best practices 14.6.1 Do not use a ratio of part:whole as a response variable – instead add the denominator as a covariate 14.6.2 Do not use change from baseline as a response variable – instead add the baseline measure as a covariate 14.6.3 Do not “test for balance” of baseline measures A test of the null hypothesis of no difference in mean at baseline is a “test for balance.” Researchers frequently test for balance at baseline and use the p-value of the test to decide the next step: 1) if \\(p &gt; 0.05\\), conclude that the pre-treatment means “do not differ” and use something like a simple t test of the post-treatment means, 2) if \\(p &lt; 0.05\\), then use the change score, or the percent change, as the response in a simple t-test, or 3) if \\(p &lt; 0.05\\), then use use a linear model with the pre-treatment value as a covariate. Here, and in general, hypothesis tests used to decide which of several ways to proceed do not make sense. First, a null-hypothesis significance test cannot tell you that there is “no difference” – this is not what null-hypothesis tests do. Second, any \\(p\\)-value after the initial test isn’t strictly valid as it does not take into account this decision step, but this is minor. Third, it doesn’t matter; there will always be some difference in the actual means of the initial measures and, consequently, the conditional expectation of the final measures, or change in measures, or percent change will be dependent on this initial difference. So, if one has initial measures, one should use an linear model that adjusts for baseline measures to estimate the treatment effect in pre-post designs. And, if one isn’t planning on taking an initial measure, then maybe you should, because the initial measure used in a linear model allows a better estimate of the treatment effect, as discussed above in Adding covariates can increases the precision of the effect of interest. 14.7 Best practices 2: Use a covariate instead of normalizing a response "],["factorial.html", "Chapter 15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”) 15.1 A linear model with crossed factors estimates interaction effects 15.2 Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data) 15.3 Understanding the linear model with crossed factors 1 15.4 Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data) 15.5 Understanding the linear model with crossed factors 2 15.6 Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data) 15.7 Understanding the linear model with crossed factors 3 15.8 Issues in inference 15.9 Two-way ANOVA 15.10 More issues in inference 15.11 Working in R 15.12 Hidden Code", " Chapter 15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”) # X &lt;- model.matrix(m1) # y_col &lt;- insight::find_response(m1) # y &lt;- model.frame(m1)[, y_col] # b &lt;- (solve(t(X)%*%X)%*%t(X)%*%y)[,1] emm_b &lt;- function(m1_emm){ # works only for 2 x 2 and specs have to be in # same order as model if(!is.data.frame(m1_emm)){ m1_emm &lt;- summary(m1_emm) } mu &lt;- m1_emm[, &quot;emmean&quot;] b &lt;- c(mu[1], mu[2] - mu[1], mu[3] - mu[1], mu[4]) b[4] &lt;- b[4] - (b[1]+b[2]+b[3]) return(b) } 15.1 A linear model with crossed factors estimates interaction effects A factorial experiment is one in which there are two or more factor variables (categorical \\(X\\)) that are crossed, resulting in a group for each combination of the levels of each factor. Each specific combination is a different treatment. A linear model with crossed factors is used to estimate interaction effects, which occur when the effect of the level of one factor is conditional on the level of the other factors. Estimation of the interaction effect is necessary for inferences about “Something different” – Estimation of a treatment effect relative to a control effect (Example 1 – TLR9-/- mice) “It depends” – Estimation of the effect of background condition on an effect (Example 2 – XX mice) “More than the sum of the parts” – Estimation of synergy, a non-additive effect (Example 3 – plant root growth) Inferences like these are common in the experimental biology literature but they are made using the wrong statistics. The correct statistic – the interaction effect – is easy to compute but rarely computed. Alert. NHST encourages thinking such as “there is or isn’t an interaction effect” This dichotomous thinking can only lead to disasters of biblical proportion, dogs and cats living together. Interaction effects in biology are ubiquitous. Sometimes they are small enough to ignore. 15.1.1 An interaction is a difference in simple effects In this chapter, I’ll describe an interaction effect using different descriptions (Examples 1-3). Here, I want to emphasize that an interaction effect is a difference of differences. To clarify this, I’ll introduce a fake experiment. A research group has evidence that a certain gene product (CGP) is an intermediary between intestinal microbiota and obesity. The researchers transfer feces from either lean mice or obese mice into either wildtype mice or CGP-/- mice to investigate the effect of microbiota and CGP on body weight. The design has two factors (donor and genotype), each with two levels (donor: “Lean”, “Obese; genotype:”WT”, “KO”), which makes this a \\(2 \\times 2\\) (crossed or factorial) design. There are \\(n=6\\) replicates for each treatment. A good way to visualize the treatment combinations in a crossed design is with a \\(m \\times p\\) table showing all combinations of the \\(m\\) levels of factor 1 (\\(\\texttt{donor}\\)) against the \\(p\\) levels of factor 2 (\\(\\texttt{genotype}\\)) (15.1). Table 15.1: Mean body weight of mice in four treatment groups of the 2 x 2 factorial experiment. WT KO Effect Lean 33.2 34.1 0.9 Obese 41.9 32.7 -9.2 Effect 8.7 -1.4 -10.1 The upper-left \\(2 \\times 2\\) part of the table contains the fake mean body weight of each treatment group. These means are known as cell means. The first two elements in the “Effect” column contains the difference of the two cells to the left – these are the effects of genotype conditional on the level of donor. The first two elements in the “Effect” row contains the difference of the two cells above – these are the effects of donor conditional on the level of genotype. The effects described in items 2 and 3 are known as the simple effects. The value in red is the difference of the two simple effects above it. It is also the difference of the two simple effects to the left. These differences are equal. This is the interaction effect. In an experiment with a single factor with four levels, all six pairwise comparisons may be of interest. In a \\(2 \\times 2\\) factorial experiment, it is the four simple effects and the interaction effect that should pique the interest of the researcher. In this fake experiment, we want to know the effect of obese donor treatment in the KO mice compared to the effect of obese donor treatment in the WT mice. That is, we want the contrast of these two simple effects. \\[ (\\operatorname{Obese/KO} - \\operatorname{Lean/KO}) - (\\operatorname{Obese/WT} - \\operatorname{Lean/WT}) \\] This contrast is the interaction effect. An interaction effect is a difference of differences. 15.1.2 A linear model with crossed factors includes interaction effects The factorial linear model for the fake data is \\[ \\begin{align} \\texttt{body_weight} &amp;= \\beta_0 + \\beta_1 (\\texttt{donor}_{\\texttt{Obese}}) + \\beta_2 (\\texttt{genotype}_{\\texttt{KO}})\\ + \\\\ &amp;\\quad \\ \\beta_3 (\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}) + \\varepsilon \\end{align} \\] What are the variables? \\(\\texttt{donor}_{\\texttt{Obese}}\\) is the indicator variable for the “Obese” level of the factor \\(\\texttt{donor}\\). It contains the value 1 if the donor was “Obese” and 0 otherwise. \\(\\texttt{genotype}_{\\texttt{KO}}\\) is the indicator variable for the “KO” level of the factor \\(\\texttt{genotype}\\). It contains the value 1 if the genotype is “KO” and 0 otherwise. \\(\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}\\) is the indicator variable for the interaction between the “Obese” level of \\(\\texttt{donor}\\) and the “KO” level of \\(\\texttt{genotype}\\). This “:” character to indicate interaction follows the R formula convention in the LME4 package. Many sources use a \\(\\times\\) symbol instead of the colon. The variable contains the value 1 if the mouse is assigned to both “Obese” and “KO” and 0 otherwise. This value is the product of the values in \\(\\texttt{donor}_{\\texttt{Obese}}\\) and \\(\\texttt{genotype}_{\\texttt{KO}}\\). What are the parameters? This linear model has set “Lean” as the reference level of \\(\\texttt{donor}\\) and “WT” as the reference level of \\(\\texttt{genotype}\\). This make the Lean/WT mice the control. \\(\\beta_0\\) is the true mean of the control, which is the “Lean/WT” group. \\(\\beta_1\\) is the true effect of donor in the WT mice (the effect of manipulating the donor factor but not the genotype factor). It is the difference between the true means of the “Obese/WT” group and the “Lean/WT” group. The mean of the “Obese/WT” group is \\(\\beta_0 + \\beta_1\\). This is the expectation if we start with the control and then add the effect of Obese donor. \\(\\beta_2\\) is the true effect of genotype in the mice given feces from lean donors (the effect of manipulating the genotype factor but not the donor factor). It is the difference between the true means of the “Lean/KO” group and the “Lean/WT” group. The mean of the “Lean/KO” group is \\(\\beta_0 + \\beta_2\\). This is the expectation if we start with the control and then add the effect of KO genotype. \\(\\beta_3\\) is the true interaction effect of \\(\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}\\) The mean of the Obese/KO group is \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\). The expected mean of the Obese/KO group, if the factors are additive, which means the interaction effect is zero, is \\(\\beta_0 + \\beta_1 + \\beta_2\\). The interaction effect is the difference between the actual mean of the Obese/KO group and this additive mean. The interaction effect is what’s left-over, after you’ve added Obese effect and the KO effect to the control. 15.1.3 factorial experiments are frequently analyzed as flattened linear models in the experimental biology literature Often, researchers analyze data from a factorial experiment with a one-way ANOVA followed by pairwise tests (or by a simple series of separate t-tests). For the fake experiment, this linear model is \\[ \\begin{align} \\texttt{body_weight} &amp;= \\beta_0 + \\beta_1 (\\texttt{treatment}_{\\texttt{Obese}}) + \\beta_2 (\\texttt{treatment}_{\\texttt{KO}}) \\ + \\\\ &amp; \\quad \\ \\beta_3 (\\texttt{treatment}_{\\texttt{Obese + KO}}) + \\varepsilon \\end{align} \\] I refer to this as a flattened model (the table of treatment combinations has been flattened into a single row). Inference from a factorial or flattened model is the same. We can get the same pairwise contrasts or interaction contrasts from either model. That said, a factorial model nudges researchers to think about the analysis as a factorial design, which is good, because interaction effects are an explicit component of a factorial model. 15.2 Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data) To introduce a linear model with crossed factors (categorical \\(X\\) variables), I’ll use data from a set of experiments designed to measure the effect of the toll-like receptor protein TLR9 on the activation of excercise-induced AMP-activated protein kinase (AMPK) and downstream sequelae of this activation, including glucose transport (from outside to inside the cell) by skeletal muscle cells. Article source: TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise Public source The data are from multiple experiments in Figure 2. Data source 15.2.1 Understand the experimental design Background. Exercise (muscle activity) stimulates AMPK activated glucose uptake (transport from outside to inside the muscle cell). Instead of natural exercise, which can stimulate multiple systems, the researchers directly activate the muscle with electrical stimulation. Research question There are two ways to think about the question – both produce the same answer. How much does TLR9 knockout inhibit the expected increase of glucose uptake following electrical stimulation? The expected increase comes from the contrast of the positive and negative controls. Call this the knockout-induced stimulation effect. How much does TLR9 knockout inhibit glucose uptake during muscle stimulation compared to effect of TLR9 knockout during muscle rest? Call this the stimulation-induced TLR9-/- effect. Response variable \\(\\texttt{glucose_uptake}\\) (nmol per mg protein per 15 min) – the rate of glucose transported into the cell. Factor 1 – \\(\\texttt{genotype}\\) (“WT”, “KO”). “WT” (reference level) – C57BL/6J mice with intact TLR gene (TLR+/+) “KO” – TLR-/- mice on a C57BL/6J background Factor 2 \\(\\texttt{stimulation}\\) (“Rest”, “Active”) – Two levels: “Rest” (reference level) – muscle that has not been stimulated. “Active” – electrical stimulation of muscle to induce contraction and contractile-related cell changes These two factors create the three control treatments and the one focal treatment: WT/Rest – Negative control. Expect non-exercise (low) level uptake. WT/Active – Positive control. Expect high uptake. KO/Rest – Method control. Unsure of KO effect on uptake in Rest, which is why we need this control. KO/Active – Focal treatment. At about same level of KO/Rest if TLR9-/- completely inhibits electrical stimulation of glucose uptake Design – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. Planned Contrasts The two ways of framing the research question suggest either of the following sets of contrasts. Both ways of framing generate a treatment contrast that includes the focal treatment and a control contrast that does not include the focal treatment. The question pursued by the experiment is addressed with the contrast of the treatment and control contrasts, which is is the estimate of the interaction effect. There is no difference in inference between the two ways of framing the question – the interaction effects are equivalent. The two framings simply give two different ways of viewing an interaction effect. Framing 1: How much does TLR9 knockout inhibit the expected increase of glucose uptake during stimulation? (KO/Active - KO/Rest) – effect of Stimulation in KO mice. This is our treatment contrast. If TLR9 is necessary for glucose uptake, then this should be zero. If positive, there are non-TLR9 paths. (WT/Active - WT/Rest) – This is the positive control contrast – it is what we know based on prior knowledge and what we want to compare the treatment effect to. This should be positive based on prior knowledge. We need to control for the expected increase in 2 using the contrast: (KO/Active - KO/Rest) - (WT/Active - WT/Rest) – This contrast is the interaction effect. This focal contrast is what we need to estimate the knockout-induced stimulation effect. Framing 2: What is stimulation-induced TLR9-/- effect? (KO/Active - WT/Active) – effect of KO during muscle stimulation. This is our treatment contrast. If TLR9 is necessary for glucose uptake, then this should be big and negative if the KO effect at rest is small. (KO/Rest - WT/Rest) – effect of KO when the muscle is not stimulated. This is the methodological control contrast. If TLR9-/- KO has non-muscle-stimulation paths to glucose uptake, this will be something other than zero. We need to control for any (KO/Rest - WT/Rest) effect using the contrast: (KO/Active - WT/Active) - (KO/Rest - WT/Rest) – This contrast is the interaction effect. This focal contrast is what we need to estimate the stimulation-induced TLR9-/- effect. Our planned contrasts are: (WT/Active - WT/Rest) – positive control contrast (KO/Rest - WT/Rest) – methodological control contrast (KO/Active - WT/Active) – treatment contrast (KO/Active - WT/Active) - (KO/Rest - WT/Rest) – interaction contrast 15.2.2 Fit the linear model exp2j_m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j) 15.2.3 Inference The coefficient table exp2j_m1_coef &lt;- tidy(exp2j_m1, conf.int = TRUE) exp2j_m1_coef %&gt;% kable(digits = c(1,2,3,1,4,2,2)) %&gt;% kable_styling() term estimate std.error statistic p.value conf.low conf.high (Intercept) 6.75 0.419 16.1 0.0000 5.89 7.61 stimulationActive 3.45 0.592 5.8 0.0000 2.23 4.67 genotypeKO 0.78 0.639 1.2 0.2338 -0.54 2.10 stimulationActive:genotypeKO -2.31 0.885 -2.6 0.0152 -4.13 -0.48 emmeans table exp2j_m1_emm &lt;- emmeans(exp2j_m1, specs = c(&quot;stimulation&quot;, &quot;genotype&quot;)) exp2j_m1_emm %&gt;% kable(digits = c(1,1,2,3,1,2,2)) %&gt;% kable_styling() stimulation genotype emmean SE df lower.CL upper.CL Rest WT 6.75 0.419 25 5.89 7.61 Active WT 10.20 0.419 25 9.34 11.06 Rest KO 7.53 0.483 25 6.53 8.53 Active KO 8.67 0.447 25 7.75 9.59 The contrasts table # exp2j_m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table wt_rest &lt;- c(1,0,0,0) wt_active &lt;- c(0,1,0,0) ko_rest &lt;- c(0,0,1,0) ko_active &lt;- c(0,0,0,1) # contrasts are the difference in the vectors created above # these planned contrasts are described above # 1. (WT/Active - WT/Rest) -- positive control contrast # 2. (KO/Rest - WT/Rest) -- methodological control contrast # 3. (KO/Active - WT/Active) -- treatment contrast # 4. (KO/Active - WT/Active) - (KO/Rest - WT/Rest) -- interaction exp2j_m1_planned &lt;- contrast( exp2j_m1_emm, method = list( &quot;(WT/Active - WT/Rest)&quot; = c(wt_active - wt_rest), &quot;(KO/Rest - WT/Rest)&quot; = c(ko_rest - wt_rest), &quot;(KO/Active - WT/Active)&quot; = c(ko_active - wt_active), &quot;KO:Active Ixn&quot; = c(ko_active - wt_active) - (ko_rest - wt_rest) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) exp2j_m1_planned %&gt;% kable(digits = c(0,3,4,0,3,3,2,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value (WT/Active - WT/Rest) 3.448 0.5919 25 2.229 4.667 5.83 0.00000 (KO/Rest - WT/Rest) 0.780 0.6393 25 -0.537 2.097 1.22 0.23379 (KO/Active - WT/Active) -1.527 0.6127 25 -2.789 -0.265 -2.49 0.01967 KO:Active Ixn -2.307 0.8855 25 -4.131 -0.484 -2.61 0.01523 # double check with automated contrasts # contrast(exp2b.2_m1_emm, method = c(&quot;revpairwise&quot;)) 15.2.4 Plot the model ggplot_the_model( exp2j_m1, exp2j_m1_emm, exp2j_m1_planned, palette = pal_okabe_ito_blue, legend_position = &quot;bottom&quot;, y_label = &quot;Glucose uptake\\n(nmol per mg protein\\nper 15 min)&quot;, effect_label = &quot;Difference in glucose uptake&quot;, rel_heights = c(0.44,1), ) 15.2.5 alternaPlot the model exp2j_m1_pairs &lt;- contrast(exp2j_m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) b &lt;- emm_b(exp2j_m1_emm) dodge_width &lt;- 0.4 gg &lt;- ggplot_the_response( exp2j_m1, exp2j_m1_emm, exp2j_m1_pairs[c(1,3,4),], palette = pal_okabe_ito_blue, legend_position = &quot;bottom&quot;, y_label = &quot;Glucose uptake\\n(nmol per mg protein\\nper 15 min)&quot;, y_pos = c(13.6, 13.2, 13.3) ) + geom_segment(x = 2 + dodge_width/2 - 0.05, y = b[1] + b[2] + b[3], xend = 2 + dodge_width/2 + 0.05, yend = b[1] + b[2] + b[3], linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + geom_bracket( x = 2.25, y = b[1] + b[2] + b[3], yend = b[1] + b[2] + b[3] + b[4], label = paste0(&quot;ixn p = &quot;, fmt_p_value_rmd(exp2j_m1_planned[4,&quot;p.value&quot;])), text.size = 3, text.hjust = 0, color = &quot;black&quot;) gg Figure 15.1: Dashed gray line is expected additive mean of KO/Active 15.3 Understanding the linear model with crossed factors 1 Researchers in experimental biology report almost exclusively the p-values of the simple effects (Table 15.1) in data from factorial experiments. These can be computed even if the data are flattened into a single “treatment” factor. But it is the interaction effect that is necessary to make many of the inferences made by the researchers. To see why, we need to understand what the coefficients in the coefficient table are. 15.3.1 What the coefficients are To understand the coefficients, it helps to use the means in the emmeans table to construct a factorial table of the cell means Table 15.2: Cell mean table Rest Active WT 6.75 7.53 KO 10.20 8.67 The linear model fit to the \\(\\texttt{exp2j}\\) data is \\[ \\begin{align} \\texttt{glucose_uptake} &amp;= \\beta_{0} + \\beta_{1}(\\texttt{stimulation}_{\\texttt{Active}}) + \\beta_{2}(\\texttt{genotype}_{\\texttt{KO}}) \\ + \\\\ &amp;\\quad \\ \\beta_{3}(\\texttt{stimulation}_{\\texttt{Active}} : \\texttt{genotype}_{\\texttt{KO}}) + \\varepsilon\\ \\end{align} \\] and the fit coefficients are term estimate std.error statistic p.value conf.low conf.high (Intercept) 6.7501 0.42 16.1280 0.000 5.888 7.612 stimulationActive 3.4482 0.59 5.8258 0.000 2.229 4.667 genotypeKO 0.7801 0.64 1.2202 0.234 -0.537 2.097 stimulationActive:genotypeKO -2.3072 0.89 -2.6056 0.015 -4.131 -0.484 Explainer Understand what the rows of the coefficient table are. There are four parameters in the fit linear model – the rows are the statistics for the estimates of these parameters. These estimates are the coefficients of the model. The \\(\\texttt{(Intercept)}\\) (\\(b_0\\)) is the mean \\(\\texttt{glucose_uptake}\\) of the reference level, which was set to the WT/Rest group (Figure 15.2). This is the mean in the upper left cell in Table 15.2. The \\(\\texttt{stimulationActive}\\) coefficient (\\(b_2\\)) is the estimate of the the added effect of Stimulation when the genotype factor is at its reference level, and so is the mean in the upper right cell minus the mean in the upper left cell in Table 15.2. This coefficient is not a mean – it is a difference of means (Figure 15.2). The mean of the WT/Active group is \\(b_0 + b_2\\). The \\(\\texttt{genotypeKO}\\) coefficient (\\(b_1\\)) is the estimate of the the added effect of knocking out TLR9 when the stimulation factor is at its reference level, and so is the mean in the lower left cell minus the mean in the upper left cell in Table 15.2. This coefficient is not a mean – it is a difference of means (Figure 15.2). The mean of the KO/Rest group is \\(b_0 + b_1\\). The \\(\\texttt{stimulationActive:genotypeKO}\\) coefficient (\\(b_3\\)) is the estimate of the interaction effect between \\(\\texttt{genotype}\\) and \\(\\texttt{stimulation}\\). It is not the mean in the lower right cell minus the mean in the upper left cell. Instead, it is the mean in the lower right cell minus the expected mean in the lower right cell if the genotype and activity treatment effects were additive. The expected additive-mean in the lower right cell (KO/Active) is \\(b_0 + b_1 + b_2\\). The mean of KO/stimulation is \\(b_0 + b_1 + b_2 + b_3\\) (Figure 15.2). The interaction effect is a non-additive effect. Think about this. Adding the Stimulation alone adds 3.45 nmol glucose per protein per 15 min to the uptake rate of the reference. Adding “KO” alone adds 0.78 glucose per protein per 15 min to the uptake rate of the reference. If these effects were purely additive, then adding both Stimulation and KO to the reference rate should result in a mean of 6.75 + 3.45 + 0.78 = 10.98 glucose per protein per 15 min. The modeled mean for KO/Active is 8.67 glucose per protein per 15 min. The difference observed - additive is 8.67 - 10.98 = -2.31 glucose per protein per 15 min. Compare this difference to the interaction coefficient in the coefficient table. Reinforce your understanding of “non-additive” in item 5. The interaction is a non-additive effect because the mean of the combined treatment is something different than if we were to just add the KO and Active effects. But this effect is additive in the linear model. This is what linear models are – a reference mean plus the sum of a bunch of effects. Understand what these rows are not. The \\(\\texttt{stimulationActive}\\) row is not the same as the “stimulation” term in a Type III ANOVA table (the ANOVA table produced in GraphPad Prism or JMP). The p-values will be different because the p-values are testing different hypotheses. In the coefficient table, the \\(\\texttt{stimulationActive}\\) p-value is testing the difference of the means of WT/Active and WT/Rest. The stimulation term in a Type III ANOVA table is testing if there is an overall stimulation effect, which is estimated as the average of the two stimulation contrasts (WT/Active - WT/Rest) and (KO/Active - KO/Rest). The average of these two contrasts is not often of interest (but sometimes is – see below). Figure 15.2: The coefficients of a linear model with two crossed factors, explained. 15.3.2 The interaction effect is something different Item 6 in the coefficient table explainer stated “The interaction is a non-additive effect because the mean of the combined treatment is something different than if we were to just add the KO and Active effects. The something different is the interaction effect. If the interaction effect were zero, the expected effect of stimulation in the KO mice would be the same as the expected effect of stimulation in the WT mice (Figure 15.3. This would suggest the underlying physiological changes between Rest and Active in the KO mice is”more of the same” physiological changes in the WT mice. But, because of the interaction, the underlying physiological changes between Rest and Active in the KO mice is “something different” to that of physiological changes in the WT mice (Figure 15.3). The biological reasons causing interaction effects are highly variable and are what makes Biology fun. Additive effects (no interaction) may occur when combined treatments act independently of each other. This might occur in the glucose uptake response if knocking out TLR9 opens a path to glucose uptake that is different from and independent of the paths activated by electrial stimulation. Positive, or synergistic interaction effects may occur when combined treatments augment each other’s ability to affect the response (see Example 3 below). This could occur in the glucose uptake response if knocking out TLR9 opens a path to glucose uptake that is different the paths activated by electrial stimulation but also makes the paths activated by stimulation more sensitive to stimulation. Negative, or antagonistic interaction effects may occur when combined treatments interfere with each other’s ability to affect the response. This could occur in the glucose uptake response if TLR9 is on the path from stimulation to glucose uptake. Knocking out TLR9 interferes with this path. If TLR9 is required, we’d expect the interaction effect to be the same magnitude but opposite sign of the control effect – that is, complete antagonism of the control effect. Previous experiments suggested this negative interaction. The measurement of this effect was the purpose of experiment \\(\\texttt{exp2j}\\). Figure 15.3: The interaction is something different, not more of the same. To get the observed difference (4), take the expected difference (3) and add something different (5). 15.3.3 Why we want to compare the treatment effect to a control effect The purpose of the experiment is to infer a TLR9 role in the regulation of muscle-stimulated glucose transport, that is, a stimulation-induced TLR9-/- effect. If TLR9 is in the pathway from muscle activity to glucose transport, we expect some kind of recovery to baseline (Rest) values in the KO/Active group. But TLR9 may also (or alternatively) have a role in non-stimulation-induced glucose uptake. How do we make an inference about a stimulation-induced TLR9-/- effect using this experiment? Researchers typically look at the treatment effect (KO/Active - WT/Active) and conclude a stimulation-induced TLR9-/- effect if it’s big (and negative) If the treatment effect is the correct contrast for inference, why bother with the measures of glucose uptake during Rest? The reason the treatment effect alone is the wrong contrast for inferring the stimulation-induced TLR9 effect is because it is confounded by the control effect, which is the contrast (KO/Rest - WT/Rest) (Figure 15.4). Figure 15.4: Why the interaction effect is the stimulation-induced TLR9-/- effect To unconfound the inference, subtract the confounder: (KO/Active - WT/Active) - (KO/Rest - WT/Rest) This is the interaction effect. Consequences of interpreting the treatment effect KO/Active - WT/Active as the stimulation-induced TLR9-/- effect are highlighted in Figure 15.5. In these plots, the data are those from \\(\\texttt{exp2j}\\) but with the values in the KO groups shifted up or down to create the scenarios. Scenario 1. The positive control has big effect AND KO has no effect during rest AND the stimulation-induced TLR9-/- effect is equal in magnitude but opposite direction to the Active effect in WT (Figure 15.5A). The stimulation-induced TLR9-/- effect is conspicuous from the plot, if our sample means are close to the true means and we have high precision. The simple effect measures the stimulation-induced TLR9-/- effect correctly – but this is because the simple effect is equal in magnitude (but opposite sign) to the interaction effect. Many experiments in the literature are pretty similar to this scenario. Scenario 2. The positive control has big effect AND KO has a negative effect during rest BUT there is zero stimulation-induced TLR9-/- effect – that is, the interaction is zero (Figure 15.5B). There is an effect of KO during stimulation but this effect is no different that that occuring during Rest. So this cannot be a contraction induced TLR9 effect. Simple effect 1 inflates the effect. Scenario 3. The positive control has big effect AND KO has a positive effect during rest AND there is a stimulation-induced TLR9-/- effect that is equal to that in scenario 1 (Figure 15.5C). The positive effect of KO during Rest masks the stimulation-induced TLR9-/- effect. The simple effect underestimates the stimulation-induced TLR9-/- effect. This is similar to what is happening in Experiment 2j. Figure 15.5: Scenarios to show the consequence of inferring the stimulation-induced TLR9-/- effect from the simple effect (KO/Active - WT/Active). The simple effect and interaction effect lines extend from the KO/Active mean to either the KO/Rest mean (simple) or the expected mean of KO/Active if the two factors were additive (interaction). 15.3.4 The order of the factors in the model tells the same story differently The order of the factors in the model formula doesn’t matter for the coefficients, the estimated marginal means, or the contrasts. It can matter for ANOVA (more on this below) but not “tests after ANOVA”. But the order does matter to how the researchers communicates the results to themselves and in the report (Figure 15.6). The order is also a natural consequence of the two different ways of framing the research question. Figure 15.6: It may take some work but these plots show the same four means and effects. The only difference is how we communicate the story to ourselves and to others. A) the order of the factors in the model is stimulation * genotype. B) the order of the factors in the model is genotype * stimulation. 15.3.5 Power for the interaction effect is less than that for simple effects The interaction contrast is the difference of two simple contrasts and, consequently, the variance of the interaction contrast is twice that of the simple contrasts. And, consequently, the SE of the interaction estimate is \\(\\sim 1.4 \\times\\) larger than the precision of the two simple effects that form the contrast (1.4 is $sqrt{2}. The exact value will depend on differences in sample size among groups). The consequence of this is wider confidence intervals and larger p-values for the interaction contrast compared to a simple contrast of the same effect size. 15.3.6 Planned comparisons vs. post-hoc tests The contrasts computed above were planned based on questions motivating the experiment. The planned contrasts contain three of the four simple effects. The four simple effects are exp2j_m1_pairs &lt;- contrast(exp2j_m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) exp2j_m1_pairs %&gt;% kable(digits = c(1,1,1,2,3,1,2,2,1,6)) %&gt;% kable_styling() genotype stimulation contrast estimate SE df lower.CL upper.CL t.ratio p.value WT . Active - Rest 3.45 0.592 25 2.23 4.67 5.8 0.000004 KO . Active - Rest 1.14 0.659 25 -0.22 2.50 1.7 0.095500 . Rest KO - WT 0.78 0.639 25 -0.54 2.10 1.2 0.233785 . Active KO - WT -1.53 0.613 25 -2.79 -0.27 -2.5 0.019668 There are six pairwise comparisons for this experiment. Two of these are not a simple effect: KO/Active - WT/Rest WT/Active - KO/Rest These are the contrasts of the diagonal pairs in the cell-means table (Table 15.2). In a factorial design, we generally are not interested in these diagonal contrasts. They could be reported in a supplement. Recognize that if you are adjusting p-values for multiple tests, and you do not care about these contrasts but have included them in the computation of the adjustment, then your adjusted p-values are conservative. 15.4 Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data) 15.4.1 Understand the experimental design Research question 1. What is the effect of the X chromosome complement on lipid-related disease markers? 2. What is the effect of sex (female or male gonad) on lipid-related disease markers? 3. How conditional is the sex effect on the chromosome complement? 4. How conditional is the effect of X chromosome complement on the level of sex? Response variable \\(\\texttt{lesian_area}\\) – atherosclerotic lesian area in aortic sinus. Factor 1 – \\(\\texttt{sex}\\) (“Female”, “Male”). This is not the typical sex factor that is merely observed but is a manipulated factor. Sex is determined by the presence or absence of SRY on an autosome using the Four Core Genotype mouse model. SRY determines the gonad that develops (ovary or testis). “Female” does not have the autosome with SRY. “Male” has the autosome with SRY. Factor 2 – \\(\\texttt{chromosome}\\) (“XX”, “XY”). The sex chromosome complement is not observed but manipulated. In “XX”, neither sex chromosome has SRY as the natural condition. In “XY”, SRY has been removed from the Y chromosome. Design – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. “Female/XX” is the control. “Male/XX” adds the autosomal SRY gene (and testes instead of an ovary). “Female/XY” replaces the “X” complement with the engineered Y complement. “Male/XY” has both the autosomal SRY and the engineered Y complement. sex chromosome treatment chromosome complement autosome gonad Female XX FXX X WT ovary Male XX MXX X SRY+ testis Female XY FXY Y (sry-) WT ovary Male XY MXY Y (sry-) SRY+ testis The research question suggest the following planned contrasts What is the effect of \\(\\texttt{chromosome}\\) on lipid-related disease markers? (Female/XX - Female/XY) – effect of XX in mice with female gonad. If hypothesis is true, this should be large, negative. (Male/XX - Male/XY) – effect of XX in mice with male gonad. If hypothesis is true, this should be large, negative. What is the effect of \\(\\texttt{sex}\\) on lipid-related disease markers? (Male/XX - Female/XX) – effect of sex in XX mice. (Male/XY - Female/XY) – effect of sex in XY in mice. How conditional are the effects? (Male/XX - Male/XY) - (Female/XX - Female/XY) – Interaction. In this experiment, there is no treatment contrast or control contrast. Instead, all four simple contrasts are of equal interest. 15.4.2 Fit the linear model exp3e_m1 &lt;- lm(lesian_area ~ sex*chromosome, data = exp3e) 15.4.3 Check the model ggcheck_the_model(exp3e_m1) 15.4.4 Inference from the model The coefficient table # step 2 - get the coefficient table exp3e_m1_coef &lt;- tidy(exp3e_m1, conf.int = TRUE) term estimate std.error statistic p.value conf.low conf.high (Intercept) 0.540 0.0353 15.29 0.0000 0.464 0.616 sexMale -0.018 0.0500 -0.37 0.7172 -0.126 0.089 chromosomeXY -0.290 0.0530 -5.48 0.0001 -0.404 -0.177 sexMale:chromosomeXY 0.143 0.0749 1.90 0.0776 -0.018 0.303 The emmeans table # step 3 - get the modeled means exp3e_m1_emm &lt;- emmeans(exp3e_m1, specs = c(&quot;sex&quot;, &quot;chromosome&quot;)) exp3e_m1_emm %&gt;% summary() %&gt;% kable(digits = c(1,1,3,4,1,3,3)) %&gt;% kable_styling() sex chromosome emmean SE df lower.CL upper.CL Female XX 0.540 0.0353 14 0.464 0.616 Male XX 0.522 0.0353 14 0.446 0.597 Female XY 0.250 0.0395 14 0.165 0.335 Male XY 0.374 0.0395 14 0.289 0.459 The contrasts table # m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table fxx &lt;- c(1,0,0,0) mxx &lt;- c(0,1,0,0) fxy &lt;- c(0,0,1,0) mxy &lt;- c(0,0,0,1) # contrasts are the difference in the vectors created above # the focal contrasts are in the understand the experiment section # 1. (FXY - FXX) # 2. (MXY - MXX) # 3. (MXX - FXX) # 4. (MXY - FXY) # 5. Interaction exp3e_m1_planned &lt;- contrast(exp3e_m1_emm, method = list( &quot;FXY - FXX&quot; = c(fxy - fxx), &quot;MXY - MXX&quot; = c(mxy - mxx), &quot;MXX - FXX&quot; = c(mxx - fxx), &quot;MXY - FXY&quot; = c(mxy - fxy), &quot;Interaction&quot; = c(mxy - mxx) - c(fxy - fxx) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) # check # exp3e_m1_ixn &lt;- contrast(exp3e_m1_emm, # interaction = c(&quot;revpairwise&quot;), # by = NULL) exp3e_m1_planned %&gt;% kable(digits = c(0,3,4,0,3,3,2,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value FXY - FXX -0.290 0.0530 14 -0.404 -0.177 -5.48 0.00008 MXY - MXX -0.148 0.0530 14 -0.261 -0.034 -2.79 0.01458 MXX - FXX -0.018 0.0500 14 -0.126 0.089 -0.37 0.71716 MXY - FXY 0.124 0.0559 14 0.004 0.244 2.22 0.04309 Interaction 0.143 0.0749 14 -0.018 0.303 1.90 0.07762 Explainer The two simple effects and the interaction are computed separately. If we want to adjust for three comparisons, I would use the Holm method. The magnitude of the estimated effect of XX in mice with male gonads is about 1/2 that in mice with female gonads. This difference is the magnitude of the interaction. Don’t infer “no effect” given the p-value of the interaction. The estimate of the interaction effect has the same magnitude as the estimated effect of XX in mice with male gonads and about 1/2 the magnitude as the estimated effect of XX in mice with female gonads. The interaction p-value suggests caution in our confidence of the sign of this effect. 15.4.5 Plot the model exp3e_m1_plot &lt;- ggplot_the_model( fit = exp3e_m1, fit_emm = exp3e_m1_emm, fit_pairs = exp3e_m1_planned, palette = pal_okabe_ito_blue, legend_position = &quot;bottom&quot;, y_label = expression(paste(&quot;Lesian area (&quot;, mm^2, &quot;)&quot;)), effect_label = expression(paste(&quot;Effect (&quot;, mm^2, &quot;)&quot;)), contrast_rows = &quot;all&quot;, rel_heights = c(0.5,1) ) exp3e_m1_plot ### alternaPlot the model exp3e_m1_pairs &lt;- contrast(exp3e_m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) b &lt;- emm_b(exp3e_m1_emm) dodge_width &lt;- 0.4 gg &lt;- ggplot_the_response( exp3e_m1, exp3e_m1_emm, exp3e_m1_pairs[1:4,], palette = pal_okabe_ito_blue, legend_position = &quot;bottom&quot;, y_label = expression(paste(&quot;Lesian area (&quot;, mm^2, &quot;)&quot;)), y_pos = c(0.75, 0.78, 0.72, 0.72) ) + geom_segment(x = 2 + dodge_width/2 - 0.05, y = b[1] + b[2] + b[3], xend = 2 + dodge_width/2 + 0.05, yend = b[1] + b[2] + b[3], linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + geom_bracket( x = 2.25, y = b[1] + b[2] + b[3], yend = b[1] + b[2] + b[3] + b[4], label = paste0(&quot;ixn p = &quot;, fmt_p_value_rmd(exp3e_m1_planned[5,&quot;p.value&quot;])), text.size = 3, text.hjust = 0, color = &quot;black&quot;) gg Figure 15.7: Dashed gray line is expected additive mean of KO/Active 15.5 Understanding the linear model with crossed factors 2 15.5.1 Conditional and marginal means Table 15.3: Conditional (white background) and marginal (color background) means from full factorial model fit to lesian area data Female Male mean XX 0.540 0.522 0.531 XY 0.250 0.374 0.312 mean 0.395 0.448 The conditional means from the fit model are shown in the upper left \\(2 \\times 2\\) block (white background) of Table 15.3. These means are conditional on the level of \\(\\texttt{sex}\\) and \\(\\texttt{chromosome}\\). For the linear model with two crossed factor here (with no continuous covariates), these conditional means are equal to the sample means of the treatment. The values in the last row and column are the marginal means, which are the means of the associated row or column cells (these values are in the margins of the table). More generally, marginal refers to a statistic averaged across multiple levels of another variable. The marginal means of the \\(\\texttt{chromosome}\\) levels (orange background) are the means of the “XX” and “XY” rows. The marginal means of the \\(\\texttt{sex}\\) levels (blue background) are the means of the “Female” and “Male” columns. Note that the marginal means are simple averages across cell means and not weighted averages where the weights are the sample size used to compute the conditional (cell) means. 15.5.2 Simple (conditional) effects In a factorial experiment with crossed A and B factors, there is an effect of factor A (relative to the reference, or another level of factor A) for each of the p levels of factor B. And, there is an effect of factor B (relative to the reference, or another level of factor B) for each of the m levels of factor A. These effects of one factor at each of the levels of the other factor are called the simple effects. I prefer conditional effects, since the value of the effect is conditional on the level of the other factor. For the mouse lesian area experiment, there is a chromosome effect at sex = Female and a different effect at sex = Male. Similarly, there is a sex effect at chromosome = XX and a different effect at chromosome = XY. Table 15.4: Conditional (or “simple”) effects of the factorial model. chromosome sex contrast estimate SE df lower.CL upper.CL t.ratio p.value XX . Male - Female -0.018 0.050 14 -0.126 0.089 -0.370 0.71716 XY . Male - Female 0.124 0.056 14 0.004 0.244 2.224 0.04309 . Female XY - XX -0.290 0.053 14 -0.404 -0.177 -5.479 0.00008 . Male XY - XX -0.148 0.053 14 -0.261 -0.034 -2.786 0.01458 The first two rows are the conditional effects of \\(\\texttt{sex}\\) in each of the levels of \\(\\texttt{chromosome}\\). The last two rows are the conditional effects of \\(\\texttt{chromosome}\\) in each of the levels of \\(\\texttt{sex}\\). To help understand conditional effects, I add these to the \\(m \\times p\\) table of treatment combination means (Table 15.5). The values in the right-side column (orange) are the conditional effects of \\(\\texttt{sex}\\) at each level of \\(\\texttt{chromosome}\\) These values are the difference of the means in the associated row. For example, the conditional effect of \\(\\texttt{sex}\\) when chromosome = XY is 0.124 (second value in orange column). The values in the bottom row (blue) are the conditional effects of \\(\\texttt{chromosome}\\) at each level of \\(\\texttt{sex}\\). These values are the difference of the means in the associated column. For example, the conditional effect of \\(\\texttt{chromosome}\\) when sex = Female is -0.29 (first value in blue row). Note that the first conditional effect for each factor has a corresponding row in the table of coefficients of the fit model because these are the effects for that factor at the reference level of the other factor. Table 15.5: Conditional (“simple”) effects of sex (orange background) and chromosome (blue background) on lesian area. The conditional means of each combination of the factor levels are in the cells with the white background. The simple effect is the difference in the means of the associated row or column. Female Male simple XX 0.54 0.522 -0.018 XY 0.25 0.374 0.124 simple -0.29 -0.148 15.5.3 Marginal effects The average of the conditional effects for a factor are the marginal effects, or the main effects in ANOVA terminology. ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## Male - Female 0.0529 0.0375 14 1.411 0.1800 ## ## Results are averaged over the levels of: chromosome ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## XY - XX -0.219 0.0375 14 -5.844 &lt;.0001 ## ## Results are averaged over the levels of: sex I’m showing the full output from the emmeans package output to highlight the warning that the inference “may be misleading” because of the interaction effect in the linear model. This is a healthy warning that I follow up on below. In Table 15.6, I add the marginal effects to the table of conditional effects from above (Table 15.5) Table 15.6: Marginal effects of sex (orange) and chromosome (blue) on lesian area. Simple effects are in grey cells. The conditional means of each combination of the factor levels are in the white cells. Female Male simple marginal XX 0.54 0.522 -0.018 XY 0.25 0.374 0.124 simple -0.29 -0.148 -0.219 marginal 0.053 15.5.4 The additive model Marginal effects can be useful for summarizing a general trend, but, like any average, might not be especially meaningful if there is large heterogeneity of the simple effects, which occurs when the interaction effect is large. If an interaction effect is small, and we want to summarize the results as general trends (“sex does this, chromosome does that”), then the best practice strategy is to refit a new linear model that estimates the effects of the two factors as if the interaction were equal to zero. \\[ \\begin{equation} \\texttt{lesian_area} = \\beta_0 + \\beta_1 \\texttt{sex}_\\texttt{Male} + \\beta_2 \\texttt{chromosome}_\\texttt{XY} + \\varepsilon \\tag{15.1} \\end{equation} \\] Model @ref{eq:twoway-reduced} is a reduced model because one of the terms has been removed from the model. This particular reduced model is often referred to as the additive model, since it excludes the interaction term, which is non-additive effect (the indicator variable is the product of two “main” indicator variables). In R, this model is exp3e_m2 &lt;- lm(lesian_area ~ sex + chromosome, data = exp3e) The model coefficients of the additive model are Table 15.7: Model coefficients of additive model. term estimate std.error statistic p.value conf.low conf.high (Intercept) 0.508 0.034 15.06 0.000 0.436 0.580 sexMale 0.045 0.040 1.11 0.283 -0.041 0.131 chromosomeXY -0.219 0.041 -5.39 0.000 -0.306 -0.132 Explainer \\(\\texttt{sexMale}\\) is the average of the two conditional effects of “Male” (one at chromosome = \"XX\" and one at chromosome = \"XY). \\(\\texttt{chromosomeXY}\\) is the average of the two conditional effects of “XY” (one at sex = \"Female\" and one at sex = \"Male). \\(\\texttt{(Intercept)}\\) is the expected value without the added \\(\\texttt{sexMale}\\) or \\(\\texttt{chromosomeXY}\\) effects. This is a very abstract “average” of Female/XX and is not the average value in the Female/XX group. The conditional effects of the reduced model are Table 15.8: Conditional effects of additive model. chromosome sex contrast estimate SE df lower.CL upper.CL t.ratio p.value XX . Male - Female 0.04495 0.040 15 -0.041 0.131 1.11 0.28292 XY . Male - Female 0.04495 0.040 15 -0.041 0.131 1.11 0.28292 . Female XY - XX -0.21895 0.041 15 -0.306 -0.132 -5.39 0.00007 . Male XY - XX -0.21895 0.041 15 -0.306 -0.132 -5.39 0.00007 Explainer In an additive model, all conditional effects for one factor are the same for each level of the other factor. This makes sense. If the model fit is additive, the interaction effect is set to zero by the model and there cannot be differences in conditional effects among the contrasts at each of the levels of the other factor (otherwise, there would be an interaction). A more sensible way of thinking about this is, it doesn’t make sense to compute or discuss conditional effects in an additive model. Instead, an additive model automatically estimates marginal effects. Compare the table of marginal effects of the additive model to the table of marginal effects of the full model. The estimates for the chromosome effect are the same but the t-values and p-values differ because of different degrees of freedom (the full model estimates one more parameter, the interaction effect). The estimates for the sex effect are not the same between the two tables because of an imbalance of sample size. In the computation of the marginal effect of chromosome, the two simple effects both have sample size of 5 and 4. But in the computation of the marginal effect of sex, one simple effect has sample size of 5 and 5 while the other has a simple effect of 4 and 4. 15.5.5 Reduce models for the right reason Unless one factor truly has no effect, there will always be an interaction. As stated above, interactions are ubiquitous. If an interaction is small, it can make sense to drop the interaction term and re-fit an additive model to estimate marginal effects in order to present a simplified picture of what is going on, with the recognition that these estimates are smoothing over the heterogenity in conditional (simple) effects that truly exist. Aided and abetted by statistics textbooks for biologists, there is a long history of researchers dropping an interaction effect because the interaction \\(p&gt;0.05\\). A good rule of thumb is, don’t make model decisions based on p-values. It doesn’t make any sense. The \\(p\\)-value is an arbitrary dichotomization of a continuous variable. Would it make sense to behave differently if the interaction were \\(p=0.06\\) vs. \\(p=0.04\\), given that these two p-values are effectively identical? A \\(p\\)-value is not evidence that an effect is zero, or “doesn’t exist”, or even that an effect is “trivially small”. This is because \\(p\\)-values are a function of measurement error, sampling error, and sample size, in addition to effect size. The interaction p-value for the lesion-area data is 0.078. Should we refit the additive model and report a simpler story of “a” chromosome effect and “a” sex effect? This reduced model isn’t invalid and it is useful. Some considerations. there is certainly a real interaction between these two factors and this interaction reflects interesting biology. For this example, I might report both – the additive effect in the main paper (since the big chromosome complement effect is the story) and the conditional effects in the supplement, which might further work on investigating the underlying biology. Or maybe two sets of p-values on the plot? There are lots of unexplored ways to provide more “ways” of looking at the results. Regardless, for this example, I would not avoid reporting the interaction effect and the conditional effects somewhere. The estimated interaction effect (0.14 mm\\(^2\\)) is moderately large relative to the four simple effects. It’s much bigger than the marginal effect of sex (Table 15.8) and about 2/3 the size of the marginal effect of chromosome (Table 15.8). A response plot of both models (Figure 15.8) can help understanding and the decision of which model to report. Figure 15.8: A. Conditional means and p-values of conditional effects. B. Marginal means and p-values of marginal effects. 15.5.6 The marginal means of an additive linear model with two factors can be weird To better understand the marginal effects computed from the additive model, let’s compare the emmeans table of the factorial and additive models. (#tab:twoway-exp3e_m1_emm)Conditional means of the lesian area data computed from the factorial model. sex chromosome emmean SE df lower.CL upper.CL Female XX 0.540 0.035 14 0.464 0.616 Male XX 0.522 0.035 14 0.446 0.597 Female XY 0.250 0.039 14 0.165 0.335 Male XY 0.374 0.039 14 0.289 0.459 (#tab:twoway-exp3e_m2_emm)Marginal means of the lesian area data computed from the additive model. sex chromosome emmean SE df lower.CL upper.CL Female XX 0.508 0.034 15 0.436 0.580 Male XX 0.553 0.034 15 0.481 0.625 Female XY 0.289 0.036 15 0.212 0.367 Male XY 0.334 0.036 15 0.257 0.412 Explainer The means in the conditional means table (Table @ref(tab:twoway-exp3e_m1_emm)) are equal to the sample means. These are conditional on \\(\\texttt{sex}\\) and \\(\\texttt{chromosome}\\). The means in the marginal means table (Table @ref(tab:twoway-exp3e_m2_emm)) are not equal to the sample means. These are modeled means of the four groups from a model in which there is no interaction effect, so all conditional effects for one factor are the same for each level of the other factor. If you take the difference of Male - Female for both the XX and the XY groups, these will be the same. All data has some measured interaction (even if there is no true interaction. But remember, interaction is ubiquitous in biology). The larger this interaction, the more weird the marginal means because these are less compatible with the data. 15.6 Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data) To explain what synergy is and why it is estimated by the interaction effect, this example uses data from an experiment measuring the effect of two defense signals on the defense response in Maize plants. In response to herbivory from insects, maize, and other plants, release multiple, chemical signals into the air (chemicals that evaporate into the air are known as volatile compounds). These chemicals signal the plant, and neighboring plants, to secrete anti-herbivory hormones, including abcisic acid and jasmonic acid. The researchers investigated the effects of two volatile compounds, (Z)‐3‐hexenyl acetate (HAC) and Indole, on the defense response both each without the other and in combination. The example data come from Figure 1c, which is the effect of HAC and Indole on tissue concentrations of the hormone jasmonic acid (JA). The design is fully crossed with two factors, each with two levels: \\(\\texttt{hac}\\), with levels “HAC-” and “HAC+”, and \\(\\texttt{indole}\\), with levels (“Indole-” and “Indole+”). HAC- HAC+ Indole- Control HAC Indole+ Indole HAC+Indole 15.6.1 Examine the data qplot(x = treatment, y = ja, data = exp1) Too few points for box plot. control variance is small. No obvious implausible points. fit with lm but recognize small n warning for any inference. 15.6.2 Fit the model exp1c_m1 &lt;- lm(ja ~ hac * indole, data = exp1) 15.6.3 Model check ggcheck_the_model(exp1c_m1) The distribution looks like a sample from a Normal. The variance looks like it increases with the mean. This suggest gls modeling heterogeneity. 15.6.4 Inference from the model exp1c_m1_coef &lt;- tidy(exp1c_m1, conf.int = TRUE) exp1c_m1_coef %&gt;% kable(digits = 3) %&gt;% kable_styling() term estimate std.error statistic p.value conf.low conf.high (Intercept) 43.862 4.881 8.986 0.000 33.515 54.210 hacHAC+ 15.615 6.903 2.262 0.038 0.982 30.249 indoleIndole+ 13.104 6.903 1.898 0.076 -1.529 27.738 hacHAC+:indoleIndole+ 29.813 9.762 3.054 0.008 9.118 50.508 exp1c_m1_emm &lt;- emmeans(exp1c_m1, specs = c(&quot;hac&quot;, &quot;indole&quot;)) exp1c_m1_emm %&gt;% kable(digits = c(1,1,1,2,1,1,1)) %&gt;% kable_styling() hac indole emmean SE df lower.CL upper.CL HAC- Indole- 43.9 4.88 16 33.5 54.2 HAC+ Indole- 59.5 4.88 16 49.1 69.8 HAC- Indole+ 57.0 4.88 16 46.6 67.3 HAC+ Indole+ 102.4 4.88 16 92.0 112.7 # exp1c_m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table ref &lt;- c(1,0,0,0) hac &lt;- c(0,1,0,0) indole &lt;- c(0,0,1,0) hac_indole &lt;- c(0,0,0,1) # contrasts are the difference in the vectors created above # these planned contrasts are described above # 1. (hac+/indole- - hac-/indole-) # add hac # 2. (hac-/indole+ - hac-/indole-) # add indole # 3. (hac+/indole+ - hac-/indole+) - (hac+/indole- - hac-/indole-) # Interaction exp1c_m1_planned &lt;- contrast( exp1c_m1_emm, method = list( &quot;hac+&quot; = c(hac - ref), &quot;indole+&quot; = c(indole - ref), &quot;HAC/Indole Ixn&quot; = c(hac_indole - indole) - (hac - ref) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) exp1c_m1_planned %&gt;% kable(digits = c(1,2,3,1,2,2,2,5)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value hac+ 15.62 6.903 16 0.98 30.25 2.26 0.03796 indole+ 13.10 6.903 16 -1.53 27.74 1.90 0.07583 HAC/Indole Ixn 29.81 9.762 16 9.12 50.51 3.05 0.00758 15.6.5 Plot the model 15.6.6 Alternative plot Figure 15.9: An alternative plot for showing the estimate of synergy. Gray, dashed line is the expected mean of the HAC + Indole group if the interaction is zero. 15.7 Understanding the linear model with crossed factors 3 15.7.1 Thinking about the coefficients of the linear model \\[ \\begin{equation} \\texttt{ja} = \\beta_0 + \\beta_1 (\\texttt{hac}_\\texttt{HAC+}) + \\beta_2 (\\texttt{indole}_\\texttt{Indole+}) + \\beta_3 (\\texttt{hac}_\\texttt{HAC+}:\\texttt{indole}_\\texttt{Indole+}) +\\varepsilon \\tag{15.2} \\end{equation} \\] The linear model makes it easy to understand synergy in the \\(2 \\times 2\\) design. If we start with the mean of the reference (the group without the added HAC or Indole), then \\(\\beta_1\\) is the extra bit due to adding HAC, \\(\\beta_2\\) is the extra bit due to adding Indole, and \\(\\beta_3\\) is the extra bit due to synergy between HAC and Indole. A positive \\(\\beta_3\\) means the combined treatment effect is more than the sum of the parts. Table 15.9: Coefficient table of the factorial model term estimate std.error statistic p.value conf.low conf.high (Intercept) 43.86 4.9 8.986 0.00 33.51 54.21 hacHAC+ 15.62 6.9 2.262 0.04 0.98 30.25 indoleIndole+ 13.10 6.9 1.898 0.08 -1.53 27.74 hacHAC+:indoleIndole+ 29.81 9.8 3.054 0.01 9.12 50.51 Again, the interaction is a non-additive effect. Adding HAC alone increases JA concentration by 15.6 ng per g FW. Adding Indole alone increases JA concentration by 13.1 ng per g FW. If these effects were purely additive, then adding both HAC and Indole to the Control mean should result in a mean of 43.9 + 15.6 + 13.1 = 72.6 ng per g FW in the HAC+Indole group. The modeled mean is 102.4 ng per g FW. The difference (observed - additive) is 102.4 - 72.6 = 29.8 ng per g FW. This is the estimated interaction effect in the coefficient table. Figure 15.10: Synergy is the bit needed to get to the HAC + Indole mean after adding the HAC effect and the Indole effect to the Control mean 15.8 Issues in inference 15.8.1 For pairwise contrasts, it doesn’t matter if you fit a factorial or a flattened linear model Compare the pairwise comparisons of the Experiment 2j glucose uptake data using a factorial linear model and a flattened linear model. Factorial: m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j) m1_emm &lt;- emmeans(m1, specs = c(&quot;stimulation&quot;, &quot;genotype&quot;)) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;tukey&quot;) %&gt;% summary(infer = TRUE) m1_pairs %&gt;% kable(digits = c(3)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Active WT - Rest WT 3.448 0.592 25 1.820 5.076 5.826 0.000 Rest KO - Rest WT 0.780 0.639 25 -0.978 2.539 1.220 0.620 Rest KO - Active WT -2.668 0.639 25 -4.427 -0.910 -4.173 0.002 Active KO - Rest WT 1.921 0.613 25 0.236 3.606 3.136 0.021 Active KO - Active WT -1.527 0.613 25 -3.212 0.158 -2.493 0.086 Active KO - Rest KO 1.141 0.659 25 -0.671 2.953 1.733 0.329 Flattened: m2 &lt;- lm(glucose_uptake ~ treatment, data = exp2j) m2_emm &lt;- emmeans(m2, specs = c(&quot;treatment&quot;)) m2_pairs &lt;- contrast(m2_emm, method = &quot;revpairwise&quot;, adjust = &quot;tukey&quot;) %&gt;% summary(infer = TRUE) m2_pairs %&gt;% kable(digits = c(3)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value WT Active - WT Rest 3.448 0.592 25 1.820 5.076 5.826 0.000 KO Rest - WT Rest 0.780 0.639 25 -0.978 2.539 1.220 0.620 KO Rest - WT Active -2.668 0.639 25 -4.427 -0.910 -4.173 0.002 KO Active - WT Rest 1.921 0.613 25 0.236 3.606 3.136 0.021 KO Active - WT Active -1.527 0.613 25 -3.212 0.158 -2.493 0.086 KO Active - KO Rest 1.141 0.659 25 -0.671 2.953 1.733 0.329 15.8.2 For interaction contrasts, it doesn’t matter if you fit a factorial or a flattened linear model Factorial model: # using m1 from above m1_ixn &lt;- contrast(m1_emm, interaction = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) m1_ixn %&gt;% kable(digits = c(3)) %&gt;% kable_styling() stimulation_revpairwise genotype_revpairwise estimate SE df lower.CL upper.CL t.ratio p.value Active - Rest KO - WT -2.307 0.885 25 -4.131 -0.484 -2.606 0.015 Flattened model: # need to compute the interaction as a contrast # using m2 from the previous chunk # m2_emm # print in console to get row numbers # set the mean as the row number from the emmeans table wt_rest &lt;- c(1,0,0,0) wt_active &lt;- c(0,1,0,0) ko_rest &lt;- c(0,0,1,0) ko_active &lt;- c(0,0,0,1) # contrasts are the difference in the vectors created above # 4. (KO/Active - WT/Active) - (KO/Rest - WT/Rest) -- interaction m2_ixn &lt;- contrast(m2_emm, method = list( &quot;KO:Active Ixn&quot; = c(ko_active - wt_active) - (ko_rest - wt_rest) )) %&gt;% summary(infer = TRUE) m2_ixn %&gt;% kable(digits = c(3)) %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value KO:Active Ixn -2.307 0.885 25 -4.131 -0.484 -2.606 0.015 15.8.3 Adjusting p-values for multiple tests Inflated Type I error in pairwise tests is a concern among some statisticians and many reviewers, readers, and colleagues. It is not the frequency of individual tests that is inflated in a batch of pairwise tests but the “family-wise” error rate – given either the six pairwise tests or the four simple-effect tests or in a \\(2 \\times 2\\) design, what is the frequency of at least one Type I error within this family of tests? For all six pairwise tests of the \\(2 \\times 2\\) design, the default adjustment is the Tukey HSD (Honestly Significant Difference) method. But if we limit the comparisons to the four simple effects, we need some other adjustment, if we want to adjust. Here are six different adjustment methods (including no adjustment) for the simple effects. Table 15.10: P-values from seven different methods of adjustment. The Tukey-all p-value is from all six pairwise comparisons. All other p-values are from the four simple effects. contrast estimate Tukey Bonferroni Sidak Holm MvT FDR None WT: Active - Rest 3.45 0.000 0.000 0.000 0.000 0.000 0.000 0.000 KO: Active - Rest 1.14 0.329 0.382 0.331 0.191 0.275 0.127 0.095 Rest: KO - WT 0.78 0.620 0.935 0.655 0.234 0.561 0.234 0.234 Active: KO - WT -1.53 0.086 0.079 0.076 0.059 0.066 0.039 0.020 Table 15.10 shows p-values adjusted using seven different methods (including no adjustment) for the Experiment 2j glucose uptake data. The Tukey correction is applied to all six pairwise tests in the code above. Tukey – The Tukey HSD adjustment corrects for the expected correlation among the tests. The other corrections are applied to only the four simple effects. Bonferroni – The Bonferroni adjustment assumes independence among the tests. This isn’t true for posthoc tests because every contrast includes a group that is in other tests (that is, the different contrasts have common data). The consequence of the correlation among the tests is that the Bonferroni adjustment is too conservative (the actual rate of Type I error will be smaller than the nominal rate). Even with independent test, the Bonferroni method is less powerful than the methods below. Sidak – The Sidak correction has increased power relative to Bonferroni but does not explicitly account for correlated tests. It’s type I error rate is too conservative. Holm – The Holm variant of the Bonferroni adjustment has increased power relative to the Bonferroni but does not explicitly account for correlated tests. It’s type I error rate is too conservative. Mvt – The multivariate t adjustment corrects for the empirically measured correlation. FDR – The FDR adjustment does not attempt to correct for Type I error but for the False Discovery Rate. The difference between these concepts is important. Further thoughts on multiple adjustment, focussing on the highlighted row in the table. The most transparent best practice is to report all, unadjusted p-values. Reporting all unadjusted p-values allows a reader to compute whatever adjustment they deem appropriate for the question they are addressing. Reporting all p-values could overwhelm the principal message of a figure. Instead, report focal (planned comparison) p-values in the figure and a table of all p-values in the supplement. There should be a table of these for every experiment in the supplement. This point raises the question of what “all” means. In some designs, “all” could mean all treatment vs. control comparisons. In others, “all” could mean all pairwise comparisons. In others, “all” could mean all pairwise comparisons and all interactions. In a Fisherian world-view of p-values, there just isn’t any real difference in inference between \\(p = 0.02\\) (no adjustment) and \\(p = 0.086\\) (Tukey adjustment from all pairwise comparisons). Fisher expected decisions based on multiple replications of the experiment, all (or nearly) with \\(p &lt; 0.05\\). From a Fisherian-world-view, then, we have bigger how-to-do-science issues than multiple testing if we are making decisions entirely on \\(p = 0.02\\) versus \\(p = 0.086\\). From a Neyman-Pearson testing world-view, what is important is the family of tests. Using all six pairwise comparisons or all four simple effects as the family because this is how the software assigned it is rather mindless statistics. 15.9 Two-way ANOVA Both wet-bench and ecology/evolution experimental biologists do ANOVA. Many textbooks, advisors, and colleagues tell researchers to report “tests after ANOVA” – what I call pairwise contrasts – but in modern computing, pairwise contrasts and ANOVA tables are both computed from the same underlying regression model. The ANOVA is not necessary for any inference, a point I return to below. Let’s use the glucose uptake experiment (Example 1) to explore the ANOVA table. 15.9.1 How to read a two-way ANOVA table Table 15.11: ANOVA table of glucose uptake experiment (Example 1). Effect df F p.value stimulation 1, 25 26.86 &lt;.0001 genotype 1, 25 0.71 .41 stimulation:genotype 1, 25 6.79 .02 Explainer This ANOVA table is computed using Type III sum of squares and should match the output from most statistics software including Graphpad Prism. The relevance of this is addressed below. In general, an ANOVA table has a row for each term in the underlying linear model. ANOVA is a method for decomposing the total variance into batches and each of these terms is a batch. For the linear model with two crossed factors, there is a term for each factor and a term for the interaction. \\(\\texttt{stimulation}\\) and \\(\\texttt{genotype}\\) are the main (or 1st order) effects. \\(\\texttt{stimulation:genotype}\\) is the interaction (or 2nd order) effect. These terms are the row names of the table. Some ANOVA functions in R also include a row for the variance of the residuals from the fit model. This row can be useful for learning what the values in ANOVA table are but only one statistic from the row (the residual df) is especially useful for reporting. In Table 15.11, this residual df is included in the statistics for each term. Many ANOVA tables contain additional SS (sum of squares) and MSE (mean square error) columns. The MSE is the variance of the term and used to compute the F statistic. The SS is used to compute the MSE. Since these columns are used for computation but not reporting, Table 15.11 excludes these. df (Degree of freedom) – If the term is a factor, the df will equal the number of levels (\\(k\\)) for the factor minus 1. Think of it this way: the contribution of the variance due to a factor is a function of the variability of the \\(k\\) level means around the grand mean. How many degrees of independent variation do these level means have, given that we know the grand mean? The answer is \\(k-1\\) – once the values of \\(k-1\\) level means are written down, the \\(k\\)th level mean has no freedom to vary; its value has to be \\(k\\bar{\\bar{Y}} - \\sum_i^{k-1}{Y_i}\\). For an interaction term, the df is the product of the df of each of the factors in the interaction. F (F-value or F-ratio) – This is the test statistic. It is the ratio of the MSE for the term divided by the MSE of the residual (this is strictly true for only “Fixed effects” ANOVA, which we have here). p.value – the p-value for the test statistic. F is compared to an F-distribution, which is a distribution of F-values under the null. 15.9.2 What do the main effects in an ANOVA table mean? It is very common in the literature to see researchers report the rows of an ANOVA table by stating something like “We found an effect of stimulation on glucose uptake (\\(F_{1,25} = 26.9\\), \\(p &lt; 0.0001\\))”. Thinking of a main effect term in an ANOVA table as “an effect” can be misleading. What does “an effect” mean? Afterall, there are two effects of \\(\\texttt{stimulation}\\) in this experiment, one in the wildtype mice and one in the TLR9-/- mice. Main effects in ANOVA tables are about average effects. A main effect in a two-way ANOVA table is an “overall” marginal effect. Recall that a marginal effect of the level of Factor A is the average of the conditional effects at each level of B (Section 15.5.3). For a \\(2 \\times 2\\) table, there is only one marginal effect for each factor and, as a consquence, the p value for the main effect term of \\(\\texttt{stimulation}\\) in the ANOVA table (Table 15.12) is equal to the p-value of the marginal effect of \\(\\texttt{stimulation}\\) in the contrast table (Table 15.13). The “main term” effect of stimulation is illustrated in Figure ??. Table 15.12: Reprinting the ANOVA table of the glucose uptake experiment (Example 1) to show more decimal places of the p-value. num Df den Df F Pr(&gt;F) stimulation 1 25 26.86 0.0000232 genotype 1 25 0.71 0.4068630 stimulation:genotype 1 25 6.79 0.0152305 Table 15.13: Marginal effect of stimulation on glucose uptake. contrast estimate SE df lower.CL upper.CL t.ratio p.value Active - Rest 2.29 0.443 25 1.38 3.21 5.183 0.0000232 Again, main effects in ANOVA tables are about average effects. Is the average effect what we want to report? The answer is, it depends. It does not depend on the p-value of the interaction (the answer in many textbooks and websites) although the p-value is, perhaps, not irrelevant. Rather, it depends on the research question motivating the experiment. The research question in Experiment 2j specifically predicted a different treatment (KO/Active - KO/Rest) and control effect (WT/Active - WT/Rest). The average of these two effects isn’t of interest. Any inference about the average effect of stimulation (the p-value from the ANOVA table or the marginal effect size or CIs) doesn’t answer any question motivating the experiment. A more general discussion of when we might be interested in the main effect term of an ANOVA table (or better, marginal effects) was in section xxx and below in section xxx. Figure 15.11: The main effect of stimulation in the ANOVA table tests the average of the conditional effects of stimulation. The average of the conditional effects is the difference between the marginal means of Active and of Rest. 15.10 More issues in inference 15.10.1 Longitudinal experiments – include Time as a random factor (better than repeated measures ANOVA) 15.11 Working in R 15.11.1 Model formula A linear model with two crossed factors is specified in the model formula as y ~ A*B where \\(\\texttt{A}\\) is the first factor, and \\(\\texttt{B}\\) is the second factor. R expands this formula to y ~ 1 + A + B + A:B where the colon indicates an interaction (multiplicative) effect. m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j) term estimate std.error statistic p.value conf.low conf.high (Intercept) 6.75 0.419 16.13 0.0000000 5.89 7.61 stimulationActive 3.45 0.592 5.83 0.0000045 2.23 4.67 genotypeKO 0.78 0.639 1.22 0.2337850 -0.54 2.10 stimulationActive:genotypeKO -2.31 0.885 -2.61 0.0152305 -4.13 -0.48 The order of the factors in the model formula doesn’t matter for the values of the coefficients, the estimated marginal means, or the contrasts. It can matter for ANOVA (more on this below) but not “tests after ANOVA”. m1_b &lt;- lm(glucose_uptake ~ genotype * stimulation, data = exp2j) term estimate std.error statistic p.value conf.low conf.high (Intercept) 6.75 0.419 16.13 0.0000000 5.89 7.61 genotypeKO 0.78 0.639 1.22 0.2337850 -0.54 2.10 stimulationActive 3.45 0.592 5.83 0.0000045 2.23 4.67 genotypeKO:stimulationActive -2.31 0.885 -2.61 0.0152305 -4.13 -0.48 The additive model is specified by the formula y ~ A + B m2 &lt;- lm(glucose_uptake ~ stimulation + genotype, data = exp2j) term estimate std.error statistic p.value conf.low conf.high (Intercept) 7.27 0.408 17.82 0.0000000 6.43 8.10 stimulationActive 2.42 0.487 4.97 0.0000368 1.42 3.42 genotypeKO -0.42 0.489 -0.86 0.3954703 -1.43 0.58 15.11.2 Using the emmeans function 15.11.2.1 Conditional means table # model m1 fit above # m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j) m1_emm &lt;- emmeans(m1, specs = c(&quot;stimulation&quot;, &quot;genotype&quot;)) m1_emm ## stimulation genotype emmean SE df lower.CL upper.CL ## Rest WT 6.75 0.419 25 5.89 7.61 ## Active WT 10.20 0.419 25 9.34 11.06 ## Rest KO 7.53 0.483 25 6.53 8.53 ## Active KO 8.67 0.447 25 7.75 9.59 ## ## Confidence level used: 0.95 Notes Printing the emmeans object displays useful information. Here, this information includes the confidence level used. If the object is printed using kable() %&gt;% kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost. emmeans computes the modeled means of all combinations of the levels of the factor variables specified in specs. If there are two factor variables in the model, and both are passed to specs, then the modeled means of all combinations of the levels of the two variables are computed. If only one factor variable is passed, then the marginal means (averaged over all levels of the missing factor) are computed (see below). 15.11.2.2 Marginal means m1_emm_stimulation &lt;- emmeans(m1, specs = c(&quot;stimulation&quot;)) ## NOTE: Results may be misleading due to involvement in interactions m1_emm_stimulation ## stimulation emmean SE df lower.CL upper.CL ## Rest 7.14 0.320 25 6.48 7.8 ## Active 9.43 0.306 25 8.80 10.1 ## ## Results are averaged over the levels of: genotype ## Confidence level used: 0.95 Notes In a model with two crossed factors, y ~ A * B, the marginal means of the levels of \\(\\texttt{A}\\), averaged over all levels of \\(\\texttt{B}\\) are computed by setting specs = to \\(\\texttt{A}\\) only. Remember that the specs argument sets the values of the predictors for which we want a mean. By excluding \\(\\texttt{B}\\), we don’t get the means of the levels of \\(\\texttt{A}\\) at each level of \\(\\texttt{B}\\) but averaged across the levels of \\(\\texttt{B}\\).emmeans “knows” to average across the levels of \\(\\texttt{B}\\) because \\(\\texttt{B}\\) is in the model. 15.11.3 Contrasts 15.11.3.1 Planned contrasts wt_rest &lt;- c(1,0,0,0) wt_active &lt;- c(0,1,0,0) ko_rest &lt;- c(0,0,1,0) ko_active &lt;- c(0,0,0,1) m1_planned &lt;- contrast( m1_emm, method = list( &quot;(WT/Active - WT/Rest)&quot; = c(wt_active - wt_rest), &quot;(KO/Rest - WT/Rest)&quot; = c(ko_rest - wt_rest), &quot;(KO/Active - WT/Active)&quot; = c(ko_active - wt_active), &quot;KO:Active Ixn&quot; = c(ko_active - wt_active) - (ko_rest - wt_rest) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast estimate SE df lower.CL upper.CL t.ratio p.value (WT/Active - WT/Rest) 3.45 0.592 25 2.23 4.67 5.83 0.0000 (KO/Rest - WT/Rest) 0.78 0.639 25 -0.54 2.10 1.22 0.2338 (KO/Active - WT/Active) -1.53 0.613 25 -2.79 -0.27 -2.49 0.0197 KO:Active Ixn -2.31 0.885 25 -4.13 -0.48 -2.61 0.0152 15.11.3.2 All pairwise effects m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, adjust = &quot;tukey&quot;, level = 0.95) %&gt;% summary(infer = TRUE) # add the 95% CIs m1_pairs ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## Active WT - Rest WT 3.45 0.592 25 1.820 5.076 5.826 &lt;.0001 ## Rest KO - Rest WT 0.78 0.639 25 -0.978 2.539 1.220 0.6202 ## Rest KO - Active WT -2.67 0.639 25 -4.427 -0.910 -4.173 0.0017 ## Active KO - Rest WT 1.92 0.613 25 0.236 3.606 3.136 0.0212 ## Active KO - Active WT -1.53 0.613 25 -3.212 0.158 -2.493 0.0857 ## Active KO - Rest KO 1.14 0.659 25 -0.671 2.953 1.733 0.3287 ## ## Confidence level used: 0.95 ## Conf-level adjustment: tukey method for comparing a family of 4 estimates ## P value adjustment: tukey method for comparing a family of 4 estimates Notes Note that printing the contrast object displays useful information, including the method of adjustment for multiple tests. If the object is printed using kable() %&gt;% kable_styling() (as in the “Inference” and “Understanding” sections above), only the table is printed and the additional information is lost. The method argument is used to control the set of contrasts that are computed. See below. The adjust argument controls if and how to adjust for multiple tests. Each method has a default adjustment method. See below. The level argument controls the percentile boundaries of the confidence interval. The default is 0.95. Including this argument with this value makes this level transparent. 15.11.3.3 Simple effects m1_simple &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;fdr&quot;) %&gt;% summary(infer = TRUE) # add the 95% CIs genotype stimulation contrast estimate SE df lower.CL upper.CL t.ratio p.value WT . Active - Rest 3.45 0.592 25 1.86 5.04 5.83 0.0000 KO . Active - Rest 1.14 0.659 25 -0.63 2.91 1.73 0.1273 . Rest KO - WT 0.78 0.639 25 -0.94 2.50 1.22 0.2338 . Active KO - WT -1.53 0.613 25 -3.18 0.12 -2.49 0.0393 15.11.3.4 Interaction contrasts The interaction contrasts can be computed as in Planned contrasts above or using the argument “interaction =” m1_ixn &lt;- contrast(m1_emm, interaction = &quot;revpairwise&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) # add the 95% CIs stimulation_revpairwise genotype_revpairwise estimate SE df lower.CL upper.CL t.ratio p.value Active - Rest KO - WT -2.31 0.885 25 -4.13 -0.48 -2.61 0.0152 15.11.3.5 Method argument m1_pairs &lt;- contrast(m1_emm, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) %&gt;% summary(infer = TRUE) # add the 95% CIs m1_pairs %&gt;% kable() %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Rest WT - Active WT -3.4482471 0.5918936 25 -5.1439551 -1.7525392 -5.825789 0.0000269 Rest WT - Rest KO -0.7800781 0.6393182 25 -2.6116520 1.0514958 -1.220172 0.2337850 Rest WT - Active KO -1.9211230 0.6126681 25 -3.6763474 -0.1658985 -3.135667 0.0086970 Active WT - Rest KO 2.6681690 0.6393182 25 0.8365951 4.4997430 4.173460 0.0009510 Active WT - Active KO 1.5271242 0.6126681 25 -0.2281003 3.2823486 2.492580 0.0295018 Rest KO - Active KO -1.1410449 0.6585984 25 -3.0278545 0.7457647 -1.732535 0.1145997 m1_pairs &lt;- contrast(m1_emm, method = &quot;trt.vs.ctrl&quot;, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) # add the 95% CIs m1_pairs %&gt;% kable() %&gt;% kable_styling() contrast estimate SE df lower.CL upper.CL t.ratio p.value Active WT - Rest WT 3.4482471 0.5918936 25 2.2292194 4.667275 5.825789 0.0000045 Rest KO - Rest WT 0.7800781 0.6393182 25 -0.5366223 2.096779 1.220172 0.2337850 Active KO - Rest WT 1.9211230 0.6126681 25 0.6593094 3.182936 3.135667 0.0043485 Notes method = \"pairwise\" and method = \"revpairwise\" compute all pairwise comparisons. I prefer “revpairwise” because the contrasts that include the reference are in the direction non-reference minus reference. method = \"trt.vs.ctrl\" gives a very flattened picture of the model results and constrains what we can infer from the results. 15.11.3.6 Adjustment for multiple tests set the adjust = argument to “none” – no adjustment “dunnettx” – Dunnett’s test is a method used when comparing all treatments to a single control. For a factorial design, this only makes sense for a flattened analysis. “tukey” – Tukey’s HSD method is a method used to compare all pairwise comparisons. “bonferroni” – Bonferroni is a general purpose method to compare any set of multiple tests. The test is conservative. A better method is “holm” “holm” – Holm-Bonferroni is a general purpose method like the Bonferroni but is more powerful. “fdr” – controls the false discovery rate not the Type I error rate for a family of tests. One might use this in an exploratory experiment. “mvt” – based on the multivariate t distribution and using covariance structure of the variables. 15.11.4 Practice safe ANOVA Many researchers are unaware of the different ways of computing the sum of squares in an ANOVA table. The F-value and p-value are both functions of the sum of squares. When the cells of a two-way ANOVA are not balanced (different sample sizes among the cells), the different ways of computing the sum of squares can matter. There is a great deal of sound and fury about which methods should be used and which avoided. For many research questions and the experiments used to address these, these arguments are moot because the ANOVA table is simply unnecessary for inference. Instead, fit the linear model and compute the contrasts of interest. If your reviewer/advisor/boss wants an ANOVA table and wants this table to match that in Graphpad Prism or JMP, then you need a table computed from Type III sum of squares. If using a linear model to compute the ANOVA (remember that ANOVA was developed without fitting linear models), then the linear model needs to be fit using a model matrix constructed with indicator variables using effects coding. There are two safe ways to do this in R. 15.11.4.1 The afex aov_4 function # .I is a data.table function that returns the row number exp2j[, fake_id := paste(&quot;mouse&quot;, .I)] m1_aov4 &lt;- aov_4(glucose_uptake ~ stimulation * genotype + (1|fake_id), data = exp2j) nice(m1_aov4, MSE = FALSE)[,-4] %&gt;% # delete ges column kable() %&gt;% kable_styling() Effect df F p.value stimulation 1, 25 26.86 *** &lt;.0001 genotype 1, 25 0.71 .41 stimulation:genotype 1, 25 6.79 * .02 Notes The afex package has three function names for generating the same ANOVA table and statistics – here I’m using aov_4 because this functions uses a linear model formula argument (specifically, that used in the lme4 package), which is consistent with the rest of this text. The formula includes the addition of a random factor ((1|id)) even though there really is no random factor in this model. See Models with random factors – Blocking and pseudoreplication for more information on random factors. The random factor (the factor variable “id” created in the line before the fit model line) identifies the individual mouse from which the response variable was measured. Because the response was only measured once on each individual mouse, “id” is not really a random factor but the addition of this in the model formula is necessary for the aov_4 function to work. 15.11.4.2 The car Anova function type3 &lt;- list(stimulation = contr.sum, genotype = contr.sum) m1_type3 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j, contrasts = type3) Anova(m1_type3, type=&quot;3&quot;)[-1, -1] %&gt;% # delete row 1 and col 1 kable(digits = c(1,2,5)) %&gt;% kable_styling() Df F value Pr(&gt;F) stimulation 1 26.86 0.00002 genotype 1 0.71 0.40686 stimulation:genotype 1 6.79 0.01523 Residuals 25 Notes car::Anova has arguments for reporting the Type II and Type III sum of squares. Background: The default model matrix in the lm function uses dummy (or treatment) coding. For a Type 3 SS ANOVA (the kind that matches that in Graphpad Prism or JMP), we need to tell lm to use sum (or deviation) coding. The best practice method for changing the contrasts in the model matrix is using the contrasts argument within the lm function, as in the code above to fit m1_type3. This is the safest practice because this sets the contrasts only for this specific fit. The coefficients of m1_type3 will be different from those using the default contrasts. Except when computing a Type III ANOVA, this text uses the default contrasts throughout because the coefficients make sense for the kinds of experiments in experimental biology. If using sum (“type III”) coding, the intercept will be the grand mean and the coefficients of the non-reference levels (the effects) will be their deviations from the grand mean. I don’t find this definition of “effects” very useful for most experiments in biology (but “useful” is largely a function of “used to”). The contrasts (differences in the means among pairs of groups) in the contrast table will be the same, regardless of the contrast coding. Danger!. Many online sites suggest this bit of code before a Type III ANOVA using car::Anova: options(contrasts = c(\"contr.sum\", \"contr.poly\") If you’re reading this book, you almost certainly don’t want to do this because this code resets how R computes coefficients of linear models and SS of ANOVA tables. This will effect all future analysis until the contrasts are set to something else or a new R session is started. 15.11.5 Better to avoid these m1_aov &lt;- aov(glucose_uptake ~ stimulation * genotype, data = exp2j) summary(m1_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## stimulation 1 41.75 41.75 29.796 1.14e-05 *** ## genotype 1 1.28 1.28 0.913 0.3485 ## stimulation:genotype 1 9.51 9.51 6.789 0.0152 * ## Residuals 25 35.03 1.40 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # same as exp2j_m1 in the Example 1 section m1 &lt;- lm(glucose_uptake ~ stimulation * genotype, data = exp2j) anova(m1) ## Analysis of Variance Table ## ## Response: glucose_uptake ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## stimulation 1 41.755 41.755 29.7959 1.143e-05 *** ## genotype 1 1.279 1.279 0.9128 0.34853 ## stimulation:genotype 1 9.514 9.514 6.7890 0.01523 * ## Residuals 25 35.034 1.401 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notes Note the differences from the the afex table and the car Anova table. The p-values for the interaction term is the same – this will always be the case for the highest order interaction term in the model. The p-values for \\(\\texttt{stimulation}\\) and \\(\\texttt{genotype}\\) terms are different. The difference will be a function of the degree of imbalance. Here, this difference doesn’t make a difference for inference. In some data sets, the difference is catastrophic. Many introduction to statistics textbooks and websites teach the base R aov function. This function is only useful if 1) the data are balanced and 2) we care about ANOVA. Don’t use this. If you need the ANOVA table use either afex or car Anova. The base R anova is useful if you know what you are doing with it. Otherwise, use either afex or car Anova. 15.12 Hidden Code 15.12.1 Import exp2j (Example 1) data_from &lt;- &quot;TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise&quot; file_name &lt;- &quot;41586_2020_1992_MOESM4_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) treatment_levels &lt;- c(&quot;WT Rest&quot;, &quot;WT Active&quot;, &quot;KO Rest&quot;, &quot;KO Active&quot;) exp2j_wide &lt;- read_excel(file_path, sheet = &quot;2j&quot;, range = &quot;A5:D13&quot;, col_names = TRUE) %&gt;% data.table() colnames(exp2j_wide) &lt;- treatment_levels exp2j &lt;- melt(exp2j_wide, measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_uptake&quot;) %&gt;% na.omit() # danger! exp2j[, c(&quot;genotype&quot;, &quot;stimulation&quot;) := tstrsplit(treatment, &quot; &quot;, fixed = TRUE)] genotype_levels &lt;- c(&quot;WT&quot;, &quot;KO&quot;) stimulation_levels &lt;- c(&quot;Rest&quot;, &quot;Active&quot;) exp2j[, genotype := factor(genotype, levels = genotype_levels)] exp2j[, stimulation := factor(stimulation, levels = stimulation_levels)] # View(exp2j) 15.12.2 Import exp3e lesian area data (Example 2) data_from &lt;- &quot;XX sex chromosome complement promotes atherosclerosis in mice&quot; file_name &lt;- &quot;41467_2019_10462_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # fig 3e exp3e_wide &lt;- read_excel(file_path, sheet = &quot;Figure 3E&quot;, range = &quot;A3:K4&quot;, col_names = FALSE) %&gt;% data.table() %&gt;% transpose(make.names = 1) sex_levels &lt;- colnames(exp3e_wide) exp3e &lt;- melt(exp3e_wide, measure.vars = sex_levels, variable.name = &quot;sex&quot;, value.name = &quot;lesian_area&quot;) # convert lesian_area to mm^2 from µm^2 exp3e[, lesian_area := lesian_area/10^6] # convert sex variable to factor exp3e[, sex := factor(sex, levels = sex_levels)] # create chromosome column and convert to factor chromosome_levels &lt;- c(&quot;XX&quot;, &quot;XY&quot;) exp3e[, chromosome := rep(rep(chromosome_levels, each = 5), 2)] exp3e[, chromosome := factor(chromosome, levels = chromosome_levels)] # researchers treatment levels exp3e[, treatment := rep(c(&quot;FXX&quot;, &quot;FXY&quot;, &quot;MXX&quot;, &quot;MXY&quot;), each = 5)] # two rows with missing response so delte these rows as there is # no useful information in them exp3e &lt;- na.omit(exp3e) # be careful with a global na.omit # View(exp3e) # highlight and run to see data 15.12.3 Import Exp1c JA data (Example 3) data_from &lt;- &quot;Integration of two herbivore-induced plant volatiles results in synergistic effects on plant defense and resistance&quot; file_name &lt;- &quot;Data for Dryad.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp1 &lt;- read_excel(file_path, sheet = &quot;Fig. 1&quot;, range = &quot;A3:K23&quot;, # 1 blank column col_names = TRUE) %&gt;% data.table() %&gt;% clean_names() setnames(exp1, old = &quot;treat&quot;, new = &quot;treatment&quot;) exp1[treatment == &quot;Control&quot;, c(&quot;hac&quot;, &quot;indole&quot;) := list(&quot;HAC-&quot;, &quot;Indole-&quot;)] exp1[treatment == &quot;HAC&quot;, c(&quot;hac&quot;, &quot;indole&quot;) := list(&quot;HAC+&quot;, &quot;Indole-&quot;)] exp1[treatment == &quot;Indole&quot;, c(&quot;hac&quot;, &quot;indole&quot;) := list(&quot;HAC-&quot;, &quot;Indole+&quot;)] exp1[treatment == &quot;HAC+Indole&quot;, c(&quot;hac&quot;, &quot;indole&quot;) := list(&quot;HAC+&quot;, &quot;Indole+&quot;)] treatment_levels &lt;- c(&quot;Control&quot;, &quot;HAC&quot;, &quot;Indole&quot;, &quot;HAC+Indole&quot;) exp1[, treatment := factor(treatment, levels = treatment_levels)] exp1[, hac := factor(hac)] # levels in correct order exp1[, indole := factor(indole)] # levels in correct order # View(exp1) "],["part-vi-expanding-the-linear-model.html", "Part VI – Expanding the Linear Model", " Part VI – Expanding the Linear Model "],["lmm.html", "Chapter 16 Models for non-independence – linear mixed models 16.1 Liberal inference from pseudoreplication 16.2 Conservative inference from failure to identify blocks 16.3 Introduction to models for non-independent data (linear mixed models) 16.4 Experimental designs in experimental bench biology 16.5 Building the linear (mixed) model for clustered data 16.6 Example 1 – A random intercepts and slopes explainer (demo1) 16.7 Example 2 – experiments without subsampling replication (exp6g) 16.8 Example 3 – Factorial experiments and no subsampling replicates (exp5c) 16.9 Example 4 – Experiments with subsampling replication (exp1g) 16.10 Statistical models for experimental designs 16.11 Which model and why? 16.12 Working in R 16.13 Hidden code", " Chapter 16 Models for non-independence – linear mixed models Probably no chapter in this book is more important for the best-practice analysis of experimental data than this chapter. Why? Because many if not most experimental data violates the assumption of independence and any analysis using standard t-tests and ANOVA will always lead to quantitative error in inference (confidence intervals and p-values) and often lead to qualitative errors in inference (statements about “significance”). Standard analysis of non-independent data can lead to absurdly liberal inference (the p-values are far lower than the data support), but can also lead to moderately conservative inference (the p-values are higher than the data supports). Liberal inference generates false discovery and lures researchers down dead-end research pathways. Conservative inference steers researchers away from true discovery. Linear Mixed Models are an extension of linear models that appropriately adjust inferential statistics for non-independent data. Paired t-tests and Repeated Measures ANOVA are classical tests that are special cases of linear mixed models. Linear mixed models are more flexible than these classical tests because the models can include added covariates or more complex models generally. And, linear mixed models can be extended to Generalized Linear Mixed Models for counts, binary responses, skewed responses, and ratios. Before introducing experimental designs that generate non-independent data and the models used to analyze these, let’s explore two ubiquitous examples, one that leads to liberal inference and one that leads to conservative inference. 16.1 Liberal inference from pseudoreplication Researchers are interested in regulation and repair of DNA double-stranded breaks and use a proximity ligation assay (PLA) of HeLa cells to investigate the number of damage response events (“foci”) per cell with and without an inhibitor of transcription elongation (DRB). The number of foci in each of fifty cells per treatment is measured. The experiment is replicated three times. The researchers use a t-test to compare the effect of DRB on foci count and naively include all measures in the analysis. What is naive about the analysis? The fifty measures per cell are technical replicates and the values within a cell are not independent of each other because they share aspects of cell environment not shared by values in other cells. Including technical replicates in an analysis without accounting for this non-independence is a kind of pseudoreplication To show how this naive analysis results in extremely liberal inference and an increase in false discovery, I simulate this experiment using a case in which there is no effect of DRB treatment, so a low p-value indicates a false-discovery. The simulation is simplified with the two following conditions: 1) the model pretends that count data are normally distributed (this is because we want to focus on pseudoreplication and not a misspecified distribution) and 2) the model pretends that values from each treatment within an experiment are independent (this is because we want to focus on pseudoreplication). In this naive analysis of the experiment, the researcher finds an effect of treatment with a p-value of 0.000073 and uses this small p-value to justify a decision to move forward with follow-up experiments. But this low p-value is not supported by the data – this discovery is false. This very small p-value is not an example of a “rare event”. In fact, if the researcher repeats the experiment 1000 times, then the median p-value is 0.000195 and 72.3% of the 1000 p-values lead to the same false discovery if 0.05 is used to make the decision to move forward. While this example is fake, I see this naive analysis a lot: tumor area of multiple tumors per mouse, islet area of multiple islets per mouse, number of vesicles docked to a membrane in multiple cells per mouse, the number of mitochondria in multiple cells per mouse, the number of neurites in multiple neurons in multiple cells per mouse, etc. etc. Indeed, when I’m looking for examples of pseudoreplication to teach, I just look for figures with a bunch of points per treatment – something similar to this plot of the fake data experiment. Regardless, this is a huge source of false discovery that could disappear overnight. 16.2 Conservative inference from failure to identify blocks Researchers frequently divide a litter among treatment levels (“littermate control”) but only once have I seen researchers use this design to increase statistical power and decrease the rate of failed discovery. Here is a fake data example. Researchers are interested in sensory regulation of wound healing and use the Nav1.8cre/Rosa26DTA mouse model to investigate the role of dorsal root ganglion (the location of peripheral sensory neuron cell bodies) in this management. Two mice from each of six litters are sampled, one Nav1.8cre sib with an ablated dorsal root ganglion and one Rosa26DTA “littermate control” with an intact dorsal root ganglion. The researchers use a naive t-test to compare the effect of ablation on muscle area (12 days after wounding). What is naive about the analysis? Each mouse is a block and the values within a block are not independent of each other because they share aspects of genetics and maternal environment not shared by values in other blocks (mice). In general, variation among blocks (mice) adds correlated noise to data and failure to account for blocks will often lead to more conservative inference. To show how this naive analysis results in conservative inference and an increase in failed discovery, I simulate this experiment using a case in which there is an effect of ablation treatment, so a low p-value indicates a true discovery. In this naive analysis of the experiment, the p-value of the treatment is 0.26 and uses this large p-value to put the grad student onto a different project. Too bad, because the effect is real. The p-value from a paired t-test (the values of the two mice within a litter are a pair), which is equivalent to a special case of a linear mixed model, is 0.0012. This small p-value using a model that accounts for the non-independence is not an example of a “rare event”. In fact, if the researcher repeats the experiment 1000 times, 72.5% of the paired t-test p-values are less than 0.05 while only 12.1% of the classical t-test p-values are less than 0.05. While this example is fake, I see this naive analysis a lot – littermate controls is very, very common but other examples are too, included replicated experiments (each experiment is a block). There are certain instances where researchers do recognize non-independence and do use a paired t-test but the vast majority of blocked designs (occurring in almost all experimental biology papers) go unrecognized. This is a huge source of failed discovery that could disappear overnight. 16.3 Introduction to models for non-independent data (linear mixed models) This chapter is about models for correlated error, including linear models with added random factors, which are known as linear mixed models. In classical hypothesis testing, a paired t-test, repeated measures ANOVA, and mixed-effect ANOVA are equivalent to specific cases of linear mixed models. Linear mixed models are used for analyzing data composed of subsets – or batches – of data that were measured from the “same thing”, such as multiple measures within a mouse, or multiple mice within a litter. Batched data results in correlated error, which violates a key assumption of linear models (and their “which test” equivalents) and muddles statistical inference unless the correlated error is modeled, explicitly or implicitly. In some experimental designs (blocked designs), failure to model the correlated error reduces precision and power, contributing to reduced rates of discovery or confirmation. In other designs (nested designs), failure to model the correlated error results in falsely high precision and low p-values, leading to increased rates of false discovery. The falsely high precision is due to pseudoreplication. I think it’s fair to infer from the experimental biology literature, that experimental biologists don’t recognize the ubiquitousness of batched data and correlated error. This is probably the biggest issue in inference in the field (far more of an issue than say, a t-test on non-normal data). What do I mean by “batch” and how can correlated error both increase and decrease false discovery? Consider an experiment to measure pancreatic islet area in response to two experimental factors: \\(\\texttt{genotype}\\) (WT, KO) and \\(\\texttt{treatment}\\) (presence/absence of some drug believed to be an agonist of the knocked out protein). While it may seem like the data from this experiment should be analyzed using the ANOVA option in GraphPad Prism (or, as advocated in this book, a general linear model that is equivalent to the ANOVA), the best practice statistical model actually depends on the experimental design. Experimental design matters because different designs introduce different patterns of correlated error due to shared genetics and environment. Recall that inference from a linear model (including t-tests and ANOVA) assumes independence (Chapter xxx) – that is, each response value has no relationship to any other value, other than that due to treatment. Lack of independence results in patterns of correlation among the residuals, or correlated error. Something like the first experiment below (Design 1) is the necessary design to use the statistics that have been covered in this book to this point, without extreme violation of the independence assumption. But many (most?) experiments in experimental bench biology do not look like the design in Design 1 below. Instead, many (most?) experiments are variants of Designs 2-4, all of which have extreme violations of the independence assumption. Interestingly, some of these violations result in conservative statistics and reduced, true discovery rate while others result in liberal statistics and increased, false discovery rate. Design 1. The design in Figure @ref(fig:lmm-biological-replicates_1) is a factorial design with two factors, \\(\\texttt{genotype}\\) and \\(\\texttt{treatment}\\), each with two levels. Twenty mice of the same sex, each from a different litter from a unique dam and sire mating, are randomly sampled and assigned to one \\(genotype \\times treatment\\) combination (five mice per combination). All mice are housed individually (20 cages). The pancreatic tissue from all mice is prepared in a single batch and the area of a single islet is measured from each mouse. The entire experiment is carried out at the same time and each component (tissue preparation, measuring) is carried out by the same person (these could be different people for each component). This is a Completely Randomized Design (CRD). The five replicate mice per treatment are treatment replicates (often called biological replicates in experimental biology. A CRD does not have batched data. Design 2. In the design in Figure ??, four littermates are randomly sampled from five litters, each with a different dam and sire. Within each litter, mice are randomly assigned to each of the four treatment combinations (one per combination). Each litter is randomly assigned to cage with only a single litter per cage. All other aspects of this design are as in Design 1. This is a Randomized Complete Block Design. The five replicate mice per treatment are the treatment replicates. Each litter/cage combination is a type of batch called a block. A blocked design typically functions to reduce noise in the model fit (this increases power) and to reduce the number of litters and cages needed for an experiment. The four measures of Islet Area within a litter/cage (one per mouse) are not independent of each other. Each cage has four mice from the same litter and these mice share genetic and maternal factors that contribute to mouse anatomy and physiology that are not shared by mice in other litters. Additionally, each cage has a unique set of environmental factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff, and behavioral interactions among the mice. All response measures within a litter/cage share the component of the error variance unique to that litter/cage and, as a consequence, the error (residuals) within a litter/cage are more similar to each other than they are to the residuals among litters/cages. Design 3. The design in Figure ?? is exactly like that in Design 2, except that the researchers take three measures of iselet area per mouse. The three measures are subsampled replicates. Experimental biologists often call these technical replicates, especially when the multiple measures are taken from the same preparation. Subsampling is a kind of nested design in which one variable is nested within (as opposed to crossed with) another variable. Here, the subsampled variable (subsample_id) is nested within the mouse_id variable. In addition to each litter/cage being a batch, each mouse is a batch. Each mouse has a unique set of factors that contribute to the error variance of the measures of the response in that mouse. All response measures within a mouse share the component of the error variance unique to that mouse and, as a consequence, the error (residuals) within a mouse are more similar to each other than they are to the residuals between mice Design 4. The design in Figure ?? is a variation of Design 2, but the five treatment replicates of each combination are housed together in the same cage. In this design, each litter is a batch and each cage is a batch but these are different batches, unlike Design 2. In each of these experiments, there is systematic variation at multiple levels: among treatments due to treatment effects and among batches due to batch effects. Batches come in lots of flavors, including experiment, litter, cage, flask, plate, slide, donor, and individual. The among-treatment differences in means are the fixed effects. The among-batch differences are the random effect. An assumption of modeling random effects is that the batches are a random sample of the batches that could have been sampled. This is often not strictly true as batches are often convenience samples (example: the human donors of the Type 2 diabetes beta cells are those that were in the hospital). The variation among batches/lack of independence within batches has different consequences on the uncertainty of the estimate of a treatment effect. The batches in Experiment 1 contain all treatment combinations. The researcher is interested in the treatment effect but not the variation due to differences among the batches. The batches are nuissance factors that add additional variance to the response, with the consequence that estimates of treatment effects are less precise, unless the variance due to the batches is explicitly modeled. Modeling a batch that contains some or all treatment combinations will increase precision and power. Batches that contain at least two treatment combinations are known as blocks. A block that contains all treatment combinations is a complete block. A block that contains fewer than all combinations is an incomplete block. Including block structure in the design is known as blocking. Blocks are non-experimental factors. Adding a blocking factor to a statistical model is used to increase the precision of an estimated treatment effect. Design 2 is an example of a randomized complete block design. In Design 3, there are multiple measures per mouse and the design is a randomized complete block design with subsampling. The subsampling is not the kind of replication that can be used to infer the among treatment effect because the treatment assignment was not at the level of the subsamples. The treatment replicates are the litters/cages, because it was at this level that treatment assignment was randomized. A statistical analysis of all measures from a subsampled design without modeling the correlated error due to the subsampling is a kind of pseudoreplication. Pseudoreplication results in falsely precise standard errors and false small p-values and, consequently, increased rates of false discovery. In Design 4, the treatment is randomized to batch, so each batch contains only a single treatment level. In these segregated experimental designs, the variation among batches that arises from non-treatment related differences among batches confounds the variation among batches due to a true treatment effect. Design 4 is an extreme example of this – there is only a single cage with a specific treatment combination. Imagine 1) the true effect of a treatment combination is zero and 2) an aggressive mouse in the control cage stimulates the stress response in the other mice and this stress response has a large effect on the value of the response variable measured by the researchers. The researcher is fooled into thinking that the treatment caused the difference in the response. In all of these designs, it is important for the researcher to identify the experimental unit and the measurement unit. The experimental unit is the entity that was randomly assigned the treatment. In designs 1 – 3, the experimental unit is the mouse. In experiment 4, the experimental unit is the cage. The measurement unit is the entity that was measured. In designs 1, 2, and 4, the measurement unit is the mouse. In design 3, the measurement unit is specific islet that was measured. Pseudoreplication. In pseudoreplication, the degrees of freedom used to compute the test statistic and the p-value are inflated given the experimental design and research question. An example: A researcher wants to investigate the effect of some protein on mitochondrial biogenesis and designs an experiment with a wildtype (WT) and a conditional knockout (KO) mouse. Mitochondrial counts from twenty cells in one WT mouse and one KO mouse are measured and the researcher uses a t-test to compare counts. The sample size used to compute the standard error in the denominator of the t-value is 20. The t-distribution used to compute the p-value uses 38 df (20 measures times two groups minus two estimated parameters). This is wrong. The df are inflated and the estimate of the standard error of the difference (the denominator of the t-value) is falsely small. The correct sample size for this design is 1 and the correct df is zero. The sample size and df are inflated for this design because the treatment was randomized to mouse and not to cell. Mouse is the experimental unit – the number of experimental units is what gives the degrees of freedom. The df are correct for inference about the two individuals (how compatible are the data and a model of sampling from the same individual?), but not for inference about the effect of genotype. We cannot infer anything about genotype with a sample size of 1, even with 20 measures per mouse, because any effect of treatment is completely confounded with other differences between the two mice. 16.4 Experimental designs in experimental bench biology Given the basic principles above, let’s consider the kinds of experimental designs seen in experimental bench biology (Fig. ??). Figure 16.1: Experimental designs in experimental bench biology. Images created with BioRender.com Figure 16.2: Experimental designs in experimental bench biology. Images created with BioRender.com 16.4.1 Notation for models i = 1..t (treatments) j = 1..b (blocks) k = 1..r (experimental replications within a block or within a CRD with no block structure) m = 1..s (subsamples or technical replicates) 16.4.2 Completely Randomized Design (CRD) The Completely Randomized Design experiment in Figure ??A has a single factor, \\(\\texttt{treatment}\\) with two levels (“Cn” and “Tr”). Five mice are randomly assigned to each treatment level. Each mouse is bred from a different litter and housed in a separate cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the treatment (biological) replicates. The design is completely randomized because there is no subgrouping due to batches. What kinds of subgrouping does this design avoid? By using a single mouse per litter, there are no litter batches and subsets of mice don’t share litter effects – common litter responses to the Cn or Tr treatments. Each litter has a unique set of factors that contribute to the error variance of the measure of the response. Siblings from the same dam and sire share more genetic variation than non-siblings and this shared genetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. Siblings from the same litter share the same history of maternal factors (maternal effects, including epigenetic effects) specific to the pregnancy and even the history of events leading up to the pregnancy. This shared non-genetic and epigenetic variation contributes to phenotypes (including the response to treatment) that are more likely to be similar to each other than to non-siblings. All response measures within a litter share the genetic, maternal environmental, and epigenetic components of the error variance unique to that litter and, as a consequence, the error (residuals) within a litter are more similar to each other than they are to the residuals between litters. By housing each mouse in it’s own cage, there is no cage batch and subsets of mice don’t share cage effects – common cage responses to the Cn or Tr treatments. As stated earlier, each cage has a unique set of factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff and behavioral interactions among the mice. All response measures within a cage share the component of the error variance unique to that cage and, as a consequence, the error (residuals) within a cage are more similar to each other than they are to the residuals between cages. Examples: Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. A single measure per mouse is taken. Mouse is the experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=1\\). Ten cell cultures are created. Five cultures are randomly assigned to control and five to treatment. A single measure per culture is taken. Culture is the experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=1\\). div style=“background-color:#cccccc; text-align:left; vertical-align: middle; padding:20px 47px;”&gt; While most data from experiments in bench biology are analyzed as if the experimental design is a CRD, a good question is, what fraction of these actually are CRD? We know that many (most?) mouse experiments will have batched measures and correlated responses because most experiments are conducted with multiple mice per litter and/or cage (or equivalents in other model systems). Many cell culture data come from multiple replicates of the whole experiment – the experiment functions as a block. And time series experiments include multiple measures on the same experimental unit over time. Except for time series experiments, researchers in experimental bench biology are using almost exclusively statistical tests that assume independence of errors (the tests appropriate for CRDs). How this mismatch between experimental design and statistical practice affects the rate of false and true discovery in cell and molecular biology is entirely unknown. 16.4.3 Completely Randomized Design with Subsampling (CRDS) or “Nested Design” The Completely Randomized Design with Subsampling experiment in Fig. ??B is exactly like the CRD except that the researchers measure multiple values of the response variable from each mouse under the same condition (that is, not different in treatment or time). The multiple measures are subsampled (technical) replicates. Do not confuse subsampled replicates with measures of the response under different conditions in the same mouse, for example a measure from one brain slice under the control treatment and a measure from a second brain slice under the drug treatment. This example is a kind of Randomized Complete Block Design, which is outlined next and the core design in this chapter. Do not confuse subsampled replicates with a measures of the response at different times in the same mouse, for example, the plasma glucose levels at baseline and at five post-baseline time points. This example is a kind of longitudinal design, which is outlined below and more thoroughly in the chapter Linear models for longitudinal experiments. Do not confuse subsampled replicates with measures of different response variables from the same mouse, for example measures of the weights of five different skeletal muscles. This example is a kind of multiple response which is addressed xxx. The technical replicates are a kind of pseudoreplication. The general linear model y ~ treatment fit to these data, including t-tests and traditional ANOVA, will have falsely high precision and falsely low p-values. Examples: Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Multiple measures per mouse are taken. Example: five measures of Islet Area are measured in each pancreas. Mouse is the experimental unit. Each of the measures is a technical replicate because the treatment is not randomly assigned to each islet but to the whole mouse. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=5\\). Ten cell cultures are created. Five cultures are randomly assigned to control and five to treatment. Multiple measures per culture are taken. Example: Mitochondrial counts are measured from five cells in each culture. Culture is the experimental unit. Each of the counts is a technical replicate because the treatment is not randomly assigned to each cell but to the whole culture. \\(t=2\\), \\(b=0\\), \\(r=5\\), and \\(s=5\\). Notes: Subsampling can occur at multiple levels. Example: Ten mice from separate litters are sampled. Five mice are randomly assigned to control. Five mice are randomly assigned to treatment. Five neurons in a slice of brain tissue are identified in each mouse. From each neuron, the length of five dendrite spines are measured. The five measures of spine length are “nested within” neuron and the five neurons are “nested within” mouse. Nested subsampling can quickly lead to massive pseudoreplication and false discovery. 16.4.4 Randomized Complete Block Design (RCBD) The Randomized Complete Block Design experiment in Figure ??C is similar to the CRD except that all treatment combinations (two here) are randomly assigned to sibling mice within a litter. Here, two mice from each litter are randomly selected and one is randomly assigned to “Cn” and the other to “Tr”. Each litter is randomly assigned to a unique cage. The researchers measure a single value of the response variable from each mouse. The five replicate mice per treatment are the treatment replicates. The litters (or cage) are the blocks. In this design, litter and cage effects are confounded but this has no consequence on the statistical model and inference unless the researchers want to explicitly estimate these effects separately. Compared to the CRD, this design requires fewer resources (five litters instead of ten, five cages instead of ten). Compared to the general linear model fit to data from the CRD (y ~ treatment), including t-tests and traditional ANOVA, the linear mixed model fit to the RCBD has increased precision and power. While many researchers seem to be designing experiments similar to this (“littermate controls”), most are failing to fit a statistical model that accounts for the batching and taking advantage of the increased precision and power. Examples: Ten mice are sampled. In each mouse, one forelimb is assigned to control and the other forelimb is assigned to treatment. Only a single measure on each side is taken. Limb is the experimental unit. Mouse is a block. \\(t=2\\), \\(b=10\\), \\(r=1\\), and \\(s=1\\). Ten litters are sampled. In each litter, one sib is assigned to control and the other sib is assigned to treatment. Only a single measure on each sib is taken. Mouse is the experimental unit. Litter is a block. \\(t=2\\), \\(b=10\\), \\(r=1\\), and \\(s=1\\). Two mice, each from a separate litter are sampled. One is randomly assigned to control and the other to treatment. Only a single measure of the response variable is taken per mouse. The experiment is replicated five times (five different days, each with a newly made set of reagents and machine calibrations). Mouse is the experimental unit. Experiment is a block. \\(t=2\\), \\(b=5\\), \\(r=1\\), and \\(s=1\\). 16.4.5 Randomized Split Plot Design (RSPD) The Randomized Split Plot Design experiment in Figure ??D is similar to the RCBD except that there is now a second experimental factor that is crossed with the first experimental factor. An individual mouse acts as single experimental unit for one factor (here, \\(\\texttt{genotype}\\) with levels “WT” and “KO”) but acts as a block for the second experimental factor (here, \\(\\texttt{treatment}\\) with levels “Cn” and “Tr”). The first factor (\\(\\texttt{genotype}\\)) is the main plot – the levels of the factor are randomly assigned to the main plots. The second factor (\\(\\texttt{treatment}\\)) is the subplot – the levels of this factor are randomly assigned to the subplots. \\(\\texttt{Litter}\\) is a replicated block. Also in Figure ??D, a 2 x 2 RCBD with the same two experimental factors is shown for comparison. In the 2 x 2 RCBD, four mice per block (litter) are each randomly assigned one of the 2 x 2 combinations of \\(\\texttt{genotype}\\) and \\(\\texttt{treatment}\\)). 16.4.6 Generalized Randomized Complete Block Design (GRCBD) The Generalized Randomized Complete Block Design experiment in Figure ??E is similar to the RCBD except that two treatment replicates per block (litter/cage) are assigned. Important and somewhat not intuitive Because the treatment replicates within a litter share common error variance, these do not act like independent replicates. One consequence of this is, the sample size (\\(n\\)) is five and not ten (\\(litters \\times treatment replicates\\)). The general linear model y ~ treatment fit to these data, including t-tests and traditional ANOVA, will generally have falsely high precision and falsely low p-values. 16.4.7 Nested Randomized Complete Block Design (NRCBD) The Nested Randomized Complete Block Design experiment in Figure ??F is similar to the RCBD except that there are now replicated experiments. This is a nested block design with litter (a block) nested within experiment (a block). 16.4.8 Longitudinal Randomized Complete Block Design (LRCBD) The Longitudinal Randomized Complete Block Design experiment in Figure ??G is similar to the RCBD except that there are multiple measures of the response variable taken, each taken at a different time point, including baseline (time zero). 16.4.9 Variations due to multiple measures of the response variable Similar to the CRDS above, the other basic designs can include subsampling, resulting in, for example, RCBDS or RSPDS. If there is subsampling within subsampled units, then we can designated these with “SS”, for example RCBDSS. Similar to the LRCBD above, the other basic designs can include longitudinal sampling, resulting in, for example, LCRD or LRSPD. 16.5 Building the linear (mixed) model for clustered data Notation i = 1..t (treatments) j = 1..b (blocks) k = 1..r (experimental replications within a block) m = 1..s (subsamples or technical replicates) \\[y_i = \\beta_0 + \\beta_i treatment_i + (\\gamma_j block_j) + (\\gamma_{ij} block_j treatment_i) + \\varepsilon\\] 16.6 Example 1 – A random intercepts and slopes explainer (demo1) To introduce linear mixed models, I’m using data from Experiment 1g below. The design is \\(2 \\times 2\\) factorial with 4-5 mice per treatment combination. To simplify the explanation of random intercepts and random slopes in linear models with added random factors (linear mixed models), I flatten the analysis to a single treatment factor (\\(\\texttt{treatment}\\)) with four levels (“Control”, “Tr1”, “Tr2”, “Tr3). The response is percent germinal centers (\\(\\texttt{gc}\\)) in secondary lymphoid tissue. The experiment was replicated 4 times. Each replication is a batch. This batch information is in the variable \\(\\texttt{experiment_id}\\). Further detail isn’t necessary at this point. Figure 16.3A is a response plot of the linear model lm(gc ~ treatment) fit to the whole data set, ignoring the fact that the data were collected in batches. This is the complete pooling fit. Figure 16.3B is a response plot of the linear model lm(gc ~ treatment) fit to the means of each treatment combination from each experiment. This is the means pooling fit. Figure 16.3: A. Response plot of the linear model gc ~ treatment fit to all exp1g data. B. Response plot of the linear model gc ~ treatment fit to the experiment means of the exp1g data. 16.6.1 Batched measurements result in clustered residuals Figure 16.4A is a plot of the residuals of the complete-pooling fit against \\(\\texttt{experiment_id}\\). The residuals are clustered by experiment. All residuals from experiment 1 are positive. All residuals from experiment 2 are negative. Residuals from experiment 3 are generally positive. Residuals from experiment 4 seem pretty random. This clustering by experiment is the same in the plot of the residuals of the means-pooling fit against \\(\\texttt{experiment_id}\\) (Figure 16.4B). The residuals are not independent in either fit. If you asked me to guess the sign of a residual and gave me the information that the measure was from experiment 1, I’d be correct 100% of the time. If the residuals were independent, I’d be correct, on average, 50% of the time. Independent residuals are randomly scattered about zero for within each experiment (Figure 16.4C). Figure 16.4: A. Residuals of the model fit to all demo1 data. B. Residuals of the model fit to the mean demo1 data. 16.6.2 Clustered residuals result in correlated error An assumption of inference from a linear model is independence – each response is independently drawn from a distribution of random values. In Experiment 1g, the experiments are batches and the batched data results in correlated error unless modeled. One way to see this correlated error is to use the residuals from the means-pooled fit. aggregate the data by computing the means for each treatment level within each experiment. fit the fixed effect model (the model without added random factors) to the aggregated data compute the residuals from the model cast (or spread) the residuals for each treatment into its own column. This creates a 4 rows (experiment) \\(\\times\\) 4 columns (treatments) matrix of residuals. Compute the correlations among the four treatment combination columns. This is the correlated error due to the batch effect of \\(\\texttt{experiment_id}\\). Table 16.1: Residuals of fixed affect model fit to aggregated data. The residuals are split into each treatment. experiment_id Control Tr1 Tr2 Tr3 exp_1 2.578125 1.64075 2.2425 3.027125 exp_2 -2.229375 -2.91625 -5.7475 -4.435375 exp_3 -0.615375 1.09975 3.4625 3.697125 exp_4 0.266625 0.17575 0.0425 -2.288875 I’ve used GGally::ggpairs to compute and display the correlations as a matrix. The lower triangle of matrix elements contains the scatterplot of the residuals for the treatment combination defined by the row and column headers. The upper triangle of elements contains the Pearson correlation. With only four experiment residuals per treatment combination, large correlations will be common. But all correlations are large, positive values. The asterisks indicate values that would be an unexpected surprise under a null model of no correlation. We could explicitly model correlated error with a linear model for correlated error using the nlme::gls function, using a model for the correlated error that matches our knowledge of how the data were generated (from experiment batches). In this chapter, we implicitly model the correlated error using a linear model with added random factors – a linear mixed model. What we explicitly model in a linear mixed model is hierarchical levels of variance. 16.6.3 In blocked designs, clustered residuals adds a variance component that masks treatment effects The variance among the experiments within a treatment is much greater than the variance among the treatment means. A consequence of this is, the experiment effect masks the effect of treatment. We can manually unmask this by compute the experiment means across all treatment combinations. create a gc variable without variation among experiment means (“adjusted for experiment_id”). Figure 16.5: Adjusting for variance among experiments. The black, dashed line is the grand-mean response. In the left panel, the colored, dashed lines are the mean gc for each experiment, ignoring treatment. In the right panel, the individual values have been shifted (adjusted) by centering the experiment means. This has the effect of reducing the error variance – the spread of the values around the treatment means (large black dots). In Figure 16.5, the black dots are the modeled means of each treatment combination. The small colored dots are the measured values of the response for each \\(\\texttt{experiment_id}\\) in the left panel and the experiment-adjusted values in the right panel. The black, dashed line is the grand-mean response. The colored, dashed lines are the means of all responses in each experiment. These means are equal in the right panel (and covered by the black line) because the variation among the means has been adjusted away. What is left is error variation uncontaminated by \\(\\texttt{experiment_id}\\). In Experiment 1g, \\(\\texttt{experiment_id}\\) is a nuissance variable – it adds to the noise. In the exercise above, the effects of the treatment variables are adjusted for the elevation of batch effects on the overal batch mean. Linear mixed models are more sophisticated than this. In a linear mixed model, the effects of the treatment variables are adjusted for the elevations of batch effects on the intercept and batch effects on the slopes (or some combination of these). These are the random intercepts and random slopes. 16.6.4 Linear mixed models are linear models with added random factors A linear model adds some combination of random intercepts and random slopes to a linear model. \\[ \\begin{equation} \\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon \\tag{16.1} \\end{equation} \\] A random intercept for experiment j is the sum of the fixed intercept (\\(\\beta_0\\)) and a random intercept effect (\\(\\gamma_{0j}\\)). I’ve embedded these within parentheses to show how these combine into the random intercept. A random slope for batch j is the sum of the fixed slope (\\(\\beta_k\\)) for the non-reference level \\(k\\) and a random slope effect (\\(\\gamma_{kj}\\)). I’ve embedded these within parentheses to show how these combine into the random slopes. There is a different \\(\\gamma_{0j}\\) for each experiment. There is a different \\(\\gamma_{kj}\\) for each combination of non-reference level and experiment. The \\(\\gamma_0j\\) and \\(\\gamma_kj\\) are modeled as if the values for each experiment is a random draw from an infinite number of experiments. This is why \\(\\gamma_{0j}\\) and \\(\\gamma_{kj}\\) are random effects. In contrast, \\(\\beta_0\\) and the three \\(\\beta_k\\) for the non-reference treatment levels are the same for all experiments – this is why \\(\\beta_0\\) is known as fixed effects (technically, \\(\\beta_0\\) is a mean and not an effect). Model (16.1) is fit to the Example 1 data using lme4::lmer() demo1_m1 &lt;- lmer(gc ~ treatment + (treatment | experiment_id), data = demo1) Notes (treatment | experiment_id) specifies a random intercept for all levels of \\(\\texttt{experiment_id}\\) and a random slope for all combinations the levels of \\(\\texttt{experiment_id}\\) and the non-reference levels of \\(\\texttt{treatment}\\) 16.6.5 What the random effects are Random intercepts model batch effects in the reference treatment level. 16.6A illustrates random intercepts and random intercept effects. The large, colored dots are the modeled means of each experiment for each treatment combination. For the reference treatment level (“Control”), each mean is the sum of the estimated fixed intercept (\\(b_0\\)), shown by the dashed gray line, and the estimated random intercept effect (\\(g_{0j}\\)) for experiment j. The random intercept effects are the vertical, colored lines. Random slopes model the effect of treatment on batch effects in the non-reference treatment levels. 16.6B illustrates random slopes and random slope effects, focusing on the slopes for the 2nd non-reference treatment level (“Tr2”). The angled black line is the estimated fixed slope \\(b_2\\) for this level. The colored lines are the random slopes for each experiment. The pale, gray dots are where the modeled means at the Tr2 level would be if there were no random slope effect – as if we took the large colored dots at “Control” and rigidly shifted them up the black line to “Tr2”. The estimated random slope effects \\(\\mathrm{g}_{2j}\\) are the difference between these large, gray dots and the modeled means. Figure 16.6: What random intercepts and slopes are. (A) A random intercept for batch \\(j\\) is the difference between the fixed intercept and the modeled mean for batch \\(j\\) in the reference treatment level. The random intercepts for experiments 1 (\\(\\mathrm{g}_{0.1}\\)) and 2 (\\(\\mathrm{g}_{0.2}\\)) are shown with brackets. (B) The fixed slope for the “Tr2” is illustrated with a bracket. The large, grey dots are the expected values for each batch (experiment_id) in the “Tr2” treatment if the random slope effects are zero. A random slope for batch \\(j\\) is the difference between the expected value for batch j and the modeled mean for batch \\(j\\). The random slope for experiment 2 in “Tr2” (\\(\\mathrm{g}_{2.2}\\)) is shown with a bracket. 16.6.6 In a blocked design, a linear model with added random effects increases precision of treatment effects \\[ \\begin{equation} \\texttt{gc}_{jk} = \\beta_{0} + \\beta_{k} \\texttt{treatment}_{k} + (\\gamma_{0j} + \\gamma_{kj}\\texttt{treatment}_{k} + \\varepsilon) \\tag{16.2} \\end{equation} \\] If the random intercepts and random slopes aren’t modeled, this among-experiment variance is shifted to the error variance because the intercept effects and slope effects aren’t estimated but absorbed by the error – everything in Model (16.2) will be estimated by the residuals. As a consequence, the estimate of \\(\\sigma\\) (the square root of the error variance) for the linear mixed model is smaller than that for the linear model with only fixed effects. # sigma for the lmm m1 &lt;- lmer(gc ~ treatment + (treatment | experiment_id), data = demo1) summary(m1)$sigma ## [1] 1.931339 # sigma for the fixed lm m2 &lt;- lm(gc ~ treatment, data = demo1) summary(m2)$sigma ## [1] 3.376402 The consequence of the smaller estimate of \\(\\sigma\\) in the linear mixed model on inference (confidence intervals and p-values) depends on the number of subsamples, the variance of the random effects relative to the variance of the residual error, and the correlation among the random effects. 16.6.7 The correlation among random intercepts and slopes Again, here is the linear mixed model fit to the experiment 1g data. \\[ \\begin{equation} \\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon \\end{equation} \\] Think about how this model generates data. We have four experiments, so we randomly draw four \\(\\gamma_{0j}\\) from a normal distribution with some variance \\(\\sigma_{0}^2\\). And, for each non-reference treatment, we randomly draw four \\(\\gamma_{0k}\\) from a normal distribution with some variance \\(\\sigma_{k}^2\\). This gives us a matrix of four columns (one random intercept and three random slopes) and four rows (four experiments). intercept slope 1 slope 2 slope 3 \\(\\gamma_{0.1}\\) \\(\\gamma_{1.1}\\) \\(\\gamma_{2.1}\\) \\(\\gamma_{3.1}\\) \\(\\gamma_{0.2}\\) \\(\\gamma_{1.2}\\) \\(\\gamma_{2.2}\\) \\(\\gamma_{3.2}\\) \\(\\gamma_{0.3}\\) \\(\\gamma_{1.3}\\) \\(\\gamma_{2.3}\\) \\(\\gamma_{3.3}\\) \\(\\gamma_{0.4}\\) \\(\\gamma_{1.4}\\) \\(\\gamma_{2.4}\\) \\(\\gamma_{3.4}\\) To randomly sample these values, the model needs not only the variances (\\(\\sigma_k^2\\)) for each column (random effect) but also a correlation for each pair of columns. These correlations are the off-diagonal elements of the correlation matrix of random effects. 1 COR(\\(\\gamma_{0}\\), \\(\\gamma_{1}\\)) COR(\\(\\gamma_{0}\\), \\(\\gamma_{2}\\)) COR(\\(\\gamma_{0}\\), \\(\\gamma_{3}\\)) COR(\\(\\gamma_{1}\\), \\(\\gamma_{0}\\)) 1 COR(\\(\\gamma_{1}\\), \\(\\gamma_{2}\\)) COR(\\(\\gamma_{1}\\), \\(\\gamma_{3}\\)) COR(\\(\\gamma_{2}\\), \\(\\gamma_{0}\\)) COR(\\(\\gamma_{2}\\), \\(\\gamma_{1}\\)) 1 COR(\\(\\gamma_{2}\\), \\(\\gamma_{3}\\)) COR(\\(\\gamma_{3}\\), \\(\\gamma_{0}\\)) COR(\\(\\gamma_{3}\\), \\(\\gamma_{1}\\)) COR(\\(\\gamma_{3}\\), \\(\\gamma_{2}\\)) 1 In the models fit in this text, a researcher doesn’t specify these variances and correlations. Instead, these are parameters estimated by the model. Here is a summary of the estimates of the variances of the random effects and of the correlations among the random effects for the linear mixed model fit to the experiment 1g data. ## Groups Name Std.Dev. Corr ## experiment_id (Intercept) 1.7710 ## treatmentTr1 0.9642 -0.025 ## treatmentTr2 2.9026 0.458 0.876 ## treatmentTr3 2.9910 0.300 0.879 0.938 ## Residual 1.9313 The first four values in the column “Std.Dev.” are the square roots of the estimated variances for the random effects given in the column “Name”. The last value in column “Std.Dev.” is the square roots of the estimate of \\(\\sigma^2\\) (the error variance). The (lower) triangular matrix of values under “Corr” are the estimates of the correlations among the random effects. The variance of the random effects and the correlation among the random effects creates the correlated error described above but do not confuse these different correlations (this is easy to confuse, you are not alone). A compact way to view these variances and correlations is a matrix with the random effect standard deviations on the diagonal and the correlations on the off-diagonal. I’ll refer to this as the VarCorr matrix after the lme4 function used to get the values. Table 16.2: The Varcorr matrix. Standard deviations of random effects on the diagonal. Correlations of random effects on the off-diagonal. (Intercept) 1.77 treatmentTr1 -0.03 0.96 treatmentTr2 0.46 0.88 2.90 treatmentTr3 0.30 0.88 0.94 2.99 It’s probably not worth trying to understand the experimental reason underneath the correlations among the random effects. But, researchers might want to sleuth out why a lab is getting high random intercept and slope variances, relative to the error variances, as these could indicate potential sources of improvement in lab protocols. 16.6.8 Clustered residuals create heterogeneity among treatments The variances of the four treatments are treatment Var Control 7.8 Tr1 5.2 Tr2 16.2 Tr3 16.7 In chapter xxx, I stated that heterogeneity of variances can arise because of clustered data. Why does clustered data generate heterogeneity? Let’s keep peeking at the linear mixed model fit to the experiment 1g data. \\[ \\begin{equation} \\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon \\end{equation} \\] In a linear model with fixed effects only, the expected variance for any treatment for any treatment is \\(\\sigma^2\\). But if the data are batched, the expected variances include components due to the batch and these batch components depend on the treatment. This creates heterogeneity. To understand this, first some rules of expected variance. The random variable \\(\\texttt{C}\\) is the sum of two random variables \\(\\texttt{A}\\) and \\(\\texttt{B}\\). The variances of these variables are \\(\\sigma_{C}^2\\), \\(\\sigma_{A}^2\\), and \\(\\sigma_{B}^2\\). The expected variance of \\(\\texttt{C}\\) if \\(\\texttt{A}\\) and \\(\\texttt{B}\\) are independent (uncorrelated) is \\(\\sigma_{C}^2 = \\sigma_{A}^2 + \\sigma_{B}^2\\) (this equation should look familiar). The expected variance of \\(\\texttt{C}\\) if \\(\\texttt{A}\\) and \\(\\texttt{B}\\) are not independent (correlated) is \\(\\sigma_{C}^2 = \\sigma_{A}^2 + \\sigma_{B}^2 + 2\\sigma_{A}\\ \\sigma_{B}\\ \\rho_{A,B}\\) where \\(\\rho_{A,B}\\) is the expected correlation between \\(\\texttt{A}\\) and \\(\\texttt{B}\\) (this equation might also look familiar). Here is some code to better know expected variances of the sum of two correlated random variables. # copy, paste, and explore n &lt;- 10^4 rho &lt;- 0.6 # change this to any value between -1 and 1 b &lt;- sqrt(abs(rho)) z &lt;- rnorm(n) A &lt;- b*z + sqrt(1-b^2)*rnorm(n) B &lt;- sign(rho)*b*z + sqrt(1-b^2)*rnorm(n) cor(A,B) # should be close to rho C &lt;- A + B sd(A)^2 # should be close to 1 sd(B)^2 # should be close to 1 sd(C)^2 # should be close to 1^2 + 1^2 + 2*1*1*rho sd(A)^2 + sd(B)^2 + 2*sd(A)*sd(B)*cor(A,B) # should equal previous line Using these rules and the standard deviations of the random effects given above we can compute the expected variances of the treatment groups given the fit model. For the variance of the reference (“Control”) group, we need to add to \\(\\sigma^2\\) the variance of the random intercept using rule #1 (the residuals are not correlated with random intercepts or slopes). The modeled variance is a less than the actual variance of the Control group. summary(m1)$sigma^2 + 1.7710^2 ## [1] 6.866512 For the variance of a non-reference group, we need to add to \\(\\sigma^2\\) the variance of the random intercept and the variance of the random slope for the treatment and the component due to the correlation between the random slope for the treatment and the random intercept. For Tr2, this is # error + intercept + slope + cor(intercept, slope) (summary(m1)$sigma^2) + (1.7710^2) + (2.9026^2) + (2 * 1.7710 * 2.9026 * 0.458) ## [1] 20.0003 which is a bit higher than the measured variance. 16.6.9 Linear mixed models are flexible One more look at the linear mixed model fit to the experiment 1g data. \\[ \\begin{equation} \\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon \\end{equation} \\] The linear mixed model specifies both a random intercept and a random slope but a researcher might limit the model to the random intercept only, or less commonly, the random slope only. Or a researcher might replace the random slope with a second random intercept that captures variance in the batch by treatment combinations like a random slope. Or a researcher might model the structure (correlated error and heterogeneity of variances) in the residuals in addition to adding random factors to the model. 16.6.10 A random intercept only model demo1_m2 &lt;- lmer(gc ~ treatment + (1 | experiment_id), data = demo1) Notes Model demo1_m2 specifies only a random intercept for each of the levels of \\(\\texttt{experiment_id}\\). The exclusion of the random slopes ia a kind of model simplification. In experiments without subsampling, random slopes cannot be added to the model because there is no variation with a treatment by batch combination. In experiments with subsampling, a researcher might exclude a random slope term for several reasons, including it is the culture in many subfields to only include a random intercept (no, this is not a good reason) the computation of the model fit returned a convergence warning model comparison suggested that a model with the random slope was too complex given the data. A useful statistic for comparing models with different random effects specifications is the AIC, which is introduced in Section 16.6.12 below. 16.6.11 A model including an interaction intercept demo1_m3 &lt;- lmer(gc ~ treatment + (1 | experiment_id) + (1 | experiment_id:treatment), data = demo1) Notes (1 | experiment_id:treatment) models a random intercept for all combinations of the levels of \\(\\texttt{experiment_id}\\) and \\(\\texttt{treatment}\\). This interaction intercept is an alternative to a random slope for modeling treatment-specific batch effects. 16.6.12 AIC and model selection – which model to report? Three different linear mixed models were fit to the Example 1 data: demo1_m1, demo1_m2, and demo1_m3. Which model do we report? A useful stastistic for this decision is a statistic known as the AIC. Table 16.3: AIC of the two alternative models fit to Example 1 data Model AIC demo1_m1 344.1529 demo1_m2 344.0765 demo1_m3 337.5332 Notes AIC (Akaike Information Criterion) is a relative measure of model quality. Compare this to \\(R^2\\), which is an absolute measure of goodness of fit. The AIC formula has two parts, one is a kind of goodness of fit (like \\(R^2\\)) and the other is a penalty based on the number of parameters in the model. As the goodness of fit increases, the AIC goes down. As the number of parameters increases, the AIC goes up. The model with the lowest AIC is the highest quality model. The actual value of AIC, unlike \\(R^2\\), does not have any absolute meaning; it is only meaningful relative to the AICs computed from fits to different models to the same data. The AIC of the three models suggests that we report Model demo1_m3. 16.6.13 The specification of random effects matters Inference will often be very different between these two models as they make very different Table 16.4: Contrasts from the three linear mixed models fit to the Example 1 data. contrast estimate SE df lower.CL upper.CL t.ratio p.value intercepts + slopes (m1) Tr1 - Control 2.54 0.804 2.978 -0.03 5.11 3.16 0.05150 Tr2 - Control 5.61 1.591 2.997 0.54 10.67 3.52 0.03885 Tr3 - Control 2.85 1.631 2.997 -2.35 8.04 1.74 0.17949 intercepts only (m2) Tr1 - Control 2.64 0.755 66.005 1.13 4.15 3.49 0.00086 Tr2 - Control 5.65 0.766 66.014 4.12 7.18 7.38 0.00000 Tr3 - Control 2.94 0.765 66.000 1.41 4.47 3.85 0.00027 intercept + interaction intercept (m3) Tr1 - Control 2.54 1.214 8.912 -0.21 5.29 2.09 0.06616 Tr2 - Control 5.60 1.219 9.061 2.84 8.35 4.59 0.00128 Tr3 - Control 2.87 1.219 9.052 0.12 5.63 2.36 0.04269 Notes Inference from the intercept only model (demo1_m2) is almost certainly too optimistic based on simulations that show that intercept only models can be highly anti-conservative (too narrow confidence intervals and too small p-values). Humans have evolved to make up rational explanations – do not convince yourself that the model with the smallest p-values is the scientifically most rational model. 16.6.14 Mixed Effect and Repeated Measures ANOVA Two-way mixed-effect ANOVA (some fields would call this a repeated measures ANOVA) is equivalent to Model demo1_m3 in this case. More generally, the two are equal in balanced designs – the same number of subsamples in all treatment x batch combinations. demo1_m4 &lt;- aov_4(gc ~ treatment + (treatment | experiment_id), data = demo1) Notes 1. The function afex::aov4 is used for specifying ANOVA models that are special cases of linear mixed models. 2. The model formula in Model demo1_m2 looks exactly like that the random intercepts and slopes model (Model demo1_m1) but these are not the same. 3. One can use either a univariate or multivariate model for mixed or repeated measures ANOVA – see section 16.7.11. Table 16.5: Contrasts from the two-way mixed effect ANOVA model demo1_m4. contrast estimate SE df lower.CL upper.CL t.ratio p.value Tr1 - Control 2.50 0.599 3 0.60 4.41 4.18 0.02488 Tr2 - Control 5.58 1.559 3 0.62 10.54 3.58 0.03734 Tr3 - Control 2.84 1.586 3 -2.20 7.89 1.79 0.17095 16.6.15 Pseudoreplication 16.7 Example 2 – experiments without subsampling replication (exp6g) This example introduces linear mixed models for batches that contain all treatment levels of a single factor but no subsampling replication. In this example, the batch is the individual mouse (\\(\\texttt{mouse_id}\\)). There are four measures of the response variable on each mouse, one measure per treatment level. When there is no subsampling replication, we cannot add a random slope to the model because there is only a single observation at each treatment level and a slope would fit the point at the reference level and the point at the non-reference level perfectly. However, we can explicitly model variation in the correlated error and heterogeneity in the variances among treatments as an alternative to modeling a random slope. Reversing a model of Parkinson’s disease with in situ converted nigral neurons Public source Source figure: Fig. 6g Source data: Source Data Fig. 6 16.7.1 Understand the data In this study, the researchers investigated the effectiveness of knocking down the protein PTBP1 to induce astrocytes to convert to neurons in a motor processing region of the brain. Experimental lesions of this region of the brain is a model of Parkinson’s disease. In Experiment 6g, the researchers Generated a lesion in the motor processing region using 6-hydroxydopamine (6-OHDA). The lesion disrupts the ability to control the contralateral (opposite side) forelimb. One month after the lesion, measured the percent of ipsilateral (same side) forepaw touches (the forelimb extending out and touching the surface) in a test of exploration in a new environment (the “cylinder test”). The expected percent in an intact mouse is 50%. In a lesioned mouse, the percent should be much greater than 50% since there is less control of the contralateral limb. The measure at this point is in the treatment “Lesion”. This is the positive control. Converted astrocytes in the lesion to functional neurons by knocking down PTBP1. Two months after knockdown, gave the mouse saline and remeasured percent ipsilateral touches in a cylinder test. If the knockdown worked as expected, there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment “Saline”. The comparison with Lesion is a focal test. Inhibited neuron action in the converted neurons using clozapine-N-oxide (CNO), which suppresses neuron electrical activity. Then, remeasured percent ipsilateral touches in a cylinder test. If the CNO worked as expected, there should be much greater than 50% ipsilateral touches since there should be re-loss of control of the contralateral limb. The measure at this point is in the treatment “CNO”. The comparison with Saline is a focal test. Allowed three days for the CNO to degrade, then, remeasured percent ipsilateral touches in a cylinder test. If the CNO degraded as expected, the converted neurons should be functional and there should be closer to 50% ipsilateral touches. The measure at this point is in the treatment “Post_CNO”. The comparison with CNO is a focal test. The design is \\(4 \\times 1\\) – a single treatment with four levels (“Lesion”, “Saline”, “CNO”, “Post_CNO”) The planned contrasts are Saline - Lesion. This measures the effect of the knockdown and conversion of astrocytes to functional neurons. CNO - Saline. This measures the effect of inhibiting the converted neurons to test if it was these and not some other neurons that account for the effect in contrast 1. Post_CNO - CNO. This is probing the same expectation as contrast 2. 16.7.2 Model fit and inference 16.7.2.1 Fit the model exp6g_m1a &lt;- lmer(touch ~ treatment + (1|mouse_id), data = exp6g) # alt model exp6g_m1b &lt;- lme(touch ~ treatment, random = ~1|mouse_id, correlation = corSymm(form = ~ 1 | mouse_id), weights = varIdent(form = ~ 1 | treatment), data = exp6g) AIC(exp6g_m1a, exp6g_m1b) ## df AIC ## exp6g_m1a 6 191.3367 ## exp6g_m1b 15 198.3888 # report model a exp6g_m1 &lt;- exp6g_m1a exp6g_m1b overparameterizes, report exp6g_m1a (see Alternative models for exp6g below) 16.7.2.2 Inference from the model exp6g_m1_coef &lt;- cbind(coef(summary(exp6g_m1)), confint(exp6g_m1)[-c(1:2),]) # exp5c_m1_coef %&gt;% # kable(digits = c(2,3,1,1,4,2,2)) %&gt;% # kable_styling() exp6g_m1_emm &lt;- emmeans(exp6g_m1, specs = c(&quot;treatment&quot;)) treatment emmean SE df lower.CL upper.CL Lesion 83.3 3.44 18.9 76.1 90.47 Saline 62.8 3.44 18.9 55.5 69.96 CNO 81.4 3.44 18.9 74.2 88.63 Post_CNO 58.8 3.44 18.9 51.6 66.00 # exp6g_m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table lesion &lt;- c(1,0,0,0) saline &lt;- c(0,1,0,0) cno &lt;- c(0,0,1,0) post_cno &lt;- c(0,0,0,1) exp6g_m1_planned &lt;- contrast(exp6g_m1_emm, method = list( &quot;Saline - Lesion&quot; = c(saline - lesion), &quot;CNO - Saline&quot; = c(cno - saline), &quot;Post_CNO - CNO&quot; = c(post_cno - cno) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast estimate SE df lower.CL upper.CL t.ratio p.value Saline - Lesion -20.52 4.071 18 -29.07 -11.96 -5.04 0.00009 CNO - Saline 18.67 4.071 18 10.12 27.23 4.59 0.00023 Post_CNO - CNO -22.63 4.071 18 -31.18 -14.08 -5.56 0.00003 16.7.2.3 Plot the model Figure 16.7: Treatment effect on ipsilateral touch, as percent of all touches. Gray dots connected by lines are individual mice. 16.7.2.4 Alternaplot the model Figure 16.8: Ipsilateral touch (percent of all touches) response to different treatments. Gray dots connected by lines are individual mice. 16.7.3 The model exp6g_m1 adds a random intercept but not a random slope The model fit to the exp6g data is \\[ \\begin{equation} \\texttt{touch}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{jk}) \\texttt{treatment}_{k} + \\varepsilon \\end{equation} \\] Notes Again, in experiments without subsampling, we cannot add a random slope to the model (for each mouse, there is a single observation at each treatment level so a slope would fit the two points perfectly). 16.7.4 The fixed effect coefficients of model exp6g_m1 The fixed effect coefficients of model exp6g_m1 are Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 83.27 3.441 18.9 24.20 0.0000 76.72 89.81 treatmentSaline -20.52 4.071 18.0 -5.04 0.0001 -28.25 -12.78 treatmentCNO -1.84 4.071 18.0 -0.45 0.6565 -9.58 5.90 treatmentPost_CNO -24.47 4.071 18.0 -6.01 0.0000 -32.21 -16.73 Notes The interpretation of the fixed effectcs coefficients have the usual interpretation (see The coefficients of a linear model using dummy coding have a useful interpretation). Figure 16.9 is a reminder. Figure 16.9: Fixed effects estimated by exp6g_m1. \\(b_0\\) is the modeled mean of the Lesian treatment. \\(b_1\\) is the difference (Saline - Lesian). \\(b_2\\) is the difference (CNO - Lesian). \\(b_3\\) is the difference (Post_CNO - Lesian). 16.7.5 The random intercept coefficients of exp6g_m1 Table 16.6: Random intercept coefficients (g_0j) for exp6g_m1. For each mouse, the random intercept is the sum of the fixed intercept (\\(b_0\\)) and the random intercept effect \\(g_{0j}\\). mouse_id random intercept b_0 g_0j mouse_1 82.874 83.268 -0.393 mouse_2 83.640 83.268 0.372 mouse_3 77.499 83.268 -5.769 mouse_4 79.943 83.268 -3.325 mouse_5 89.286 83.268 6.018 mouse_6 82.732 83.268 -0.536 mouse_7 86.901 83.268 3.633 Notes The random intercept effect \\(g_{0j}\\) is the difference between the modeled mean for mouse j and the mean of the reference treatment. Unlike the fixed intercept (\\(b_0\\)), \\(g_{0j}\\) is an effect. Figure ?? illustrates the random intercept coefficients \\(g_{0j}\\) for model exp6g_m1. The random intercept coefficients are often (but not always) treated as a source of nuisance variation – that is, the coefficients are not generally of interest and the values are not typically reported. Figure 16.10: Random intercept effects for model exp6g_m1. The pale, colored dots are the measured percent ipsilateral touch values for each mouse for each treatment. The dashed, gray lines are the modeled means for each treatment. The dashed grey line for the reference level (“Lesion”) is the fixed intercept. The dark, colored dots at the reference level are the random intercepts. The value of each random intercept is the sum of the fixed intercept and the random intercept effect for that mouse. The vertical, colored line segments at the reference level are the random intercept effects \\(g_{0j}\\). The length of the segment is the residual from the dashed, gray line to the pale dot. The random intercept effects for mice 1 and 2 are too short to see. 16.7.6 The random and residual variance and the intraclass correlation of model exp6g_m1 ## Groups Name Std.Dev. ## mouse_id (Intercept) 4.9882 ## Residual 7.6155 Notes The first element in “Std.Dev.” is the estimate of \\(\\sqrt{\\sigma^2_{0j}}\\), the standard deviation among the donors due to the random effect of \\(\\texttt{mouse_id}\\). The second element in “Std.Dev.” is \\(\\sqrt{\\sigma^2_{0}}\\), the estimate of the standard deviation of the error variance (\\(\\varepsilon^2\\)). Remember that the residuals of the model are the estimates of \\(\\varepsilon\\). The ratio \\(\\frac{\\sigma^2_{0j}}{\\sigma^2_{0j} + \\sigma^2_{0}}\\) (the ratio of the among-block variance to total random variance) is known as the intraclass correlation. This correlation is an estimate of the correlated error due to the by-mouse clustering if the model were fit without the added random intercept. For exp6g_m1, this correlation is 0.3. The intraclass correlation ranges between 0 and 1 and makes a pretty good qualitative indicator of repeatability. 16.7.7 The linear mixed model exp6g_m1 increases precision of treatment effects, relative to a fixed effects model Let’s compare the effects estimated by the linear mixed model exp6g_m1 with a linear model that ignores donor (a fixed effects model). exp6g_m2 &lt;- lm(touch ~ treatment, data = exp6g) Figure 16.11: A. Inference from a linear mixed model with blocking factor (mouse_id) added as a random intercept. B. Inference from a fixed effects model. Figure 16.11A is a plot of the effects from the linear mixed model exp6g_m1 that models the added variance due to mouse. Figure 16.11B is a plot of the effects from the fixed effect model exp6g_m2 that ignores the added variance due to mouse. The 95% confidence intervals of the treatment effects in model exp6g_m1 are slightly smaller than those in model exp6g_m2. Adding \\(\\texttt{mouse_id}\\) as a random factor to the linear model increases the precision of the estimate of the treatment effects by eliminating the among-mouse component of variance from the error variance. The error variance (the estimate of $^2) in the linear mixed model and the fixed effects model is summary(exp6g_m1)$sigma^2 ## [1] 57.99549 summary(exp6g_m2)$sigma^2 ## [1] 82.8777 The error variance is 43% higher in the fixed effects model. In the linear mixed model, the variance lost from the error was shifted to the random intercept. We can track this shift with the table of the two variance components of the linear mixed model, shown above, and here again (the values are the square roots of the variances). VarCorr(exp6g_m1) ## Groups Name Std.Dev. ## mouse_id (Intercept) 4.9882 ## Residual 7.6155 The sum of the two variance components of the linear mixed model is equal to the error variance of the fixed effects model: sum(as.data.frame(VarCorr(exp6g_m1))$vcov) ## [1] 82.8777 16.7.8 Alternative models for exp6g Linear mixed models are very flexible, a topic which is too advanced for this text. Here I want to focus on an alternative model because of its relevance to repeated measures ANOVA. exp6g_m1a &lt;- lmer(touch ~ treatment + (1 | mouse_id), data = exp6g) exp6g_m1b &lt;- lme(touch ~ treatment, random = ~1|mouse_id, correlation = corSymm(form = ~ 1 | mouse_id), weights = varIdent(form = ~ 1 | treatment), data = exp6g) Notes Two models are fit. The two models have the same fixed effect but differ in how they model the random effect and the pattern of correlations in the residuals. The difference in specification determines the error variance and degrees of freedom for computing uncertainty. I’ll return to inference and the problem of “which model to choose” in a moment. First, how do these models differ? Model exp6g_m1a is the same model analyzed and explained above. This model specifies a random intercept for each level of \\(\\texttt{mouse_id}\\). Model exp6g_m1b has the same random effects as exp6g_m1a but adds two additional arguments, a correlation argument that explicitly models correlated error in the residuals and a weights argument that models heterogeneity in the residuals. Consider the error (the residuals) of the fixed effect model touch ~ treatment cast into a matrix with the residuals for each treatment in its own column. This matrix of residuals will be a \\(7 \\times 4\\) (7 mice, 4 treatments) matrix that looks like the table of residuals in Example 1 (Table 16.1). Model exp6g_m1a implicitly models compound symmetric correlated error. This means the correlation between every pair of columns of the residual matrix is the same. the variances of each of the columns of the residual matrix is the same. Model exp6g_m1a assumes zero correlation among the columns of a residual matrix from Model exp6g_m1a. This is because the source of the correlated residuals from the fixed model has been modeled by the random intercept. Model exp6g_m1b does not assume compound symmetric correlated error. the argument correlation = corSymm(form = ~ 1 | mouse_id) in model exp6g_m1b explicitly models different correlations for all pairs of treatment combinations – that is, unstructured correlated error. the argument weights = varIdent(form = ~ 1 | treatment) in model exp6g_m1b explicitly models heterogeneity in the residuals among the levels of $. Model exp6g_m1b makes fewer assumptions for inference. The trade-off is the estimation of more parameters and the potential of overfitting. A univariate model of a repeated measures ANOVA fit to the exp6g data is equivalent to model exp6g_m1a. See section 16.7.10. A multivariate model of a repeated measures ANOVA fit to the exp6g data is equivalent to model exp6g_m1b. See section 16.7.11. Table 16.7: Planned contrasts from the two alternative models. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp6g_m1a Saline - Lesion -20.52 4.071 18 -29.07 -11.96 -5.04 0.000085 CNO - Saline 18.67 4.071 18 10.12 27.23 4.59 0.000228 Post_CNO - CNO -22.63 4.071 18 -31.18 -14.08 -5.56 0.000028 exp6g_m1b Saline - Lesion -20.52 3.172 18 -27.18 -13.85 -6.47 0.000004 CNO - Saline 18.67 3.057 18 12.25 25.10 6.11 0.000009 Post_CNO - CNO -22.63 5.610 18 -34.42 -10.84 -4.03 0.000779 Planned comparisons of the two models are given in the tables above. The effect estimates are the same but inference differs quantitatively among the models (but not qualitatively) because of how each model models the error. Which model do we report? One way to evaluate the models is a statistic known as the AIC. Table 16.8: AIC of the two alternative models fit to exp6g data Model AIC exp6g_m1a 191.3367 exp6g_m1b 198.3888 AIC (Akaike Information Criterion) is a relative measure of model quality. Compare this to \\(R^2\\), which is an absolute measure of goodness of fit. The AIC formula has two parts, one is a kind of goodness of fit (like \\(R^2\\)) and the other is a penalty based on the number of parameters in the model. As the goodness of fit increases, the AIC goes down. As the number of parameters increases, the AIC goes up. The model with the lowest AIC is the highest quality model. The actual number, however does not have any absolute meaning; it is only meaningful relative to the AICs computed from fits to different models using the same data. The AICs of models exp6g_m1a and models exp6g_m1b suggest that model exp6g_m1b is too complex given the data. This has relevance for the repeated measures ANOVA analysis in the next section. 16.7.9 Paired t-tests and repeated measures ANOVA are special cases of linear mixed models A paired t-test is a special case of linear mixed model fit to data from a randomized complete block design with no subsampling and only two treatment levels (see Lack of independence in the Violations chapter). A repeated measures ANOVA is a special case of linear mixed model fit to data from a randomized complete block design with no subsampling and more than two treatment levels (see Lack of independence in the Violations chapter). Experiment 6g is a randomized complete block design with no subsampling and four treatments. A “which test” key in a traditional, experimental statistics textbook would guide a researcher to analyze these data using repeated measures ANOVA. There are two methods of repeated measures anova, the univariate model and the multivariate model. 16.7.10 Classical (“univariate model”) repeated measures ANOVA of exp6g exp6g_aov1 &lt;- aov_4(touch ~ treatment + (treatment | mouse_id), data = exp6g) Table 16.9: Planned contrasts for the linear mixed model exp6g_m1a and the univariate model of the repeated measures ANOVA. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp6g_m1a Saline - Lesion -20.52 4.071 18 -29.07 -11.96 -5.04 0.000085 CNO - Saline 18.67 4.071 18 10.12 27.23 4.59 0.000228 Post_CNO - CNO -22.63 4.071 18 -31.18 -14.08 -5.56 0.000028 univariate RM-ANOVA Saline - Lesion -20.52 3.172 6 -28.28 -12.75 -6.47 0.000649 CNO - Saline 18.67 3.057 6 11.19 26.15 6.11 0.000877 Post_CNO - CNO -22.63 5.610 6 -36.36 -8.90 -4.03 0.006853 Notes In addition to the assumptions of the linear model outlined in the Violations chapter, the classical (“univariate model”) repeated measures ANOVA assumes sphericity, which is the equality of the variances of all pairwise differences among treatment combinations. The univariate model of the classic RM ANOVA is equivalent to Model exp6g_m1a. 16.7.11 “Multivariate model” repeated measures ANOVA Table 16.10: Planned contrasts for the linear mixed model exp6g_m1b and the multivariate model of the repeated measures ANOVA. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp6g_m1b Saline - Lesion -20.52 3.172 18 -27.18 -13.85 -6.47 0.000004 CNO - Saline 18.67 3.057 18 12.25 25.10 6.11 0.000009 Post_CNO - CNO -22.63 5.610 18 -34.42 -10.84 -4.03 0.000779 multivariate RM-ANOVA Saline - Lesion -20.52 3.172 6 -28.28 -12.75 -6.47 0.000649 CNO - Saline 18.67 3.057 6 11.19 26.15 6.11 0.000877 Post_CNO - CNO -22.63 5.610 6 -36.36 -8.90 -4.03 0.006853 Notes The sphericity assumption of classical repeated measures ANOVA is relaxed if the model is fit using the “multivariate model”. The multivariate model repeated measures ANOVA is a linear model with a multivariate response and not a linear mixed model. In the multivariate model, each treatment combination is a different response variable and there is a single row for each level of the random factor (\\(\\texttt{mouse_id}\\) in Experiment 6g). The multivariate model is a different way of handling the correlated error that occurs when conceiving of the design as univariate. The multivariate repeated measures ANOVA is equivalent to the linear mixed model exp6g_m1b. The SEs of the contrasts are the same but the degrees of freedom differ. Because of the increased df of the linear mixed model, the CIs are narrower and the p-value is smaller – that is the linear mixed model is less conservative than the repeated measures ANOVA. There is no correct degrees of freedom for a linear mixed model like this and emmeans outputs one way to compute these (with options for others, none of which are equivalent to those from the multivariate model RM-ANOVA). 16.7.12 Linear mixed models vs repeated measures ANOVA Many modern textbooks encourage researchers to use linear mixed models instead of repeated measures ANOVA for randomized complete block designs (with or without subsampling) because linear mixed models do not exclude random units (subject/mouse/donor/cage) with missing measures of one of the treatment combinations. Example 2 (diHOME exp2a) – A repeated measures ANOVA is a special case of a linear mixed model is an example of this kind of missing data. if there is subsampling, linear mixed models do not aggregate the random-unit data, that is, linear mixed models do not simply compute the means and ignore the variance of the sample within each random unit. linear mixed models allow for modeling additional sources of correlated error, that is, an experiment may have two or more random factor variables (for example, donor and experiment). linear mixed models allow a researcher to model different patterns of correlated error. This is especially important in longitudinal experiments. linear mixed models can be generalized to model sampling from non-normal distributions – these are generalized linear mixed models. 16.7.13 Modeling \\(\\texttt{mouse_id}\\) as a fixed effect \\[ \\begin{equation} \\texttt{touch} \\sim \\texttt{treatment + mouse_id} \\end{equation} \\tag{16.3} \\] Model (16.3) (using R formula syntax) is a linear model with the block \\(\\texttt{mouse_id}\\) added as a fixed covariate instead of a random intercept. The coefficients of the fit model are exp6g_m3 &lt;- lm(touch ~ treatment + mouse_id, data = exp6g) exp6g_m3_coef &lt;- cbind(coef(summary(exp6g_m3)), confint(exp6g_m3)) Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 82.65 4.551 18.2 0.0000 73.08 92.21 treatmentSaline -20.52 4.071 -5.0 0.0001 -29.07 -11.96 treatmentCNO -1.84 4.071 -0.5 0.6565 -10.39 6.71 treatmentPost_CNO -24.47 4.071 -6.0 0.0000 -33.02 -15.92 mouse_idmouse_2 1.21 5.385 0.2 0.8245 -10.10 12.52 mouse_idmouse_3 -8.51 5.385 -1.6 0.1315 -19.82 2.81 mouse_idmouse_4 -4.64 5.385 -0.9 0.4003 -15.95 6.67 mouse_idmouse_5 10.15 5.385 1.9 0.0758 -1.17 21.46 mouse_idmouse_6 -0.23 5.385 0.0 0.9671 -11.54 11.09 mouse_idmouse_7 6.37 5.385 1.2 0.2521 -4.94 17.69 Notes The coefficients include a slope for the seven non-reference levels of mouse_id We typically don’t care about these. In a randomized complete block design with only 1 replicate of each treatment combination per block, like that in Experiment 6g, inference about treatment effects is exactly that same between this model and the random intercept model exp6g_m1. Compare the SE, CI and p-value for the coefficients of the treatment effects to those from the linear mixed model exp6g_m1. Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 83.27 3.441 18.9 24.20 0.0000 76.72 89.81 treatmentSaline -20.52 4.071 18.0 -5.04 0.0001 -28.25 -12.78 treatmentCNO -1.84 4.071 18.0 -0.45 0.6565 -9.58 5.90 treatmentPost_CNO -24.47 4.071 18.0 -6.01 0.0000 -32.21 -16.73 In a randomized complete block design with only 1 replicate of each treatment combination per block, like that in Experiment 6g, inference about treatment means differs between this model and the random intercept model – the SE of the means of the fixed effect model are smaller than the SE of the means of the linear mixed model. Compare the SE and CI of the (Intercept) (the mean of the reference treatment combination) between the fixed effect and linear mixed model. The equivalence of inference in the treatment effect between the fixed effect and linear mixed model holds only for balanced randomized complete block designs – where all blocks contain all treatment combinations and the subsampling replicate size is the same for all treatment combinations for all blocks. This means, outside of special cases like Experiment 6g, the choice between adding a blocking variable as a random or fixed factor depends on assumptions about the model. 16.8 Example 3 – Factorial experiments and no subsampling replicates (exp5c) Example 3 is similar to example 2 in that there is no subsampling replication and we cannot add random slopes to the linear mixed model. Example 3 differs in that the design is factorial – there are two, crossed fixed factors. Consequently, there are several alternative models with different sets of random intercepts. The reported model includes two random intercepts, one of which models differences in batch effects among treatment levels (treatment by batch interactions). This interaction intercept is an alternative to random slope for modeling treatment by batch interactions. Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity Source figure: Fig. 5c Source data: Source Data Fig. 5 16.8.1 Understand the data The data for Example 1 are from Figure 5c. Six muscle source cells were used to start six independent cultures. Cells from each culture were treated with either a negative control (“Scr”) or a siRNA (“siNR4A3”) that “silences” expression of the NR4A3 gene product by cleaving the mRNA. Glucose uptake in the two cell types was measured at rest (“Basal”) and during electrical pulse stimulation (“EPS”). The design is a \\(2 \\times 2\\) Randomized complete block with no subsampling. There are two factors each with two levels: \\(\\texttt{treatment}\\) (“Scr”, “siNR4A3”) and \\(\\texttt{activity}\\) (“Basal”, “EPS”). Each source cell is a block. All four treatment combinations were measured once per block. 16.8.2 Examine the data The plot shows a strong donor effect. 16.8.3 Model fit and inference exp5c_m1a &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor), data = exp5c) exp5c_m1b &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor) + (1 | donor:treatment) + (1 | donor:activity), data = exp5c) exp5c_m1c &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor) + (1 | donor:treatment), data = exp5c) exp5c_m1d &lt;- lme(glucose_uptake ~ treatment * activity, random = ~ 1 | donor, correlation = corSymm(form = ~ 1 | donor), weights = varIdent(form = ~ 1|t.by.a), data = exp5c) # check AIC AIC(exp5c_m1a, exp5c_m1b, exp5c_m1c, exp5c_m1d) ## df AIC ## exp5c_m1a 6 8.153813 ## exp5c_m1b 8 10.052676 ## exp5c_m1c 7 8.052676 ## exp5c_m1d 15 14.763024 # check VarCorr model c VarCorr(exp5c_m1c) # fine ## Groups Name Std.Dev. ## donor:treatment (Intercept) 0.089519 ## donor (Intercept) 0.352097 ## Residual 0.090685 # report 1c (based on AIC and VarCorr check) exp5c_m1 &lt;- exp5c_m1c exp5c_m1b (equivalent to univariate repeated measures ANOVA) is singular fit. don’t use. Trivial difference in AIC between exp5c_m1a and exp5c_m1c. Nothing in VarCorr with exp5c_m1c raises red flags. Report exp5c_m1c. 16.8.3.1 Check the model ggcheck_the_model(exp5c_m1) fine. 16.8.3.2 Inference from the model exp5c_m1_coef &lt;- cbind(coef(summary(exp5c_m1)), confint(exp5c_m1)[-c(1:3),]) exp5c_m1_coef %&gt;% kable(digits = c(2,3,1,1,4,2,2)) %&gt;% kable_styling() Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 1.34 0.153 5.8 8.8 0.0001 1.02 1.66 treatmentsiNR4A3 0.08 0.074 8.5 1.1 0.2994 -0.07 0.23 activityEPS 0.15 0.052 10.0 2.9 0.0163 0.05 0.25 treatmentsiNR4A3:activityEPS -0.27 0.074 10.0 -3.6 0.0048 -0.41 -0.12 exp5c_m1_emm &lt;- emmeans(exp5c_m1, specs = c(&quot;treatment&quot;, &quot;activity&quot;)) treatment activity emmean SE df lower.CL upper.CL Scr Basal 1.34 0.153 5.8 0.96 1.72 siNR4A3 Basal 1.42 0.153 5.8 1.04 1.80 Scr EPS 1.49 0.153 5.8 1.11 1.87 siNR4A3 EPS 1.31 0.153 5.8 0.93 1.68 # exp5c_emm # print in console to get row numbers # set the mean as the row number from the emmeans table scr_basal &lt;- c(1,0,0,0) siNR4A3_basal &lt;- c(0,1,0,0) scr_eps &lt;- c(0,0,1,0) siNR4A3_eps &lt;- c(0,0,0,1) exp5c_m1_planned &lt;- contrast(exp5c_m1_emm, method = list( &quot;(Scr EPS) - (Scr Basal)&quot; = c(scr_eps - scr_basal), &quot;(siNR4A3 EPS) - (siNR4A3 Basal)&quot; = c(siNR4A3_eps - siNR4A3_basal), &quot;Interaction&quot; = c(siNR4A3_eps - siNR4A3_basal) - c(scr_eps - scr_basal) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast estimate SE df lower.CL upper.CL t.ratio p.value (Scr EPS) - (Scr Basal) 0.15 0.052 10 0.03 0.27 2.88 0.016 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.052 10 -0.23 0.00 -2.22 0.051 Interaction -0.27 0.074 10 -0.43 -0.10 -3.61 0.005 16.8.3.3 Plot the model Figure 16.12: Treatment effect on glucose uptake. Gray dots connected by lines are individual donors. 16.8.3.4 Alternaplot the model Figure 16.13: Glucose response to different treatments. Gray dots connected by lines are individual donors. Dashed gray line is expected additive mean of “siNR4A3 EPS”. Notes Many researchers might look at the wide confidence intervals relative to the short distance between the means and think “no effect”. The confidence intervals are correct, they simply are not meant to be tools for inferring anything about differences in means. This is one of many reasons why plots of means and error bars can be misleading for inference, despite the ubiquity of their use for communicating results. And, its why I prefer the effects-and-response plots, which explicitly communicate correct inference about effects. 16.8.4 Why we care about modeling batch in exp5c Figure 16.14 shows the modeled means of the four treatment combinations and the individual values colored by donor. It is pretty easy to see that the glucose uptake values for donors 4 and 5 are well above the mean for all four treatments. And, the values for donors 1, 2, and 3 are well below the mean for all four treatments. The values for donor 6 are near the mean for all four treatments. Figure 16.14: Why we care about blocking. The black dots are the modeled means of each treatment combination. The colored dots are the measured values of the response for each donor. The position of a donor relative to the mean is easy to see with these data. Let’s compare the effects estimated by the linear mixed model exp5c_m1 with a linear model that ignores donor. exp5c_m2 &lt;- lm(glucose_uptake ~ treatment * activity, data = exp5c) Figure 16.15: A. Inference from a linear mixed model with blocking factor (donor) added as a random intercept. B. Inference from a fixed effects model. Figure 16.15A is a plot of the effects from the linear mixed model that models the correlated error due to donor. Figure 16.15B is a plot of the effects from the fixed effect model that ignores the correlated error due to donor. Adding \\(\\texttt{donor}\\) as a factor to the linear model increases the precision of the estimate of the treatment effects by eliminating the among-donor component of variance from the error variance. 16.8.5 The linear mixed model exp5c_m1 adds two random intercepts There are two random intercepts in the linear mixed model exp5c_m1 fit to the exp5c data (exp5c_m1 is copied from exp5c_m1c in Model fit and inference). \\[ \\begin{align} \\texttt{glucose_uptake}_j = \\ &amp;(\\beta_0 + \\gamma_{0j} + \\gamma_{0jk}) + \\beta_1 (\\texttt{treatment}_\\texttt{siNR4A3}) + \\beta_2 (\\texttt{activity}_\\texttt{EPS}) \\ + \\\\ &amp;\\beta_3 (\\texttt{treatment}_\\texttt{siNR4A3}:\\texttt{activity}_\\texttt{EPS}) + \\varepsilon \\end{align} \\tag{16.4} \\] The random intercept effect \\(\\gamma_{0j}\\) models donor variation in the reference level. j indexes donor j. A coefficient is estimated for each donor. The random intercept effect \\(\\gamma_{0jk}\\) models the variation due to the \\(\\texttt{donor}\\) by \\(\\texttt{treatment}\\) combinations. A coefficient is estimated for each of the 6 (donors) \\(\\times\\) 2 (levels of \\(\\texttt{treatment}\\)). k indexes treatment level k. This interaction intercept is an alternative to random slope for modeling treatment by batch interactions. Unlike a random slope, there is a \\(\\gamma_{0jk}\\) coefficient for each donor in the reference level. 16.8.6 The fixed effect coefficients of model exp5c_m1 The fixed effect coefficients of model exp5c_m1 are Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 1.34 0.153 5.8 8.77 0.0001 1.02 1.66 treatmentsiNR4A3 0.08 0.074 8.5 1.11 0.2994 -0.07 0.23 activityEPS 0.15 0.052 10.0 2.88 0.0163 0.05 0.25 treatmentsiNR4A3:activityEPS -0.27 0.074 10.0 -3.61 0.0048 -0.41 -0.12 Notes The interpretation of the fixed effectcs coefficients have the usual interpretation for a factorial linear model. Figure 16.16 is a reminder. Figure 16.16: Fixed effects estimated by exp5c_m1. The light gray point is the expected value of the GPR174- F treatment if genotype and sex were additive. 16.8.7 The random effect coefficients of model exp5c_m1 Table 16.11: Random intercept effects for each donor (g_0j) and for each donor:treatment combination (g_0jk) for exp5c_m1. There are two random intercepts. random_intercept_j is the sum of the fixed intercept (b_0) and g_0j. random_intercept_jk is the sum of the fixed intercept (b_0) and g_0jk. donor:treatment random_intercept_j random_intercept_jk b_0 g_0j g_0jk donor_1:Scr 1.018 1.349 1.341 -0.323 0.008 donor_1:siNR4A3 1.018 1.312 1.341 -0.323 -0.029 donor_2:Scr 1.033 1.283 1.341 -0.307 -0.058 donor_2:siNR4A3 1.033 1.379 1.341 -0.307 0.038 donor_3:Scr 1.168 1.310 1.341 -0.173 -0.031 donor_3:siNR4A3 1.168 1.360 1.341 -0.173 0.020 donor_4:Scr 1.785 1.302 1.341 0.445 -0.039 donor_4:siNR4A3 1.785 1.408 1.341 0.445 0.068 donor_5:Scr 1.744 1.433 1.341 0.403 0.092 donor_5:siNR4A3 1.744 1.275 1.341 0.403 -0.066 donor_6:Scr 1.296 1.368 1.341 -0.045 0.028 donor_6:siNR4A3 1.296 1.310 1.341 -0.045 -0.031 Notes The random intercept “random_intercept_j” is the sum of the fixed intercept \\(b_0\\) and the random intercept effect \\(g_{0j}\\) for donor j. The \\(g_{0j}\\) estimate the \\(\\gamma_{0j}\\) in Model (16.4). Note that \\(g_{0j}\\) is the same for both treatment levels within a donor. The illustration of these random effects is similar to that for Example 2 exp6g_m1 (Figure 16.10. The random intercept “random_intercept_jk” is the sum of the fixed intercept \\(b_0\\) and the random intercept effect \\(g_{0jk}\\) for donor j in treatment level k (with the reference level equal to 1). The \\(g_{0jk}\\) estimate the \\(\\gamma_{0jk}\\) in Model (16.4). The \\(g_{0jk}\\) differ for each combination of \\(\\texttt{donor}\\) and \\(\\texttt{treatment}\\). The donor:treatment random intercepts (“random_intercept_jk”) are shown with the dark, colored dots in Figure 16.17A. The distance from the dark colored dot to the dashed, gray line (the modeled mean for the treatment:activity combination) is the random intercept effect estimate of \\(\\gamma_{0jk}\\). The set of random intercept effects for the donor:treatment combinations (\\(g_{0jk}\\)) are the same between the two activity levels. This is seen in Figure 16.17A - compare the pattern of dark, colored dots in the two EPS treatments to the two Basal treatments. The combined random intercept effects (\\(g_{0j} + g_{0jk}\\)) are shown in Figure 16.17B as the distance between the dark, colored dots and the dashed, gray lines. There is some communication decision-making in Figure 16.17. The actual y-values of the dark, colored dots are the random intercept effect plus the treatment-combination modeled mean. Thus, in A, the dark colored dots are equal to “random_intercept_jk” only at the reference level (since the reference model mean is the fixed intercept). Figure 16.17: Random intercepts for model exp5c_m1. The pale, colored dots are the measured glucose uptake values for each donor at each treatment combination. The dashed, gray lines are the modeled means for each treatment combination. (A) The distance from a dark colored dot to the dashed, gray line is the random intercept effect \\(g_{0jk}\\). (B) The distance from a dark colored dot to the dashed, gray line is the combined random intercept effect \\(g_{0j} + g_{0jk}\\). 16.8.8 Alternative models for exp5c exp5c_m1a &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor), data = exp5c) exp5c_m1b &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor) + (1 | donor:treatment) + (1 | donor:activity), data = exp5c) ## boundary (singular) fit: see help(&#39;isSingular&#39;) exp5c_m1c &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor) + (1 | donor:treatment), data = exp5c) exp5c_m1d &lt;- lme(glucose_uptake ~ treatment * activity, random = ~ 1 | donor, correlation = corSymm(form = ~ 1 | donor), weights = varIdent(form = ~ 1|t.by.a), data = exp5c) Notes Four models are fit. The models specify the same fixed effects but different random effects or patterns of correlated error. Again, we care about the fixed effects – this is the point of the experiment – but the specification of the random effects determines the error variance and degrees of freedom for computing uncertainty. Model exp5c_m1a specifies a random intercept for each level of \\(\\texttt{donor}\\). Model exp5c_m1b adds two additional random intercepts to Model exp5c_m1a. The code (1 | donor:treatment) adds an intercept for each combination of \\(\\texttt{donor}\\) and \\(\\texttt{treatment}\\). The code (1 | donor:activity) adds an intercept for each combination of \\(\\texttt{donor}\\) and \\(\\texttt{activity}\\). This model is the equivalent of the univariate model of a repeated measures ANOVA of these data (see Classical (“univariate model”) repeated measures ANOVA below). Model exp5c_m1c adds only one additional random intercept ((1 | donor:treatment)). It is a simplification of Model exp5c_m1c. This is the model reported and explained above. Model exp5c_m1d has the same random effects as exp5c_m1a but models unstructured correlated error and heterogeneity of the error, as described for Example 2: Alternative models for exp6g. This model is equivalent to the multivariate model of a repeated measures ANOVA of these data (see “Multivariate model” repeated measures ANOVA below). lmer returns a message “boundary (singular) fit: see ?isSingular” for exp5c_m1b. The model was fit but the message means that we should be cautious about (or simply avoid) interpreting any inferential statistics (SEs, CIs, p-values). Looking at the estimates of the estimated parameters of the random effects in the table below (the variances of the effects and the covariances among the effects), Model exp5c_m1b estimates zero variance for the \\(\\texttt{donor:activity}\\) random intercept. This suggests that we could simplify exp5c_m1b by removing (1 | donor:activity) from the model – this is exp5c_m1c. Think about this warning. If we fit using repeated measures ANOVA (next section), we won’t get a warning message but we still may be overfitting. VarCorr(exp5c_m1b) ## Groups Name Std.Dev. ## donor:activity (Intercept) 0.000000 ## donor:treatment (Intercept) 0.089519 ## donor (Intercept) 0.352092 ## Residual 0.090686 Planned comparisons of the four alternative models are given in Table 16.12. The effect estimates are the same but inference differs slightly among the models because of how each model partitions error variance to either random effects or the residuals. Table 16.12: Planned contrasts from the four alternative models. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp5c_m1a (Scr EPS) - (Scr Basal) 0.15 0.067 15 0.01 0.29 2.25 0.040 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.067 15 -0.26 0.03 -1.73 0.105 Interaction -0.27 0.095 15 -0.47 -0.06 -2.81 0.013 exp5c_m1b (Scr EPS) - (Scr Basal) 0.15 0.052 10 0.03 0.27 2.88 0.016 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.052 10 -0.23 0.00 -2.22 0.051 Interaction -0.27 0.074 5 -0.46 -0.08 -3.61 0.015 exp5c_m1c (Scr EPS) - (Scr Basal) 0.15 0.052 10 0.03 0.27 2.88 0.016 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.052 10 -0.23 0.00 -2.22 0.051 Interaction -0.27 0.074 10 -0.43 -0.10 -3.61 0.005 exp5c_m1d (Scr EPS) - (Scr Basal) 0.15 0.040 15 0.07 0.24 3.82 0.002 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.063 15 -0.25 0.02 -1.85 0.083 Interaction -0.27 0.083 15 -0.44 -0.09 -3.22 0.006 Table 16.13: AIC of the four alternative models firt to exp5c data Model AIC exp5c_m1a 8.153813 exp5c_m1b 10.052675 exp5c_m1c 8.052675 exp5c_m1d 14.763024 The use of AIC for selecting among models with different random effects was described above. The AICs of the models suggest that models exp5c_m1b and exp5c_m1d are too complex given the data. The AICs of models exp5c_m1a and exp5c_m1c are effectively the same. I reported exp5c_m1c simply to allow me to explain the donor:treatment random effect. 16.8.9 Classical (“univariate model”) repeated measures ANOVA exp5c_aov1 &lt;- aov_4(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c) Table 16.14: Planned contrasts for the linear mixed model exp5c_m1c and the univariate model of the repeated measures ANOVA. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp5c_m1c (Scr EPS) - (Scr Basal) 0.15 0.052 10 0.03 0.27 2.88 0.016 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.052 10 -0.23 0.00 -2.22 0.051 Interaction -0.27 0.074 10 -0.43 -0.10 -3.61 0.005 univariate RM-ANOVA (Scr EPS) - (Scr Basal) 0.15 0.040 5 0.05 0.25 3.82 0.012 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.063 5 -0.28 0.04 -1.85 0.123 Interaction -0.27 0.083 5 -0.48 -0.05 -3.22 0.023 Notes In addition to the assumptions of the linear model outlined in the Violations chapter, the classical (“univariate model”) repeated measures ANOVA assumes sphericity, which is the equality of the variances of all pairwise differences among treatment combinations. The univariate model of the repeated measures ANOVA is equivalent to Model exp5c_m1c. Inference (SEs, CIs, p-values) differs slightly in this here because the estimated variance for the \\(\\texttt{donor:activity}\\) random intercept of Model exp5c_m1c is zero. In cases where all variances are greater than zero and the design is balanced (no block is missing a treatment combination), the two tables would be identical. 16.8.10 “Multivariate model” repeated measures ANOVA of exp5c Table 16.15: Planned contrasts for the linear mixed model exp5c_m1d and the multivariate model of the repeated measures ANOVA. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp5c_m1d (Scr EPS) - (Scr Basal) 0.15 0.040 15 0.07 0.24 3.82 0.002 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.063 15 -0.25 0.02 -1.85 0.083 Interaction -0.27 0.083 15 -0.44 -0.09 -3.22 0.006 multivariate RM-ANOVA (Scr EPS) - (Scr Basal) 0.15 0.040 5 0.05 0.25 3.82 0.012 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.063 5 -0.28 0.04 -1.85 0.123 Interaction -0.27 0.083 5 -0.48 -0.05 -3.22 0.023 Notes The sphericity assumption of the repeated measures ANOVA is relaxed if the model is fit using the “multivariate model”. The multivariate model repeated measures ANOVA is a linear model with a multivariate response and not a linear mixed model. In the multivariate model, each treatment combination is a different response variable and there is a single row for each level of the random factor (\\(\\texttt{donor}\\) in Experiment 5c). The multivariate model is a different way of handling the correlated error that occurs when conceiving of the design as univariate. The multivariate repeated measures ANOVA can be specified using the linear mixed model exp5c_m1d. Compared to the contrasts from the multivariate model of the repeated measures ANOVA above, the SEs of the contrasts are the same but the degrees of freedom differ. Because of the increased df of the linear mixed model, the CIs are narrower and the p-value is smaller – that is the linear mixed model is less conservative than the repeated measures ANOVA. There is no correct degrees of freedom for a linear mixed model like this and emmeans outputs one way to compute these (with options for others, none of which are equivalent to those from the multivariate model RM-ANOVA). 16.8.11 Modeling \\(\\texttt{donor}\\) as a fixed effect \\[ \\begin{equation} \\texttt{glucose_uptake} \\sim \\texttt{treatment * activity + donor} \\end{equation} \\tag{16.5} \\] Model (16.5) (using R formula syntax) is a linear model with the block \\(\\texttt{donor}\\) added as a fixed covariate instead of a random intercept. exp5c_m3 &lt;- lm(glucose_uptake ~ treatment * activity + donor, data = exp5c) The coefficients of the fit model are Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 1.00 0.071 14.0 0.0000 0.85 1.15 treatmentsiNR4A3 0.08 0.067 1.2 0.2453 -0.06 0.22 activityEPS 0.15 0.067 2.2 0.0402 0.01 0.29 donordonor_2 0.02 0.082 0.2 0.8444 -0.16 0.19 donordonor_3 0.16 0.082 1.9 0.0757 -0.02 0.33 donordonor_4 0.81 0.082 9.8 0.0000 0.63 0.98 donordonor_5 0.76 0.082 9.2 0.0000 0.59 0.94 donordonor_6 0.29 0.082 3.5 0.0030 0.12 0.47 treatmentsiNR4A3:activityEPS -0.27 0.095 -2.8 0.0132 -0.47 -0.06 Notes The coefficients include a slope for the five non-reference levels of donor. We typically don’t care about these. Table 16.16: Planned contrasts for the linear mixed model exp5c_m1a and the fixed effect model exp5c_m3. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp5c_m1a (Scr EPS) - (Scr Basal) 0.15 0.067 15 0.01 0.29 2.25 0.040 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.067 15 -0.26 0.03 -1.73 0.105 Interaction -0.27 0.095 15 -0.47 -0.06 -2.81 0.013 exp5c_m3 (Scr EPS) - (Scr Basal) 0.15 0.067 15 0.01 0.29 2.25 0.040 (siNR4A3 EPS) - (siNR4A3 Basal) -0.12 0.067 15 -0.26 0.03 -1.73 0.105 Interaction -0.27 0.095 15 -0.47 -0.06 -2.81 0.013 Notes In a randomized complete block design with only 1 replicate of each treatment combination per block, like that in Experiment 5c, inference about treatment effects is exactly that same between this fixed effect model and the random intercept model exp5c_m1a. The equivalence of inference in the treatment effect between the fixed effect and linear mixed model holds only for balanced randomized complete block designs – where all blocks contain all treatment combinations and the subsampling replicate size is the same for all treatment combinations for all blocks. This means, outside of special cases like Experiment 5c, the choice between adding a blocking variable as a random or fixed factor depends on assumptions about the model. 16.9 Example 4 – Experiments with subsampling replication (exp1g) This example is from a design with batches (independent experiments) that contain all treatment levels of a single factor and subsampling replication. These data were used to introduce linear mixed models in Example 1. The design of the experiment is \\(2 \\times 2\\) factorial. Example 1 flattened the analysis to simplify explanation of random intercepts and random slopes. Here, the data are analyzed with a factorial model. A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity Public source Source figure: Fig. 1g Source data: Source Data Fig. 1 16.9.1 Understand the data The researchers in this paper are interested in discovering mechanisms causing the lower antibody-mediated immune response in males relative to females. The data in Fig. 1 are from a set of experiments on mice to investigate how the G-protein coupled receptor protein GPR174 regulates formation of the B-cell germinal center in secondary lymph tissue. GPR174 is a X-linked gene. Response variable \\(\\texttt{gc}\\) – germinal center size (%). The units are the percent of cells expressing germinal center markers. Factor 1 – \\(\\texttt{sex}\\) (“M”, “F”). Male (“M”) is the reference level. Factor 2 – \\(\\texttt{chromosome}\\) (“Gpr174+”, “Gpr174-”). “Gpr174-” is a GPR174 knockout. The wildtype (“Gpr174+”) condition is the reference level. Design – \\(2 \\times 2\\), that is, two crossed factors each with two levels. This results in four groups, each with a unique combination of the levels from each factor. “M Gpr174+” is the control. “M Gpr174+” is the knockout genotype in males (“knockout added”). “F Gpr174+” is the wildtype female (“X chromosome added”). “F Gpr174-” is the knockout female (“knockout and X chromosome added”. 16.9.2 Examine the data ggplot(data = exp1g, aes(x = treatment, y = gc, color = experiment_id)) + geom_point(position = position_dodge(0.4)) 16.9.3 Fit the model # three slope parameters exp1g_m1a &lt;- lmer(gc ~ genotype * sex + (genotype * sex | experiment_id), data = exp1g) VarCorr(exp1g_m1a) # looks fine ## Groups Name Std.Dev. Corr ## experiment_id (Intercept) 1.77082 ## genotypeGpr174- 0.96192 -0.023 ## sexF 2.90154 0.459 0.878 ## genotypeGpr174-:sexF 1.33169 -0.314 -0.625 -0.706 ## Residual 1.93155 # one slope parameter but capturing all treatment combinations exp1g_m1b &lt;- lmer(gc ~ genotype * sex + (treatment | experiment_id), data = exp1g) # intercept interactions exp1g_m1c &lt;- lmer(gc ~ genotype * sex + (1 | experiment_id) + (1 | experiment_id:genotype) + (1 | experiment_id:sex) + (1 | experiment_id:genotype:sex), data = exp1g) VarCorr(exp1g_m1c) # id:genotype is low ## Groups Name Std.Dev. ## experiment_id:genotype:sex (Intercept) 0.4046670 ## experiment_id:sex (Intercept) 1.6927499 ## experiment_id:genotype (Intercept) 0.0001216 ## experiment_id (Intercept) 2.5487069 ## Residual 1.9792428 # drop id:genotype which has low variance exp1g_m1d &lt;- lmer(gc ~ genotype * sex + (1 | experiment_id) + (1 | experiment_id:sex) + (1 | experiment_id:sex:genotype), data = exp1g) AIC(exp1g_m1a, exp1g_m1b, exp1g_m1c, exp1g_m1d) ## df AIC ## exp1g_m1a 15 344.1579 ## exp1g_m1b 15 344.1529 ## exp1g_m1c 9 337.7602 ## exp1g_m1d 8 335.7602 # go with exp1g_m1d. exp1g_m1 &lt;- exp1g_m1d 16.9.4 Inference from the model exp1g_m1_coef &lt;- coef(summary(exp1g_m1)) exp1g_m1_coef ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 8.435718 1.612602 4.638595 5.231123 0.004202015 ## genotypeGpr174- 2.568365 0.712122 5.760582 3.606637 0.012094327 ## sexF 5.583860 1.397388 4.024010 3.995927 0.015995349 ## genotypeGpr174-:sexF -5.304752 1.013552 5.918159 -5.233823 0.002033657 # order of factors reversed in specs because I want sex to be # main x-axis variable in plot exp1g_m1_emm &lt;- emmeans(exp1g_m1, specs = c(&quot;sex&quot;, &quot;genotype&quot;)) # exp1g_m1_emm # print in console to get row numbers # set the mean as the row number from the emmeans table wt_m &lt;- c(1,0,0,0) wt_f &lt;- c(0,1,0,0) ko_m &lt;- c(0,0,1,0) ko_f &lt;- c(0,0,0,1) # simple effects within males and females + interaction # 1. (ko_m - wt_m) # 2. (ko_f - wt_f) exp1g_contrasts &lt;- list( &quot;(Gpr174- M) - (Gpr174+ M)&quot; = c(ko_m - wt_m), &quot;(Gpr174- F) - (Gpr174+ F)&quot; = c(ko_f - wt_f), &quot;Interaction&quot; = c(ko_f - wt_f) - c(ko_m - wt_m) ) exp1g_m1_planned &lt;- contrast(exp1g_m1_emm, method = exp1g_contrasts, adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast estimate SE df lower.CL upper.CL t.ratio p.value (Gpr174- M) - (Gpr174+ M) 2.57 0.714 5.784 0.81 4.33 3.60 0.012 (Gpr174- F) - (Gpr174+ F) -2.74 0.722 6.100 -4.50 -0.98 -3.79 0.009 Interaction -5.30 1.016 5.942 -7.80 -2.81 -5.22 0.002 Notes The direction of the estimated effect is opposite in males and females 16.9.5 Plot the model Figure 16.18: Treatment effect on germinal center (GC) formation. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means. 16.9.6 Alternaplot the model Figure 16.19: Germinal center (GC) formation in response to treatment. Small, pale, colored dots are independent experiments. Intermediate size colored dots are experiment means. Dashed gray line is expected additive mean of Gpr174- F 16.9.7 Understanding the alternative models exp1g_m1a &lt;- lmer(gc ~ genotype * sex + (genotype * sex | experiment_id), data = exp1g) exp1g_m1b &lt;- lmer(gc ~ genotype * sex + (treatment | experiment_id), data = exp1g) exp1g_m1c &lt;- lmer(gc ~ genotype * sex + (1 | experiment_id) + (1 | experiment_id:genotype) + (1 | experiment_id:sex) + (1 | experiment_id:genotype:sex), data = exp1g) exp1g_m1d &lt;- lmer(gc ~ genotype * sex + (1 | experiment_id) + (1 | experiment_id:sex) + (1 | experiment_id:genotype:sex), data = exp1g) Notes All four models model the same fixed effects. The models differ only in how they model the random effects. Model exp1g_m1a fits one random intercept, two random slopes, and one random interaction. \\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment_id}\\) on the intercept \\(\\gamma_{1j}\\) – a random slope modeling the effect of the non-reference level of “Gpr174-” on the batch effect of \\(\\texttt{experiment_id}\\). This is a \\(\\texttt{experiment_id} \\times \\texttt{genotype}\\) interaction. \\(\\gamma_{2j}\\) – a random slope modeling the effect of “F” (female) on the batch effect of \\(\\texttt{experiment_id}\\). This is a \\(\\texttt{experiment_id} \\times \\texttt{sex}\\) interaction. \\(\\gamma_{3j}\\) – a random slope modeling the effect of the interaction effect of “Gpr174-” and “F” on the batch effect of \\(\\texttt{experiment_id}\\). This is a \\(\\texttt{experiment_id} \\times \\texttt{sex} \\times \\texttt{genotype}\\) interaction. Model exp1g_m1b fits one random intercept and three random slopes \\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment_id}\\) on the intercept. This is modeling the same thing as \\(\\gamma_{0j}\\) in Model exp1g_m1a. \\(\\gamma_{1j}\\) – a random slope modeling the effect of “Gpr174- M” on the batch effect of \\(\\texttt{experiment_id}\\). This is modeling the same thing as \\(\\gamma_{1j}\\) in Model exp1g_m1a. \\(\\gamma_{2j}\\) – a random slope modeling the effect of “Gpr174+ F” on the batch effect of \\(\\texttt{experiment_id}\\). This is modeling the same thing as \\(\\gamma_{2j}\\) in Model exp1g_m1a. \\(\\gamma_{3j}\\) – a random slope modeling the effect of “M Gpr174- F” on the batch effect of \\(\\texttt{experiment_id}\\). This is modeling the added variance accounted for by the random interaction \\(\\gamma_{2j}\\) in Model exp1g_m1a but in a different way. Model exp1g_m1c fits four random intercepts \\(\\gamma_{0j}\\) – a random intercept modeling batch effects of \\(\\texttt{experiment_id}\\) on the intercept. This is modeling the same thing as \\(\\gamma_{0j}\\) in Model exp1g_m1a. \\(\\gamma_{0jk}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment_id}\\) and \\(\\texttt{genotype}\\). This is very similar to the variance modeled by \\(\\gamma_{1j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jk}\\) are independent (uncorrelated) of draws from the other random intercepts. \\(\\gamma_{0jl}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment_id}\\) and \\(\\texttt{sex}\\). This is very similar to the variance modeled by \\(\\gamma_{2j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jl}\\) are independent (uncorrelated) of draws from the other random intercepts (review The correlation among random intercepts and slopes if this doesn’t make sense). \\(\\gamma_{0jkl}\\) – a random intercept modeling the effects of the combination of \\(\\texttt{experiment_id}\\), \\(\\texttt{genotype}\\) and \\(\\texttt{sex}\\). This is very similar to the variance modeled by \\(\\gamma_{3j}\\) in Model exp1g_m1a except the draws from \\(\\gamma_{0jkl}\\) are independent (uncorrelated) of draws from the other random intercepts. Model exp1g_m1d fits the same random intercepts as Model exp1g_m1c but excludes \\(\\gamma_{0jl}\\) (the random intercept for the experiment_id by genotyp combination. This was excluded because of the low variance of this component in the fit model. 16.9.8 The VarCorr matrix of models exp1g_m1a and exp1g_m1b The random effect similarity of models exp1g_m1a and exp1g_m1b can be seen in the estimated variance components and correlations among the random effects. Table 16.17: The Varcorr matrix. Standard deviations of random effects on the diagonal. Correlations of random effects on the off-diagonal. exp1g_m1a (Intercept) 1.7708 genotypeGpr174- -0.0232 0.9619 sexF 0.4589 0.8776 2.9015 genotypeGpr174-:sexF -0.3138 -0.6246 -0.7056 1.3317 exp1g_m1b (Intercept) 1.7710 treatmentGpr174- M -0.0251 0.9640 treatmentGpr174+ F 0.4585 0.8761 2.9022 treatmentGpr174- F 0.2999 0.8791 0.9384 2.9905 16.9.9 The linear mixed model has more precision and power than the fixed effect model of batch means # means pooling model exp1g_m2 &lt;- lm(gc ~ sex * genotype, data = exp1g_means) Table 16.18: Planned contrasts for the linear mixed model exp1g_m1 and the fixed effects model of experiment means exp1g_m2. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp1g_m1 (lmm) (Gpr174- M) - (Gpr174+ M) 2.57 0.714 5.8 0.81 4.33 3.60 0.012 (Gpr174- F) - (Gpr174+ F) -2.74 0.722 6.1 -4.50 -0.98 -3.79 0.009 Interaction -5.30 1.016 5.9 -7.80 -2.81 -5.22 0.002 exp1g_m2 (lm means pooling) (Gpr174- M) - (Gpr174+ M) 2.50 2.257 12.0 -2.41 7.42 1.11 0.289 (Gpr174- F) - (Gpr174+ F) -2.73 2.257 12.0 -7.65 2.18 -1.21 0.249 Interaction -5.24 3.192 12.0 -12.20 1.72 -1.64 0.127 Notes A fixed effects model fit to batch-means pooled data is strongly conservative and will result in less discovery. Means pooling does not make the data independent in a randomized complete block design. A linear mixed model of batch-means pooled data is a mixed-effect ANOVA (Next section. Also see Section 13.1.1 in the Issues chapter). 16.9.10 Fixed effect models and pseudoreplication # complete pooling model exp1g_m3 &lt;- lm(gc ~ sex * genotype, data = exp1g) (#tab:lmm-exp1g_m3-compare)Planned contrasts for the linear mixed model exp1g_m1 and the fixed effects model exp1g_m3 with complete pooling. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp1g_m1 (lmm) (Gpr174- M) - (Gpr174+ M) 2.57 0.714 5.8 0.81 4.33 3.60 0.012 (Gpr174- F) - (Gpr174+ F) -2.74 0.722 6.1 -4.50 -0.98 -3.79 0.009 Interaction -5.30 1.016 5.9 -7.80 -2.81 -5.22 0.002 exp1g_m3 (lm complete pooling) (Gpr174- M) - (Gpr174+ M) 2.44 1.111 69.0 0.22 4.65 2.20 0.032 (Gpr174- F) - (Gpr174+ F) -2.53 1.125 69.0 -4.77 -0.28 -2.25 0.028 Interaction -4.97 1.581 69.0 -8.12 -1.81 -3.14 0.002 Notes Complete pooling is strongly anti-conservative and will result in increased false discovery. Complete pooling is a type of pseudoreplication – the subsamples have been analyzed as if they are independent replicates. Subsamples are not independent. For the Experiment 1g data, the 95% confidence intervals are wider and the p-values are larger in the complete pool model exp1g_m3 compared to the linear mixed model exp1g_m1. This is an unusual result. 16.9.11 Mixed-effect ANOVA exp1g_m1_aov &lt;- aov_4(gc ~ sex * genotype + (sex * genotype | experiment_id), data = exp1g) Notes The formula has the same format as that in Example 3 for repeated measures ANOVA on RCB designs with no subsampling. In biology, this is often called a mixed effect ANOVA with two fixed factors and one random factor. The data are aggregated prior to fitting the model – this means the subsamples are averaged within each batch by treatment combination. The mixed-effect ANOVA is equivalent to the linear mixed model exp1g_m1c if there are the same number of replicates in each treatment combination and the same number of subsamples in all treatment by \\(\\texttt{experiment_id}\\) combinations. The design is not balanced in this example. (#tab:lmm-exp1g_m1_aov-pairs)Planned contrasts from mixed ANOVA compared to lmm equivalent of mixed ANOVA and lowest AIC lmm. contrast estimate SE df lower.CL upper.CL t.ratio p.value exp1g_m1_aov (mixed ANOVA) (Gpr174- M) - (Gpr174+ M) 2.50 0.599 3.000 0.60 4.41 4.18 0.025 (Gpr174- F) - (Gpr174+ F) -2.73 0.808 3.000 -5.30 -0.16 -3.39 0.043 Interaction -5.24 1.087 3.000 -8.70 -1.78 -4.82 0.017 exp1g_m1_c (lmm equivalent of mixed ANOVA) (Gpr174- M) - (Gpr174+ M) 2.57 0.715 5.784 0.80 4.33 3.59 0.012 (Gpr174- F) - (Gpr174+ F) -2.74 0.724 6.100 -4.50 -0.97 -3.78 0.009 Interaction -5.30 1.017 2.972 -8.56 -2.05 -5.21 0.014 exp1g_md (lmm with min AIC) (Gpr174- M) - (Gpr174+ M) 2.57 0.714 5.784 0.81 4.33 3.60 0.012 (Gpr174- F) - (Gpr174+ F) -2.74 0.722 6.100 -4.50 -0.98 -3.79 0.009 Interaction -5.30 1.016 5.942 -7.80 -2.81 -5.22 0.002 16.10 Statistical models for experimental designs 16.10.1 Models for Completely Randomized Designs (CRD) lm0 &lt;- lm(y ~ treatment, data = figx) 16.10.2 Models for batched data (CRDS, RCBD, RSPD, GRCBD, NRCBD) 16.10.2.1 Linear models 16.10.2.1.1 fixed effect model lm1 &lt;- lm(y ~ treatment + block, data = figx) 16.10.2.1.2 linear model of batch means figx_means &lt;- figx[, .(y = mean(y)), by = .(treatment, batch)] lm2 &lt;- lm(y ~ treatment, data = figx_means) 16.10.2.2 linear mixed models (“random effects” models) 16.10.2.2.1 Random intercept model lmm1 &lt;- lmer(y ~ treatment + (1 | block), data = figx) 16.10.2.2.2 lmm for correlated error lmm2 &lt;- lme(y ~ treatment, random = ~1 | block, correlation = corSymm(form = ~ 1 | block), weights = varIdent(form = ~ 1 | treatment), data = figx) 16.10.2.2.3 random intercept and slope model lmm3 &lt;- lmer(y ~ treatment + (treatment | block), data = figx) 16.10.2.2.4 random interaction intercept model lmm4 &lt;- lmer(y ~ treatment + (1 | block) + (1 | block:treatment), data = figx) 16.10.2.2.5 random interaction model with subsampling lmm5 &lt;- lmer(y ~ treatment + (1 | block) + (1 | block:treatment) + (1 | block:treatment:replicate), data = figx) 16.10.2.2.6 CRD split plot model # tr1 is the main plot # tr2 is the subplot # main_plot is tr1:rep, where rep is the rep id lmm6 &lt;- lmer(y ~ tr1 * tr2 + (1 | main_plot), data = figx) 16.10.2.2.7 RCBD split plot model # tr1 is the main plot # tr2 is the subplot # main_plot is tr1:block lmm7 &lt;- lmer(y ~ tr1 * tr2 + (1 | block) + (1 | block:tr1), data = figx) lmm7 &lt;- lmer(y ~ tr1 * tr2 + block + (1 | block:tr1), data = figx) # the two versions are numerically equivalent 16.10.2.3 ANOVA models (“mixed models”, “repeated measures ANOVA”) 16.10.2.3.1 Multivariate repeated measures ANOVA aov1 &lt;- aov_4(y ~ treatment + (treatment | block), data = figx) 16.10.2.3.2 Univariate repeated measures ANOVA aov2 &lt;- aov_4(y ~ treatment + (treatment | block), include_aov = TRUE, data = figx) 16.10.2.3.3 Pairwise paired, t-tests pptt &lt;- pairwise_t_tests(y_col = &quot;y&quot;, g_col = &quot;treatment&quot;, id_col = &quot;block&quot;, data = figx) 16.11 Which model and why? 16.11.1 CRD No batch to model! 16.11.2 CRDS lmm1 – random intercept model lm2 – linear model of batch means Notes These are numerically equivalent and should result in same estimates, SE, CIs, and p-values These are equivalent to a Nested t-test or Nested ANOVA 16.11.3 RCBD Reasonable models: lmm1 – random intercept model lmm2 – lmm for correlated error lm1 – fixed effect model aov1 – multivariate RM-ANOVA aov2 – univariate RM-ANOVA pptt – pairwise, paired t-test (Cannot use lmm3 or lmm4 because there is no replication of each block:treatment combination.) Assumptions: lmm1, lm1, and aov2 assume compound symmetry. sphericity. lmm2, aov1, and pptt do not assume compound symmetry and sphericity. Notes: lmm1 is the standard. If the number of treatments = 2, then lmm1 is eqivalent with a paired t-test. If the design is balanced (all blocks have a single value for both treatments) AND the number of treatments = 2 then all six methods are numerically equivalent. If the design is balanced AND the number of treatments &gt; 2, then lmm1, lm1, and aov2 are numerically equivalent lmm2, aov1, pptt result in the same estimates and SE but lmm2 has more df, so the CIs, and p-values of lmm2 are less conservative. If the number of treatments = 2 AND the design is not balanced (at least one block is missing value for one treatment level) then the linear models (lm1, aov1, aov2, pptt) are equivalent but the linear mixed models (lmm1, lmm2) differ from each other and from the linear models. If a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power (the loss of power depends partly on the number of missing values). If a treatment within a block is missing, the block is deleted only in the comparisons including the missing treatment in the pairwise, paired t-tests. This makes pptt more powerful than aov1. the lmm models are very flexible – covariates can be added or these can be used as generalized lmms for non-normal distributions. the lmm models and especially lmm2 sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if the among-block component of variance is small. Performance: Table 16.19: Type I error rate the RCBD statistical models under different levels of correlation structure among the observations. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text. Contrast Cor Error lm0 lm1 aov1 aov2 pptt lmm2 sim 1 - high correlated error Cn, Tr1 0.299 0.047 0.110 0.058 0.110 0.058 0.068 Cn, Tr2 0.412 0.027 0.090 0.052 0.090 0.052 0.070 Tr1, Tr2 0.662 0.000 0.005 0.063 0.005 0.063 0.070 sim 2 - low correlated error Cn, Tr1 0.165 0.044 0.072 0.055 0.072 0.055 0.063 Cn, Tr2 0.164 0.051 0.077 0.060 0.077 0.060 0.076 Tr1, Tr2 0.238 0.016 0.028 0.062 0.028 0.062 0.079 sim 3 - equal correlated error Cn, Tr1 0.771 0.001 0.057 0.058 0.057 0.058 0.069 Cn, Tr2 0.773 0.001 0.049 0.056 0.049 0.056 0.067 Tr1, Tr2 0.769 0.002 0.056 0.056 0.056 0.056 0.074 sim 4 - zero correlated error Cn, Tr1 0.025 0.050 0.047 0.042 0.047 0.042 0.061 Cn, Tr2 0.010 0.054 0.054 0.046 0.054 0.046 0.061 Tr1, Tr2 0.000 0.056 0.060 0.053 0.060 0.053 0.076 aov1, pptt, and lmm2 have well-behaved type I error rates (lmm2 is a little anti-conservative) regardless of correlation structure – False discoveries should be well controlled. lm1, lmm1, and aov2 have poorly-behaved type I error rates when there is a block:treatment interaction (sim 1 and 2), resulting in heterogenous correlation structure (remember that this is moot when there are only two treatment levels) – this consequence is small when the block and block:treatment variance is small (small correlations in residuals). When there is block:treatment interaction, these models are conservative in contrasts associated with large residual correlation, resulting in low power, and anti-conservative in contrasts associated with small residual correlations. resulting in high false discovery rates. Best Practices: If the number of treatments = 2 and the design is balanced: it doesn’t matter which method you use. the design is not balanced: use lmm1 or lmm2 If the number of treatments &gt; 2 and the design is balanced: use aov1/pptt (these are numerically equivalent). lmm1 is the standard but it assumes equal correlated error among treatment levels and equal standard errors among contrasts. lmm2 explicitly models heterogeneity of correlations and variances but 1) the model often fails and 2) has slightly anti-conservative Type I error. the design is not balanced: lmm2 is attractive if the model runs. pptt is attractive if there are few missing values. If there are covariates, then start with lmm2. Use lmeControl if lmm2 fails. If this fails, use pairwise lmm1, which is the same as pairwise t-tests but allows covariates to be added to the model. Note that the covariate will be modeled independently in each pair, which effectively models the consequence of a treatment:covariate interaction. 16.11.4 RSPD Reasonable models: lmm4 – random interaction intercept model Assumptions: Notes: Performance: Table 16.20: Type I error rate for the RSPD statistical models under different models of random variance. The design is 2 (main plot: WT, KO) x 3 (subplot: Cn, Tr1, Tr2) with ten blocks. All four simulations have a component of variance due to block. Sim 1 includes a block:main plot component, Sim 2 includes a block:sub plot component, Sim 3 includes both block:main and block:sub components. Type I error rates for the 9 simple effect contrasts were averaged within the two sets: contrast = main is the aggregate of the single main plot contrast (KO - WT) at each level of the subplot factor. contrast = sub is the aggregate of the three subplot contrasts (Tr1 - Cn, Tr2 - Cn, Tr2 - Tr1) at each level of the main plot factor. The Cor Error column is the average correlated error of the residuals fit by a simple linear model y ~ treatment. Model abbreviations as in text. sim_id sig_block sig_ss sig_main sig_sub contrast r lm0 lm1 lmm1 lmm3 lmm4 lmm5 lmm6 sim 1 - block:main plot 1 0.8 0.5 0.6 0.0 main 0.494 0.010 0.199 0.087 0.051 0.052 0.083 0.054 1 0.8 0.5 0.6 0.0 sub 0.783 0.000 0.052 0.011 0.052 0.052 0.011 0.052 sim 2 - block:sub plot 2 0.8 0.5 0.0 0.6 main 0.782 0.000 0.002 0.003 0.002 0.003 0.042 0.041 2 0.8 0.5 0.0 0.6 sub 0.497 0.008 0.053 0.066 0.068 0.065 0.050 0.053 sim 3 - block:main + block:sub 3 0.8 0.5 0.6 0.4 main 0.549 0.007 0.111 0.062 0.038 0.039 0.068 0.054 3 0.8 0.5 0.6 0.4 sub 0.689 0.001 0.056 0.026 0.057 0.056 0.024 0.054 sim 4 - block only 4 0.8 0.5 0.0 0.0 main 0.707 0.002 0.049 0.049 0.044 0.045 0.056 0.054 4 0.8 0.5 0.0 0.0 sub 0.704 0.002 0.057 0.055 0.061 0.060 0.052 0.059 16.11.5 RCBDS lmm3 – random intercept and slopes model lmm4 – random interaction intercept model lmm1 – random intercept model on batch means lmm2 – lmm for correlated error on batch means aov1 – multivariate RM-ANOVA aov2 – univariate RM-ANOVA pptt – pairwise, paired t-test Notes lmm3 is the “maximal model” – it fits the most parameters and has the fewest assumptions. lmm1 on the full data is the minimal model. I’m not recommending this at all as this is pseudoreplication and will result in very anti-conservative inference. lmm4 is less conservative than lmm3 (and makes more assumptions). lmm1 and lmm2 here are the models for RCBD designs but using the aggregated data - that is the subsampled replicates within a batch (block:treatment combination) are averaged. If design is balanced lmm3 and aov1 result in same estimates, SE, CIs, and p-values lmm4, lmm1, and aov2 result in same estimates, SE, CIs, and p-values lmm2 has same estimates and SE as lmm3/aov1 but more df so less conservative If a treatment within a block is missing, the whole block is deleted in the RM-ANOVA models. This reduces power the lmm models are very flexible – covariates can be added or these can be used as generalized lmms for non-normal distributions. the lmm models, and especially lmm2 and lmm3, sometimes (often?) fail to converge, especially with small samples (small number of blocks) or if within block correlation is small lmm1, lmm4, and aov2 will be less conservative (more power but at higher type I error/false positive rate) Best practices If balanced and no covariates then use lmm3/lmm2/aov1 if discovery is expensive (want to avoid false positives) or lmm4/aov2/lmm1 if discovery is cheap (can afford false positives). lmm2 is less conservative than lmm3/aov1. Use lmm3 if interested in the variance at different levels and/or covariance structure. If unbalanced, or if there are covariates, then start with lmm3 or lmm2. Use lmeControl if lmm2 fails. If these fail, go to 1. 16.11.6 GRCBD lmm3 – random intercept and slopes model lmm4 – random interaction intercept model lmm1 – random intercept model on block:treatment means lmm2 – lmm for correlated error on block:treatment means aov1 – multivariate RM-ANOVA aov2 – univariate RM-ANOVA lm1 – fixed effect model on all data pptt – pairwise, paired t-test on block:treatment means 16.11.7 GRCBDS lmm3 – random intercept and slopes model lmm4 – random interaction intercept model lmm1 – random intercept model on block:treatment means lmm2 – lmm for correlated error on block:treatment means aov1 – multivariate RM-ANOVA aov2 – univariate RM-ANOVA lm1 – fixed effect model on all block:treatment:rep means pptt – pairwise, paired t-test on block:treatment:rep means 16.12 Working in R 16.12.1 Plotting models fit to batched data 16.12.1.1 Models without subsampling - Experiment 6g Data wrangling necessary for plot: # convert contrast table to a data.table exp6g_m1_planned_dt &lt;- data.table(exp6g_m1_planned) # create a pretty p-value column exp6g_m1_planned_dt[, pretty_p := pvalString(p.value)] # add group1 and group2 columns to exp1g_m1_planned exp6g_m1_planned_dt[, group1 := c(&quot;Saline&quot;, &quot;CNO&quot;, &quot;Post_CNO&quot;)] exp6g_m1_planned_dt[, group2 := c(&quot;Lesion&quot;, &quot;Saline&quot;, &quot;CNO&quot;)] Experiments are colored: gg1 &lt;- ggplot(data = exp6g, aes(x = treatment, y = touch, color = mouse_id)) + geom_point(position = position_dodge(width = 0.2)) + geom_line(aes(group = mouse_id), position = position_dodge(width = 0.2), color = &quot;gray80&quot;) + geom_point(data = summary(exp6g_m1_emm), aes(y = emmean), color = &quot;black&quot;, size = 3) + geom_errorbar(data = summary(exp6g_m1_emm), aes(y = emmean, ymin = lower.CL, ymax = upper.CL), color = &quot;black&quot;, width = .05) + # pvalue brackets stat_pvalue_manual(exp6g_m1_planned_dt, label = &quot;pretty_p&quot;, y.position = c(99,97,95), size = 2.5, tip.length = 0.01) + ylab(&quot;Percent ipsilateral touch&quot;) + scale_color_manual(values = pal_okabe_ito_blue) + theme_pubr() + theme( axis.title.x = element_blank(), # no x-axis title legend.position = &quot;none&quot; ) + NULL Treatments are colored: gg2 &lt;- ggplot(data = exp6g, aes(x = treatment, y = touch)) + geom_point(aes(group = mouse_id), position = position_dodge(width = 0.2), color = &quot;gray&quot;) + geom_line(aes(group = mouse_id), position = position_dodge(width = 0.2), color = &quot;gray80&quot;) + geom_point(data = summary(exp6g_m1_emm), aes(y = emmean, color = treatment), size = 3) + geom_errorbar(data = summary(exp6g_m1_emm), aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = treatment), width = .05) + # pvalue brackets stat_pvalue_manual(exp6g_m1_planned_dt, label = &quot;pretty_p&quot;, y.position = c(99,97,95), size = 2.5, tip.length = 0.01) + ylab(&quot;Percent ipsilateral touch&quot;) + scale_color_manual(values = pal_okabe_ito_blue) + theme_pubr() + theme( axis.title.x = element_blank(), # no x-axis title legend.position = &quot;none&quot; ) + NULL #gg2 16.12.1.2 Models with subsampling - Experiment 1g Data wrangling necessary for plot: # add treatment column to emmeans table exp1g_m1_emm_dt &lt;- summary(exp1g_m1_emm) %&gt;% data.table exp1g_m1_emm_dt[, treatment := paste(genotype, sex)] exp1g_m1_emm_dt[, treatment := factor(treatment, levels = levels(exp1g$treatment))] # create table of means for each treatment * experiment_id combination exp1g[, group_mean := predict(exp1g_m1)] exp1g_m1_emm2 &lt;- exp1g[, .(group_mean = mean(group_mean)), by = .(treatment, experiment_id)] # add group1 and group2 columns to exp1g_m1_planned exp1g_m1_planned_dt &lt;- data.table(exp1g_m1_planned) exp1g_m1_planned_dt[, pretty_p := pvalString(p.value)] exp1g_m1_planned_dt[, group1 := c(&quot;Gpr174- M&quot;, &quot;Gpr174- F&quot;, &quot;&quot;)] exp1g_m1_planned_dt[, group2 := c(&quot;Gpr174+ M&quot;, &quot;Gpr174+ F&quot;, &quot;&quot;)] # get coefficients of model b &lt;- exp1g_m1_coef[, &quot;Estimate&quot;] # get interaction p p_ixn &lt;- exp1g_m1_planned_dt[contrast == &quot;Interaction&quot;, pretty_p] exp1g_plot_a &lt;- ggplot(data = exp1g, aes(x = treatment, y = gc, color = experiment_id)) + # modeled experiment by treatment means geom_point(data = exp1g_m1_emm2, aes(y = group_mean, color = experiment_id), position = position_dodge(width = 0.4), alpha = 1, size = 2) + geom_line(data = exp1g_m1_emm2, aes(y = group_mean, group = experiment_id, color = experiment_id), position = position_dodge(width = 0.4)) + # raw data geom_point(position = position_dodge(width = 0.4), alpha = 0.3 ) + # modeled treatment means geom_point(data = exp1g_m1_emm_dt, aes(y = emmean), color = &quot;black&quot;, size = 3) + # geom_errorbar(data = exp1g_m1_emm_dt, # aes(y = emmean, # ymin = lower.CL, # ymax = upper.CL), # color = &quot;black&quot;, # width = .05) + # pvalue brackets stat_pvalue_manual(exp1g_m1_planned_dt[1:2], label = &quot;pretty_p&quot;, y.position = c(20,20), size = 2.5, tip.length = 0.01) + # additive line + interaction p bracket geom_segment(x = 3.85, y = b[1] + b[2] + b[3], xend = 4.15, yend = b[1] + b[2] + b[3], linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + geom_bracket( x = 4.2, y = b[1] + b[2] + b[3], yend = b[1] + b[2] + b[3] + b[4], label = paste0(&quot;interaction\\np = &quot;, p_ixn), text.size = 3, text.hjust = 0, color = &quot;black&quot;) + ylab(&quot;GC (%)&quot;) + scale_color_manual(values = pal_okabe_ito_blue) + theme_pubr() + coord_cartesian(xlim = c(1, 4.1)) + theme( axis.title.x = element_blank(), # no x-axis title legend.position = &quot;none&quot; ) + NULL exp1g_plot_a &lt;- factor_wrap(exp1g_plot_a) #exp1g_plot_a # get coefficients of model b &lt;- exp1g_m1_coef[, &quot;Estimate&quot;] # get interaction p p_ixn &lt;- exp1g_m1_planned_dt[contrast == &quot;Interaction&quot;, pretty_p] dodge_width = 0.6 exp1g_plot_b &lt;- ggplot(data = exp1g, aes(x = treatment, y = gc, group = experiment_id)) + # modeled experiment by treatment means geom_point(data = exp1g_m1_emm2, aes(y = group_mean, color = experiment_id), position = position_dodge(width = dodge_width), alpha = 1, size = 2) + geom_line(data = exp1g_m1_emm2, aes(y = group_mean, group = experiment_id, color = experiment_id), position = position_dodge(width = dodge_width)) + # raw data geom_point(position = position_dodge(width = dodge_width), color = &quot;gray80&quot; ) + # modeled treatment means geom_point(data = exp1g_m1_emm_dt, aes(y = emmean), color = &quot;black&quot;, size = 3) + # geom_errorbar(data = exp1g_m1_emm_dt, # aes(y = emmean, # ymin = lower.CL, # ymax = upper.CL), # color = &quot;black&quot;, # width = .05) + # pvalue brackets stat_pvalue_manual(exp1g_m1_planned_dt[1:2], label = &quot;pretty_p&quot;, y.position = c(20,20), size = 2.5, tip.length = 0.01) + # additive line + interaction p bracket geom_segment(x = 4 - dodge_width/2, y = b[1] + b[2] + b[3], xend = 4 + dodge_width/2, yend = b[1] + b[2] + b[3], linetype = &quot;dashed&quot;, color = &quot;gray&quot;) + geom_bracket( x = 4 + dodge_width/1.9, y = b[1] + b[2] + b[3], yend = b[1] + b[2] + b[3] + b[4], label = paste0(&quot;interaction\\np = &quot;, p_ixn), text.size = 3, text.hjust = 0, color = &quot;black&quot;) + ylab(&quot;GC (%)&quot;) + scale_color_manual(values = pal_okabe_ito_blue) + theme_pubr() + coord_cartesian(xlim = c(1, 4.2)) + theme( axis.title.x = element_blank(), # no x-axis title legend.position = &quot;none&quot; ) + NULL exp1g_plot_b &lt;- factor_wrap(exp1g_plot_b) # exp1g_plot_b 16.12.2 Repeated measures ANOVA (randomized complete block with no subsampling) # this is the rm-ANOVA m1 &lt;- aov_4(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c) # lmm equivalent m2 &lt;- lmer(glucose_uptake ~ treatment * activity + (1 | donor) + (1 | donor:treatment) + (1 | donor:activity), data = exp5c) # random intercept and slope model that is *not* equivalent # this isn&#39;t solvable because there is no subsampling m3 &lt;- lmer(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c) Notes afex computes the repeated measures anova model using both aov (classical univariate repeated measures ANOVA) and using lm with multiple response variables (the multivariate repeated measures ANOVA). As of this writing, the output from aov is the default. Given only one measure for each donor within a \\(\\texttt{treatment} \\times \\texttt{activity}\\) combination, the linear mixed model m2 is equivalent to the univariate repeated measures model m1. The model formula in m1 looks like that in the linear mixed model m3 but the two models are not equivalent. 16.12.2.1 univariate vs. multivariate repeated measures ANOVA Use the multivariate model unless you want to replicate the result of someone who used a univariate model. By default, aov_4 computes only the multivariate model (prior to version xxx, the default was to compute both models). # default mode -- should be multivariate exp5c_aov &lt;- aov_4(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c) # force aov_4 to compute univariate model exp5c_aov1 &lt;- aov_4(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c, include_aov = TRUE) # explicitly exclude the univariate model exp5c_aov2 &lt;- aov_4(glucose_uptake ~ treatment * activity + (treatment * activity | donor), data = exp5c, include_aov = FALSE) Notes The include_aov = TRUE argument forces output from aov_4 to include the univariate model. contrasts from multivariate model # These three should give equivalent results # exp5c_aov was fit with the default -- multivariate model only emmeans(exp5c_aov, specs = c(&quot;treatment&quot;, &quot;activity&quot;)) %&gt;% contrast(method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) ## activity treatment contrast estimate SE df t.ratio p.value ## Basal . siNR4A3 - Scr 0.0813 0.1012 5 0.804 0.4581 ## EPS . siNR4A3 - Scr -0.1858 0.0359 5 -5.180 0.0035 ## . Scr EPS - Basal 0.1510 0.0395 5 3.821 0.0124 ## . siNR4A3 EPS - Basal -0.1161 0.0626 5 -1.855 0.1228 # exp5c_aov included the univariate model but the default is still the multivariate model emmeans(exp5c_aov1, specs = c(&quot;treatment&quot;, &quot;activity&quot;)) %&gt;% contrast(method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) ## activity treatment contrast estimate SE df t.ratio p.value ## Basal . siNR4A3 - Scr 0.0813 0.1012 5 0.804 0.4581 ## EPS . siNR4A3 - Scr -0.1858 0.0359 5 -5.180 0.0035 ## . Scr EPS - Basal 0.1510 0.0395 5 3.821 0.0124 ## . siNR4A3 EPS - Basal -0.1161 0.0626 5 -1.855 0.1228 # passing `model = &quot;multivariate&quot;` makes the model choice transparent emmeans(exp5c_aov1, specs = c(&quot;treatment&quot;, &quot;activity&quot;), model = &quot;multivariate&quot;) %&gt;% contrast(method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) ## activity treatment contrast estimate SE df t.ratio p.value ## Basal . siNR4A3 - Scr 0.0813 0.1012 5 0.804 0.4581 ## EPS . siNR4A3 - Scr -0.1858 0.0359 5 -5.180 0.0035 ## . Scr EPS - Basal 0.1510 0.0395 5 3.821 0.0124 ## . siNR4A3 EPS - Basal -0.1161 0.0626 5 -1.855 0.1228 Notes If our rmANOVA model is fit with the default specification (exp5c_aov1), we can get the multivariate output using the model = \"multivariate\" argument within emmeans (not the contrast function!). Or, if our fit excluded the univariate model (exp5c_aov2), then we don’t need the model argument. contrasts from univariate model # get the univariate model results using $aov emmeans(exp5c_aov1$aov, specs = c(&quot;treatment&quot;, &quot;activity&quot;)) %&gt;% contrast(method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) ## activity treatment contrast estimate SE df t.ratio p.value ## Basal . siNR4A3 - Scr 0.0813 0.0759 8.60 1.071 0.3132 ## EPS . siNR4A3 - Scr -0.1858 0.0759 8.60 -2.448 0.0380 ## . Scr EPS - Basal 0.1510 0.0524 9.39 2.884 0.0173 ## . siNR4A3 EPS - Basal -0.1161 0.0524 9.39 -2.218 0.0525 Notes Passing exp5c_aov1 to emmeans will return the contrasts from the multivariate. change this to the univariate model by passing exp5c_aov1$aov. 16.12.2.2 The ANOVA table nice(exp5c_aov1, correction = &quot;GG&quot;) ## Anova Table (Type 3 tests) ## ## Response: glucose_uptake ## Effect df MSE F ges p.value ## 1 treatment 1, 5 0.02 0.68 .006 .45 ## 2 activity 1, 5 0.01 0.30 .0006 .61 ## 3 treatment:activity 1, 5 0.01 10.37 * .04 .02 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 nice(exp5c_aov1, correction = &quot;none&quot;) ## Anova Table (Type 3 tests) ## ## Response: glucose_uptake ## Effect df MSE F ges p.value ## 1 treatment 1, 5 0.02 0.68 .006 .45 ## 2 activity 1, 5 0.01 0.30 .0006 .61 ## 3 treatment:activity 1, 5 0.01 10.37 * .04 .02 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Notes The Greenhouse-Geiger (“GG”) correction is the default. While the correction = \"GG\" argument is not needed, it makes the script more transparent. For these data the Greenhouse-Geiger correction doesn’t make a difference in the table. 16.13 Hidden code 16.13.1 Import exp5c data_from &lt;- &quot;Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity&quot; file_name &lt;- &quot;41467_2019_13869_MOESM6_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp5c_wide &lt;- read_excel(file_path, sheet = &quot;Fig5c&quot;, range = &quot;A2:M3&quot;, col_names = FALSE) %&gt;% data.table() %&gt;% transpose(make.names = 1) activity_levels &lt;- c(&quot;Basal&quot;, &quot;EPS&quot;) treatment_levels &lt;- names(exp5c_wide) exp5c_wide[, activity := rep(activity_levels, each = 6)] exp5c_wide[, activity := factor(activity, levels = activity_levels)] exp5c &lt;- melt(exp5c_wide, id.vars = &quot;activity&quot;, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_uptake&quot;) exp5c[, treatment := factor(treatment, levels = treatment_levels)] exp5c[, donor := rep(paste0(&quot;donor_&quot;, 1:6), 4)] 16.13.2 Import exp1g data_from &lt;- &quot;A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity&quot; file_name &lt;- &quot;41586_2019_1873_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) exp1g_wide &lt;- read_excel(file_path, sheet = &quot;Fig 1g&quot;, range = &quot;B4:E25&quot;, col_types = c(&quot;numeric&quot;), col_names = FALSE) %&gt;% data.table() genotype_levels &lt;- c(&quot;Gpr174+&quot;, &quot;Gpr174-&quot;) sex_levels &lt;- c(&quot;M&quot;, &quot;F&quot;) g.by.s_levels &lt;- do.call(paste, expand.grid(genotype_levels, sex_levels)) colnames(exp1g_wide) &lt;- g.by.s_levels exp_levels &lt;- paste0(&quot;exp_&quot;, 1:4) exp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check! exp1g_wide[, experiment_id := factor(experiment_id)] #check! exp1g &lt;- melt(exp1g_wide, id.vars = &quot;experiment_id&quot;, measure.vars = g.by.s_levels, variable.name = &quot;treatment&quot;, value.name = &quot;gc&quot;) %&gt;% # cell count na.omit() exp1g[, c(&quot;genotype&quot;, &quot;sex&quot;):= tstrsplit(treatment, &quot; &quot;, fixed = TRUE)] exp1g[, genotype := factor(genotype, levels = genotype_levels)] exp1g[, sex := factor(sex, levels = sex_levels)] exp1g_means &lt;- exp1g[, .(gc = mean(gc)), by = .(treatment, genotype, sex, experiment_id)] "],["pre-post.html", "Chapter 17 Linear models for longitudinal experiments – I. pre-post designs 17.1 Best practice models 17.2 Common alternatives that are not recommended 17.3 Advanced models 17.4 Understanding the alternative models 17.5 Example 1 – a single post-baseline measure (pre-post design) 17.6 Working in R 17.7 Hidden code", " Chapter 17 Linear models for longitudinal experiments – I. pre-post designs In a Longitudinal experiment, the response is measured in the same set of individuals over multiple time points. For example, in a glucose tolerance test (GTT), plasma glucose levels are measured at 0, 15, 30, 60, and 120 minutes (or some variation of this) after glucose has been given to the individual. The initial measure (time 0) is the baseline measure and subsequent measures are post-baseline measures. In a pre-post experiment, there is a single post-baseline measure. This chapter focuses on pre-post designs. The next chapter generalizes this information to longitudinal experiments with more than one post-baseline measure. For this chapter, I’ll use \\(pre_i\\) and \\(post_i\\) for the baseline and post-baseline measures of the response variable of indiviual \\(i\\). The change from baseline for individual \\(i\\) is \\(change_i = post_i - pre_i\\). The change from baseline is often called a change score. Imagine two versions of an experiment where researchers are interested in the effect of different intestinal microbiomes on body weight. In the intial experiment of the study, the researchers harvest intestinal microbiomes from five mice fed on chow and from five mice fed on a high-fat diet (HFD) and transplant these into ten, male C57bl/6 mouse randomly assigned to donor (this is done by fecal transplant). We have five replicates of the non-obese microbiome and five of the obese microbiome. Weight is measured at baseline (the day of the transplant) and four weeks later. The expected difference at baseline is zero, because the mice are sampled from the same population and the treatment was randomly assigned. Additional experiments identified the potential role of inflammation-induced obesity in response to microbiome-derived lipopolysaccharide (LPS). In the second experiment, the researchers transplant intestinal microbiota from ten HFD mice into five male, wild-type C57bl/6 mice and five male, C57bl/6 mice in which the LPS receptor (CD14) has been knocked-out. Weight is measured at baseline (the day of the transplant) and four weeks later. The expected difference at baseline is not zero. It is not zero because the mice in each treatment are sampled from different populations (different genotypes). Even if we think that genotype shouldn’t have an effect on weight at baseline, the expected difference is not zero because the treatment is not randomized at baseline but prior to baseline. This distinction is important because the best practice linear model depends on it. 17.1 Best practice models If the treatment is randomized at baseline to individuals sampled from the same population Model \\(\\mathcal{M}_1\\) – “ANCOVA model” model formula: post ~ pre + treatment a linear model that estimates the effect as the coefficient of treatment adjusted for the baseline measure, with \\(post\\) as the response. The response variable can either be the post-baseline measure or the change-score – the estimate of the treatment effect is the same. This is the best practice model if the expected difference at baseline is zero. If the treatment is randomized prior to baseline (the groups are not composed of individuals sampled from the same population at baseline) Model \\(\\mathcal{M}_2\\) – “change-score model” model formula: change ~ treatment a linear model that estimates the effect as the coefficient of \\(treatment\\), with the change score as the response. This is not the same as Model \\(\\mathcal{M}_1\\) as there is no covariate in the model. This is the best practice model if there is no expectation of no expected difference at baseline. These two models present vary different ways of thinking about what the treatment effect is. In the ANCOVA model, we think of the treatment effect as the difference between treated and control means at the post-baseline time. Because the post-baseline measure of individual i is partly determined by it’s value at baseline, variation in baseline values partly masks the effect of treatment. By adding baseline value as a covariate in the model, this masking effect of initial variation is eliminated. In the change-score model, we think of the treatment effect as the mean difference in the change from baseline. The response changes from baseline to post-baseline in all individuals and we can compute the mean change for both treated (\\(\\overline{post_{tr} - pre_{tr}}\\)) and control (\\(\\overline{post_{cn} - pre_{cn}}\\)) levels. The treatment effect is the difference in these means. \\[\\begin{align} \\mathrm{effect} &amp;= \\overline{change}_{tr} - \\overline{change}_{cn}\\\\ &amp;= (\\overline{post_{tr} - pre_{tr}}) - (\\overline{post_{cn} - pre_{cn}}) \\end{align}\\] In the change-score model, the treatment effect is a difference of differences. Differences of differences are interactions and an alternative way to estimate the same effect is with the interaction coefficient (\\(\\beta_3\\)) of the factorial linear model \\[\\begin{equation} weight = \\beta_0 + \\beta_1 treatment + \\beta_2 time + \\beta_3 treatment:time + \\varepsilon \\end{equation}\\] 17.2 Common alternatives that are not recommended \\(\\mathcal{M}_3\\) – “post model” model formula: post ~ treatment a linear model that estimates the effect as the coefficient of treatment, with \\(post\\) as the response. this model is valid but is not taking advantage of the increased precision of \\(\\mathcal{M}_1\\) \\(\\mathcal{M}_4\\) – “interaction model” model formula: y ~ treatment*time a factorial fixed-effects model that estimates the effect as the coefficient of the interaction \\(treatment:time\\). The response is the pre and post values stacked into a single column \\(y\\). this will give the same estimate as Model \\(\\mathcal{M}_2\\) but the standard error of the effect will incorrect because of the unmodeled correlated error due to multiple (pre and post) measures taken from the same individual. Never use this for pre-post or longitudinal models. \\(\\mathcal{M}_4\\) – “repeated measures model” doesn’t estimate the fixed effect but does compute a p-value of this effect as the \\(treatment \\times time\\) interaction term of the ANOVA table. This is effectively equivalent to change-score model \\(\\mathcal{M}_2\\). makes highly contstraining assumptions A repeated measures model is more common when there are multiple post-baseline measures 17.3 Advanced models \\(\\mathcal{M}_6\\) – “linear mixed model” model formula: y ~ treatment*time + (1|id) a factorial linear mixed model that estimates the effect as the coefficient of the interaction \\(treatment:time\\). The response is the pre and post values stacked into a single column \\(y\\). the correlated error is implicitly modeled using the random factor \\(id\\). This will give the same estimate as Model \\(\\mathcal{M}_2\\) and the same confidence interval and p-value if the Satterthwaite degrees of freedom are used. \\(\\mathcal{M}_7\\) – “fixed effects with correlated error model” model formula: y ~ treatment*time linear model with fixed effects and correlated error due to repeated measures within individuals. This looks like the fixed effect model but the error is explicitly modeled to allow 1) correlation due to id and 2) heterogeneity due to time. The advantage of explicitly modeling the correlated error is the flexibility in the model of the error. estimates the effect as the coefficient of the interaction \\(treatment:time\\). The response is the pre and post values stacked into a single column \\(y\\). This is an alternative to the change-score model, but, as in the change-score model, use this only if the expected difference at baseline is not zero. \\(\\mathcal{M}_8\\) – “constrained LDA” model formula: y ~ time + treatment:time As in a \\(\\mathcal{M}_7\\), this is linear model with fixed effects and correlated error due to repeated measures within individuals. Unlike \\(\\mathcal{M}_7\\), the treatment levels are constrained to have the same baseline mean, which is why there is no coefficient for \\(treatment\\). This model is known as Constrained Longitudinal Data Analysis (cLDA). The estimated treatment effect is effectively equal to that of the ANCOVA model (\\(\\mathcal{M}_1\\)) but, compared to inference from the ANCOVA model, the SE is smaller, the CIs are narrower, and the p-value is smaller. This increased precision and power, however, comes at a cost to increased Type I error. This is an interesting alternative to the ANCOVA model. 17.4 Understanding the alternative models The data used to explore the alternative models for analyzing pre-post data are from Figure 3F of Reed et al. 2020. Source article: Reed, M.D., Yim, Y.S., Wimmer, R.D. et al. IL-17a promotes sociability in mouse models of neurodevelopmental disorders. Nature 577, 249–253 (2020). https://doi.org/10.1038/s41586-019-1843-6 Source data The data is from an experiment estimating the effect of xxx on a sociability in mice. For some of the models, we need a wide version of the data.table and for other models, we need a long version. In the wide version, values of \\(pre\\) and \\(post\\) are in separate columns and the data.table includes the column \\(change\\). In the long version, values of \\(pre\\) and \\(post\\) are stacked into a single column, and the column \\(time\\) (with values “pre” and “post”) is added to identify the time period of the measurement. There is no \\(change\\) column because it is not needed in the analyses using the data in long format. Table 17.1: First four rows of sociability data in wide format treatment id pre post change Vehicle mouse_1 63.99 62.8 -1.2 Vehicle mouse_2 58.48 59.5 1.0 Vehicle mouse_3 59.77 42.5 -17.3 Vehicle mouse_4 51.08 55.5 4.5 Table 17.2: First four rows of sociability data in long format. treatment time id sociability Vehicle pre mouse_1 64.0 Vehicle pre mouse_2 58.5 Vehicle pre mouse_3 59.8 Vehicle pre mouse_4 51.1 17.4.1 (M1) Linear model with the baseline measure as the covariate (ANCOVA model) Model \\(mathcal{M}_1\\) is a linear model with the baseline variable added as a covariate. This is almost universally referred to as the “ANCOVA model”. \\[\\begin{equation} \\mathcal{M}_1\\;\\;\\;post = \\beta_0 + \\beta_1 pre + \\beta_2 treatment + \\varepsilon \\end{equation}\\] The model formula is post ~ pre + treatment Table 17.3: Coefficient table of Model M1 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 20.4 18.86 1.1 0.295 -19.4 60.2 pre 0.6 0.34 1.8 0.093 -0.1 1.3 treatmentIL-17 12.7 4.38 2.9 0.010 3.5 21.9 The third row (treatmentIL-17) of coefficient table ?? of Model \\(\\mathcal{M}_1\\) contains the statistics of interest. The estimate is the treatment effect adjusted for the baseline value, or “what the treatment effect would be if all the baseline values were equal”. Adding the covariate increases precision, which is why the ANCOVA model is preferred to a simple analysis of the post-baseline values (the “post model”). 17.4.2 (M2) Linear model of the change score (change-score model) Model \\(\\mathcal{M}_2\\) is a linear model with the change score used as the response and treatment factor as the only \\(X\\) variable. This is equivalent to a Student’s t-test of the change scores. \\[\\begin{equation} \\mathcal{M}_2\\;\\;\\;change = \\beta_0 + \\beta_1 treatment + \\varepsilon \\end{equation}\\] The model formula is change ~ treatment Table 17.4: Coefficient table of Model M2 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) -1.2 3.21 -0.4 0.724 -7.9 5.6 treatmentIL-17 11.7 4.33 2.7 0.015 2.6 20.8 The second row (treatmentIL-17) of coefficient table ?? of Model \\(\\mathcal{M}_2\\) contains the treatment effect, which is the simple difference between the mean “IL-17” and “Vehicle” change scores. 17.4.3 (M3) Linear model of post-baseline values without the baseline as a covariate (post model) Model \\(\\mathcal{M}_3\\) is a linear model with the post-baseline measure only. This is equivalent to a Student’s t-test of the post-baseline measures. \\[\\begin{equation} \\mathcal{M}_3\\;\\;\\;post = \\beta_0 + \\beta_1 treatment + \\varepsilon \\end{equation}\\] The model formula is post ~ treatment Table 17.5: Coefficient table of Model M3 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 53.5 3.37 15.9 0.000 46.4 60.6 treatmentIL-17 14.2 4.55 3.1 0.006 4.6 23.7 The second row (treatmentIL-17) of coefficient table ?? of Model \\(\\mathcal{M}_3\\) contains the treatment effect, which is the simple difference between the “IL-17” and “Vehicle” means. If the experiment includes baseline measures and the expected difference at baseline is zero, the ANCOVA model (\\(\\mathcal{M}_1\\)) should be used because the addition of the baseline measure as a covariate increases precision. If the experiment includes baseline measures and the expected difference at baseline is not zero, the change-score model (\\(\\mathcal{M}_2\\)) should be used. 17.4.4 (M4) Linear model with factorial fixed effects (fixed-effects model) Model \\(\\mathcal{M}_4\\) is a linear model with \\(treatment\\) and \\(time\\) crossed. \\[\\begin{equation} \\mathcal{M}_4\\;\\;\\;sociability = \\beta_0 + \\beta_1 treatment + \\beta_2 time + \\beta_3 treatment:time + \\varepsilon \\end{equation}\\] The model formula is post ~ treatment*time Table 17.6: Coefficient table of Model M4 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 54.7 2.85 19.2 0.0 48.878 60.4 treatmentIL-17 2.5 3.84 0.6 0.5 -5.319 10.3 timepost -1.2 4.03 -0.3 0.8 -9.327 7.0 treatmentIL-17:timepost 11.7 5.43 2.2 0.0 0.692 22.7 17.4.5 (M5) Repeated measures ANOVA Table 17.7: ANOVA table of Model M5. num Df den Df MSE F ges Pr(&gt;F) treatment 1 18 99.6 6.89 0.21 0.017 time 1 18 46.5 4.71 0.08 0.044 treatment:time 1 18 46.5 7.30 0.11 0.015 17.4.6 (M6) Linear mixed model Model \\(\\mathcal{M}_6\\) is the linear mixed model \\[\\begin{equation} \\mathcal{M}_6\\;\\;\\;sociability = (\\beta_0 + \\beta_{0j}) + \\beta_1 treatment + \\beta_2 time + \\beta_3 treatment:time + \\varepsilon \\end{equation}\\] where \\(\\beta_{0j}\\) is the random intercept for mouse \\(j\\). The model formula is sociability ~ treatment*time + (1|id) Table 17.8: Coefficient table of Model M6 Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 54.7 2.85 31.8 19.2 0.000 49.2 60.1 treatmentIL-17 2.5 3.84 31.8 0.6 0.524 -4.9 9.8 timepost -1.2 3.21 18.0 -0.4 0.724 -7.4 5.1 treatmentIL-17:timepost 11.7 4.33 18.0 2.7 0.015 3.2 20.2 The estimate of the treatment effect is not the 2nd row (treatmentIL-17) but the interaction row (treatmentIL-17:timepost) of coefficient table ?? of Model \\(\\mathcal{M}_6\\). The coefficient of treatmentIL-17 (\\(b_1\\)) is an estimate of the effect of treatment at baseline. The coefficient of timepost (\\(b_2\\)) is the “slope” of the model – it is the estimate of the effect of the treatment on the reference (“Vehicle”) group. The coefficient of the interaction (\\(b_3\\)) is an estimate of the difference in slope between the Vehicle and IL-17 groups, that is, the difference in the response to treatment. This is the treatment effect. The inferential statistics for the treatment effect (the interaction effect!) for the LMM (\\(\\mathcal{M}_6\\)) are the same as those for the change-score model ($_3) except for the confidence intervals, which are too narrow in the coefficient table of the LMM. A better method for computing the confidence limit of the treatment effect for the LMM is to compute the interaction contrast of the model. The confidence interval of the contrast computed with the Satterthwaite degrees of freedom are identicle to the confidence intervals from the change-score model. Table 17.9: Contrasts table of Model M6. treatment_trt.vs.ctrl time_trt.vs.ctrl estimate SE df lower.CL upper.CL t.ratio p.value (IL-17) - Vehicle post - pre 11.7 4.33 18 2.6 20.8 2.7 0.015 17.4.7 (M7) Linear model with correlated error Table 17.10: Coefficient table of Model M7 Value Std.Error t-value p-value 2.5 % 97.5 % (Intercept) 54.7 2.21 24.8 0.0 50.331 59.0 treatmentIL-17 2.5 2.98 0.8 0.4 -3.360 8.3 timepost -1.2 3.21 -0.4 0.7 -7.455 5.1 treatmentIL-17:timepost 11.7 4.33 2.7 0.0 3.217 20.2 Table 17.11: Contrasts table of Model M7. treatment_trt.vs.ctrl time_trt.vs.ctrl estimate SE df lower.CL upper.CL t.ratio p.value (IL-17) - Vehicle post - pre 11.7 4.33 18 2.6 20.8 2.7 0.014 17.4.8 (M8) Constrained fixed effects model with correlated error (cLDA model) Model \\(mathcal{M}_7\\), the Constrained Longitudinal Data Analysis (cLDA) is a linear model with correlated error. This is similar to a linear mixed model \\[\\begin{align} \\mathcal{M}_7\\;\\;\\;sociability &amp;= \\beta_0 + \\beta_1 time + \\beta_2 treatment:time + \\varepsilon_j\\\\ $\\varepsilon_j \\sim N(0, \\sigma^2R)$ \\end{align}\\] The model formula is sociability ~ time + treatment*time Table 17.12: Coefficient table of Model M8 Value Std.Error t-value p-value 2.5 % 97.5 % (Intercept) 56.0 1.47 38.1 0.0000 53.1 58.9 timepost -1.7 3.15 -0.5 0.5945 -7.9 4.5 treatment_il_17_timepost 12.7 4.17 3.0 0.0043 4.5 20.9 17.4.9 Comparison table Table 17.13: Estimated treatment effects from Models 1-8 Model Estimate SE t Pr(&gt;|t|) 2.5 % 97.5 % ANCOVA 12.68555 4.375280 2.899369 0.0099759 3.4545187 21.91659 change-score 11.71171 4.334247 2.702133 0.0145854 2.6057977 20.81763 post 14.18421 4.545934 3.120198 0.0059121 4.6335591 23.73486 factorial 11.71171 5.433377 2.155513 0.0378825 0.6923141 22.73111 RM-ANOVA 0.0145854 LMM-coef 11.71171 4.334247 2.702133 0.0145854 3.2497551 20.17367 LMM-contrast 11.71171 4.334247 2.702133 0.0145854 2.6057978 20.81763 CorErr-coef 11.71171 4.334247 2.702133 0.0104425 3.2167443 20.20668 CorErr-contrast 11.71171 4.334247 2.702133 0.0144958 2.6124382 20.81099 cLDA 12.68555 4.172748 3.040094 0.0043266 4.5071126 20.86399 17.5 Example 1 – a single post-baseline measure (pre-post design) 17.5.0.1 Fit the model m1 &lt;- lm(post ~ pre + treatment, data = fig3f) 17.5.0.2 Model checking set.seed(1) qqPlot(m1, id = FALSE) The residuals are within the bounds reasonably expected of a sample from a normal distribution spreadLevelPlot(m1) ## ## Suggested power transformation: 0.1054906 The residuals do not show conspicuous heterogeneity. 17.5.0.3 Inference from the model m1_coef &lt;- cbind(coef(summary(m1)), confint(m1)) knitr::kable(m1_coef, digits = c(1,2,1,3,1,1), caption = &quot;Coefficient table of model m1&quot;) %&gt;% kable_styling() Table 17.14: Coefficient table of model m1 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 20.4 18.86 1.1 0.295 -19.4 60.2 pre 0.6 0.34 1.8 0.093 -0.1 1.3 treatmentIL-17 12.7 4.38 2.9 0.010 3.5 21.9 Sociability in IL-17 treated fmr1 mice is 12.7 (95% CI: 3.5, 21.9, \\(p = 0.01\\)) percentage points higher than that in Control mice. 17.5.0.4 Plot the model 17.6 Working in R (M1) Linear model with the baseline measure as the covariate (ANCOVA model) m1 &lt;- lm(post ~ pre + treatment, data = fig3f) m1_coef &lt;- cbind(coef(summary(m1)), confint(m1)) knitr::kable(m1_coef, digits = c(1,2,1,3,1,1), caption = &quot;Coefficient table of Model M1&quot;) %&gt;% kable_styling() Table 17.15: Coefficient table of Model M1 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 20.4 18.86 1.1 0.295 -19.4 60.2 pre 0.6 0.34 1.8 0.093 -0.1 1.3 treatmentIL-17 12.7 4.38 2.9 0.010 3.5 21.9 (M2) Linear model of the change score (change-score model) m2 &lt;- lm(change ~ treatment, data = fig3f) m2_coef &lt;- cbind(coef(summary(m2)), confint(m2)) knitr::kable(m2_coef, digits = c(1,2,1,3,1,1), caption = &quot;Coefficient table of Model M2&quot;) %&gt;% kable_styling() Table 17.16: Coefficient table of Model M2 Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) -1.2 3.21 -0.4 0.724 -7.9 5.6 treatmentIL-17 11.7 4.33 2.7 0.015 2.6 20.8 (M5) Repeated measures ANOVA use aov_4 from the afex package, which allos input of a lmer model formula. Note that time is coded here as a random slope. This model would fail in lmer because there is only one time:id measure. Table 17.17: ANOVA table of Model M5. num Df den Df MSE F ges Pr(&gt;F) treatment 1 18 99.6 6.89 0.21 0.017 time 1 18 46.5 4.71 0.08 0.044 treatment:time 1 18 46.5 7.30 0.11 0.015 (M6) Linear mixed model m6 &lt;- lmer(sociability ~ treatment*time + (1|id), data = fig3f_long) m6_coef &lt;- cbind(coef(summary(m6)), confint(m6)[3:6,]) m6_coef %&gt;% knitr::kable(digits = c(1,2,1,1,3,1), caption = &quot;Coefficient table of Model M6&quot;) %&gt;% kable_styling() Table 17.18: Coefficient table of Model M6 Estimate Std. Error df t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 54.7 2.85 31.8 19.2 0.000 49.2 60.1 treatmentIL-17 2.5 3.84 31.8 0.6 0.524 -4.9 9.8 timepost -1.2 3.21 18.0 -0.4 0.724 -7.4 5.1 treatmentIL-17:timepost 11.7 4.33 18.0 2.7 0.015 3.2 20.2 Use lmer.df = \"Satterthwaite\" as an argument in emmeans (not contrast!) to get the satterthwaite df for the interaction contrast. m6_emm &lt;- emmeans(m6, specs = c(&quot;treatment&quot;, &quot;time&quot;), lmer.df = &quot;Satterthwaite&quot;) m6_pairs &lt;- contrast(m6_emm, interaction = c(&quot;trt.vs.ctrl&quot;), by = NULL) %&gt;% summary(infer = TRUE) # do not use profile or Wald intervals of coefficients, which are # similar/equal to lmer.df = &quot;asymptotic&quot; knitr::kable(m6_pairs, digits = c(1,1,1,2,0,1,1,1,3), caption = &quot;Contrasts table of Model M6.&quot;) %&gt;% kable_styling() Table 17.19: Contrasts table of Model M6. treatment_trt.vs.ctrl time_trt.vs.ctrl estimate SE df lower.CL upper.CL t.ratio p.value (IL-17) - Vehicle post - pre 11.7 4.33 18 2.6 20.8 2.7 0.015 (M7) Linear model with correlated error m7 &lt;- gls(sociability ~ treatment*time, data = fig3f_long, weights = varIdent(form= ~ 1 | time), correlation= corSymm(form=~ 1| id)) m7_coef &lt;- cbind(coef(summary(m7)), confint(m7)) m7_coef %&gt;% knitr::kable(digits = c(1,2,1,1,3,1), caption = &quot;Coefficient table of Model M7&quot;) %&gt;% kable_styling() Table 17.20: Coefficient table of Model M7 Value Std.Error t-value p-value 2.5 % 97.5 % (Intercept) 54.7 2.21 24.8 0.0 50.331 59.0 treatmentIL-17 2.5 2.98 0.8 0.4 -3.360 8.3 timepost -1.2 3.21 -0.4 0.7 -7.455 5.1 treatmentIL-17:timepost 11.7 4.33 2.7 0.0 3.217 20.2 Use mode = \"satterthwaite\" as an argument in emmeans (not contrast!) to get the satterthwaite df for the interaction contrast. m7_emm &lt;- emmeans(m7, specs = c(&quot;treatment&quot;, &quot;time&quot;), mode = &quot;satterthwaite&quot;) m7_pairs &lt;- contrast(m7_emm, interaction = c(&quot;trt.vs.ctrl&quot;), by = NULL) %&gt;% summary(infer = TRUE) knitr::kable(m7_pairs, digits = c(1,1,1,2,0,1,1,1,3), caption = &quot;Contrasts table of Model M7.&quot;) %&gt;% kable_styling() Table 17.21: Contrasts table of Model M7. treatment_trt.vs.ctrl time_trt.vs.ctrl estimate SE df lower.CL upper.CL t.ratio p.value (IL-17) - Vehicle post - pre 11.7 4.33 18 2.6 20.8 2.7 0.015 (M8) Constrained fixed effects model with correlated error (cLDA model) cLDA function clda &lt;- function(data, y_col, treatment_col = &quot;treatment&quot;, time_col = &quot;time&quot;, id_col = &quot;id&quot;){ # warning - not debugged for missing dt &lt;- setDT(data) n_groups &lt;- length(unique(dt[, get(treatment_col)])) n_times &lt;- length(unique(dt[, get(time_col)])) # create model matrix with no treatment effect at baseline # part 1 - get the model matrix form &lt;- formula(paste0(&quot; ~ &quot;, treatment_col,&quot; * &quot;, time_col)) model_matrix &lt;- model.matrix(form, data = dt) # part 2 - exclude columns treat_cols &lt;- 2:n_groups exc &lt;- c(1, treat_cols) # constrained model matrix X &lt;- model_matrix[, -exc] %&gt;% data.table() %&gt;% clean_names() dt &lt;- cbind(dt, X) rhs &lt;- paste0(names(X), collapse = &quot; + &quot;) model_form &lt;- formula(paste0(y_col, &quot; ~ &quot;, rhs)) weight_form &lt;- formula(paste0(&quot;~ 1 | &quot;, time_col)) cor_form &lt;- formula(paste0(&quot;~ 1 | &quot;, id_col)) fit &lt;- gls(model_form, data = dt, weights = varIdent(form = weight_form), correlation= corSymm(form = cor_form)) return(fit) } m8 &lt;- clda(fig3f_long, y_col = &quot;sociability&quot;, treatment_col = &quot;treatment&quot;, time_col = &quot;time&quot;, id_col = &quot;id&quot;) m8_coef &lt;- cbind(coef(summary(m8)), confint(m8)) m8_coef %&gt;% knitr::kable(digits = c(1,2,1,4,1,1), caption = &quot;Coefficient table of Model M8&quot;) %&gt;% kable_styling() Table 17.22: Coefficient table of Model M8 Value Std.Error t-value p-value 2.5 % 97.5 % (Intercept) 56.0 1.47 38.1 0.0000 53.1 58.9 timepost -1.7 3.15 -0.5 0.5945 -7.9 4.5 treatment_il_17_timepost 12.7 4.17 3.0 0.0043 4.5 20.9 17.7 Hidden code 17.7.1 Import and wrangle mouse sociability data data_from &lt;- &quot;IL-17a promotes sociability in mouse models of neurodevelopmental disorders&quot; file_name &lt;- &quot;41586_2019_1843_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # response - fmr1 # vehicle fig3f_vehicle &lt;- read_excel(file_path, sheet = &quot;Fig 3f&quot;, range = &quot;DA2:DI3&quot;, # 1 blank column col_names = FALSE) %&gt;% data.table() %&gt;% transpose() %&gt;% setnames(old = c(&quot;V1&quot;, &quot;V2&quot;), new = c(&quot;pre&quot;, &quot;post&quot;)) fig3f_il17 &lt;- read_excel(file_path, sheet = &quot;Fig 3f&quot;, range = &quot;DL2:DV3&quot;, # 1 blank column col_names = FALSE) %&gt;% data.table() %&gt;% transpose() %&gt;% setnames(old = c(&quot;V1&quot;, &quot;V2&quot;), new = c(&quot;pre&quot;, &quot;post&quot;)) treatment_levels &lt;- c(&quot;Vehicle&quot;, &quot;IL-17&quot;) fig3f &lt;- rbind(data.table(treatment = treatment_levels[1], fig3f_vehicle), data.table(treatment = treatment_levels[2], fig3f_il17)) fig3f[, treatment := factor(treatment, levels = treatment_levels)] fig3f[, id := paste0(&quot;mouse_&quot;, .I)] # add id for each mouse fig3f[, background := &quot;fmr1&quot;] # which mouse group? fig3f[, change := post - pre] # change score # melt to long fig3f_long &lt;- melt(fig3f, id.vars = c(&quot;treatment&quot;, &quot;id&quot;, &quot;change&quot;), measure.vars = c(&quot;pre&quot;, &quot;post&quot;), variable.name = &quot;time&quot;, value.name = &quot;sociability&quot;) fig3f_long[, time := factor(time, levels = c(&quot;pre&quot;, &quot;post&quot;))] fig3f_long[, id := factor(id)] "],["counts.html", "Chapter 18 Linear models for counts, binary responses, skewed responses, and ratios – Generalized Linear Models 18.1 Introducing Generalized Linear Models using count data examples 18.2 Example 1 – GLM models for count responses (“angiogenic sprouts” fig3a) 18.3 Understanding Example 1 18.4 Example 2 – Use a GLM with an offset instead of a ratio of some measurement per total (“dna damage” data fig3b) #glm_offset 18.5 Understanding Example 2 18.6 Example 3 – GLM models for binary responses 18.7 Working in R 18.8 Model checking GLMs 18.9 Hidden code", " Chapter 18 Linear models for counts, binary responses, skewed responses, and ratios – Generalized Linear Models 18.1 Introducing Generalized Linear Models using count data examples Biologists frequently count stuff, and design experiments to estimate the effects of different factors on these counts. Count data can cause numerous problems with linear models that assume a normal, conditional distribution, including 1) counts are discrete, and can be zero or positive integers only, 2) counts tend to bunch up on the small side of the range, creating a distribution with a positive skew, 3) a sample of counts can have an abundance of zeros, and 4) the variance of counts increases with the mean (see Figure 18.1 for some of these properties). Some count data can be approximated by and reasonably modeled with a normal distribution. More often, count data should modeled with a Poisson distribution or negative binomial distribution using a generalized linear model. Poisson and negative binomial distributions are discrete probability distributions with two important properties: 1) the distribution contains only zero and positive integers and 2) the variance is a function of the mean. Back before modern computing and fast processors, count data were often analyzed by either transforming the response or by non-parametric hypothesis tests. One reason to prefer a statistical modeling approach with a GLM is that we can get interpretable parameter estimates. By contrast, both the analysis of transformed data and non-parametric hypothesis tests are really tools for computing “correct” p-values. Figure 18.1: Histogram of the number of angiogenic sprouts in response to two of the four treatment combinations for the experiment in Example 1. 18.1.1 The Generalized Linear Model (GLM) As outlined in Two specifications of a linear model, a common way that biological researchers are taught to think about a response variable is \\[ response = expected + error \\] or, using the notation of this text, \\[ \\begin{align} y &amp;= \\beta_0 + \\beta_1 \\texttt{treatment} + \\varepsilon \\\\ \\varepsilon &amp;\\sim \\operatorname{Normal}(0, \\sigma^2) \\tag{18.1} \\end{align} \\] That is, we can think of a response as the sum of some systematic part (\\(\\beta_0 + \\beta_1 \\texttt{treatment}\\)) and a stochastic (“random error”) part (\\(\\varepsilon\\)), where the stochastic part is a random draw from a normal distribution with mean zero and variance \\(\\sigma^2\\). This way of thinking about the generation of the response is useful for linear models, and model checking linear models, but is much less useful for thinking about generalized linear models or model checking generalized liner models. For example, if we want to model the number of angiogenic sprouts in response to some combination of GAS6 and treatment using a Poisson distribution, the following is the wrong way to think about the statistical model \\[ \\begin{align} \\texttt{sprouts} = &amp;\\ \\beta_0 + \\beta_1 \\texttt{treatment}_{\\texttt{GAS6}} + \\beta_2 \\texttt{genotype}_{\\texttt{FAK_ko}} + \\\\ &amp;\\ \\beta_3 \\texttt{treatment}_{\\texttt{GAS6}}:\\texttt{genotype}_{\\texttt{FAK_ko}} + \\varepsilon_i\\\\ \\varepsilon \\sim &amp;\\ \\operatorname{Poisson}(\\lambda) \\tag{18.2} \\end{align} \\] That is, we should not think of a count as the sum of a systematic part and a random draw from a Poisson distribution. Why? Because it is the counts, conditional on the \\(\\texttt{treatment}\\) and \\(\\texttt{genotype}\\), that are poisson distributed, not the residuals from the fit model. Thinking about the distribution of count data using model (18.2) leads to absurd consequences. For example, if we set the mean of the Poisson “error” to zero (like with a normal distribution), then the error term for every observation would have to be zero (because the only way to get a mean of zero with non-negative integers is if every value is zero). Or, if the study is modeling the effect of a treatment on the counts (that is, the \\(X\\) are dummy variables) then \\(\\beta_0\\) is the expected mean count of the control (or reference) group. But if we add non-zero Poisson error to this, then the mean of the control group would be larger than \\(\\beta_0\\). This doesn’t make sense. And finally, equation (18.2) generates a continuous response, instead of an integer, because \\(\\beta_0\\) and \\(\\beta_1\\) are continuous. A better way to think about the data generation for a linear model that naturally leads to the correct way to think about data generation for a generalized linear model, is \\[ \\begin{align} y_i &amp;\\sim N(\\mu_i, \\sigma^2)\\\\ \\mathrm{E}(y_i| \\texttt{treatment}) &amp;= \\mu_i\\\\ \\mu_i &amp;= \\beta_0 + \\beta_1 \\texttt{treatment}_i \\tag{18.3} \\end{align} \\] That is, a response is a random draw from a normal distribution with mean \\(mu\\) (not zero!) and variance \\(\\sigma^2\\). Line 1 is the stochastic part of this specification. Line 3 is the systematic part. The specification of a generalized linear model has both stochastic and systematic parts but adds a third part, which is a link function connecting the stochastic and systematic parts. The stochastic part, which is a probability distribution from the exponential family (this is sometimes called the “random part”) \\[ \\begin{equation} y_i \\sim \\operatorname{Prob}(\\mu_i) \\end{equation} \\] the systematic part, which is a linear predictor (I like to think about this as the deterministic part) \\[ \\begin{equation} \\eta = \\beta_0 + \\beta_1 \\texttt{treatment}_i \\end{equation} \\] 3. a link function connecting the two parts \\[ \\begin{equation} \\eta_i = g(\\mu_i) \\end{equation} \\] \\(\\mu\\) (the Greek symbol mu) is the conditional mean (or expectation \\(\\mathrm{E}(Y|X)\\)) of the response on the response scale and \\(\\eta\\) (the Greek symbol eta) is the conditional mean of the response on the link scale. The response scale is the scale of the raw measurements and has units of the raw measurements. The link scale is the scale of the transformed mean. A GLM models the response with a distribution specified in the stochastic part. The probability distributions introduced in this chapter are the Poisson and Negative Binomial. The natural link function, and default link function in R, for the Poisson and Negative Binomial is the “log link”, \\(\\eta = log(\\mu)\\). More generally, while each distribution has a natural (or, “canonical”) link function, one can use alternatives. Given this definition of a generalized linear model, a linear model is a GLM with a normal distribution and an Identity link (\\(\\eta = \\mu\\)). \\[ \\begin{align} y_i &amp;\\sim \\operatorname{Normal}(\\mu_i, \\sigma^2)\\\\ \\mathrm{E}(y_i| \\texttt{treatment}) &amp;= \\mu_i\\\\ \\eta &amp;= \\beta_0 + \\beta_1 \\texttt{treatment}\\\\ \\mu_i &amp;= \\eta_i \\end{align} \\] Think about the link function and GLM more generally like this: the GLM constructs a linear model that predicts the conditional means on the link scale. If the model uses a “log link” (\\(\\eta_i = \\mathrm{log}(\\mu_i)\\)), then \\(\\eta_i\\) – the conditional mean on the link scale – is the log of the modeled mean on the response scale. The modeled mean on the response scale is the inverse link function. \\[ \\begin{equation} \\mu_i = g^{-1}(\\eta_i) \\end{equation} \\] For a log link, a modeled mean is \\(\\mathrm{exp}(\\eta_i)\\))$. Importantly, in a GLM, the individual data values are not transformed. A GLM with a log link is not the same as a linear model on log transformed data. When modeling counts using the Poisson or negative binomial distributions with a log link, the link scale is linear, and so the effects are additive on the link scale, while the response scale is nonlinear (it is the exponent of the link scale), and so the effects are multiplicative on the response scale. If this doesn’t make sense now, an example is worked out below. The inverse of the link function backtransforms the parameters from the link scale back to the response scale. So, for example, a prediction on the response sale is \\(\\mathrm{exp}(\\hat{\\eta})\\) and a coefficient on the response scale is \\(\\mathrm{exp}(b_j)\\). 18.1.2 Kinds of data that are modeled by a GLM If a response is a count then use a Poisson, quasi-Poisson or negative binomial family with a log link. If a response is binary response, for example, presence/absence or success/failure or survive/die, then use the binomial family with a logistic link (other link functions are also useful). This is classically known as logistic regression. If a response is a fraction that is a ratio of counts, for example the fraction of total cells that express some marker, then use the binomial family with a logistic link. Think of each count as a “success”. If a response is a fraction of counts per “effort” (cells per area or volume or time), then use a Poisson, quasi-Poisson or negative binomial family with a log link on the raw count and include the measure of effort as an offset. If a response is continuous but the variance increases with mean then use the gamma family If a response is a fraction of a continuous measure per “effort” (tumor area per total area or volume or time), then use a gamma family on the raw measure and include the measure of effort as an offset. 18.2 Example 1 – GLM models for count responses (“angiogenic sprouts” fig3a) 18.2.1 Understand the data Article source Lechertier, Tanguy, et al. “Pericyte FAK negatively regulates Gas6/Axl signalling to suppress tumour angiogenesis and tumour growth.” Nature communications 11.1 (2020): 1-14. data source The researchers designed a set of experiments to investigate the effects of pericyte derived FAK (focal adhesion kinase, a protein tyrosine kinase) on the Gas6/Axl pathway regulating angiogenesis promoting tumor growth. Pericytes are cells immediately deep to the endothelium (the epithelial lining) of the smallest blood vessels, including capillaries, arterioles, and venules. Angiogenesis is the growth of new blood vessels. GAS6 (growth arrest specific 6) is a protein commonly expressed in tumors. The example data is from the experiment for Figure 3a. The design is \\(2 \\times 2\\) – two factors (\\(\\texttt{treatment}\\) and \\(\\texttt{genotype}\\)) each with two levels. Factor 1: \\(\\texttt{treatment}\\) reference level: “PBS”. Phosphate buffered saline added to tissue. This is the control treatment. treatment level: “GAS6”. Added to tissue in solution. The experiment is designed to test its effect on promoting angiogenesis in the development of tumors. Factor 2: \\(\\texttt{genotype}\\) reference level: “FAK_wt”. The functional genotype. treatment level: “FAK_ko”. Tissue-specific FAK deletion in pericytes. The experiment is designed to test the effect of pericyte-derived FAK on slowing angiogenesis in the development of tumors. If true, then its deletion should result in increased tumor development. The four treatment by genotype combinations are Control (“PBS FAK_wt”) – negative control FAK_ko (“PBS FAK_ko”) – PBS control. Unknown response given deletion of putative angiogenesis inhibitor but no added putative angiogenesis promotor GAS6 (“GAS6 FAK_wt”) – GAS6 control. GAS6 expected to promote angiogenesis but this the response is expected to be inhibited by some amount by FAK GAS6+FAK_ko (“GAS6 FAK_ko”) – focal treatment. Expected positive angiogenesis The planned contrasts are (PBS FAK_ko) - (PBS FAK_wt) – the effect of FAK deletion given the control treatment (GAS6 FAK_ko) - (GAS6 FAK_wt) – the effect of FAK deletion given the GAS6 treatment ((PBS FAK_ko) - (PBS FAK_wt)) - ((GAS6 FAK_ko) - (GAS6 FAK_wt)). The interaction effect giving the effect of the combined treatment relative to the individual effects. 18.2.2 Model fit and inference 18.2.2.1 Fit the models fig3a_m1 &lt;- lm(sprouts ~ treatment * genotype, data = fig3a) fig3a_m2 &lt;- glm(sprouts ~ treatment * genotype, family = &quot;poisson&quot;, data = fig3a) fig3a_m3 &lt;- glm.nb(sprouts ~ treatment * genotype, data = fig3a) 18.2.2.2 Check the linear model ggcheck_the_model(fig3a_m1) Notes left panel shows classic right skew conditional distribution with larger values much larger than expected with a normal distribution. right panel shows heterogeneity and specifically the variance increasing with the mean. 18.2.2.3 Check the poisson model Check shape and homogeneity # from the DHARMa package fig3a_m2_simulation &lt;- simulateResiduals(fittedModel = fig3a_m2, n = 250) plot(fig3a_m2_simulation, asFactor = FALSE) Notes poisson glm fails to generated scaled residuals approximating uniform distribution. Check dispersion fig3a_m2_simulation_refit &lt;- simulateResiduals(fittedModel = fig3a_m2, n = 250, refit = TRUE) fig3a_m2_test_dispersion &lt;- testDispersion(fig3a_m2_simulation_refit) Notes large overdispersion Check zero inflation fig3a_m2_test_zi &lt;- testZeroInflation(fig3a_m2_simulation_refit) Notes The data has too many zeros relative to the expected number from a poisson GLM. 18.2.2.4 Check the negative binomial model Check shape and homogeneity # from the DHARMa package fig3a_m3_simulation &lt;- simulateResiduals(fittedModel = fig3a_m3, n = 250) plot(fig3a_m3_simulation, asFactor = FALSE) Notes uniform q-q for negative binomial GLM looks good. spread-location plot looks good. Check dispersion fig3a_m3_simulation_refit &lt;- simulateResiduals(fittedModel = fig3a_m3, n = 250, refit = TRUE) fig3a_m3_test_dispersion &lt;- testDispersion(fig3a_m3_simulation_refit) Notes good Check zero inflation fig3a_m3_test_zi &lt;- testZeroInflation(fig3a_m3_simulation_refit) 18.2.2.5 Inference from the model fig3a_m3_coef &lt;- cbind(coef(summary(fig3a_m3)), confint(fig3a_m3)) Estimate Std. Error z value Pr(&gt;|z|) 2.5 % 97.5 % (Intercept) 0.90 0.231 3.9 0.000 0.45 1.36 treatmentGAS6 1.30 0.287 4.5 0.000 0.74 1.86 genotypeFAK_ko 0.04 0.357 0.1 0.900 -0.65 0.75 treatmentGAS6:genotypeFAK_ko 0.46 0.425 1.1 0.275 -0.37 1.30 fig3a_m3_emm &lt;- emmeans(fig3a_m3, specs = c(&quot;treatment&quot;, &quot;genotype&quot;), type=&quot;response&quot;) treatment genotype response SE df asymp.LCL asymp.UCL PBS FAK_wt 2.47 0.6 Inf 1.57 3.88 GAS6 FAK_wt 9.05 1.5 Inf 6.48 12.64 PBS FAK_ko 2.58 0.7 Inf 1.52 4.40 GAS6 FAK_ko 15.04 2.4 Inf 11.06 20.46 # fig3a_m3_emm # print in console to get row numbers # set the mean as the row number from the emmeans table pbs_fak_wt &lt;- c(1,0,0,0) gas6_fak_wt &lt;- c(0,1,0,0) pbs_fak_ko &lt;- c(0,0,1,0) gas6_fak_ko &lt;- c(0,0,0,1) #1. (PBS FAK_ko) - (PBS FAK_wt) #2. (GAS6 FAK_ko) - (GAS6 FAK_wt) #3. ((PBS FAK_ko) - (PBS FAK_wt)) - (GAS6 FAK_ko) - (GAS6 FAK_wt). fig3a_m3_planned &lt;- contrast( fig3a_m3_emm, method = list( &quot;(PBS FAK_ko) - (PBS FAK_wt)&quot; = c(pbs_fak_ko - pbs_fak_wt), &quot;(GAS6 FAK_ko) - (GAS6 FAK_wt)&quot; = c(gas6_fak_ko - gas6_fak_wt), &quot;Interaction&quot; = c(gas6_fak_ko - gas6_fak_wt) - c(pbs_fak_ko - pbs_fak_wt) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast ratio SE df asymp.LCL asymp.UCL null z.ratio p.value (PBS FAK_ko) / (PBS FAK_wt) 1.05 0.373 Inf 0.52 2.10 1 0.12516 0.9 (GAS6 FAK_ko) / (GAS6 FAK_wt) 1.66 0.385 Inf 1.06 2.62 1 2.19446 0.0 Interaction 1.59 0.676 Inf 0.69 3.66 1 1.09082 0.3 Notes 18.2.2.6 Plot the model 18.2.2.7 Alternaplot the model 18.3 Understanding Example 1 18.3.1 Modeling strategy Instead of testing assumptions of a model using formal hypothesis tests before fitting the model, a better strategy is to 1) fit one or more models based on initial evaluation of the data, and then do 2) model checking using diagnostic plots, diagnostic statistics, and simulation (see Section All statistical analyses should be followed by model checking). For the fig3a data, I fit a linear model, a Poisson GLM, and a negative binomial GLM. I use the diagnostic plots and statistics to help me decide which model to report. 18.3.2 Model checking fits to count data We use the fit models to check the compatibility between the quantiles of the observed residuals and the distribution of expected quantiles from the family in the model fit if the observed distribution is over or under dispersed if there are more zeros than expected by the theoretical distribution. If so, the observed distribution is zero-inflated 18.3.2.1 Checking the linear model fig3a_m1 – a Normal-QQ plot Figure ??A shows a histogram of the residuals from the fit linear model. The plot shows that the residuals seem to be clumped at the negative end of the range, which suggests that a model with a normally distributed conditional outcome (or normal error) is not well approximated. Figure 18.2: Diagnostic plots of angiogenic sprout data (fig3a). A) Distribution of the residuals of the fit linear model. B) Normal-QQ plot of the residuals of the fit linear model. A better way to investigate this is with the Normal-QQ plot in Figure ??B, which plots the sample quantiles for a variable against their theoretical quantiles. If the conditional outcome approximates a normal distribution, the points should roughly follow the robust regression line, the points should be largely inside the 95% CI (gray) bounds for sampling a normal distribution with the variance estimated by the model, and the robust regression line should be largely inside the CI bounds. For the sprout data, the points are above the line at the positive end, hug the upper bound of the 95% CI at the negative end, are well above the 95% CI at the positive end, and the robust regression is distinctly shallower than a line bisecting the CI bounds. At the left (negative) end, the observed values are more positive than the theoretical values. Remembering that this plot is of residuals, if we think about this as counts, this means that our smallest counts are not as small as we would expect given the mean, the variance, and a normal distribution. This shouldn’t be surprising – the counts range down to zero and counts cannot be below zero. At the positive end, the sample values are again more positive than the theoretical values. Thinking about this as counts, this means that are largest counts are larger than expected given the mean, the variance, and a normal distribution. This pattern is what we’d expect of count data. 18.3.2.2 Checking the linear model fig3a_m1 – Spread-level plot for checking homoskedasticity A linear model also assumes that the error is homoskedastic – the error variance is not a function of the value of the \\(X\\) variables). Non-homoskedastic error is heteroskedastic. I will typically use “homogenous variance” and “heterogenous variance” since these terms are more familiar to biologists. The fit model can be checked for homogeneity using a spread-level (also known as a scale-location) plot, which comes in several forms. I like a spread-level plot that is a scatterplot of the positive square-root of the standardized residuals against the fitted values (remember that the fitted values are the values computed by the linear predictor of the model – they are the “predicted values” of the observed data). If the residuals approximate a normal distribution, then a regression line through the scatter should be close to horizontal. The regression line in the spread-level plot of the fit of the linear model to the sprout data shows a distinct increase in the “scale” (the square root of the standardized residuals) with increased fitted value, which is expected of data sampled from a distribution in which the variance increases the mean. 18.3.2.3 Two distributions for count data – Poisson and Negative Binomial The pattern in the Normal-QQ plot in Figure ??B should discourage a researcher from modeling the data with a normal distribution and instead model the data with an alternative distribution using a Generalized Linear Model. There is no unique mapping between observed data and a data generating mechanism with a specific distribution, so this decision is not as easy as thinking about the data generation mechanism and then simply choosing the “correct” distribution. Section 4.5 in Bolker (xxx) is an excellent summary of how to think about the generating processes for different distributions in the context of ecological data. Since the response in the angiogenic sprouts data are counts, we need to choose a distribution that generates integer values, such as the Poisson or the negative binomial. Poisson – A Poisson distribution is the probability distribution of the number of occurrences of some thing (a white blood cell, a tumor, or a specific mRNA transcript) generated by a process that generates the thing at a constant rate per unit effort (duration or space). This constant rate is the parameter \\(\\lambda\\), which is the expectation (the expected mean of the counts), so \\(\\mathrm{E}(Y) = \\mu = \\lambda\\). Because the rate per effort is constant, the variance of a Poisson variable equals the mean, \\(\\sigma^2 = \\mu = \\lambda\\). Figure ?? shows three samples from a Poisson distribution with \\(\\lambda\\) set to 1, 5, and 10. The plots show that, as the mean count (\\(\\lambda\\)) moves away from zero, a Poisson distribution 1) becomes less skewed and more closely approximates a normal distribution and 2) has an increasingly low probability of including zero (less than 1% zeros when the mean is 5). A Poisson distribution, then, is useful for count data in which the conditional variance is close to the conditional mean. Very often, biological count data are not well approximated by a Poisson distribution because the variance is either less than the mean, an example of underdispersion5, or greater than the mean, an example of overdispersion6. A useful distribution for count data with overdispersion is the negative binomial. Negative Binomial – The negative binomial distribution is a discrete probability distribution of the number of successes that occur before a specified number of failures \\(k\\) given a probability p of success. This isn’t a very useful way of thinking about modeling count data in biology. What is useful is that the Negative Binomial distribution can be used simply as way of modeling an “overdispersed” Poisson process. Using the parameterization in the MASS::glm.nb function, the mean of a negative binomial variable is \\(\\mu\\) and the variance is \\(\\sigma^2 = \\mu + \\frac{\\mu^2}{\\theta}\\). As a method for modeling an overdispersed Poisson variable, \\(\\theta\\) functions as a **dispersion parameter* controlling the amount of overdispersion and can be any real, positive value (not simply a positive integer), including values less than 1. As \\(\\theta\\) approaches positive infinity, the “overdispersion” bit \\(\\frac{\\mu^2}{\\theta}\\) goes to zero and the variance goes to \\(\\mu\\), which is the same as the Poisson. 18.3.2.4 Model checking a GLM I – the quantile-residual uniform-QQ plot Normal-QQ plots were introduced in Section @ref{normal-qq} of the Model Checking chapter and applied to the linear model fit of the angiogenic sprout data in Section 18.3.2.1 above. We cannot use a Normal-QQ plot with a Poisson or negative binomial GLM fit because the residuals from this fit are not expected to be normally distributed. An alternative to a Normal-QQ plot for a GLM fit is a quantile-residual uniform-QQ plot of observed quantile residuals. (#fig:glm-fig3a_m2-check-poisson-again)Quantile-residual uniform-QQ plot of the Poisson GLM fit to the angiogenic sprouts (fig3a) data. Notes The x-axis (“Expected”) contains the expected quantiles from a uniform distribution. The y-axis (“Observed”) contains the observed quantile residuals from a GLM fit, which are the residuals from the fit model that are transformed in a way that the expected distribution is uniform under the fit model family. This means that we’d expect the quantile residuals to closely approximate the expected quantiles from a uniform distribution. If the approximation is close, the points will fall along the “y = x” line in the plot. The gray shaded area is a 95% confidence interval computed using a parametric bootstrap. At any value of the expected quantile, the interval will include an observed quantile 95% of the time. This gray area gives us a sense of the variability we’d get when we fit models to random samples from the specified model. In the quantile-residual QQ plot for Model fig3a_m2, the observed residuals are far outside the 95% boundary. The observed residuals are smaller than expected at the negative (left) end and larger than expected at the right (high) end. This means the residuals are more spread out than expected for a Poisson sample. The data are overdispersed for this model. Understand that overdispersion is not a property of data but of the residuals from a specific model fit to the data. Misconceivable – A common misconception is that if the distribution of the response approximates a Poisson distribution, then the residuals of a GLM fit with a Poisson distribution should be normally distributed, which could then be checked with a Normal-QQ plot, and homoskedastic, which could be checked with a scale-location plot. Neither of these is true because a GLM does not transform the data and, in fact, the model definition does not specify anything about the distribution of an “error” term – there is no \\(\\varepsilon\\) in the model definition above! This is why thinking about the definition of a linear model by specifying an error term with a normal distribution can be confusing and lead to misconceptions when learning GLMs. 18.3.2.5 Model checking a GLM II – Spread-level plot for checking homoskedasticity (#fig:glm-fig3a_m2-spreadlevel-again)Spread-level plot of the Poisson GLM fit to the angiogenic sprouts (fig3a) data. Notes The three curves are quantile regressions fit to the 25% (the first quartile), 50% (the second quartile or median), and 75% (the third quartile) quantiles conditional on the fitted value (the x-axis). The \\(p\\%\\) quantile regression line goes through the \\(p\\%\\) quantile of the \\(Y\\) values at each value of \\(X\\). Think of it like this: at any location along the x-axis, there is a spread of points on the y-axis that contains a first quartile, a median, and a third quartile. The 25%, 50%, and 75% quantile regression lines go through these points. Why fit the three quartiles? The classic spread-level plot typically fits a regression, which is the expected (mean) residual, conditional on the fitted value (for example, the plot in section @ref(glm-fig3a_m1-spreadlevel)). Researchers typically look at this and think, “the whole spread of points is becoming more variable as the fitted value increases” but the regression is only modeling the mean. By fitting the first, second, and third quartile regressions, this spread-level plot evaluates deviations from expected spread not just at the mean but in the lower and upper halves of the spread. 18.3.2.6 Model checking a GLM III – Checking dispersion (#fig:glm-fig3a_m2-check-dispersion-again)Dispersion plot of the negative binomial GLM fit to the angiogenic sprouts (fig3a) data. Notes This plot is a histogram of the sum of squared Pearson residuals of fake data sampled from the fit model. Pearson residuals are the raw residuals divided by the square root of the fitted value. Remember that in the Poisson distribution, the variance is equal to the expectation (mean), so a Pearson residual is the raw residual divided by the standard deviation of the residual. A way to think about this is, Pearson residuals “correct” for the heterogeneity in variance that arises among groups with different mean counts. The sum of squared Pearson residuals is a measure of the dispersion of the residuals. The red line is the observed sum of the squared Pearson residuals of the fit model. If the observed dispersion approximates that expected from sampling from the fit model, the red line will be within the histogram. The red line here is far larger than expected given the histogram, which indicates that the residuals are overdispersed given the fit model. Overdispersion will be common with Poisson GLM fits to biological data. 18.3.2.7 Model Checking a count GLM – Check zero inflation Counts can have the value zero. Data that have more zeros than expected given a fit count GLM model (Poisson, quasi-Poisson, negative binomial) is zero-inflated. zero_inflation_test &lt;- testZeroInflation(simulation_output) This plot is a histogram of the number of zeros in each of the fake data sets generated by the fit model. An observed number of zeros at the extremes of this distribution are unlikely given the fit model. The number of zeros in the observed data is greater than expected by the model. 18.3.3 Biological count data are rarely fit well by a Poisson GLM. Instead, fit a quasi-poisson or negative binomial GLM model. Here are the diagnostic plots of the negative binomial GLM fit to the fig3a data (#fig:glm-fig3a_m3-qq-again)Quantile-residual uniform-QQ plot of the negative binomial GLM fit to the angiogenic sprouts (fig3a) data. (#fig:glm-fig3a_m3-spreadlevel-again)Spread-level plot of the negative binomial GLM fit to the angiogenic sprouts (fig3a) data. (#fig:glm-fig3a_m3-check-dispersion-again)Dispersion plot of the negative binomial GLM fit to the angiogenic sprouts (fig3a) data. 18.3.4 A GLM is a linear model on the link scale The negative-binomial GLM fit to the angiogenic sprout data (fig3a) is \\[ \\begin{align} \\texttt{sprouts}_i &amp;\\sim \\operatorname{NB}(\\mu_i, \\theta)\\\\ \\mathrm{E}(\\texttt{sprouts}\\ | \\ \\texttt{treatment, genotype}) &amp;= \\mu\\\\ \\mu_i &amp;= \\mathrm{exp}(\\eta_i)\\\\ \\eta_i &amp;= \\beta_0 \\ + \\\\ &amp; \\quad \\; \\beta_1 \\texttt{treatment}_{\\texttt{GAS6}} \\ + \\\\ &amp; \\quad \\; \\beta_2 \\texttt{genotype}_{\\texttt{FAK_ko}} \\ + \\\\ &amp; \\quad \\; \\beta_3 \\texttt{treatment}_{\\texttt{GAS6}}:\\texttt{genotype}_{\\texttt{FAK_ko}} \\tag{18.4} \\end{align} \\] The first line of Model (18.4) is the stochastic part stating the response is modeled as a random Negative Binomial variable with conditional mean \\(\\mu_i\\), and variance \\(\\mu + \\frac{\\mu^2}{\\theta}\\). Fitting the model to the data estimates \\(\\mu_i\\) for all i. \\(\\mu_i\\) will be the same for all mice within each treatment by genotype combination because they share the same conditions (the values of the indicator variables for treatment, genotype, and their interaction). The second line states the \\(\\mu\\) is the mean conditional on the value of \\(\\texttt{treatment}\\) and \\(\\texttt{genotype}\\) The third line connects the conditional mean on the link scale (\\(\\eta\\)) with the conditional mean on the response scale (\\(\\mu\\)). This is the backtransformation. The fourth line is the linear predictor – it is a linear model on the link scale. The linear predictor includes three indicator variables. Remember that a linear model is a model in which the coefficients are additive, meaning that the coefficients do not have exponents or are not multiplied by each other. 18.3.5 Coeffecients of a Generalized Linear Model with a log-link function are on the link scale. The coefficients of the fit negative binomial model are Estimate Std. Error z value Pr(&gt;|z|) 2.5 % 97.5 % (Intercept) 0.90446 0.231 3.9 0.000 0.45 1.36 treatmentGAS6 1.29805 0.287 4.5 0.000 0.74 1.86 genotypeFAK_ko 0.04462 0.357 0.1 0.900 -0.65 0.75 treatmentGAS6:genotypeFAK_ko 0.46382 0.425 1.1 0.275 -0.37 1.30 Notes The coefficients are on the link scale, which is a linear (additive) scale. 18.3.5.1 The intercept of a Generalized Linear Model with a log-link function is the mean of the reference on the link scale In the linear model fig3a_m1, the intercept is the modeled mean of the reference group. In a GLM with a log-link (including Models fig3a_m2 and fig3a_m3), the intercept is the mean of the reference group on the link scale. The modeled mean of the reference on the response scale is computed by the backtransformaiton \\(\\exp(b_1)\\). The function “exp(x)” is the exponent, which is often written using the notation \\(e^{x}\\). The transformation between link scale and response scale is part of the specification of a Generalized Linear Model. For the negative binomial GLM fit to the fig3a data, this specification is in line 3 of the specification (Model (18.4)). Compare the computation here with the modeled mean in the emmeans table (Section 18.2.2.5 or with a computation of the sample mean. b1 &lt;- coef(fig3a_m3)[1] exp(b1) ## (Intercept) ## 2.470588 mean(fig3a[t_by_g == &quot;PBS FAK_wt&quot;, sprouts]) ## [1] 2.470588 18.3.5.2 The coefficients of the indicator variables of a Generalized Linear Model with a log-link function are effect-ratios on the response scale In the linear model fig3a_m1, the coefficient of an indicator variable is a difference in means. In a GLM with a log-link, the coefficient of an indicator variable is the difference (group minus reference) of the modeled means on the link scale. Let’s check this with some computations. The modeled means on the link scale are in the emmeans table fig3a_m3_emm_link &lt;- emmeans(fig3a_m3, specs = c(&quot;treatment&quot;, &quot;genotype&quot;)) %&gt;% summary() %&gt;% data.table() Using this table, the difference between the link-scale mean of “GAS6 FAK_wt” and “PBS FAK_wt” (the reference) is fig3a_m3_emm_link[treatment == &quot;GAS6&quot; &amp; genotype == &quot;FAK_wt&quot;, emmean] - fig3a_m3_emm_link[treatment == &quot;PBS&quot; &amp; genotype == &quot;FAK_wt&quot;, emmean] ## [1] 1.298045 And the coefficient for the \\(\\texttt{treatment_gas6}\\) indicator variable is # coefficient for treatment_gas6 b2 &lt;- coef(fig3a_m3)[2] b2 ## treatmentGAS6 ## 1.298045 In a GLM with a log-link, the exponent of the coefficient of an indicator variable is the ratio of the modeled means on the response scale. Let’s check this with some computations. The modeled means on the response scale are in the emmeans table fig3a_m3_emm_response &lt;- emmeans(fig3a_m3, specs = c(&quot;treatment&quot;, &quot;genotype&quot;), type = &quot;response&quot;) %&gt;% summary() %&gt;% data.table() Using this table, the ratio of the response-scale mean of “GAS6 FAK_wt” to the response-scale mean of “PBS FAK_wt” (the reference) is fig3a_m3_emm_response[treatment == &quot;GAS6&quot; &amp; genotype == &quot;FAK_wt&quot;, response]/ fig3a_m3_emm_response[treatment == &quot;PBS&quot; &amp; genotype == &quot;FAK_wt&quot;, response] ## [1] 3.662132 And the coefficient for the \\(\\texttt{treatment_gas6}\\) indicator variable backtransformed to the response scale is exp_b2 &lt;- exp(b2) exp_b2 ## treatmentGAS6 ## 3.662132 Since this backtransformed coefficient is both an effect and a ratio, I call it an effect ratio. It’s value is how many times bigger (or smaller if less than one) the non-reference response is relative to the reference response. The response of the GAS6 treatment is 3.66\\(\\times\\) that of the reference treatment. 18.3.6 Modeled means in the emmeans table of a Generalized Linear Model can be on the link scale or response scale – Report the response scale 18.3.6.1 The emmeans table on the link scale contains the log means (#tab:fig3a_m3_emm_link)Emmeans table on the link scale treatment genotype emmean SE df asymp.LCL asymp.UCL PBS FAK_wt 0.90446 0.23 Inf 0.45 1.36 GAS6 FAK_wt 2.20250 0.17 Inf 1.87 2.54 PBS FAK_ko 0.94908 0.27 Inf 0.42 1.48 GAS6 FAK_ko 2.71094 0.16 Inf 2.40 3.02 The values are the statistics on the link scale. The mean and CI, but not the SE, can be meaningfully backtransformed to the response scale using the exponent function to get the mean and CI on the response scale. The CIs are “asymptotic”, meaning they are computed using infinite degrees of freedom. The consequences is that the CIs will be too narrow, especially for small n. Check the math! Given asymptotic CIs, the lower CI should be (mean - 1.96 \\(\\times\\) SE) and the upper should be (mean + 1.96 \\(\\times\\) SE). 0.90446 - 1.96*0.23 ## [1] 0.45366 A GLM is linear on the link scale. This means the model is additive on the link scale. Modeled means on the link scale are computed by adding the coefficients. b &lt;- coef(fig3a_m3) # the model coefficients # the way I typically compute these pbs_fak_wt_link &lt;- b[1] gas6_fak_wt_link &lt;- b[1] + b[2] pbs_fak_ko_link &lt;- b[1] + b[3] gas6_fak_ko_link &lt;- b[1] + b[2] + b[3] + b[4] # but this is the algebra more consistent with the linear model math pbs_fak_wt_link &lt;- b[1] + b[2]*0 + b[3]*0 + b[4]*0 gas6_fak_wt_link &lt;- b[1] + b[2]*1 + b[3]*0 + b[4]*0 pbs_fak_ko_link &lt;- b[1] + b[2]*0 + b[3]*1 + b[4]*0 gas6_fak_ko_link &lt;- b[1] + b[2]*1 + b[3]*1 + b[4]*1 # combine into a table fig3a_means &lt;- data.table( group = t_by_g_levels, &quot;mean (link scale)&quot; = c(pbs_fak_wt_link, gas6_fak_wt_link, pbs_fak_ko_link, gas6_fak_ko_link) ) Table 18.1: Table of group treatment by genotype means on the link scale group mean (link scale) PBS FAK_wt 0.90446 GAS6 FAK_wt 2.20250 PBS FAK_ko 0.94908 GAS6 FAK_ko 2.71094 Compare the values here to those in the emmeans table on the link scale (Table @ref(tab:fig3a_m3_emm_link)). 18.3.6.2 The emmeans table on the response scale contains more readily interpretable means (#tab:fig3a_m3_emm_response)Emmeans table on the response scale treatment genotype response SE df asymp.LCL asymp.UCL PBS FAK_wt 2.47059 0.57 Inf 1.57 3.88 GAS6 FAK_wt 9.04762 1.54 Inf 6.48 12.64 PBS FAK_ko 2.58333 0.70 Inf 1.52 4.40 GAS6 FAK_ko 15.04348 2.36 Inf 11.06 20.46 The values in the column “response” are the modeled means on the response scale. These values are the exponent of the values in the “emmean” column of the emmeans table on the link scale. The values in the columns “asymp.LCL” and “asymp.UCL” are the 95% confidence intervals on the response scale. These values are the exponent of the values in the same columns of the emmeans table on the link scale. Don’t do additive math on the response scale! Remember, a GLM is linear on the link scale. The CIs on the response scale are not the mean plus or minus 1.96 \\(\\times\\) SE! 2.47059 - 1.96*0.57 ## [1] 1.35339 # oops Modeled means on the response scale are computed by backtransforming the modeled means on the link scale. Since the fit model used a log-link, the backtransformation is the exponent. pbs_fak_wt_response &lt;- exp(pbs_fak_wt_link) gas6_fak_wt_response &lt;- exp(gas6_fak_wt_link) pbs_fak_ko_response &lt;- exp(pbs_fak_ko_link) gas6_fak_ko_response &lt;- exp(gas6_fak_ko_link) # add a column to the fig3a_means data.table fig3a_means[, &quot;mean (response scale)&quot; := c(pbs_fak_wt_response, gas6_fak_wt_response, pbs_fak_ko_response, gas6_fak_ko_response)] Table 18.2: Table of group treatment by genotype means on both the link and response scale. group mean (link scale) mean (response scale) PBS FAK_wt 0.90446 2.47059 GAS6 FAK_wt 2.20250 9.04762 PBS FAK_ko 0.94908 2.58333 GAS6 FAK_ko 2.71094 15.04348 Compare the values here to those in the emmeans table on the response scale (Table @ref(tab:fig3a_m3_emm_response)). 18.3.7 Some consequences of fitting a linear model to count data 18.3.7.1 One – linear models can make absurd predictions plot_grid(gg1, gg2, gg3, ncol=3, labels = &quot;AUTO&quot;) Notes A prediction interval is a confidence interval of a prediction – using the fit model to predict future responses given the same conditions (here, assignment to one of the four different treatment combinations). Left panel: The prediction intervals from the linear model imply that negative sprouts could be sampled. This is absurd. Middle panel: The fit linear model is used to make 100 fake predictions in each group. Right panel: The fit negative binomial GLM is used to make 100 fake predictions in each group. Nothing absurd here. 18.3.7.2 Two – linear models can perform surprisingly well if one is only interested in p-values P-values are a function of the sampling distribution of group means and differences in means, and, due to the magic of the central limit theorem, linear models fit to count data perform surprisingly well in the sense of Type I error that approximates the nominal value Reasonable power compared to GLM models and many non-parametric tests. 18.4 Example 2 – Use a GLM with an offset instead of a ratio of some measurement per total (“dna damage” data fig3b) #glm_offset A problem that often arises in count data (and other kinds of measures) are counts that are taken in samples with different areas or volumes of tissue. As a consequence, size and treatment response are confounded – samples with higher counts may have higher counts because of a different response to treatment, a larger amount of tissue, or some combination. The common practice in experimental biology is to adjust for tissue size variation by area-normalizing the count using the ratio \\(\\frac{count}{area}\\) and then testing for a difference in the normalized count using either a linear model NHST (t-test/ANOVA) or a non-parametric NHST (Mann-Whitney-Wilcoxan). These practices raise at least two statistical issues. The ratio will have some kind of ratio distribution that violates the normal distribution assumption of the linear model. A Mann-Whitney-Wilcoxan does not estimate meaningful effects, is less powerful than GLM models, and is not flexible for complex designs including factorial or covariate models. A better practice is to model the count using a GLM that adds the denominator of the ratio (the area or volume) as an offset in the model. A count GLM with offset also models the ratio (see section 18.5.2 below), but in a way that is likely to be much more compatible with the data. NHST of the ratios will perform okay in the sense of Type I error that is close to nominal but will have relatively low power compared to a generalized linear model with offset. If the researcher is interested in best practices including the reporting of uncertainty of estimated effects, a GLM with offset will have more useful confidence intervals – for example CIs from linear model assuming Normal error can often include absurd values such as ratios less than zero. Source article (Fernández, Álvaro F., et al. “Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice.” Nature 558.7708 (2018): 136-140.)https://www.nature.com/articles/s41586-018-0162-7 18.4.1 fig3b (“dna damage”) data These data were first introduced in the Issues chapter, Section 13.4.2. There only the age 20 month data were analyzed. Here the full dataset is analyzed. Public source Source data for Fig. 3 The example here is from Fig 3b. 18.4.2 Understand the data Response variable – number of TUNEL+ cells measured in kidney tissue, where a positive marker indicates nuclear DNA damage. Background. The experiments in Figure 3 were designed to measure the effect of a knock-in mutation in the gene for the beclin 1 protein on autophagy and tissue health in the kidney and heart. The researchers were interested in autophagy because there is evidence in many non-mammalian model organisms that increased autophagy reduces age-related damage to tissues and increases health and lifespan. BCL2 is an autophagy inhibitor. Initial experiments showed that the knock-in mutation in beclin 1 inhibits BCL2. Inhibiting BCL2 with the knock-in mutation should increase autophagy and, as a consequence, reduce age-related tissue damage. Design - \\(2 \\times 2\\) factorial with offset Factor 1: \\(\\texttt{age}\\) with levels “Young” (two months) and “Old” (twenty months). Factor 2: \\(\\texttt{genotype}\\) with levels “WT” (wildtype) and “KI” (knock-in). Offset: \\(\\texttt{area_mm2}\\). The area of the kidney tissue containing the counted cells. The planned contrasts are (Young KI) - (Young WT) – the effect of the beclin 1 knock-in mutation in the 2 month old mice. We expect this effect to be negative (more damage in the WT) but small, assuming there is little BCL2-related DNA damage by 2 months. (Old KI) - (Old WT) – the effect of the beclin 1 knock-in mutation in the 20 month old mice. We expect this effect to be negative (more damage in the WT) but large assuming there is substantial BCL2-related DNA damage by 20 months. Interaction. The magnitude of the interaction will largely be a function of the difference in BCL2-related DNA damage between 2 and 20 months and not really a function with how the knockin functions at these two ages. 18.4.3 Model fit and inference 18.4.3.1 Fit the models # lm with ratio response fig3b_m1 &lt;- lm(count_per_area ~ age * genotype, data = fig3b) # poisson offset fig3b_m2 &lt;- glm(positive_nuclei ~ age * genotype + offset(log(area_mm2)), family = &quot;poisson&quot;, data = fig3b) # nb offset fig3b_m3 &lt;- glm.nb(positive_nuclei ~ age * genotype + offset(log(area_mm2)), data = fig3b) # non-parametric fig3b_m4_2mo &lt;- wilcox.test(count_per_area ~ genotype, data = fig3b[age == &quot;Young&quot;]) fig3b_m4_20mo &lt;- wilcox.test(count_per_area ~ genotype, data = fig3b[age == &quot;Old&quot;]) 18.4.3.2 Check the models ggcheck_the_model(fig3b_m1) fig3b_m2_simulation &lt;- simulateResiduals(fittedModel = fig3b_m2, n = 250) plot(fig3b_m2_simulation, asFactor = FALSE) fig3b_m3_simulation &lt;- simulateResiduals(fittedModel = fig3b_m3, n = 250) plot(fig3b_m3_simulation, asFactor = FALSE) 18.4.3.3 Inference from the model fig3b_m3_coef &lt;- cbind(coef(summary(fig3b_m3)), confint(fig3b_m3)) Estimate Std. Error z value Pr(&gt;|z|) 2.5 % 97.5 % (Intercept) -0.07 0.508 -0.1 0.887 -0.95 1.09 ageOld 0.86 0.575 1.5 0.133 -0.39 1.91 genotypeKI -0.10 0.722 -0.1 0.887 -1.56 1.36 ageOld:genotypeKI -0.66 0.806 -0.8 0.416 -2.27 0.96 mean_area &lt;- mean(fig3b[age == &quot;Old&quot; &amp; genotype == &quot;WT&quot;, positive_nuclei]) fig3b_m3_emm &lt;- emmeans(fig3b_m3, specs = c(&quot;age&quot;, &quot;genotype&quot;), type=&quot;response&quot;, offset = log(mean_area)) age genotype response SE df asymp.LCL asymp.UCL Young WT 58.20 29.6 Inf 21.51 157.48 Old WT 137.86 37.0 Inf 81.42 233.40 Young KI 52.53 27.0 Inf 19.20 143.72 Old KI 64.57 15.3 Inf 40.55 102.80 fig3b_m3_pairs &lt;- contrast(fig3b_m3_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) # fig3b_m3_emm # print in console to get row numbers # set the mean as the row number from the emmeans table young_wt &lt;- c(1,0,0,0) old_wt &lt;- c(0,1,0,0) young_ki &lt;- c(0,0,1,0) old_ki &lt;- c(0,0,0,1) #1. (Young KI) - (Young WT) #2. (Old KI) - (Old WT) #3. Interaction fig3b_m3_planned &lt;- contrast( fig3b_m3_emm, method = list( &quot;(Young KI) - (Young WT)&quot; = c(young_ki - young_wt), &quot;(Old KI) - (Old WT)&quot; = c(old_ki - old_wt), &quot;Interaction&quot; = c((old_ki - old_wt) - (young_ki - young_wt)) # &quot;Interaction&quot; = c(old_ki - old_wt) - # c(young_ki - young_wt) ), adjust = &quot;none&quot; ) %&gt;% summary(infer = TRUE) contrast ratio SE df asymp.LCL asymp.UCL null z.ratio p.value (Young KI) / (Young WT) 0.90 0.652 Inf 0.22 3.72 1 -0.14200 0.9 (Old KI) / (Old WT) 0.47 0.168 Inf 0.23 0.95 1 -2.11608 0.0 Interaction 0.52 0.418 Inf 0.11 2.52 1 -0.81353 0.4 Notes 18.4.3.4 Plot the model 18.5 Understanding Example 2 18.5.1 An offset is an added covariate with a coefficient fixed at 1 The systematic part (the linear predictor) of the negative binomial model fit to the fig3b data is \\[ \\begin{align} \\texttt{positive_nuclei} = \\ &amp;\\beta_0 + \\beta_1 \\texttt{age}_\\texttt{Old} + \\beta_2 \\texttt{genotype}_\\texttt{KI} \\ + \\\\ &amp;\\beta_3 \\texttt{age}_\\texttt{Old}\\texttt{:genotype}_\\texttt{KI} \\ + \\\\ &amp;1.0 \\times \\textrm{log}(\\texttt{area_mm2}) \\end{align} \\] Notes The offset variable is \\(\\textrm{log}(\\texttt{area_mm2})\\). An offset is a covariate with a fixed coefficient of 1.0 – this coefficient is not estimated. Figure 18.3 illustrates the offset on the response and the log scale. Figure 18.3: What an offset looks like. A. The offset on the response scale. B. The offset on the log scale. In A and B, only the lines for the 20 month old mice are shown. On the log scale (Figure 18.3B), the offset curves are parallel because each is forced to have a slope of one on the link scale. On the response scale, these curves have different slopes – this slope is \\(\\mathrm{exp}(b_0)\\), that is, the exponent of the intercept on the log scale). The plot on the log scale (Figure 18.3B) is easy to misinterpret as a regression with a slope of 1 fit to the log transformed values. It isn’t, because individual values are not transformed in a GLM. I’m specifically referring to this space as the log and not link scale to try to avoid this misinterpretation. 18.5.2 A count GLM with an offset models the area-normalized means In the t-test or ANOVA of the difference in the area-normalized count of TUNEL+ nuclei, the systematic component of the equivalent linear model is (using only the 20 month old mice for simplicity) \\[ \\frac{\\mu}{\\texttt{area_mm2}} = \\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI} \\] where \\(\\mu\\) is the mean response (\\(\\texttt{positive_nuclei}\\)) conditional on the level of genotype. Statisticians typically refer to the ratio \\(\\frac{\\mu}{\\texttt{area_mm2}}\\) as a rate – here, it is the expected rate of observing a TUNEL+ nucleus as more area is searched. Let’s compare this to the GLM model recommended here. Recall that the conditional expecation on the link scale is \\(\\eta\\) and, using the inverse link function, \\(\\mathrm{log}(\\mu) = \\eta\\), the systematic part of the count GLM model with offset is \\[ \\begin{align} \\eta &amp;= \\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI} + \\mathrm{log}(\\texttt{area_mm2}) \\\\ \\mathrm{log}(\\mu) &amp;= \\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI} + \\mathrm{log}(\\texttt{area_mm2}) \\\\ \\mathrm{log}(\\mu) - \\mathrm{log}(\\texttt{area_mm2}) &amp;= \\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI} \\\\ \\mathrm{log}(\\frac{\\mu}{\\texttt{area_mm2}}) &amp;= \\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI} \\\\ \\frac{\\mu}{\\texttt{area_mm2}} &amp;= \\mathrm{exp}(\\beta_0 + \\beta_1 \\texttt{genotype}_\\texttt{KI}) \\\\ \\end{align} \\] That is, the GLM with offset also models the conditional mean of the area-normalized count, but in a way where the data are much more compatible with the assumptions of the statistical model (in the sense that model checks show that the data look like a sample from the statistical model). If you want the area-normalized means and their uncertainty under the assumptions of the count GLM offset model, here is some code. mean_area &lt;- mean(fig3b[, area_mm2]) # link scale # compute rate or normalized count using emmeans # setting the offset will compute mean at the specified # value of the offset # be sure the stats are on the link scale m3_emm &lt;- emmeans(fig3b_m3, specs = c(&quot;age&quot;, &quot;genotype&quot;), offset = log(mean_area)) %&gt;% summary() %&gt;% data.table() # emmeans is returning the first line in equations above. # We want the last line. So subtract the log of the offset value. m3_emm[, log_rate := emmean - log(mean_area)] # and compute the 95% asymptotic CIs m3_emm[, log_rate_lcl := log_rate - 1.96*SE] m3_emm[, log_rate_ucl := log_rate + 1.96*SE] # m3_emm # response scale m3_emm_response &lt;- emmeans(fig3b_m3, specs = c(&quot;age&quot;, &quot;genotype&quot;), offset = log(mean_area), type = &quot;response&quot;) %&gt;% summary() %&gt;% data.table() # replace the mean and CIs with the backtransformed values from the # link scale table m3_emm_rates &lt;- m3_emm_response m3_emm_rates[, response := exp(m3_emm$log_rate)] m3_emm_rates[, asymp.LCL := exp(m3_emm$log_rate_lcl)] m3_emm_rates[, asymp.UCL := exp(m3_emm$log_rate_ucl)] Table 18.3: Modeled mean area-normalized counts of TUNEL+ cells, estimated using negative binomial GLM. age genotype response SE df asymp.LCL asymp.UCL Young WT 0.93 10.79 Inf 0.34 2.5 Old WT 2.20 13.52 Inf 1.30 3.7 Young KI 0.84 9.85 Inf 0.31 2.3 Old KI 1.03 5.59 Inf 0.65 1.6 Notes Again, no ratios were computed. The mean area-normalized counts were computed using the coefficients of the GLM. Here, we used the emmeans package to do this. See section xxx below for code that does this withouth the emmeans package. Figure 18.4: Area-normalized counts modeled using negative binomial GLM with offset, with the raw count as the response (A) and linear model with the area-normalized count as the response. Notes In Panel B, the CIs of the two Young treatment groups have lower bounds that extend below zero. This is absurd – negative area-normalized-count means cannot exist. But this inference is implied by an ANOVA of these data. Panel A has the potential to lead to misconceptions because the GLM model with offset is not fit to the area-normalized data but to the raw counts. Regardless, the GLM model with offset is modeling the means of these ratios. More than I need to know. The modeled means of a count GLM on the link scale are \\(\\mathbf{X} \\mathbf{b}\\), where \\(\\mathbf{X}\\) is the model matrix and \\(\\mathbf{b}\\) is the vector of model coefficients. In a model with an offset, the values of the offset variable are not in the model matrix and the coefficient (1.0) is not in the vector of coefficients. Following the math above, this expectation is the modeled mean normalized-count (on the link scale) and not the modeled mean count conditional on some offset value (on the link scale). Again, following the math above, to compute the expected count conditional on some value of the offset, use \\(\\mathbf{X} \\mathbf{b} + \\mathrm{log}(offset)\\). But we want the rate not the mean, and we want the rate on the response scale, so simply use \\(\\mathrm{exp}(\\mathbf{X} \\mathbf{b})\\). # compute rate or normalized count using matrix algebra using # final line of equations above b &lt;- coef(fig3b_m3) X &lt;- model.matrix(fig3b_m3) fig3b[, rate := exp((X %*% b)[,1])] # compare rate to area-normalized count rates &lt;- fig3b[, .(rate_manual = mean(rate), norm_count = mean(count_per_area)), by = .(age, genotype)] m3_emm &lt;- merge(m3_emm_rates, rates, by = c(&quot;age&quot;, &quot;genotype&quot;)) Table 18.4: Mean area-normalized-counts computed using the emmeans package (\\(\\texttt{response}\\)), the matrix algebra code in this section (\\(\\texttt{rate_manual}\\)), and as the simple average of the normalized counts (\\(\\texttt{norm_count}\\)) age genotype response SE df asymp.LCL asymp.UCL g_by_age rate_manual norm_count Young WT 0.9304 10.79 Inf 0.34 2.52 WT Young 0.9304 0.9269 Young KI 0.8397 9.85 Inf 0.31 2.30 KI Young 0.8397 0.8432 Old WT 2.2039 13.52 Inf 1.30 3.73 WT Old 2.2039 2.2027 Old KI 1.0322 5.59 Inf 0.65 1.64 KI Old 1.0322 1.0284 18.5.3 Compare an offset to an added covariate with an estimated coefficient fig3b_m5 &lt;- glm.nb(positive_nuclei ~ age * genotype + log(area_mm2), data = fig3b) Table 18.5: Coefficients from model fig3b_m5 (covariate) and model fig3b_m3 (offset). Estimate Std. Error z value Pr(&gt;|z|) 2.5 % 97.5 % fig3b_m5 (covariate) (Intercept) -3.36 1.48 -2.27 0.0232 -6.27 -0.33 ageOld -0.67 0.89 -0.75 0.4528 -2.33 1.00 genotypeKI 0.24 0.70 0.35 0.7299 -1.20 1.69 log(area_mm2) 2.48 0.64 3.89 0.0001 1.19 3.74 ageOld:genotypeKI -1.16 0.78 -1.48 0.1390 -2.81 0.47 fig3b_m3 (offset) (Intercept) -0.07 0.51 -0.14 0.8871 -0.95 1.09 ageOld 0.86 0.57 1.50 0.1334 -0.39 1.91 genotypeKI -0.10 0.72 -0.14 0.8871 -1.56 1.36 ageOld:genotypeKI -0.66 0.81 -0.81 0.4159 -2.27 0.96 Notes The coefficient table for Model fig3b_m5 has a row for the added covariate \\(\\texttt{area_mm2}\\). This row is absent for Model fig3b_m3 since the coefficient is fixed at 1.0. The estimate of the coefficient of \\(\\texttt{area_mm2}\\) in Model fig3b_m5 is 2.48. A value of 1.0 – the value assumed by the offset model or an analysis of the ratio – is not very compatible with the data. A coefficient greater than 1.0 means that the rate of DNA damage (the number of TUNEL positive cells per area) increases with the size of the area measured. Does this make any biological sense? The area measured should not have a biological component unless it is a function of the size of the organ. If the area measured is a function of the size of the organ, then we would want to use the ANCOVA model and not the offset model. This increase in the rate of TUNEL positive cells is seen in the response-scale plot of the fit model in Figure ??A, where the slope of the regression line increases as \\(\\texttt{area_mm2}\\) increases. Note that on the log scale (Figure ??B), the two regression lines are parallel, as they must be in an ANCOVA linear model. 18.5.4 Issues with plotting 18.6 Example 3 – GLM models for binary responses 18.7 Working in R 18.7.1 Fitting GLMs to count data The Poisson family is specified with the base R glm() function. For negative binomial, use glm.nb from the MASS package # poisson - less likely to fit to real biological data well # because of overdispersion fit &lt;- glm(y ~ treatment, family = &quot;poisson&quot;, data = dt) # two alternatives to overdispersed poisson fit # quasipoisson fit &lt;- glm(y ~ treatment, family = &quot;quasipoisson&quot;, data=dt) # note that &quot;family&quot; is not an argument since this function is used only to fit a negative binomial distribution! fit &lt;- glm.nb(y ~ treatment, data = dt) 18.7.2 Fitting a GLM to a continuous conditional response with right skew. The Gamma family is specified with the base R glm() function. fit &lt;- glm(y ~ treatment, family = Gamma(link = &quot;log&quot;), data = dt) 18.7.3 Fitting a GLM to a binary (success or failure, presence or absence, survived or died) response The binomial family is specified with base R glm() function. # if the data includes a 0 or 1 for every observation of y fit &lt;- glm(y ~ treatment, family = &quot;binomial&quot;, data = dt) # if the data includes the frequency of success AND there is a measure of the total n dt[ , failure := n - success] fit &lt;- glm(cbind(success, failure) ~ treatment, family = &quot;binomial&quot;, data = dt) 18.7.4 Fitting Generalized Linear Mixed Models Generalized linear mixed models are fit with glmer from the lmer package. # random intercept of factor &quot;id&quot; fit &lt;- glmer(y ~ treatment + (1|id), family = &quot;poisson&quot;, data = dt) # random intercept and slope of factor &quot;id&quot; fit &lt;- glmer(y ~ treatment + (treatment|id), family = Gamma(link = &quot;log&quot;), data = dt) # Again, negative binomial uses a special function fit &lt;- glmer.nb(y ~ treatment + (treatment|id), data = dt) Another good package for GLMMs is glmmTMB from the glmmTMB package # negative binomial fit &lt;- glmmTMB(y ~ treatment + (1|id), family=&quot;nbinom2&quot;, data = dt) # nbinom1, the mean variance relationship is that of quasipoisson fit &lt;- glmmTMB(y ~ treatment + (1|id), family=&quot;nbinom1&quot;, data = dt) 18.8 Model checking GLMs The DHARMa package has an excellent set of model checking tools. The DHARMa package uses simulation to generate fake data sampled from the fit model using the function simulateResiduals. simulation_output &lt;- simulateResiduals(fittedModel = fig3a_m3, n = 250, refit = FALSE) simulation_output &lt;- simulateResiduals(fittedModel = fig3a_m2, n = 250, refit = FALSE) plot(simulation_output) plotQQunif(simulation_output) Three test statistics are superimposed. Use these p-values cautiously – they are guides and not thresholds of demarcation. The two we care about here are * The KS statistic indicates that the quantile residuals are not very compatible with a Poisson model – think of this as having a very low probability of sampling these counts from a Poisson with the estimated \\(\\lambda\\). * The dispersion statistic indicates that the value of the dispersion of the quantile residuals is not very compatible with a Poisson model – think of this as having a very low probability of sampling counts with this dispersion from a Poisson with the estimated \\(\\lambda\\). 18.9 Hidden code 18.9.1 Import Example 1 data (fig3a – “angiogenic sprouts”) data_from &lt;- &quot;Pericyte FAK negatively regulates Gas6-Axl signalling to suppress tumour angiogenesis and tumour growth&quot; file_name &lt;- &quot;41467_2020_16618_MOESM3_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) fig3a_wide &lt;- read_excel(file_path, sheet = &quot;Figure 3&quot;, range = &quot;B4:E26&quot;, col_names = FALSE) %&gt;% data.table() input_labels &lt;- c(&quot;PBS FAK_wt&quot;, &quot;PBS FAK_ko&quot;, &quot;GAS6 FAK_wt&quot;, &quot;GAS6 FAK_ko&quot;) colnames(fig3a_wide) &lt;- input_labels fig3a &lt;- melt(fig3a_wide, measure.vars = input_labels, variable.name = &quot;t_by_g&quot;, value.name = &quot;sprouts&quot;) %&gt;% na.omit() # change order of factor levels t_by_g_levels &lt;- c(&quot;PBS FAK_wt&quot;, &quot;GAS6 FAK_wt&quot;, &quot;PBS FAK_ko&quot;, &quot;GAS6 FAK_ko&quot;) fig3a[, c(&quot;treatment&quot;, &quot;genotype&quot;):= tstrsplit(t_by_g, &quot; &quot;, fixed = TRUE)] fig3a[, t_by_g := factor(t_by_g, levels = t_by_g_levels)] treatment_levels &lt;- c(&quot;PBS&quot;, &quot;GAS6&quot;) fig3a[, treatment := factor(treatment, levels = treatment_levels)] genotype_levels &lt;- c(&quot;FAK_wt&quot;, &quot;FAK_ko&quot;) fig3a[, genotype := factor(genotype, levels = genotype_levels)] the variance is less than that expected by the probability model↩︎ the variance is greater than that expected by the probability model↩︎ "],["gls.html", "Chapter 19 Linear models with heterogenous variance", " Chapter 19 Linear models with heterogenous variance "],["appendix-an-example-set-of-analyses-of-experimental-data-with-linear-models.html", "Appendix: An example set of analyses of experimental data with linear models", " Appendix: An example set of analyses of experimental data with linear models "],["ask1-bio.html", "Chapter 20 Background physiology to the experiments in Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity”", " Chapter 20 Background physiology to the experiments in Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity” A little background on the subject of the article: white adipose tissue (WAT) is composed of adipose (fat) cells that function as energy storage cells. The energy is in the form of the fatty acids in the triacylglycerols, which form large lipid drops in the cell. The stored fatty acids are released from the WAT when other organs need energy. Mammalian brown adipose tissue (BAT) is composed of adipose cells that burn the stored fat to generate heat. This is enabled by the expression of the protein uncoupling receptor 1 (UCP1) in the mitochondria. UCP1 uncouples the proton gradient across the inner mitochondrial membrane from ATP synthesis. In response to adverse health consequences of obesity, including metabolic syndrome, researchers are investigating various ways to increase BAT, or stimulate BAT activity, or transform WAT cells into more BAT-like cells, by turning up expression of UCP1. The regulation of UCP1 in WAT is a potential drug target for obesity. The researchers of the ASK1 study investigated the effects of an intracellular signaling protein (ASK1) on the browning of white adipose tissue. Previous research had suggested that 1) inflammation stimulates ASK1 activity and 2) increased ASK1 acitivty inhibits UCP1 expression (Figure 20.1. The experiments in Figure 2 of the ASK1 study follow this up and explore the question, if ASK1 is knocked out in the WAT cells, will this reverse the adverse effects of a high-fat diet, including weight gain, glucose intolerance, and liver triacylglycerol levels? Figure 20.1: Inflammation to obesity stimulates ASK1 activity. ASK1 activity inhibits UCP1 expression. For the experiments in Figure 2, the researchers created mice in which the ASK1 gene was inhibited from being expressed (or “knocked out”) in the white adipose tissue cells. The \\(ask1\\) treatment has two levels: “ASK1Δadipo”, which are the adipocyte-specific ASK1 knockout (KO) mice, and “ASK1F/F”, which are the controls. For some of the experiments, the researchers split the mice in each \\(ask1\\) treatment level and assigned these to either a Chow or a High Fat Diet (HFD). This experimental design is two-crossed factors, each with two levels, which I call a \\(2 \\times 2\\) factorial design in this text. Some of the plots are coded directly in this document. Others use functions from the chapter “Plotting functions”. But, to use these in an R Markdown document, these functions have to be saved in a “R Script” file. This script file then needs to be read at the start of the R Markdown document. I named the script file “ggplotsci.R” and placed it in a folder called “R” at the level of the project (directly within the project folder). This example was written with the Bookdown style sheet (because its part of this book), which doesn’t have one nice features of creating R Markdown documents for reports and manuscripts – code folding. In an R Markdown document with code folding, a user can toggle between showing and hiding code. The html output with code folding is here. "],["ask1-report.html", "Chapter 21 Analyses for Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity” 21.1 Setup 21.2 Data source 21.3 control the color palette 21.4 useful functions 21.5 figure 2b – effect of ASK1 deletion on growth (body weight) 21.6 Figure 2c – Effect of ASK1 deletion on final body weight 21.7 Figure 2d – Effect of ASK1 KO on glucose tolerance (whole curve) 21.8 Figure 2e – Effect of ASK1 deletion on glucose tolerance (summary measure) 21.9 Figure 2f – Effect of ASK1 deletion on glucose infusion rate 21.10 Figure 2g – Effect of ASK1 deletion on tissue-specific glucose uptake 21.11 Figure 2h 21.12 Figure 2i – Effect of ASK1 deletion on liver TG 21.13 Figure 2j", " Chapter 21 Analyses for Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity” 21.1 Setup Some plots in this document require the file “ggplotsci.R” within the “R” folder, within the project folder. knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE) # wrangling packages library(here) library(janitor) library(readxl) library(data.table) library(stringr) # analysis packages library(emmeans) library(car) # qqplot, spreadlevel library(DHARMa) # graphing packages library(ggsci) library(ggpubr) library(ggforce) library(cowplot) library(lazyWeave) #pvalstring here &lt;- here::here data_path &lt;- &quot;data&quot; ggplotsci_path &lt;- here(&quot;R&quot;, &quot;ggplotsci.R&quot;) source(ggplotsci_path) 21.2 Data source Data source: ASK1 inhibits browning of white adipose tissue in obesity This chunk assigns the path to the Excel data file for all panels of Figure 2. The data for each panel are in a single sheet in the Excel file named “Source Date_Figure 2”. data_folder &lt;- &quot;ASK1 inhibits browning of white adipose tissue in obesity&quot; file_name &lt;- &quot;41467_2020_15483_MOESM4_ESM.xlsx&quot; file_path &lt;- here(data_path, data_folder, file_name) fig_2_sheet &lt;- &quot;Source Date_Figure 2&quot; 21.3 control the color palette fig_2_palette &lt;- pal_okabe_ito[5:6] # fig_2_palette &lt;- pal_okabe_ito #fig_2_palette &lt;- pal_nature_mod 21.4 useful functions A function to import longitudinal data from Fig 2 # function to read in parts of 2b import_fig_2_part &lt;- function(range_2){ fig_2_part &lt;- read_excel(file_path, sheet = fig_2_sheet, range = range_2, col_names = TRUE) %&gt;% data.table() group &lt;- colnames(fig_2_part)[1] setnames(fig_2_part, old = group, new = &quot;treatment&quot;) fig_2_part[, treatment := as.character(treatment)] # this was read as logical fig_2_part[, treatment := group] # assign treatment group fig_2_part[, mouse_id := paste(group, .I)] return(fig_2_part) } # script to compute various area under the curves (AUC) using trapezoidal method # le Floch&#39;s &quot;incremental&quot; auc substracts the baseline value from all points. # This can create some elements with negative area if post-baseline values are less # than baseline value. # Some researchers &quot;correct&quot; this by setting any(y - ybar &lt; 0 to zero. Don&#39;t do this. auc &lt;- function(x, y, method=&quot;auc&quot;, average = FALSE){ # method = &quot;auc&quot;, auc computed using trapezoidal calc # method = &quot;iauc&quot; is an incremental AUC of Le Floch # method = &quot;pos_iauc&quot; is a &quot;positive&quot; incremental AUC of Le Floch but not Wolever # method = &quot;post_0_auc&quot; is AUC of post-time0 values # if average then divide area by duration if(method==&quot;iauc&quot;){y &lt;- y - y[1]} if(method==&quot;pos_iauc&quot;){y[y &lt; 0] &lt;- 0} if(method==&quot;post_0_auc&quot;){ x &lt;- x[-1] y &lt;- y[-1] } n &lt;- length(x) area &lt;- 0 for(i in 2:n){ area &lt;- area + (x[i] - x[i-1])*(y[i-1] + y[i]) } value &lt;- area/2 if(average == TRUE){ value &lt;- value/(x[length(x)] - x[1]) } return(value) } 21.5 figure 2b – effect of ASK1 deletion on growth (body weight) 21.5.1 figure 2b – import range_list &lt;- c(&quot;A21:N41&quot;, &quot;A43:N56&quot;, &quot;A58:N110&quot;, &quot;A112:N170&quot;) fig_2b_wide &lt;- data.table(NULL) for(range_i in range_list){ part &lt;- import_fig_2_part(range_i) fig_2b_wide &lt;- rbind(fig_2b_wide, part) } fig_2b &lt;- melt(fig_2b_wide, id.vars = c(&quot;treatment&quot;, &quot;mouse_id&quot;), variable.name = &quot;week&quot;, value.name = &quot;body_weight&quot;) fig_2b[, week := as.numeric(as.character(week))] fig_2b[, c(&quot;ask1&quot;, &quot;diet&quot;) := tstrsplit(treatment, &quot; &quot;, fixed=TRUE)] fig_2b[, week_f := factor(week)] 21.5.2 figure 2b – exploratory plots qplot(x = week, y = body_weight, data = fig_2b, color = treatment) + facet_grid(ask1~diet) no obvious outliers reduced growth rate at bigger size qplot(x = week, y = body_weight, data = fig_2b, color = treatment) + geom_smooth() loess smooth. Growth in ASK1F/F + HFD clearly greater than other three treatment combinations. 21.6 Figure 2c – Effect of ASK1 deletion on final body weight 21.6.1 Figure 2c – import range_2c &lt;- &quot;A173:BD176&quot; y_cols &lt;- c(&quot;ASK1F/F chow&quot;, &quot;ASK1Δadipo chow&quot;, &quot;ASK1F/F HFD&quot;, &quot;ASK1Δadipo HFD&quot;) fig_2c_import &lt;- read_excel(file_path, sheet = fig_2_sheet, range = range_2c, col_names = FALSE) %&gt;% transpose(make.names=1) %&gt;% data.table() %&gt;% melt(measure.vars = y_cols, variable.name = &quot;treatment&quot;, value.name = &quot;body_weight_gain&quot;) %&gt;% na.omit() fig_2c_import[, mouse_id := paste(treatment, .I, sep = &quot;_&quot;), by = treatment] 21.6.2 Figure 2c – check own computation of weight change v imported value Note that three cases are missing from fig_2c import that are in fig_2b # change colnames to char fig_2c &lt;- copy(fig_2b_wide) weeks &lt;- unique(fig_2b[, week]) setnames(fig_2c, old = colnames(fig_2c), new = c(&quot;treatment&quot;, paste0(&quot;week_&quot;, weeks), &quot;mouse_id&quot;)) fig_2c[, weight_gain := week_12 - week_0] fig_2c &lt;- fig_2c[, .SD, .SDcols = c(&quot;treatment&quot;, &quot;week_0&quot;, &quot;week_12&quot;, &quot;weight_gain&quot;)] fig_2c[, mouse_id := paste(treatment, .I, sep = &quot;_&quot;), by = treatment] fig_2c[, c(&quot;ask1&quot;, &quot;diet&quot;) := tstrsplit(treatment, &quot; &quot;, fixed=TRUE)] fig_2c_check &lt;- merge(fig_2c, fig_2c_import, by = c(&quot;mouse_id&quot;), all = TRUE) #View(fig_2c_check) 21.6.3 Figure 2c – exploratory plots qplot(x = treatment, y = weight_gain, data = fig_2c) no obvious outliers variance increases with mean, as expected from growth, suggests a multiplicative model. But start with simple lm. 21.6.4 Figure 2c – fit the model: m1 (lm) fig_2c_m1 &lt;- lm(weight_gain ~ week_0 + ask1*diet, data = fig_2c) 21.6.5 Figure 2c – check the model: m1 # check normality assumption set.seed(1) qqPlot(fig_2c_m1, id=FALSE) spreadLevelPlot(fig_2c_m1, id=FALSE) ## ## Suggested power transformation: 0.06419448 QQ indicates possible right skew but especially left side is squashed toward mean spread-level indicates variance increases with mean For p-value, this may not be too severe but for intervals, best to account for this. Try gamma with log link (which makes biological sense for growth) 21.6.6 Figure 2c – fit the model: m2 (gamma glm) fig_2c_m2 &lt;- glm(weight_gain ~ week_0 + ask1*diet, family = Gamma(link = &quot;log&quot;), data = fig_2c) 21.6.7 Figure 2c – check the model, m2 set.seed(1) fig_2c_m2_sim &lt;- simulateResiduals(fig_2c_m2, n=250) plot(fig_2c_m2_sim) well behaved QQ and spread-level 21.6.8 Figure 2c – inference from the model coef_table &lt;- cbind(coef(summary(fig_2c_m2)), exp(confint(fig_2c_m2))) %&gt;% data.table(keep.rownames = TRUE) coef_table[, Estimate:=exp(Estimate)] knitr::kable(coef_table, digits = c(0,2,2,2,4,2,2)) rn Estimate Std. Error t value Pr(&gt;|t|) 2.5 % 97.5 % (Intercept) 10.87 0.35 6.83 0.0000 5.53 21.50 week_0 0.99 0.01 -0.84 0.3997 0.96 1.02 ask1ASK1Δadipo 1.03 0.11 0.24 0.8138 0.83 1.28 dietHFD 1.56 0.08 5.45 0.0000 1.33 1.82 ask1ASK1Δadipo:dietHFD 0.79 0.13 -1.93 0.0551 0.61 1.00 fig_2c_m2_emm &lt;- emmeans(fig_2c_m2, specs = c(&quot;diet&quot;, &quot;ask1&quot;), type = &quot;response&quot;) fig_2c_m2_pairs &lt;- contrast(fig_2c_m2_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) fig_2c_m2_emm ## diet ask1 response SE df lower.CL upper.CL ## chow ASK1F/F 8.19 0.568 138 7.14 9.40 ## HFD ASK1F/F 12.75 0.548 138 11.71 13.88 ## chow ASK1Δadipo 8.41 0.721 138 7.10 9.96 ## HFD ASK1Δadipo 10.28 0.424 138 9.47 11.15 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale fig_2c_m2_pairs ## ask1 diet contrast ratio SE df lower.CL upper.CL null ## ASK1F/F . HFD / chow 1.556 0.1263 138 1.326 1.827 1 ## ASK1Δadipo . HFD / chow 1.222 0.1168 138 1.011 1.476 1 ## . chow ASK1Δadipo / (ASK1F/F) 1.026 0.1127 138 0.826 1.275 1 ## . HFD ASK1Δadipo / (ASK1F/F) 0.806 0.0485 138 0.715 0.908 1 ## t.ratio p.value ## 5.453 &lt;.0001 ## 2.097 0.0378 ## 0.236 0.8138 ## -3.587 0.0005 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale ## Tests are performed on the log scale within ASK1 Cn, HFD mean is 1.6X chow mean within ASK1 KO, HFD mean is 1.2X chow mean within chow, ASK1 KO mean is 1.0X ASK1 Cn mean within HFD, ASK1 KO mean is 0.86X ASK1 Cn mean # same as interaction effect in coefficient table contrast(fig_2c_m2_emm, interaction = &quot;pairwise&quot;, by = NULL) %&gt;% summary(infer = TRUE) ## diet_pairwise ask1_pairwise ratio SE df lower.CL upper.CL null ## chow / HFD (ASK1F/F) / ASK1Δadipo 0.785 0.0982 138 0.613 1.01 1 ## t.ratio p.value ## -1.934 0.0551 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale ## Tests are performed on the log scale the reduction in weight gain in the ASK1 KO mice compared to ASK1 CN is 0.785X. Notice that p &gt; 0.05. 21.6.9 Figure 2c – plot the model fig_2c_m2_emm_dt &lt;- summary(fig_2c_m2_emm) %&gt;% data.table fig_2c_m2_pairs_dt &lt;- data.table(fig_2c_m2_pairs) fig_2c_m2_pairs_dt[ , p_pretty := pvalString(p.value)] dodge_width &lt;- 0.8 # separation between groups # get x positions of brackets for p-values # requires looking at table and mentally figuring out # Chow is at x = 1 and HFD is at x = 2 fig_2c_m2_pairs_dt[, group1 := c(1-dodge_width/4, 1+dodge_width/4, 1-dodge_width/4, 2-dodge_width/4)] fig_2c_m2_pairs_dt[, group2 := c(2-dodge_width/4, 2+dodge_width/4, 1+dodge_width/4, 2+dodge_width/4)] pd &lt;- position_dodge(width = dodge_width) fig_2c_gg &lt;- ggplot(data = fig_2c, aes(x = diet, y = weight_gain, color = ask1)) + # points geom_sina(alpha = 0.5, position = pd) + # plot means and CI geom_errorbar(data = fig_2c_m2_emm_dt, aes(y = response, ymin = lower.CL, ymax = upper.CL, color = ask1), width = 0, position = pd ) + geom_point(data = fig_2c_m2_emm_dt, aes(y = response, color = ask1), size = 3, position = pd ) + # plot p-values (y positions are adjusted by eye) stat_pvalue_manual(fig_2c_m2_pairs_dt, label = &quot;p_pretty&quot;, y.position=c(28.5, 31, 26, 26), tip.length = 0.01) + # aesthetics ylab(&quot;Weight Gain&quot;) + scale_color_manual(values=fig_2_palette, name = NULL) + theme_pubr() + theme(legend.position=&quot;top&quot;) + theme(axis.title.x=element_blank()) + NULL fig_2c_gg 21.6.10 Figure 2c – report Results could be reported using either: (This is inconsistent with plot, if using this, the plot should reverse what factor is on the x-axis and what factor is the grouping (color) variable) Mean weight gain in ASK1F/F mice on HFD was 1.56 (95% CI: 1.33, 1.82, \\(p &lt; 0.0001\\)) times that of ASK1F/F mice on chow while mean weight gain in ASK1Δadipo mice on HFD was only 1.22 (95% CI: 1.01, 1.47, \\(p = 0.036\\)) times that of ASK1Δadipo mice on chow. This reduction in weight gain in ASK1Δadipo mice compared to ASK1F/F control mice was 0.79 times (95% CI; 0.61, 1.00, \\(p = 0.0531\\)). (This is consistent with the plot in that its comparing difference in the grouping factor within each level of the factor on the x-axis) Mean weight gain in ASK1Δadipo mice on chow was trivially larger (1.03 times) than that in ASK1F/F mice on chow (95% CI: 0.83, 1.27, \\(p = 0.81\\)) while mean weight gain in ASK1Δadipo mice on HFD was smaller (0.81 times) than that in ASK1F/F control mice on HFD (95% CI: 0.72 , 0.91, \\(p = 0.0003\\)). This reduction in weight gain in ASK1Δadipo mice compared to ASK1Δadipo mice is 0.79 times (95% CI; 0.61, 1.00, \\(p = 0.0531\\)). note to research team. The big difference in p-values between weight difference on chow and weight difference on HFD might lead one to believe there is a “difference in this difference”. Using a p-value = effect strategy, this is not supported. 21.7 Figure 2d – Effect of ASK1 KO on glucose tolerance (whole curve) 21.7.1 Figure 2d – Import range_list &lt;- c(&quot;A179:H189&quot;, &quot;A191:H199&quot;, &quot;A201:H214&quot;, &quot;A216:H230&quot;) fig_2d_wide &lt;- data.table(NULL) for(range_i in range_list){ part &lt;- import_fig_2_part(range_i) fig_2d_wide &lt;- rbind(fig_2d_wide, part) } fig_2d_wide[, c(&quot;ask1&quot;, &quot;diet&quot;) := tstrsplit(treatment, &quot; &quot;, fixed=TRUE)] # melt fig_2d &lt;- melt(fig_2d_wide, id.vars = c(&quot;treatment&quot;, &quot;ask1&quot;, &quot;diet&quot;, &quot;mouse_id&quot;), variable.name = &quot;time&quot;, value.name = &quot;glucose&quot;) fig_2d[, time := as.numeric(as.character(time))] # for plot only (not analysis!) shift &lt;- 2 fig_2d[treatment == &quot;ASK1F/F chow&quot;, time_x := time - shift*1.5] fig_2d[treatment == &quot;ASK1Δadipo chow&quot;, time_x := time - shift*.5] fig_2d[treatment == &quot;ASK1F/F HFD&quot;, time_x := time + shift*.5] fig_2d[treatment == &quot;ASK1Δadipo HFD&quot;, time_x := time + shift*1.5] 21.7.2 Figure 2d – exploratory plots qplot(x = time_x, y = glucose, color = treatment, data = fig_2d) + geom_line(aes(group = mouse_id), alpha = 0.3) * no obvious unplausible outliers but two mice w/ high values in “F/F HFD” * similar at time zero (initial effect is trivial) use AUC conditional on time 0 glucose qplot(x = time, y = glucose, data = fig_2d, color = treatment) + geom_smooth() 21.7.3 Figure 2d – fit the model 21.7.4 Figure 2d – check the model 21.7.5 Figure 2d – inference 21.7.6 Figure 2d – plot the model 21.8 Figure 2e – Effect of ASK1 deletion on glucose tolerance (summary measure) The researchers did create a table to import but this analysis uses the mean post-baseline glucose amount as the response instead of the area under the curve of over the full 120 minutes. This mean is computed as the post-baseline area under the curve divided by the duration of time of the post-baseline measures (105 minutes). This analysis will use fig_2d_wide since there is only one a single Y variable per mouse. 21.8.1 Figure 2e – message the data # AUC of post-baseline values # do this after melt as we don&#39;t need this in long format) fig_2e &lt;- fig_2d_wide fig_2e[, glucose_0 := get(&quot;0&quot;)] times &lt;- c(0, 15, 30, 45, 60, 90, 120) time_cols &lt;- as.character(times) Y &lt;- fig_2e[, .SD, .SDcols = time_cols] fig_2e[, glucose_mean := apply(Y, 1, auc, x=times, method = &quot;post_0_auc&quot;, average = TRUE)] 21.8.2 Figure 2e – exploratory plots qplot(x = treatment, y = glucose_mean, data = fig_2e) 21.8.3 Figure 2e – fit the model fig_2e_m1 &lt;- lm(glucose_mean ~ glucose_0 + ask1*diet, data = fig_2e) 21.8.4 Figure 2e – check the model # check normality assumption set.seed(1) qqPlot(fig_2e_m1, id=FALSE) spreadLevelPlot(fig_2e_m1, id=FALSE) ## ## Suggested power transformation: -0.4035073 21.8.5 Figure 2e – inference from the model fig_2e_m1_coef &lt;- coef(summary(fig_2e_m1)) fig_2e_m1_coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.6835026 1.2556730 6.9154169 2.460045e-08 ## glucose_0 1.2194720 0.2390919 5.1004329 8.592596e-06 ## ask1ASK1Δadipo -0.3488511 0.8759124 -0.3982716 6.925479e-01 ## dietHFD 4.2782121 0.7908612 5.4095613 3.185930e-06 ## ask1ASK1Δadipo:dietHFD -2.7503448 1.1288320 -2.4364518 1.937783e-02 fig_2e_m1_emm &lt;- emmeans(fig_2e_m1, specs = c(&quot;diet&quot;, &quot;ask1&quot;)) fig_2e_m1_pairs &lt;- contrast(fig_2e_m1_emm, method = &quot;revpairwise&quot;, simple = &quot;each&quot;, combine = TRUE, adjust = &quot;none&quot;) %&gt;% summary(infer = TRUE) fig_2e_m1_emm ## diet ask1 emmean SE df lower.CL upper.CL ## chow ASK1F/F 14.7 0.587 40 13.5 15.9 ## HFD ASK1F/F 18.9 0.520 40 17.9 20.0 ## chow ASK1Δadipo 14.3 0.659 40 13.0 15.7 ## HFD ASK1Δadipo 15.9 0.493 40 14.9 16.8 ## ## Confidence level used: 0.95 fig_2e_m1_pairs ## ask1 diet contrast estimate SE df lower.CL upper.CL ## ASK1F/F . HFD - chow 4.278 0.791 40 2.680 5.88 ## ASK1Δadipo . HFD - chow 1.528 0.824 40 -0.138 3.19 ## . chow ASK1Δadipo - (ASK1F/F) -0.349 0.876 40 -2.119 1.42 ## . HFD ASK1Δadipo - (ASK1F/F) -3.099 0.715 40 -4.544 -1.65 ## t.ratio p.value ## 5.410 &lt;.0001 ## 1.853 0.0712 ## -0.398 0.6925 ## -4.335 0.0001 ## ## Confidence level used: 0.95 21.8.6 Figure 2e – plot the model fig_2e_gg &lt;- ggplot_the_response( fig_2e_m1, fig_2e_m1_emm, fig_2e_m1_pairs, wrap_col=NULL, y_label = &quot;Post-baseline glucose (mmol per l)&quot;, contrast_rows = &quot;all&quot;, palette = pal_okabe_ito_4, ) fig_2e_gg 21.9 Figure 2f – Effect of ASK1 deletion on glucose infusion rate 21.9.1 Figure 2f – import range_2f &lt;- &quot;A239:I240&quot; treatment_levels &lt;- c(&quot;ASK1F/F&quot;, &quot;ASK1Δadipo&quot;) fig_2f &lt;- read_excel(file_path, sheet = fig_2_sheet, range = range_2f, col_names = FALSE) %&gt;% transpose(make.names=1) %&gt;% data.table() %&gt;% melt(measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;glucose_infusion_rate&quot;) %&gt;% na.omit() fig_2f[, treatment := factor(treatment, treatment_levels)] 21.9.2 Figure 2f – exploratory plots 21.9.3 Figure 2f – fit the model fig_2f_m1 &lt;- lm(glucose_infusion_rate ~ treatment, data = fig_2f) 21.9.4 Figure 2f – check the model 21.9.5 Figure 2f – inference fig_2f_m1_coef &lt;- summary(fig_2f_m1) %&gt;% coef() fig_2f_m1_emm &lt;- emmeans(fig_2f_m1, specs = &quot;treatment&quot;) fig_2f_m1_pairs &lt;- contrast(fig_2f_m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) fig_2f_m1_pairs ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## ASK1Δadipo - (ASK1F/F) 18.9 6.3 12 5.18 32.6 3.000 0.0111 ## ## Confidence level used: 0.95 21.9.6 Figure 2f – plot the model fig_2f_m1_emm_dt &lt;- summary(fig_2f_m1_emm) %&gt;% data.table fig_2f_m1_pairs_dt &lt;- data.table(fig_2f_m1_pairs) fig_2f_m1_pairs_dt[ , p_pretty := pvalString(p.value)] fig_2f_m1_pairs_dt[, group1 := 1] fig_2f_m1_pairs_dt[, group2 := 2] fig_2f_gg &lt;- ggplot(data = fig_2f, aes(x = treatment, y = glucose_infusion_rate, color = treatment)) + # points geom_sina(alpha = 0.5) + # plot means and CI geom_errorbar(data = fig_2f_m1_emm_dt, aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = treatment), width = 0 ) + geom_point(data = fig_2f_m1_emm_dt, aes(y = emmean, color = treatment), size = 3 ) + # plot p-values (y positions are adjusted by eye) stat_pvalue_manual(fig_2f_m1_pairs_dt, label = &quot;p_pretty&quot;, y.position=c(95), tip.length = 0.01) + # aesthetics ylab(&quot;Glucose infusion rate&quot;) + scale_color_manual(values=fig_2_palette, name = NULL) + theme_pubr() + theme(legend.position=&quot;none&quot;) + theme(axis.title.x=element_blank()) + NULL fig_2f_gg 21.10 Figure 2g – Effect of ASK1 deletion on tissue-specific glucose uptake 21.10.1 Figure 2g – import range_list &lt;- c(&quot;A244:G247&quot;, &quot;A250:H253&quot;) # import ASK1F/F fig_2g_1 &lt;- read_excel(file_path, sheet = fig_2_sheet, range = &quot;A244:G247&quot;, col_names = FALSE) %&gt;% transpose(make.names=1) %&gt;% data.table() fig_2g_1[, treatment := &quot;ASK1F/F&quot;] # import ASK1Δadipo fig_2g_2 &lt;- read_excel(file_path, sheet = fig_2_sheet, range = &quot;A250:H253&quot;, col_names = FALSE) %&gt;% transpose(make.names=1) %&gt;% data.table() fig_2g_2[, treatment := &quot;ASK1Δadipo&quot;] # combine fig_2g &lt;- rbind(fig_2g_1, fig_2g_2) 21.10.2 Figure 2g – exploratory plots 21.10.3 Figure 2g – fit the model # a more sophisticated would be a mixed model to dampen noise fig_2g_m1_ingWAT &lt;- lm(ingWAT ~ treatment, data = fig_2g) fig_2g_m1_epiWAT &lt;- lm(epiWAT ~ treatment, data = fig_2g) fig_2g_m1_Muscle &lt;- lm(Muscle ~ treatment, data = fig_2g) fig_2g_m1_BAT &lt;- lm(BAT ~ treatment, data = fig_2g) 21.10.4 Figure 2g – check the model 21.10.5 Figure 2g – inference fig_2g_infer &lt;- function(m1){ m1_emm &lt;- emmeans(m1, specs = &quot;treatment&quot;) m1_pairs &lt;- contrast(m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) return(list(emm = m1_emm, pairs = m1_pairs)) } fig_2g_m1_emm_dt &lt;- data.table(NULL) fig_2g_m1_pairs_dt &lt;- data.table(NULL) m1_list &lt;- list(fig_2g_m1_ingWAT, fig_2g_m1_epiWAT, fig_2g_m1_Muscle, fig_2g_m1_BAT) y_cols &lt;- c(&quot;ingWAT&quot;, &quot;epiWAT&quot;, &quot;Muscle&quot;, &quot;BAT&quot;) for(i in 1:length(y_cols)){ m1_infer &lt;- fig_2g_infer(m1_list[[i]]) m1_emm_dt &lt;- summary(m1_infer$emm) %&gt;% data.table fig_2g_m1_emm_dt &lt;- rbind(fig_2g_m1_emm_dt, data.table(tissue = y_cols[i], m1_emm_dt)) m1_pairs_dt &lt;- m1_infer$pairs %&gt;% data.table fig_2g_m1_pairs_dt &lt;- rbind(fig_2g_m1_pairs_dt, data.table(tissue = y_cols[i], m1_pairs_dt)) } fig_2g_m1_pairs_dt ## tissue contrast estimate SE df lower.CL upper.CL ## 1: ingWAT ASK1Δadipo - (ASK1F/F) 3.595000 1.468289 10 0.32344725 6.866553 ## 2: epiWAT ASK1Δadipo - (ASK1F/F) 1.390238 0.669957 11 -0.08432738 2.864804 ## 3: Muscle ASK1Δadipo - (ASK1F/F) 2.694048 5.675468 11 -9.79757382 15.185669 ## 4: BAT ASK1Δadipo - (ASK1F/F) 33.855000 28.715230 7 -34.04572935 101.755729 ## t.ratio p.value ## 1: 2.4484273 0.03435010 ## 2: 2.0751153 0.06222096 ## 3: 0.4746829 0.64429728 ## 4: 1.1789911 0.27691810 21.10.6 Figure 2g – plot the model # melt fig_2g fig_2g_long &lt;- melt(fig_2g, id.vars = &quot;treatment&quot;, variable.name = &quot;tissue&quot;, value.name = &quot;glucose_uptake&quot;) # change name of ASK1Δadipo label fig_2g_long[treatment == &quot;ASK1Δadipo&quot;, treatment := &quot;ASK1-/-adipo&quot;] fig_2g_m1_emm_dt[treatment == &quot;ASK1Δadipo&quot;, treatment := &quot;ASK1-/-adipo&quot;] fig_2g_m1_pairs_dt[ , p_pretty := pvalString(p.value)] fig_2g_m1_pairs_dt[, group1 := 1] fig_2g_m1_pairs_dt[, group2 := 2] fig_2g_plot &lt;- function(tissue_i, y_lab = FALSE, # title y-axis? g_lab = FALSE # add group label? ){ y_max &lt;- max(fig_2g_long[tissue == tissue_i, glucose_uptake], na.rm=TRUE) y_min &lt;- min(fig_2g_long[tissue == tissue_i, glucose_uptake], na.rm=TRUE) y_pos &lt;- y_max + (y_max-y_min)*.05 gg &lt;- ggplot(data = fig_2g_long[tissue == tissue_i], aes(x = treatment, y = glucose_uptake, color = treatment)) + # points geom_sina(alpha = 0.5) + # plot means and CI geom_errorbar(data = fig_2g_m1_emm_dt[tissue == tissue_i], aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = treatment), width = 0 ) + geom_point(data = fig_2g_m1_emm_dt[tissue == tissue_i], aes(y = emmean, color = treatment), size = 3 ) + # plot p-values (y positions are adjusted by eye) stat_pvalue_manual(fig_2g_m1_pairs_dt[tissue == tissue_i], label = &quot;p_pretty&quot;, y.position=c(y_pos), tip.length = 0.01) + # aesthetics ylab(&quot;Glucose Uptake&quot;) + scale_color_manual(values=fig_2_palette, name = NULL) + ggtitle(tissue_i)+ theme_pubr() + theme(legend.position=&quot;top&quot;) + theme(axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank()) + NULL if(y_lab == FALSE){ gg &lt;- gg + theme(axis.title.y = element_blank()) } if(g_lab == FALSE){ gg &lt;- gg + theme(legend.position=&quot;none&quot;) } return(gg) } y_cols &lt;- c(&quot;ingWAT&quot;, &quot;epiWAT&quot;, &quot;Muscle&quot;, &quot;BAT&quot;) legend &lt;- get_legend(fig_2g_plot(&quot;ingWAT&quot;, y_lab = TRUE, g_lab = TRUE)) gg1 &lt;- fig_2g_plot(&quot;ingWAT&quot;, y_lab = TRUE, ) gg2 &lt;- fig_2g_plot(&quot;epiWAT&quot;) gg3 &lt;- fig_2g_plot(&quot;Muscle&quot;) gg4 &lt;- fig_2g_plot(&quot;BAT&quot;) top_gg &lt;- plot_grid(gg1, gg2, gg3, gg4, nrow=1, rel_widths = c(1.15, 1, 1.05, 1.1)) # by eye plot_grid(top_gg, legend, nrow=2, rel_heights = c(1, 0.1)) 21.11 Figure 2h 21.12 Figure 2i – Effect of ASK1 deletion on liver TG range_2i &lt;- &quot;A265:G266&quot; treatment_levels &lt;- c(&quot;ASK1F/F&quot;, &quot;ASK1Δadipo&quot;) fig_2i &lt;- read_excel(file_path, sheet = fig_2_sheet, range = range_2i, col_names = FALSE) %&gt;% transpose(make.names=1) %&gt;% data.table() %&gt;% melt(measure.vars = treatment_levels, variable.name = &quot;treatment&quot;, value.name = &quot;liver_tg&quot;) %&gt;% na.omit() fig_2i[, treatment := factor(treatment, treatment_levels)] # View(fig_2i) 21.12.1 Figure 2i – fit the model fig_2i_m1 &lt;- lm(liver_tg ~ treatment, data = fig_2i) 21.12.2 Figure 2i – check the model set.seed(1) qqPlot(fig_2i_m1, id=FALSE) spreadLevelPlot(fig_2i_m1, id=FALSE) ## ## Suggested power transformation: 1.294553 The QQ plot looks okay, in the sense that the observed data points (open circles) fall within the boundaries set by the dashed line. spread level looks pretty good Fit normal model 21.12.3 Figure 2i – inference fig_2i_m1 &lt;- lm(liver_tg ~ treatment, data = fig_2i) fig_2i_m1_coef &lt;- cbind(coef(summary(fig_2i_m1)), confint(fig_2i_m1)) fig_2i_m1_emm &lt;- emmeans(fig_2i_m1, specs = &quot;treatment&quot;) fig_2i_m1_pairs &lt;- contrast(fig_2i_m1_emm, method = &quot;revpairwise&quot;) %&gt;% summary(infer = TRUE) fig_2i_m1_pairs ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## ASK1Δadipo - (ASK1F/F) -21.6 7.05 10 -37.3 -5.9 -3.066 0.0119 ## ## Confidence level used: 0.95 21.12.4 Figure 2i – plot the model fig_2i_m1_emm_dt &lt;- summary(fig_2i_m1_emm) %&gt;% data.table fig_2i_m1_pairs_dt &lt;- data.table(fig_2i_m1_pairs) fig_2i_m1_pairs_dt[ , p_pretty := pvalString(p.value)] fig_2i_m1_pairs_dt[, group1 := 1] fig_2i_m1_pairs_dt[, group2 := 2] fig_2i_gg &lt;- ggplot(data = fig_2i, aes(x = treatment, y = liver_tg, color = treatment)) + # points geom_sina(alpha = 0.5) + # plot means and CI geom_errorbar(data = fig_2i_m1_emm_dt, aes(y = emmean, ymin = lower.CL, ymax = upper.CL, color = treatment), width = 0 ) + geom_point(data = fig_2i_m1_emm_dt, aes(y = emmean, color = treatment), size = 3 ) + # plot p-values (y positions are adjusted by eye) stat_pvalue_manual(fig_2i_m1_pairs_dt, label = &quot;p_pretty&quot;, y.position=c(95), tip.length = 0.01) + # aesthetics ylab(&quot;Liver TG (µmol per g liver)&quot;) + scale_color_manual(values=fig_2_palette, name = NULL) + theme_pubr() + theme(legend.position=&quot;none&quot;) + theme(axis.title.x=element_blank()) + NULL fig_2i_gg 21.12.5 Figure 2i – report the model Mean TG level in ASK1Δadipo mice on a high-fat diet was 21.6 µmol/g less than that in ASK1F/F mice on a high-fat diet (95% CI: -37.3, -5.9, \\(p = 0.012\\)) (Figure xxx). 21.13 Figure 2j "],["count-sim.html", "Chapter 22 Simulations – Count data (alternatives to a t-test) 22.1 Use data similar to Figure 6f from Example 1 22.2 Functions 22.3 Simulations 22.4 Save it, Read it 22.5 Analysis", " Chapter 22 Simulations – Count data (alternatives to a t-test) library(here) library(data.table) library(readxl) library(MASS) library(lmPerm) library(nlme) library(glmmTMB) library(emmeans) library(knitr) library(kableExtra) here &lt;- here::here 22.1 Use data similar to Figure 6f from Example 1 The simulated data are modeled to look like the Figure 6f data used in Example 1 of the Violations chapter. data_folder &lt;- &quot;data&quot; data_from &lt;- &quot;Exercise reduces inflammatory cell production and cardiovascular inflammation via instruction of hematopoietic progenitor cells&quot; file_name &lt;- &quot;41591_2019_633_MOESM8_ESM.xlsx&quot; file_path &lt;- here(data_folder, data_from, file_name) # assuming mice are independent and not same mouse used for all three treatment melt_col_names &lt;- c(&quot;Sedentary&quot;, &quot;Exercise&quot;) fig6f &lt;- read_excel(file_path, sheet = &quot;Figure 6f&quot;, range = &quot;A7:B29&quot;, col_names = TRUE) %&gt;% data.table() %&gt;% melt(measure.vars = melt_col_names, variable.name = &quot;treatment&quot;, value.name = &quot;neutrophils&quot;) %&gt;% na.omit() # danger! treatment_levels &lt;- melt_col_names fig6f[, treatment := factor(treatment, levels = treatment_levels)] # neutrophils is count/10^6 fig6f[, neutrophil_count := round(neutrophils*10^6, 0)] # fig6f[1, neutrophil_count] #View(fig6f) m1 &lt;- glmmTMB(neutrophil_count ~ treatment, data = fig6f, family = nbinom2(link = &quot;log&quot;)) mu_cn_obs &lt;- exp(coef(summary(m1))$cond[&quot;(Intercept)&quot;, &quot;Estimate&quot;]) theta_obs &lt;- sigma(m1) %&gt;% round(1) # compare with glm.nb, which is used for the simulation m2 &lt;- glm.nb(neutrophil_count ~ treatment, data = fig6f) mu_cn_obs ## [1] 3633139 m2$theta ## [1] 6.607988 22.2 Functions simulator &lt;- function(n_sim = 1000, n_levels = c(10, 10), treatment_levels = c(&quot;cn&quot;, &quot;tr&quot;), mu_levels = c(10, 10), theta_levels = c(1, 1), sigma_levels = c(1,1), normal = FALSE, seed = 1){ set.seed(seed) N &lt;- sum(n_levels) mu_sim &lt;- rep(mu_levels, n_levels) theta_sim &lt;- rep(theta_levels, n_levels) sigma_sim &lt;- rep(sigma_levels, n_levels) method_list &lt;- c(&quot;lm&quot;, &quot;gls&quot;, &quot;glm_nb&quot;, &quot;glm_qp&quot;, &quot;lmp&quot;, &quot;mww&quot;) p_values &lt;- matrix(as.numeric(NA), nrow = n_sim, ncol = length(method_list)) %&gt;% data.table() colnames(p_values) &lt;- method_list fake_counts &lt;- matrix(as.numeric(NA), nrow = N, ncol = n_sim) if(normal == TRUE){ for(j in 1:n_sim){ fake_counts[, j] &lt;- rnorm(N, mean = mu_sim, sd = sigma_sim) %&gt;% round(0) } }else{ for(j in 1:n_sim){ fake_counts[, j] &lt;- rnegbin(N, mu = mu_sim, theta = theta_sim) } } for(j in 1:n_sim){ fake_data[, count := fake_counts[,j]] m1 &lt;- lm(count ~ treatment, data = fake_data) m2 &lt;- gls(count ~ treatment, data = fake_data, weights = varIdent(form = ~ 1 | treatment)) m2_pairs &lt;- contrast(emmeans(m2, specs = &quot;treatment&quot;), method = &quot;revpairwise&quot;) %&gt;% summary() m3 &lt;- glm.nb(count ~ treatment, data = fake_data) m4 &lt;- glm(count ~ treatment, data = fake_data, family = quasipoisson) # m3 &lt;- glmmTMB(count ~ treatment, # data = fake_data, # family = nbinom2(link = &quot;log&quot;))$cond # m4 &lt;- glmmTMB(count ~ treatment, # data = fake_data, # family = nbinom1(link = &quot;log&quot;))$cond m5 &lt;- lmp(count ~ treatment, data = fake_data, perm = &quot;Exact&quot;, settings = FALSE) m6 &lt;- wilcox.test(count ~ treatment, data = fake_data) p_values[j, lm := coef(summary(m1))[2, &quot;Pr(&gt;|t|)&quot;]] p_values[j, gls := m2_pairs[1, &quot;p.value&quot;]] p_values[j, glm_nb := coef(summary(m3))[2, &quot;Pr(&gt;|z|)&quot;]] p_values[j, glm_qp := coef(summary(m4))[2, &quot;Pr(&gt;|t|)&quot;]] p_values[j, lmp := coef(summary(m5))[2, &quot;Pr(Prob)&quot;]] p_values[j, mww := m6$p.value] } return(p_values) } binder &lt;- function( p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim){ sim_table &lt;- data.table( sim = sim_i, n_sim = n_sim_i, n1 = n_levels_sim[1], n2 = n_levels_sim[2], n = ifelse(n_levels_sim[1] == n_levels_sim[2], &quot;=&quot;, &quot;!=&quot;), mu1 = mu_levels_sim[1], mu2 = mu_levels_sim[2], type = ifelse(mu_levels_sim[1] == mu_levels_sim[2], &quot;type 1&quot;, &quot;power&quot;), theta1 = theta_levels_sim[1], theta2 = theta_levels_sim[2], sigma1 = sigma_levels_sim[1], sigma1 = sigma_levels_sim[2], normal = normal_sim, sim_res ) p_values &lt;- rbind(p_values, sim_table) return(p_values) } 22.3 Simulations do_it &lt;- FALSE p_values &lt;- data.table(NULL) sim_i &lt;- 0 n_sim_i = 4000 22.3.1 Type I, Pseudo-Normal distribution sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(1000, 1000) theta_levels_sim &lt;- c(NA, NA) # not used sigma_levels_sim &lt;- c(100, 100) normal_sim &lt;- TRUE if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.2 Type I, neg binom, equal n sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs) theta_levels_sim &lt;- c(theta_obs, theta_obs) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.3 Type I, neg binom, equal n, small theta sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs) theta_levels_sim &lt;- c(1, 1) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.4 Type I, neg binom, unequal n sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(12, 8) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs) theta_levels_sim &lt;- c(theta_obs, theta_obs) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.5 Power, Pseudo-Normal distribution, equal n sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(1000, 1100) theta_levels_sim &lt;- c(NA, NA) # not used sigma_levels_sim &lt;- c(100, 100) normal_sim &lt;- TRUE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.6 Power, neg binom, equal n sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs*.7) theta_levels_sim &lt;- c(theta_obs, theta_obs) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.7 Power, neg binom, small theta sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(10, 10) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs*.7) theta_levels_sim &lt;- c(1, 1) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.8 Power, neg binom, unequal n sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(12, 8) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs*.7) theta_levels_sim &lt;- c(theta_obs, theta_obs) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.9 Power, neg binom, unequal n, unequal theta What if the treatment affects the variance and the mean? sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(12, 8) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs*.7) theta_levels_sim &lt;- c(1, theta_obs) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.3.10 Type 1, neg binom, equal n, unequal theta What if the treatment affects the variance but not the mean? sim_i &lt;- sim_i + 1 n_levels_sim &lt;- c(12, 8) mu_levels_sim &lt;- c(mu_cn_obs, mu_cn_obs) theta_levels_sim &lt;- c(theta_obs, 1) sigma_levels_sim &lt;- c(NA, NA) # not used normal_sim &lt;- FALSE # fake_data has to be global for emmeans to run inside # a function treatment_levels &lt;- c(&quot;cn&quot;, &quot;tr&quot;) fake_data &lt;- data.table( treatment = factor(rep(treatment_levels, n_levels_sim), levels = treatment_levels) ) if(do_it == TRUE){ sim_res &lt;- simulator( n_sim = n_sim_i, n_levels = n_levels_sim, mu_levels = mu_levels_sim, theta_levels = theta_levels_sim, # not used sigma_levels = sigma_levels_sim, normal = normal_sim ) p_values &lt;- binder(p_values, sim_res, sim_i, n_levels_sim, mu_levels_sim, theta_levels_sim, sigma_levels_sim, normal_sim) # apply(sim_res, 2, function(x) sum(x &lt; 0.05)/n_sim_i) } 22.4 Save it, Read it sim_data_folder &lt;- &quot;sim_data&quot; sim_fn &lt;- &quot;counts_t_test_alternative.Rds&quot; sim_path &lt;- here(sim_data_folder, sim_fn) if(do_it == TRUE){ saveRDS(p_values, sim_path) }else{ p_values &lt;- readRDS(sim_path) } 22.5 Analysis method_list &lt;- c(&quot;lm&quot;, &quot;gls&quot;, &quot;glm_nb&quot;, &quot;glm_qp&quot;, &quot;lmp&quot;, &quot;mww&quot;) p_values[, lapply(.SD, function(x) sum(x &lt; 0.05)/mean(n_sim)), .SDcols = method_list, by = c(&quot;sim&quot;, &quot;normal&quot;,&quot;type&quot;, &quot;n&quot;, &quot;theta1&quot;, &quot;theta2&quot;)] %&gt;% kable(digits = c(0,0,0,0,2,2,3,3,3,3,3,3)) %&gt;% kable_styling() sim normal type n theta1 theta2 lm gls glm_nb glm_qp lmp mww 1 TRUE type 1 = 0.053 0.050 0.076 0.052 0.053 0.046 2 FALSE type 1 = 6.6 6.6 0.043 0.041 0.075 0.044 0.046 0.039 3 FALSE type 1 = 1.0 1.0 0.039 0.034 0.075 0.046 0.046 0.041 4 FALSE type 1 != 6.6 6.6 0.047 0.049 0.080 0.048 0.050 0.046 5 TRUE power = 0.573 0.567 0.653 0.572 0.571 0.528 6 FALSE power = 6.6 6.6 0.488 0.476 0.590 0.494 0.499 0.440 7 FALSE power = 1.0 1.0 0.096 0.082 0.171 0.113 0.114 0.088 8 FALSE power != 6.6 6.6 0.442 0.501 0.578 0.452 0.444 0.438 9 FALSE power != 1.0 6.6 0.034 0.082 0.144 0.054 0.041 0.045 10 FALSE type 1 != 6.6 1.0 0.127 0.103 0.139 0.116 0.132 0.144 "],["appendix-1-getting-started-with-r.html", "Appendix 1: Getting Started with R 22.6 Get your computer ready 22.7 Start learning R Studio", " Appendix 1: Getting Started with R 22.6 Get your computer ready 22.6.1 Start here Watch this video. The links for installing R and R studio are in the next sections. Andy Field’s Installing R and RStudio 22.6.2 Install R R is the core software. It runs under the hood. You never see it. To use R, you need another piece of software that provides a user interface. The software we will use for this is R Studio. Download R for your OS 22.6.3 Install R Studio R Studio is a slick (very slick) graphical user interface (GUI) for developing R projects. Download R Studio Desktop 22.6.3.1 Additional resources for installing R and R Studio** On Windows On a Mac 22.6.4 Install R Markdown In this class, we will write code to analyze data using R Markdown. R markdown is a version of Markdown. Markdown is tool for creating a document containing text (like microsoft Word), images, tables, and code that can be output to the three modern output formats: html (web pages), pdf (reports and documents), and microsoft word (okay, this isn’t modern but it is widely used). R Markdown can output pdf files. The mechanism for this is to first create a LaTeX (“la-tek”) file. LaTeX is an amazing tool for creating professional pdf documents. You do not need PDF output for BIO 414/513. The directions for installing R Markdown include directions for installing LaTeX. This is optional, for this class, but I encourage you to do it. Directions for installing R Markdown 22.6.5 (optional) Alternative LaTeX installations On Windows On a Mac 22.7 Start learning R Studio R Studio Essentials, Programming Part 1 (Writing code in RStudio) Getting Started with R Markdown Andy Field’s RStudio basics of R Markdown Data Visualisation chapter from R for Data Science Graphics for communication chapter from R for Data Science Youtube: An Introduction to The data.table Package Coursera: The data.table Package "],["appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r.html", "Appendix 2: Online Resources for Getting Started with Statistical Modeling in R", " Appendix 2: Online Resources for Getting Started with Statistical Modeling in R Roughly, in order from most elementary to most advanced Learning Statistics with R by Danielle Navarro and adapted to Bookdown (for web viewing) by Emily Kothe. Statististical Thinking for the 21st Century by Russell A. Poldrack Regression Models for Data Science in R by Brian Caffo Broadening Your Statistical Horizons: Generalized Linear Models and Multilevel Models by J. Legler and P. Roback Modern Statistics for Modern Biology The Art of Data Science by Roger D. Peng and Elizabeth Matsui "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
