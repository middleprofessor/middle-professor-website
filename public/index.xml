<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jeffrey A. Walker on Jeffrey A. Walker</title>
    <link>/</link>
    <description>Recent content in Jeffrey A. Walker on Jeffrey A. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 05 May 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The role of statistics in bench biology: Fc-dependent depletion of tumor-infiltrating regulatory T cells co-defines the efficacy of anti–CTLA-4 therapy against melanoma</title>
      <link>/post/the-role-of-statistics-in-bench-biology-fc-dependent-depletion-of-tumor-infiltrating-regulatory-t-cells-co-defines-the-efficacy-of-anti-ctla-4-therapy-against-melanoma/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-role-of-statistics-in-bench-biology-fc-dependent-depletion-of-tumor-infiltrating-regulatory-t-cells-co-defines-the-efficacy-of-anti-ctla-4-therapy-against-melanoma/</guid>
      <description>&lt;div id=&#34;experiment-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experiment 1&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/2019-01-08/fig1c.png&#34; alt=&#34;Fig 1C. GVAX+–CTLA-4 combination therapy protects against tumor outgrowth through a CD4+ T cell–dependent mechanism&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 1C. “GVAX+–CTLA-4 combination therapy protects against tumor outgrowth through a CD4+ T cell–dependent mechanism”&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;2 x 2 combination of GVAX and alpha-CTLA-4 on tumor growth in C57BL/6 wild-type and I-A-/- (no CD4+ T cell compartment) mouse lines. Combination treatment (but not either in isolation) “protected against” tumor growth in wild but not -/- mice, indicating CD4+ T cell required for protection.&lt;/p&gt;
&lt;p&gt;Treatment key: -/- control; +/- alpha-CTLA-4; -/+ GVAX; +/+ both cell key: CD4+Foxp3- Teff; CD4+Foxp3+ Treg&lt;/p&gt;
&lt;p&gt;Fig. 1C is the effect of treatment on Neff/Nreg, the ratio of number of effector (Tc) to regulatory (Treg) cells. Thoughts&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Means but no SE or CI presented.&lt;/li&gt;
&lt;li&gt;presented as 4 levels instead of 2 x 2 (this seems to be a graphpad prism thing)&lt;/li&gt;
&lt;li&gt;what is the effect of the distribution of the response (a ratio of counts) on t-tests?&lt;/li&gt;
&lt;li&gt;F-/- ratio = 1.5, +/+ ratio is 3. Only seem to care about “an effect” with no sense of effect size importance of this effect size or what the minimal effect to care is. For example what if the pattern were maintained with bigger sample and “significant” increase in +/-? How would this effect interpretation of mechanism?&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;experiment-2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experiment 2&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/2019-01-08/fig2b.png&#34; alt=&#34;Fig 2B. alpha–CTLA-4 increases the number of tumor specific CD4+ Teff and Treg cells in lymph nodes while preventing intra-tumoral Treg cell accumulation. Left panel is Teff. Middle panel is Treg. Right panel is T eff/T reg&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 2B. “alpha–CTLA-4 increases the number of tumor specific CD4+ Teff and Treg cells in lymph nodes while preventing intra-tumoral Treg cell accumulation”. Left panel is Teff. Middle panel is Treg. Right panel is T eff/T reg&lt;/p&gt;
&lt;/div&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;clear increase variance with mean&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/2019-01-08/fig2c.png&#34; alt=&#34;Fig 2C&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 2C&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;images/2019-01-08/fig2d.png&#34; alt=&#34;Fig 2D&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Fig 2D&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The role of statistics in Boström: &#34;A PGC1-alpha-dependent myokine that drives brown-fat-like development of white fat and thermogenesis&#34;</title>
      <link>/post/comments-on-the-role-of-statistics-in-bostr%C3%B6m-a-pgc1-alpha-dependent-myokine-that-drives-brown-fat-like-development-of-white-fat-and-thermogenesis/</link>
      <pubDate>Tue, 02 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/comments-on-the-role-of-statistics-in-bostr%C3%B6m-a-pgc1-alpha-dependent-myokine-that-drives-brown-fat-like-development-of-white-fat-and-thermogenesis/</guid>
      <description>&lt;div id=&#34;muscle-pgc1-a-transgenics-figure-1-and-s1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Muscle PGC1-a transgenics (Figure 1 and S1)&lt;/h1&gt;
&lt;p&gt;The transgenic mice are expressing PGC1-a in skeletal muscle. These mice seem to be overexpressing browning related proteins in white adipose tissue. The section argues that this pattern suggests muscle is secreting a myokine that signals fat.&lt;/p&gt;
&lt;div id=&#34;results-reported-in-text&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;results reported in text&lt;/h2&gt;
&lt;p&gt;Fig. 1a shows the “relative mRNA” of brown-fat-selective genes between wild-type and transgenic mice in two different fat depots. The authors write: “There were no significant alterations in the expression of brown-fat-selective genes in the interscapular brown adipose tissue or in the visceral (epididymal) white adipose tissue (Fig. 1a)”&lt;/p&gt;
&lt;p&gt;Fig. 1b is the the relative mRNA in a third fat depot. The authors believe something different is going on here between the wild-type and transgenic mice: “However, the subcutaneous fat layer (inguinal), a white adipose tissue that is particularly prone to ‘browning’ (that is, formation of multilocular, UCP1-positive adipocytes), had significantly increased levels of Ucp1 and Cidea messenger RNAs (Fig. 1b).”&lt;/p&gt;
&lt;p&gt;Fig. S1a “Similar to what has been reported, a twofold increase in Ucp1 mRNA expression was observed in the visceral, epididymal fat with 3 weeks of wheel running (Supplementary Fig. 1). However, a much larger change (approximately 25 fold) was seen in the same mice in the subcutaeneous inguinal fat depot.”&lt;/p&gt;
&lt;p&gt;Fig S1b,c - “Similarly, a small increase in Ucp1 mRNA expression was seen in the epididymal fat with repeated bouts of swimming in warm (32 uC) water (Supplementary Fig. 1); however a very large increase (65 fold) was observed in the inguinal white depot (Supplementary Fig. 1). Thus, muscle-specific expression of PGC1a drives browning of subcutaneous white adipose tissue, possibly recapitulating part of the exercise program.”&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My summary&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;authors want to show that some adipose tissue is upregulating “browning” proteins in response to pgc1-a (the (MCK)-PGC1-a transgenic).&lt;/li&gt;
&lt;li&gt;Similar response occurs in exercised mice (replication of other studies)&lt;/li&gt;
&lt;li&gt;Similar response in “cultured primary subcutaneous adipocytes with serum-free media conditioned by myocytes expressing PGC1-a” with “cells expressing green fluorescent protein (GFP)” as a control.* (Fig 1e)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1 and 2 are patterns that suggest something is 3 is an experiment to support inference from 1 and 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;comments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ucp1 effect is consistent in swimming and running mice&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-the-statistics-are-doing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What the statistics are doing?&lt;/h2&gt;
&lt;p&gt;The authors are arguing that transgenic mice differ in “browning” protein expression in inguinal white fat but not epididymal white fat or brown fat. The evidence is 2/6 p &amp;lt; 0.05 values in inguinal fat but 0/10 in the other two depots. There is no attempt to measure effect sizes and uncertainty. An increase in expression occurs for all proteins in all three tissues, except adipoq in inguinal fat, but p &amp;lt; 0.05 in only 2 of the proteins in the inguinal fat. Cumulative probability of 2/16 p if null is true is 0.19.&lt;/p&gt;
&lt;p&gt;What pattern would induce them to conclude something different from what they concluded? Does it even matter if the inguinal fat differs? What if no p-values were significant? There is a consistent effect. Is this enough to move forward? Does it matter that 4 of the p-values are not significant (and difference is in wrong direction)?&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fig 1A, the authors are using a p-value from a t-test to show no differences in expression of the five genes between white and brown adipose tissue in both wild type and transgenic mice. This isn’t what p-values do.&lt;/li&gt;
&lt;li&gt;Fig 1B - Note the effect size in the subcutaneous depot (1B) are similar to that in the other two depots. The authors are inferrering something different in subcutaneous as opposed to epidydimal or BAT depots based on pattern of p-values - but “a difference in statistical significance is not (necessarily) statistically significant” and the similarity in effect sizes suggests this is especially so here.&lt;/li&gt;
&lt;li&gt;Fig 1A,B How does use of t-test effect &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; given that the outcome is probably not approximately normally distributed? What is the distribution in these kinds of data? In general, it looks from both 1a and 1b that the variance increases with the mean.&lt;/li&gt;
&lt;li&gt;Fig S1a - Clearly the SE is not appropriate (what is the mean - 2SE in the UCP1, wheel running, subcutaneou treatment?) nor a t-test given the extreme heterogenous variance.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;key-to-figure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Key to figure&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;(MCK)-PGC1-a : These MCK-PGC1-alpha transgenic mice express mouse peroxisome proliferative activated receptor, gamma, coactivator 1 alpha under the direction of the mouse muscle creatine kinase promoter. Muscle fibers from transgenic mice exhibit a more type II oxidative phenotype than wild-type. This mutant mouse strain may be useful in studies of muscle physiology and disease, exercise and oxidative capacity, and metabolic homeostasis (&lt;a href=&#34;https://www.jax.org/strain/008231&#34; class=&#34;uri&#34;&gt;https://www.jax.org/strain/008231&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Ucp1 – uncoupling protein 1. Uncouples H+ gradient from ATP synthesis in mitochondria. Expressed in Brown Adipose Tissue (BAT)&lt;/li&gt;
&lt;li&gt;Pgc1a – Peroxisome proliferator-activated receptor gamma coactivator 1-alpha. PGC-1α is a transcriptional coactivator that regulates the genes involved in energy metabolism. It is the master regulator of mitochondrial biogenesis.[6][7][8] This protein interacts with the nuclear receptor PPAR-γ, which permits the interaction of this protein with multiple transcription factors. This protein can interact with, and regulate the activities of, cAMP response element-binding protein (CREB) and nuclear respiratory factors (NRFs)[citation needed]. It provides a direct link between external physiological stimuli and the regulation of mitochondrial biogenesis, and is a major factor causing slow-twitch rather than fast-twitch muscle fiber types.[9] Endurance exercise has been shown to activate the PGC-1α gene in human skeletal muscle.[10] Exercise-induced PGC-1α in skeletal muscle increases autophagy [11] and unfolded protein response.[12] PGC-1α protein may be also involved in controlling blood pressure, regulating cellular cholesterol homoeostasis, and the development of obesity.[13]&lt;/li&gt;
&lt;li&gt;Prdm – PR domain containing 16, also known as PRDM16, is a protein which in humans is encoded by the PRDM16 gene.[5][6] PRDM16 acts as a transcription coregulator that controls the development of brown adipocytes in brown adipose tissue.[7] Previously, this coregulator was believed to be present only in brown adipose tissue, but more recent studies have shown that PRDM16 is highly expressed in subcutaneous white adipose tissue as well.[7]&lt;/li&gt;
&lt;li&gt;Cidea – Cell death activator CIDE-A is a protein that in humans is encoded by the CIDEA gene.[5][6][7] Cidea is an essential transcriptional coactivator regulating mammary gland secretion of milk lipids.[8] This gene encodes the homolog of the mouse protein Cidea that has been shown to activate apoptosis. This activation of apoptosis is inhibited by the DNA fragmentation factor DFF45 but not by caspase inhibitors. Mice that lack functional Cidea have higher metabolic rates, higher lipolysis in brown adipose tissue and higher core body temperatures when subjected to cold. These mice are also resistant to diet-induced obesity and diabetes. This suggests that in mice this gene product plays a role in thermogenesis and lipolysis. Two alternative transcripts encoding different isoforms have been identified.[7]&lt;/li&gt;
&lt;li&gt;Adipoq – Adiponectin (also referred to as GBP-28, apM1, AdipoQ and Acrp30) is a protein hormone which is involved in regulating glucose levels as well as fatty acid breakdown. In humans it is encoded by the ADIPOQ gene and it is produced in adipose tissue.[5]&lt;/li&gt;
&lt;li&gt;Ndufs1 – NADH-ubiquinone oxidoreductase 75 kDa subunit, mitochondrial (NDUFS1) is an enzyme that in humans is encoded by the NDUFS1 gene.[5] The encoded protein, NDUFS1, is the largest subunit of complex I, located on the inner mitochondrial membrane, and is important for mitochondrial oxidative phosphorylation. Mutations in this gene are associated with complex I deficiency.[6]&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;media-from-pgc1-a-expressing-myocytes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Media from PGC1-a-expressing myocytes&lt;/h1&gt;
&lt;p&gt;Again, the first section suggests secretion of a myokine or some kind of communication. This section reports a test of direct vs. indirect effect by comparing response in a culture of subcutaneous fat cells (or “stromo vascular fraction (SVF) cells”) bathed in media from pgc1-a muscle vs. media from GFP muscle. Note that inguinal fat is subcutaneous.&lt;/p&gt;
&lt;p&gt;Fig 1e - “As shown in Fig. 1e, the media from cells expressing ectopic PGC1-a increased the mRNA levels of several brown-fat-specific genes (Fig. 1e). This suggested that PGC1-a causes the muscle cells to secrete a molecule(s) that can induce a thermogenic gene program in the cells.”&lt;/p&gt;
&lt;div id=&#34;comments-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;comments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;upward response of prdm16, ucp1, cidea and downward response of adipoq are p&amp;lt;0.05 and consistent with in vivo inquinal fat. The upward responses are concisistent with epididymal and BAT fat.&lt;/li&gt;
&lt;li&gt;responses about the same as in vivo but the errors are smaller.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;what-the-statistics-are-doing-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What the statistics are doing?&lt;/h2&gt;
&lt;p&gt;4/5 are p&amp;lt;0.05 so the p-value is a marker that something is going on, to move forward. Would the plot itself be convincing? Probably no need of effect sizes because there is no theory to interpret this. But what if n were large and the effect were small, then would the experiment move on? what is the minimum effect size to move on?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How do I interpret the AIC?</title>
      <link>/post/how-do-i-interpret-the-aic/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-do-i-interpret-the-aic/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.seascapemodels.org/rstats/2018/04/13/how-to-use-the-AIC.html&#34;&gt;This is an archive of an external source. The original is here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Date: xxx&lt;/p&gt;
&lt;p&gt;Author: Chris Brown&lt;/p&gt;
&lt;p&gt;How do I interpret the AIC?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interpreting coefficients in GLMs</title>
      <link>/post/interpreting-coefficients-in-glms/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/interpreting-coefficients-in-glms/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://environmentalcomputing.net/interpreting-coefficients-in-glms/&#34;&gt;This is an archive of an external source. The original is here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Date: November 11, 2016&lt;/p&gt;
&lt;p&gt;Author: Gordana Popovic&lt;/p&gt;
&lt;p&gt;In linear models, the interpretation of model parameters is linear. For example, if a you were modelling plant height against altitude and your coefficient for altitude was -0.9, then plant height will decrease by 1.09 for every increase in altitude of 1 unit.&lt;/p&gt;
&lt;p&gt;For generalised linear models, the interpretation is not this straightforward. Here, I will explain how to interpret the co-efficients in generalised linear models (glms). First you will want to read our pages on glms for &lt;a href=&#34;http://environmentalcomputing.net/generalised-linear-models-1/&#34;&gt;binary&lt;/a&gt; and &lt;a href=&#34;http://environmentalcomputing.net/generalised-linear-models-1/&#34;&gt;count&lt;/a&gt; data page on &lt;a href=&#34;http://environmentalcomputing.net/how-to-interpret-linear-models/&#34;&gt;interpreting coefficients in linear models&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;poisson-and-negative-binomial-glms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson and negative binomial GLMs&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; In Poisson and negative binomial glms, we use a log link. The actual model we fit with one covariate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; looks like this&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{Poisson} (\lambda) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log(\lambda) = \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;here &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the mean of Y. So if we have an initial value of the covariate &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;, then the predicted value of the mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  log(\lambda_0) = \beta_0 + \beta_1 x_0 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we now increase the covariate by 1, we get a new mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  log(\lambda_1) = \beta_0 + \beta_1 (x_0 +1) = \beta_0 + \beta_1 x_0 +\beta_1 = log(\lambda_0) + \beta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the log of the mean of Y increases by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; when we increase x by 1. But we are not really interested in how the log mean changes, we would like to know on average how Y changes. If we take the exponential of both sides&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \lambda_1 = \lambda_0 exp(\beta_1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the mean of Y is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt; when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1 unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 120
x &amp;lt;- rnorm(N)
mu &amp;lt;- exp(1+0.2*x)
Y &amp;lt;- rpois(N, lambda = mu)
glm1 &amp;lt;- glm(Y~x, family = poisson)
glm1$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           x 
##   1.0628067   0.1278092&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(glm1$coefficients[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        x 
## 1.136336&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here increasing &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1 unit multiplies the mean value of Y by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 ) = 1.25\)&lt;/span&gt;. The same thing is true for negative binomial glms as they have the same link function. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-glms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial GLMs&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; #### Logistic regression &lt;br&gt; Things become much more complicated in binomial glms. The model here is actually a model of log odds, so we need to start with an explanation of those. The odds of an event are the probability success divided by the probability of failure. So if the probability of success is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; then the odds are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Odds} = \frac{p}{1-p} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As p increases, so do the odds. The equation for a logistic regression looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{binomial} (p) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log\left(\frac{p}{1-p}\right)  =  \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Skipping some maths that is very similar to the above, we can obtain an interpretation for the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in the model in terms of the odds. When we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by one unit the odds are multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;. Odds are not the most intuitive thing to interpret, but they do increase when p increases, so that if your coefficient &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is positive, increasing &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will increase your probability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bY &amp;lt;- Y&amp;gt;0 #turning counts into presence absence
bin1 &amp;lt;- glm(bY~x,family = binomial)
summary(bin1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = bY ~ x, family = binomial)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4383   0.3373   0.3465   0.3526   0.3861  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  2.78857    0.39303   7.095 1.29e-12 ***
## x            0.06468    0.37057   0.175    0.861    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 53.366  on 119  degrees of freedom
## Residual deviance: 53.335  on 118  degrees of freedom
## AIC: 57.335
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by one unit, the odds of Y are multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 ) = 2.11\)&lt;/span&gt; &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;complementary-log-log&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complementary log-log&lt;/h4&gt;
&lt;p&gt;&lt;br&gt; Possibly a more intuitive model is a binomial regression with a complementary log-log link function. This link function is based on the assumption that you have some counts, which are Poisson distributed, but you’ve decided to turn them into presence/absence.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{binomial} (p) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log(-log(1-p)) = \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In that case you can interpret your coefficients in a similar way as the Poisson regression. When you increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1, the mean of your underlying count (which you have turned into presence/absence) is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvabund)
bin2 &amp;lt;- manyglm(bY~x, family = binomial(link = &amp;quot;cloglog&amp;quot;))
coef(bin2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     bY
## (Intercept) 1.04635177
## x           0.02013689&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interpretation is now the same as in the Poisson case, when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1, the mean of the underlying count is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-binomial-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Log binomial model&lt;/h4&gt;
&lt;p&gt;&lt;br&gt; It is possible to use a log link function with the binomial distribution &lt;code&gt;family = binomial(link = log)&lt;/code&gt;. In this case you can interpret the coefficients as multiplying the probabilities by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;, however these models can give you predicted probabilities greater than 1, and often don’t converge (don’t give an answer). &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;offsets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Offsets&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; Sometimes we know the effect of a particular variable (call it &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) on the response is proportional, so that when we double &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; we expect the response to double on average. The most common time you see this is with sampling intensity.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;glm_coefficients_image.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;If you sample soil and count critters, all other things being equal, you would expect twice the critters in twice the amount of soil. If you have a variable like this it is tempting to divide your response (count) by the amount of soil to standardise the data. Unfortunately this will take counts, which we know how to model with glms, and turn them into something we do not know how to model. Fortunately this situation is easily dealt with using offsets. First, let’s simulate some data for amount of soil, depth (our predictor variable) and count data (with a poisson distribution) where the couunts depend on how much soil was sampled.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soil &amp;lt;- exp(rbeta(N, shape1 = 8, shape2 = 1))
depth &amp;lt;- rnorm(N)
mu &amp;lt;- soil*exp(0.5+0.5*depth)
count &amp;lt;- rpois(N, lambda = mu)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can model counts with depth as our predictor and soil quantity as an offset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;off_mod &amp;lt;- glm(Y~depth+offset(log(soil)), family = poisson)
summary(off_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ depth + offset(log(soil)), family = poisson)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4974  -0.6830  -0.1568   0.5521   2.5910  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept)  0.16048    0.05368   2.989   0.0028 **
## depth        0.03153    0.05458   0.578   0.5635   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 141.22  on 119  degrees of freedom
## Residual deviance: 140.88  on 118  degrees of freedom
## AIC: 470.36
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we ignored the soil amount, we could have misleading conclusions. If the soil amount is correlated with another variable in your model, then leaving out the offset will affect the coefficient of that variable, as in the discussion of conditional/marginal interpretations &lt;a href=&#34;http://environmentalcomputing.net/how-to-interpret-linear-models/&#34;&gt;here&lt;/a&gt;. The offset will also often account for a lot of the variation in the response, so including it will give you a better model overall. What if you’re not sure if the relationship is exactly proportional? In that case just include the variable in your model as a coefficient, and the model will decide the best relationship between it and your response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef_mod &amp;lt;- glm(Y~depth+log(soil), family = poisson)
summary(coef_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ depth + log(soil), family = poisson)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4669  -0.6503  -0.2537   0.5792   2.6574  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  0.48414    0.61501   0.787    0.431
## depth        0.03267    0.05469   0.597    0.550
## log(soil)    0.64191    0.67865   0.946    0.344
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 141.93  on 119  degrees of freedom
## Residual deviance: 140.61  on 117  degrees of freedom
## AIC: 472.08
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient the model estimated is close to 1, which would be equivalent to an offset. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Gordana Popovic&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Intuition Behind Hierarchical Modeling</title>
      <link>/post/some-intuition-behind-hierarchical-modeling/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/some-intuition-behind-hierarchical-modeling/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.berryconsultants.com/intuition-behind-hierarchical-modeling/&#34;&gt;This is an archive of an external source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Date: November 13, 2017&lt;/p&gt;
&lt;p&gt;Author: Kert Viele&lt;/p&gt;
&lt;p&gt;Hierarchical modeling is a powerful tool for making inferences about multiple groups in clinical trials, whether these groups are multiple indications or tumor types in an oncology study, multiple sites in a device clinical trial, or any other source of patient heterogeneity.&lt;/p&gt;
&lt;p&gt;Hierarchical models that share information across groups form a backbone of modern Bayesian thinking. We currently employ hierarchical modeling most commonly in our oncology basket trials, see for example Berry et. al (2013)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/23983156&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/23983156&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also employ hierarchical modeling in recent work on the ADAPT antibiotic platform trial, where we share information across multiple body sites&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.berryconsultants.com/antibiotic-platform-design/&#34; class=&#34;uri&#34;&gt;https://www.berryconsultants.com/antibiotic-platform-design/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hierarchical modeling promises to decrease sample sizes and increase the power of clinical trials but also requires care in implementation. Our goal here is to explain some of the intuition behind hierarchical modeling and how that relates to the advantages and disadvantages of inferences for multiple groups.&lt;/p&gt;
&lt;p&gt;Suppose I’m running an oncology basket trial where success rates on standard of care run about 10%. These are trials that investigate a targeted agent against tumors in multiple body sites, under the assumption that if the targeted agent is effective against a particular mutation, then the agent should generally (but not always) be effective against tumors throughout the body regardless of location. These trials are also referred to as enrichment or umbrella trials in the literature, see for example Woodcock and LaVange (2016)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nejm.org/doi/full/10.1056/NEJMra1510062#t=article&#34; class=&#34;uri&#34;&gt;http://www.nejm.org/doi/full/10.1056/NEJMra1510062#t=article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If I observe four tumor types, 20 patients each, and the data indicate 3/20, 4/20, 5/20, and 6/20 successes in the four groups, then what is my best guess of the success rate for the group with 6/20 successes? Is it greater than 10%?&lt;/p&gt;
&lt;p&gt;The standard estimate for a binomial rate is to take the observed proportion 6/20=30%. If I had only collected data in that group, this estimate would be unbiased (on average it’s equal to the true rate for the group) and have the greatest precision. Statisticians have a fancy name for such an estimate, the minimum variance unbiased estimate (MVUE). And for one group, the MVUE is essentially the best estimate you can make, absent prior information. After all, being unbiased and minimum variance are really good things.&lt;/p&gt;
&lt;p&gt;But I didn’t collect data from one tumor type, I collected data from 4 types and I also know that 6/20 is the highest result among the 4 types. Should that change my estimate? And if so, how?&lt;/p&gt;
&lt;p&gt;It is well established that testing multiple hypotheses is prone to multiplicity errors. Multiple methods (Bonferroni corrections, false discovery rates, gatekeeping strategies) exist which emphasize that p=0.024 means something very different in a group of p-values as opposed to the single primary analysis from a study. See for example Yadav and Lewis (2017).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jamanetwork.com/journals/jama/article-abstract/2656795&#34; class=&#34;uri&#34;&gt;https://jamanetwork.com/journals/jama/article-abstract/2656795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Within a basket trial, we do not need to control family wise type I error. A sponsor could run separate trials in each indication and would not need to adjust alpha. Nevertheless, a sponsor would be wise to interpret the result taking into account multiplicities, both negative, in that one spurious result may be a random high, and positive, in that repeated good results are highly suggestive of a real positive trend. For example, all four groups above have observed rates in excess of 10%.&lt;/p&gt;
&lt;p&gt;Clearly, taking account of the extra information in multiple groups is fundamental to hypothesis testing. Similar phenomena apply to estimation and suggest our usual definitions of bias are limited when considering multiple groups. If I want to estimate the underlying true rates of the 4 groups from the data 3/20, 4/20, 5/20, and 6/20, the standard estimates are the sample proportions 15%, 20%, 25%, and 30%. Within each group, these estimates are unbiased and have the smallest possible variance.&lt;/p&gt;
&lt;p&gt;In the 1950s, Charles Stein made a startling discovery that, once multiple groups were considered, biased estimates are superior in terms of accuracy (mean square error). In other words, while alternative estimates may be biased, they will tend to be closer to the right answer. Their reduction in variance outweighs the bias. Stein’s arguments were technical, but we do have intuition for the results and a motivation for why hierarchical models present a natural inferential framework.&lt;/p&gt;
&lt;p&gt;One basic observation in statistics is that whenever you add noise to a system, variation increases. This seems obvious but has fundamental ramifications. Whenever we observe multiple groups, the highest observed value from the groups is likely biased high and the lower observed value from the groups is likely biased low. Returning to our example with 3/20, 4/20, 5/20, and 6/20 in the four groups, the 30% estimate for the highest group is likely an overestimate, and the 15% in the lowest group is likely an underestimate.&lt;/p&gt;
&lt;p&gt;This phenomenon is commonly seen in sports, for example in baseball where we see batting averages from multiple players. Early in the season we often see several players batting over 0.400 and can observe some extreme slumps at the same time. Neither of these extremes is likely to be real. The highest batting averages are most likely to be good players, but players who are both good and lucky. If you want to estimate how good they really are, you need to remove the luck. For the players with the highest observed averages, their estimates should be lower than their observed rates.&lt;/p&gt;
&lt;p&gt;Whenever we observe multiple groups, either at bats within batters or success rates within tumor types, the observed rates are likely farther apart than the underlying truth. If we want to estimate that underlying truth, it makes sense to move those observed rates together.&lt;/p&gt;
&lt;p&gt;Suppose the true underlying rates were 19%, 21%, 24%, and 26% in the four groups. The true standard deviation for the four groups is 3.11%. If we observe 20 observations in each group, we might get 4/20 in each and no variation, but it’s more likely we get a broader range. In fact, with random sampling the expected observed standard deviation is 9.06%, almost triple the standard deviation of the underlying truth. On average, the largest of the 4 observed rates is 32.8%, far higher than the true highest 26%. The lowest of the 4 observed rates is, on average, 12.8%, much lower than the true lowest 19%.&lt;/p&gt;
&lt;p&gt;In this example, it clearly makes sense to estimate the lowest group higher than the observed rate (after all, it’s clearly biased low) and estimate the highest group as lower than it’s observed rate (it’s clearly biased high). In effect, the estimates should be moved closer together. The resulting estimators are typically called “shrinkage estimators”.&lt;/p&gt;
&lt;p&gt;But how far to shrink? With N=20 per group and true rates of 19%, 21%, 24%, and 26%, we found&lt;/p&gt;
&lt;p&gt;–average observed SD 9.06%, almost triple SD of true rates 3.11%&lt;/p&gt;
&lt;p&gt;–average observed high rate 32.8%, true high rate 26%&lt;/p&gt;
&lt;p&gt;–average observed low rate 12.8%, true low rate 19%.&lt;/p&gt;
&lt;p&gt;What about N=200 per group? The results are a lot closer&lt;/p&gt;
&lt;p&gt;–average observed SD 4.0%, much closer to 3.11%&lt;/p&gt;
&lt;p&gt;–average observed high 27.0%, much closer to 26%&lt;/p&gt;
&lt;p&gt;–average observed low 18.1%, much closer to 19%&lt;/p&gt;
&lt;p&gt;Thus, with a large sample size, we should “shrink” far less. Suppose our true rates were farther apart, with true rates of 5%, 20%, 35%, and 50%, and we observed 20 per group. The standard deviation of the true rates is 19.4%&lt;/p&gt;
&lt;p&gt;–average observed SD 20.9%, close to true SD&lt;/p&gt;
&lt;p&gt;–average observed high 51.4%, not far from 50%&lt;/p&gt;
&lt;p&gt;–average observed low 4.7%, not far from 5%&lt;/p&gt;
&lt;p&gt;In the later examples, we should clearly shrink less than in the earlier example given the observed results (standard deviation, minimum and maximum) are much closer to their true counterparts.&lt;/p&gt;
&lt;p&gt;The key to determining the appropriate amount of shrinkage is the ratio of the true across group variance to the within group variation (the variation resulting from the sampling variation in observed N=20, or 200, observations per group). When the within group variability is much larger than the across group variability, as in our first example, we should shrink our estimates considerably. In the later examples, we should shrink less, either because the N=200 samples resulted in far less within group variation in the second example, or because the large discrepancy in underlying true rates in the third example results in large across group variation. To return to our baseball analogy, the within group variability is the “luck”, while the true across group variation determines whether groups are truly “good” or “bad”. We want to estimate away the random within group variability and leave the true across group variability.&lt;/p&gt;
&lt;p&gt;Hierarchical models estimate the appropriate amount of shrinkage by expressly modeling the within and across group variability.&lt;/p&gt;
&lt;p&gt;Part of the inferential structure describes the likelihood, where each group’s data arises from a Binomial distribution. A higher level of the prior (the levels of this structure motivate the term “hierarchical”) describes how the groups are related. If the underlying parameters in the G groups are theta_1,theta_2,…,theta_G (often these are transformed, for example to logodds), then we often model&lt;/p&gt;
&lt;p&gt;theta_1,theta_2,..,theta_G ~ N(eta,tau^2)&lt;/p&gt;
&lt;p&gt;The thetas are the true parameters, and tau is the across group variance. To finalize the model within a Bayesian perspective, we place priors on eta and tau and estimate them on the basis of the data. The resulting inference provides estimates of the within group and across group variation, and provides shrinkage estimates appropriate for the ratio between them.&lt;/p&gt;
&lt;p&gt;The details of the pros and cons of this design are longer than this blog post will allow (we may revisit it in another). Generally speaking, shrinkage estimators perform admirably and decrease required sample sizes whenever the true parameters approximately obey the N(eta,tau^2) assumption. As with any model, remember “all models are wrong, but some are useful” and, thus, exact normality is not required. Results are particularly strong in the joint null, where a drug has no effect in any group. Shrinkage estimates act as automatic multiplicity adjustments, reducing type I error. When there are consistent strong effects, the use of across group estimates greatly increases power within each group, allowing repeated promising results in multiple groups to be combined into a conclusive narrative of effectiveness. When a large range of true effects is present, the large across group variation essentially eliminates shrinkage in favor of separate estimates per group, with no gain or loss relative to standard analyses. The model above will perform poorly when the N(eta,tau^2) assumption clearly does not fit, which occurs in what we call “nugget” situations, where all groups are null, save for a single group where the drug is effective, or vice versa where the drug is effective in all but a single null group. In such situations, outlying groups are not modeled well by the across group normal distribution and performance suffers, either in terms of reduced power for a single effective group, or inflated type I error for a single effective group. Balancing the pros and cons relative to the expected performance of the groups is a key part of the design process involving any clinical trial with a hierarchical model. The design can be calibrated to properly balance these risks.&lt;/p&gt;
&lt;p&gt;(Many thanks to Anna McGlothlin and Barbara Wendelberger for helpful comments)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HarrellPlot</title>
      <link>/shiny/hdotplot/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/shiny/hdotplot/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
