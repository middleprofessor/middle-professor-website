<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jeffrey A. Walker on Jeffrey A. Walker</title>
    <link>/</link>
    <description>Recent content in Jeffrey A. Walker on Jeffrey A. Walker</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 05 May 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Applied Biostatistics Links</title>
      <link>/post/applied-biostatistics-links/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/applied-biostatistics-links/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;/post/some-intuition-behind-hierarchical-modeling/&#34;&gt;Some Intuition Behind Hierarchical Modeling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/post/interpreting-coefficients-in-glms/&#34;&gt;Interpreting coefficients in GLMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;/post/how-do-i-interpret-the-aic/&#34;&gt;How do I interpret the AIC?&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How do I interpret the AIC?</title>
      <link>/post/how-do-i-interpret-the-aic/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/how-do-i-interpret-the-aic/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.seascapemodels.org/rstats/2018/04/13/how-to-use-the-AIC.html&#34;&gt;This is an archive of an external source. The original is here&lt;/a&gt; Date: xxx Author: Chris Brown&lt;/p&gt;
&lt;p&gt;How do I interpret the AIC?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interpreting coefficients in GLMs</title>
      <link>/post/interpreting-coefficients-in-glms/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/interpreting-coefficients-in-glms/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://environmentalcomputing.net/interpreting-coefficients-in-glms/&#34;&gt;This is an archive of an external source. The original is here&lt;/a&gt; Date: November 11, 2016 Author: Gordana Popovic&lt;/p&gt;
&lt;p&gt;In linear models, the interpretation of model parameters is linear. For example, if a you were modelling plant height against altitude and your coefficient for altitude was -0.9, then plant height will decrease by 1.09 for every increase in altitude of 1 unit.&lt;/p&gt;
&lt;p&gt;For generalised linear models, the interpretation is not this straightforward. Here, I will explain how to interpret the co-efficients in generalised linear models (glms). First you will want to read our pages on glms for &lt;a href=&#34;http://environmentalcomputing.net/generalised-linear-models-1/&#34;&gt;binary&lt;/a&gt; and &lt;a href=&#34;http://environmentalcomputing.net/generalised-linear-models-1/&#34;&gt;count&lt;/a&gt; data page on &lt;a href=&#34;http://environmentalcomputing.net/how-to-interpret-linear-models/&#34;&gt;interpreting coefficients in linear models&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;poisson-and-negative-binomial-glms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Poisson and negative binomial GLMs&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; In Poisson and negative binomial glms, we use a log link. The actual model we fit with one covariate &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; looks like this&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{Poisson} (\lambda) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log(\lambda) = \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;here &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; is the mean of Y. So if we have an initial value of the covariate &lt;span class=&#34;math inline&#34;&gt;\(x_0\)&lt;/span&gt;, then the predicted value of the mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda_0\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  log(\lambda_0) = \beta_0 + \beta_1 x_0 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we now increase the covariate by 1, we get a new mean &lt;span class=&#34;math inline&#34;&gt;\(\lambda_1\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  log(\lambda_1) = \beta_0 + \beta_1 (x_0 +1) = \beta_0 + \beta_1 x_0 +\beta_1 = log(\lambda_0) + \beta_1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the log of the mean of Y increases by &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; when we increase x by 1. But we are not really interested in how the log mean changes, we would like to know on average how Y changes. If we take the exponential of both sides&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \lambda_1 = \lambda_0 exp(\beta_1)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So the mean of Y is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt; when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1 unit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N &amp;lt;- 120
x &amp;lt;- rnorm(N)
mu &amp;lt;- exp(1+0.2*x)
Y &amp;lt;- rpois(N, lambda = mu)
glm1 &amp;lt;- glm(Y~x, family = poisson)
glm1$coefficients&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)           x 
##   0.9900113   0.2431779&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;exp(glm1$coefficients[2])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        x 
## 1.275295&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So here increasing &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1 unit multiplies the mean value of Y by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 ) = 1.25\)&lt;/span&gt;. The same thing is true for negative binomial glms as they have the same link function. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-glms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial GLMs&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; #### Logistic regression &lt;br&gt; Things become much more complicated in binomial glms. The model here is actually a model of log odds, so we need to start with an explanation of those. The odds of an event are the probability success divided by the probability of failure. So if the probability of success is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; then the odds are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Odds} = \frac{p}{1-p} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As p increases, so do the odds. The equation for a logistic regression looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{binomial} (p) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log\left(\frac{p}{1-p}\right)  =  \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Skipping some maths that is very similar to the above, we can obtain an interpretation for the coefficient of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; in the model in terms of the odds. When we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by one unit the odds are multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;. Odds are not the most intuitive thing to interpret, but they do increase when p increases, so that if your coefficient &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is positive, increasing &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; will increase your probability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bY &amp;lt;- Y&amp;gt;0 #turning counts into presence absence
bin1 &amp;lt;- glm(bY~x,family = binomial)
summary(bin1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = bY ~ x, family = binomial)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.5591   0.1067   0.1656   0.2360   0.7281  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)   4.1331     0.8269   4.998 5.78e-07 ***
## x             1.3275     0.7339   1.809   0.0705 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 28.058  on 119  degrees of freedom
## Residual deviance: 24.398  on 118  degrees of freedom
## AIC: 28.398
## 
## Number of Fisher Scoring iterations: 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by one unit, the odds of Y are multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 ) = 2.11\)&lt;/span&gt; &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;div id=&#34;complementary-log-log&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Complementary log-log&lt;/h4&gt;
&lt;p&gt;&lt;br&gt; Possibly a more intuitive model is a binomial regression with a complementary log-log link function. This link function is based on the assumption that you have some counts, which are Poisson distributed, but you’ve decided to turn them into presence/absence.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Y \sim \text{binomial} (p) \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[  log(-log(1-p)) = \beta_0 + \beta_1 x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In that case you can interpret your coefficients in a similar way as the Poisson regression. When you increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1, the mean of your underlying count (which you have turned into presence/absence) is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvabund)
bin2 &amp;lt;- manyglm(bY~x, family = binomial(link = &amp;quot;cloglog&amp;quot;))
coef(bin2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    bY
## (Intercept) 1.5074349
## x           0.5388818&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interpretation is now the same as in the Poisson case, when we increase &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; by 1, the mean of the underlying count is multiplied by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-binomial-model&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Log binomial model&lt;/h4&gt;
&lt;p&gt;&lt;br&gt; It is possible to use a log link function with the binomial distribution &lt;code&gt;family = binomial(link = log)&lt;/code&gt;. In this case you can interpret the coefficients as multiplying the probabilities by &lt;span class=&#34;math inline&#34;&gt;\(exp( \beta_1 )\)&lt;/span&gt;, however these models can give you predicted probabilities greater than 1, and often don’t converge (don’t give an answer). &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;offsets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Offsets&lt;/h3&gt;
&lt;p&gt;&lt;br&gt; Sometimes we know the effect of a particular variable (call it &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;) on the response is proportional, so that when we double &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; we expect the response to double on average. The most common time you see this is with sampling intensity.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;glm_coefficients_image.jpg&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;If you sample soil and count critters, all other things being equal, you would expect twice the critters in twice the amount of soil. If you have a variable like this it is tempting to divide your response (count) by the amount of soil to standardise the data. Unfortunately this will take counts, which we know how to model with glms, and turn them into something we do not know how to model. Fortunately this situation is easily dealt with using offsets. First, let’s simulate some data for amount of soil, depth (our predictor variable) and count data (with a poisson distribution) where the couunts depend on how much soil was sampled.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soil &amp;lt;- exp(rbeta(N, shape1 = 8, shape2 = 1))
depth &amp;lt;- rnorm(N)
mu &amp;lt;- soil*exp(0.5+0.5*depth)
count &amp;lt;- rpois(N, lambda = mu)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can model counts with depth as our predictor and soil quantity as an offset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;off_mod &amp;lt;- glm(Y~depth+offset(log(soil)), family = poisson)
summary(off_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ depth + offset(log(soil)), family = poisson)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3924  -0.9488  -0.0576   0.5753   2.5984  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)   
## (Intercept)  0.14481    0.05418   2.673  0.00752 **
## depth       -0.04579    0.04937  -0.927  0.35373   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 129.85  on 119  degrees of freedom
## Residual deviance: 128.98  on 118  degrees of freedom
## AIC: 461.58
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we ignored the soil amount, we could have misleading conclusions. If the soil amount is correlated with another variable in your model, then leaving out the offset will affect the coefficient of that variable, as in the discussion of conditional/marginal interpretations &lt;a href=&#34;http://environmentalcomputing.net/how-to-interpret-linear-models/&#34;&gt;here&lt;/a&gt;. The offset will also often account for a lot of the variation in the response, so including it will give you a better model overall. What if you’re not sure if the relationship is exactly proportional? In that case just include the variable in your model as a coefficient, and the model will decide the best relationship between it and your response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef_mod &amp;lt;- glm(Y~depth+log(soil), family = poisson)
summary(coef_mod)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ depth + log(soil), family = poisson)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.43747  -1.15721   0.00747   0.57807   2.58419  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  0.87107    0.47087   1.850   0.0643 .
## depth       -0.05379    0.04997  -1.076   0.2817  
## log(soil)    0.19363    0.52176   0.371   0.7106  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 128.10  on 119  degrees of freedom
## Residual deviance: 126.69  on 117  degrees of freedom
## AIC: 461.29
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient the model estimated is close to 1, which would be equivalent to an offset. &lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author&lt;/strong&gt;: Gordana Popovic&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Some Intuition Behind Hierarchical Modeling</title>
      <link>/post/some-intuition-behind-hierarchical-modeling/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/some-intuition-behind-hierarchical-modeling/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.berryconsultants.com/intuition-behind-hierarchical-modeling/&#34;&gt;This is an archive of an external source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Date: November 13, 2017 Author: Kert Viele&lt;/p&gt;
&lt;p&gt;Hierarchical modeling is a powerful tool for making inferences about multiple groups in clinical trials, whether these groups are multiple indications or tumor types in an oncology study, multiple sites in a device clinical trial, or any other source of patient heterogeneity.&lt;/p&gt;
&lt;p&gt;Hierarchical models that share information across groups form a backbone of modern Bayesian thinking. We currently employ hierarchical modeling most commonly in our oncology basket trials, see for example Berry et. al (2013)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/23983156&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/23983156&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also employ hierarchical modeling in recent work on the ADAPT antibiotic platform trial, where we share information across multiple body sites&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.berryconsultants.com/antibiotic-platform-design/&#34; class=&#34;uri&#34;&gt;https://www.berryconsultants.com/antibiotic-platform-design/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hierarchical modeling promises to decrease sample sizes and increase the power of clinical trials but also requires care in implementation. Our goal here is to explain some of the intuition behind hierarchical modeling and how that relates to the advantages and disadvantages of inferences for multiple groups.&lt;/p&gt;
&lt;p&gt;Suppose I’m running an oncology basket trial where success rates on standard of care run about 10%. These are trials that investigate a targeted agent against tumors in multiple body sites, under the assumption that if the targeted agent is effective against a particular mutation, then the agent should generally (but not always) be effective against tumors throughout the body regardless of location. These trials are also referred to as enrichment or umbrella trials in the literature, see for example Woodcock and LaVange (2016)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nejm.org/doi/full/10.1056/NEJMra1510062#t=article&#34; class=&#34;uri&#34;&gt;http://www.nejm.org/doi/full/10.1056/NEJMra1510062#t=article&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If I observe four tumor types, 20 patients each, and the data indicate 3/20, 4/20, 5/20, and 6/20 successes in the four groups, then what is my best guess of the success rate for the group with 6/20 successes? Is it greater than 10%?&lt;/p&gt;
&lt;p&gt;The standard estimate for a binomial rate is to take the observed proportion 6/20=30%. If I had only collected data in that group, this estimate would be unbiased (on average it’s equal to the true rate for the group) and have the greatest precision. Statisticians have a fancy name for such an estimate, the minimum variance unbiased estimate (MVUE). And for one group, the MVUE is essentially the best estimate you can make, absent prior information. After all, being unbiased and minimum variance are really good things.&lt;/p&gt;
&lt;p&gt;But I didn’t collect data from one tumor type, I collected data from 4 types and I also know that 6/20 is the highest result among the 4 types. Should that change my estimate? And if so, how?&lt;/p&gt;
&lt;p&gt;It is well established that testing multiple hypotheses is prone to multiplicity errors. Multiple methods (Bonferroni corrections, false discovery rates, gatekeeping strategies) exist which emphasize that p=0.024 means something very different in a group of p-values as opposed to the single primary analysis from a study. See for example Yadav and Lewis (2017).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jamanetwork.com/journals/jama/article-abstract/2656795&#34; class=&#34;uri&#34;&gt;https://jamanetwork.com/journals/jama/article-abstract/2656795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Within a basket trial, we do not need to control family wise type I error. A sponsor could run separate trials in each indication and would not need to adjust alpha. Nevertheless, a sponsor would be wise to interpret the result taking into account multiplicities, both negative, in that one spurious result may be a random high, and positive, in that repeated good results are highly suggestive of a real positive trend. For example, all four groups above have observed rates in excess of 10%.&lt;/p&gt;
&lt;p&gt;Clearly, taking account of the extra information in multiple groups is fundamental to hypothesis testing. Similar phenomena apply to estimation and suggest our usual definitions of bias are limited when considering multiple groups. If I want to estimate the underlying true rates of the 4 groups from the data 3/20, 4/20, 5/20, and 6/20, the standard estimates are the sample proportions 15%, 20%, 25%, and 30%. Within each group, these estimates are unbiased and have the smallest possible variance.&lt;/p&gt;
&lt;p&gt;In the 1950s, Charles Stein made a startling discovery that, once multiple groups were considered, biased estimates are superior in terms of accuracy (mean square error). In other words, while alternative estimates may be biased, they will tend to be closer to the right answer. Their reduction in variance outweighs the bias. Stein’s arguments were technical, but we do have intuition for the results and a motivation for why hierarchical models present a natural inferential framework.&lt;/p&gt;
&lt;p&gt;One basic observation in statistics is that whenever you add noise to a system, variation increases. This seems obvious but has fundamental ramifications. Whenever we observe multiple groups, the highest observed value from the groups is likely biased high and the lower observed value from the groups is likely biased low. Returning to our example with 3/20, 4/20, 5/20, and 6/20 in the four groups, the 30% estimate for the highest group is likely an overestimate, and the 15% in the lowest group is likely an underestimate.&lt;/p&gt;
&lt;p&gt;This phenomenon is commonly seen in sports, for example in baseball where we see batting averages from multiple players. Early in the season we often see several players batting over 0.400 and can observe some extreme slumps at the same time. Neither of these extremes is likely to be real. The highest batting averages are most likely to be good players, but players who are both good and lucky. If you want to estimate how good they really are, you need to remove the luck. For the players with the highest observed averages, their estimates should be lower than their observed rates.&lt;/p&gt;
&lt;p&gt;Whenever we observe multiple groups, either at bats within batters or success rates within tumor types, the observed rates are likely farther apart than the underlying truth. If we want to estimate that underlying truth, it makes sense to move those observed rates together.&lt;/p&gt;
&lt;p&gt;Suppose the true underlying rates were 19%, 21%, 24%, and 26% in the four groups. The true standard deviation for the four groups is 3.11%. If we observe 20 observations in each group, we might get 4/20 in each and no variation, but it’s more likely we get a broader range. In fact, with random sampling the expected observed standard deviation is 9.06%, almost triple the standard deviation of the underlying truth. On average, the largest of the 4 observed rates is 32.8%, far higher than the true highest 26%. The lowest of the 4 observed rates is, on average, 12.8%, much lower than the true lowest 19%.&lt;/p&gt;
&lt;p&gt;In this example, it clearly makes sense to estimate the lowest group higher than the observed rate (after all, it’s clearly biased low) and estimate the highest group as lower than it’s observed rate (it’s clearly biased high). In effect, the estimates should be moved closer together. The resulting estimators are typically called “shrinkage estimators”.&lt;/p&gt;
&lt;p&gt;But how far to shrink? With N=20 per group and true rates of 19%, 21%, 24%, and 26%, we found&lt;/p&gt;
&lt;p&gt;–average observed SD 9.06%, almost triple SD of true rates 3.11%&lt;/p&gt;
&lt;p&gt;–average observed high rate 32.8%, true high rate 26%&lt;/p&gt;
&lt;p&gt;–average observed low rate 12.8%, true low rate 19%.&lt;/p&gt;
&lt;p&gt;What about N=200 per group? The results are a lot closer&lt;/p&gt;
&lt;p&gt;–average observed SD 4.0%, much closer to 3.11%&lt;/p&gt;
&lt;p&gt;–average observed high 27.0%, much closer to 26%&lt;/p&gt;
&lt;p&gt;–average observed low 18.1%, much closer to 19%&lt;/p&gt;
&lt;p&gt;Thus, with a large sample size, we should “shrink” far less. Suppose our true rates were farther apart, with true rates of 5%, 20%, 35%, and 50%, and we observed 20 per group. The standard deviation of the true rates is 19.4%&lt;/p&gt;
&lt;p&gt;–average observed SD 20.9%, close to true SD&lt;/p&gt;
&lt;p&gt;–average observed high 51.4%, not far from 50%&lt;/p&gt;
&lt;p&gt;–average observed low 4.7%, not far from 5%&lt;/p&gt;
&lt;p&gt;In the later examples, we should clearly shrink less than in the earlier example given the observed results (standard deviation, minimum and maximum) are much closer to their true counterparts.&lt;/p&gt;
&lt;p&gt;The key to determining the appropriate amount of shrinkage is the ratio of the true across group variance to the within group variation (the variation resulting from the sampling variation in observed N=20, or 200, observations per group). When the within group variability is much larger than the across group variability, as in our first example, we should shrink our estimates considerably. In the later examples, we should shrink less, either because the N=200 samples resulted in far less within group variation in the second example, or because the large discrepancy in underlying true rates in the third example results in large across group variation. To return to our baseball analogy, the within group variability is the “luck”, while the true across group variation determines whether groups are truly “good” or “bad”. We want to estimate away the random within group variability and leave the true across group variability.&lt;/p&gt;
&lt;p&gt;Hierarchical models estimate the appropriate amount of shrinkage by expressly modeling the within and across group variability.&lt;/p&gt;
&lt;p&gt;Part of the inferential structure describes the likelihood, where each group’s data arises from a Binomial distribution. A higher level of the prior (the levels of this structure motivate the term “hierarchical”) describes how the groups are related. If the underlying parameters in the G groups are theta_1,theta_2,…,theta_G (often these are transformed, for example to logodds), then we often model&lt;/p&gt;
&lt;p&gt;theta_1,theta_2,..,theta_G ~ N(eta,tau^2)&lt;/p&gt;
&lt;p&gt;The thetas are the true parameters, and tau is the across group variance. To finalize the model within a Bayesian perspective, we place priors on eta and tau and estimate them on the basis of the data. The resulting inference provides estimates of the within group and across group variation, and provides shrinkage estimates appropriate for the ratio between them.&lt;/p&gt;
&lt;p&gt;The details of the pros and cons of this design are longer than this blog post will allow (we may revisit it in another). Generally speaking, shrinkage estimators perform admirably and decrease required sample sizes whenever the true parameters approximately obey the N(eta,tau^2) assumption. As with any model, remember “all models are wrong, but some are useful” and, thus, exact normality is not required. Results are particularly strong in the joint null, where a drug has no effect in any group. Shrinkage estimates act as automatic multiplicity adjustments, reducing type I error. When there are consistent strong effects, the use of across group estimates greatly increases power within each group, allowing repeated promising results in multiple groups to be combined into a conclusive narrative of effectiveness. When a large range of true effects is present, the large across group variation essentially eliminates shrinkage in favor of separate estimates per group, with no gain or loss relative to standard analyses. The model above will perform poorly when the N(eta,tau^2) assumption clearly does not fit, which occurs in what we call “nugget” situations, where all groups are null, save for a single group where the drug is effective, or vice versa where the drug is effective in all but a single null group. In such situations, outlying groups are not modeled well by the across group normal distribution and performance suffers, either in terms of reduced power for a single effective group, or inflated type I error for a single effective group. Balancing the pros and cons relative to the expected performance of the groups is a key part of the design process involving any clinical trial with a hierarchical model. The design can be calibrated to properly balance these risks.&lt;/p&gt;
&lt;p&gt;(Many thanks to Anna McGlothlin and Barbara Wendelberger for helpful comments)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HarrellPlot</title>
      <link>/shiny/hdotplot/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/shiny/hdotplot/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
