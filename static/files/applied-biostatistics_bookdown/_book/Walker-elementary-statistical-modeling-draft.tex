\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Elements of Statistical Modeling for Experimental Biology},
            pdfauthor={Copyright 2018 Jeffrey A. Walker},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Elements of Statistical Modeling for Experimental Biology}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Copyright 2018 Jeffrey A. Walker}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Draft: 2020-02-10}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\emph{More cynically, one could also well ask ``Why has medicine not
adopted frequentist inference, even though everyone presents P-values
and hypothesis tests?'' My answer is: Because frequentist inference,
like Bayesian inference, is not taught. Instead everyone gets taught a
misleading pseudo-frequentism: a set of rituals and misinterpretations
caricaturing frequentist inference, leading to all kinds of
misunderstandings.} -- Sander Greenland

We use statistics to learn from data with uncertainty. Traditional
introductory textbooks in biostatistics implicitly or explicitly train
students and researchers to ``discover by p-value'' using hypothesis
tests (Chapter \ref{p-values}). Over the course of many chapters, the
student learns to use something like a look-up table or a dichotomous
key to choose the correct ``test'' for the data at hand, compute a test
statistic for their data, compute a \emph{p}-value based on the test
statistic, and compare the \emph{p}-value to 0.05. Textbooks typically
give very little guidance about what can be concluded if \(p < 0.05\) or
if \(p > 0.05\), but many researchers conclude, incorrectly, they have
``discovered'' an effect if \(p < 0.05\) but found ``no effect'' if
\(p > 0.05\).

This book is an introduction to the estimation of treatment effects in
biological experiments, and measures of the uncertainty of theses
estimates. Instead of a look-up-table of ``which statistical test'',
this book emphasizes a statistical (or ``regression'') modeling
approach. Compared to the hypothesis testing strategy, the advantages of
the statistical modeling strategy include

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimates of effects and uncertainty are \emph{far} more useful than
  \emph{p}-values. For example, to build useful models on the effects of
  an increasingly acidified ocean on coral growth, we want to estimate
  the \emph{direction} and \emph{magnitude} of the effects at different
  levels of acidification and how these estimates change under different
  conditions. We can compare the magnitude to a prediction of the
  magnitude from a mechanistic model of growth. We can use a magnitude
  and uncertainty to make predictions about the future of coral reefs,
  under different scenarios of ocean acidification. We can use the
  estimated effects and uncertainty to model the consequences of the
  effects of acidification on coral growth on fish production or carbon
  cycling.
\end{enumerate}

By contrast, researchers learn little from a hypothesis test -- that is,
comparing \emph{p} to 0.05. A \emph{p}-value is a measure of
compatibility between the data and the null hypothesis and,
consequently, a pretty good -- but imperfect -- tool to dampen the
frequency that we are fooled by randomness. Importantly, \emph{p}-value
\textbf{is not a measure of the size of an effect}. At most, a small
\emph{p}-value gives a researcher some confidence in the existence and
direction of an effect. But if we are investigating the effects of
acidified ocean water on coral growth, it would be absurd to conclude
from a \emph{p}-value that pH does or does not affect growth. pH affects
\emph{everything} about cell biology.

\emph{p}-values are useful but small tools in the data analysis toolkit.
Importantly, the estimation of effects and uncertainty is not a
necessary alternative to the hypothesis testing strategy. Indeed, the
\emph{p}-value returned by many hypothesis tests are computed from the
statistical model that hides beneath the hypothesis test. Throughout
this text, statistical models are used to compute a \emph{p}-value
\emph{in addition to} the estimates of effects and their uncertainty.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  A unified aproach instead of a collection of seemingly unrelated
  tests. The unifed approach is the linear model and its extensions,
  including linear mixed models and generalized linear models. All of
  these models are variations of the equation for a line \(Y = mX + b\)
  using slightly different notation for the coefficients:
\end{enumerate}

\begin{align}
y_i &\sim N(\mu_i, \theta)\\
\mathrm{E}(Y|X) &= \mu\\
\mu_i &= f(\beta_0 + \beta_1 x_i)
\end{align}

Chapter 1 explains the meaning of this \textbf{model specification} but
the point to make here is that because all linear models and their
generalizations are variations of this specification, a modeling
strategy of learning or doing statistics is more coherent than the
hypothesis testing strategy using look-up tables or dichotomous keys to
the tests. Generalizations of the basic linear model include linear
mixed models, generalized linear models, generalized additive models,
causal graphical models, multivariate models, and machine learning. This
book is not a comprehensive source for any of these methods but,
instead, \emph{a path of the critical elements leading you to the
doorway to the vast universe of each of these methods}.

\textbf{NHST Blues} -- The ``discovery by p-value'' strategy, or
Null-Hypothesis Significance Testing (NHST), has been criticized by
statisticians for many, many decades. Nevertheless, introductory
biostatistics textbooks written by both biologists and statisticians
continue to organize textbooks around a collection of hypothesis tests,
with much less emphasis on estimation and uncertainty. The NHST strategy
of learning or doing statistics is easy in that it requires little
understanding of the statistical model underneath the tests and its
assumptions, limitations, and behavior. The NHST strategy in combination
with point-and-click software enables ``mindless statistics''\footnote{Gegenrezer}
and encourages the belief that statistics is a tool like a word
processor is a tool, afterall, a rigorous analysis of one's data
requires little more than getting p-values and creating bar plots.
Indeed, many PhD programs in the biosciences require no statistics
coursework and the only training available to students is from the other
graduate students and postdocs in the lab. As a consequence, the
biological sciences literature is filled with error bars that imply data
with negative values and p-values that have little relationship to the
probability of the data under the null. More importantly for science,
the reported statistics are often not doing for the study what the
researchers and journal editors think they are doing.

\section{Math}\label{math}

\section{R and programming}\label{r-and-programming}

\chapter*{Part I: R fundamentals}\label{part-i-r-fundamentals}
\addcontentsline{toc}{chapter}{Part I: R fundamentals}

\chapter{Organization -- R Projects and R
Notebooks}\label{organization-r-projects-and-r-notebooks}

\section{Importing Packages}\label{importing-packages}

The R scripts you write will include functions in packages that are not
included in Base R. These packages need to be downloaded from an
internet server to your computer. You only need to do this once. But,
each time you start a new R session, you will need to load a package
using the \texttt{library()} function. Now is a good time to import
packages that we will use

Open R Studio and choose the menu item ``Tools'' \textgreater{}
``Install Packages''. In the ``packages'' input box, insert the names of
packages to install the package. The names can be separated by spaces or
commas, for example ``data.table, emmeans, ggplot2''. Make sure that
``install dependencies'' is clicked before you click ``Install''.
Packages that we will use in this book are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data.table -- improves functionality of data frames
\item
  readxl -- elegant importing from microsoft Excel spreadsheets
\item
  janitor -- we use the function clean\_names from this package
\item
  ggplot2 -- we use this for plotting
\item
  ggpubr -- we use this to make ggplots a bit easier
\item
  emmeans -- we use this to compute modeled means and contrasts
\item
  nlme, lme4, lmerTest -- these are packages for multilevel modeling
\item
  MASS -- we will use glm.nb from this package
\end{enumerate}

Again, once these are installed, you don't need to do this again. You
simply need to use the \texttt{library()} function at the start of a
markdown script.

\section{Create an R Studio Project for this
Class}\label{create-an-r-studio-project-for-this-class}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a project folder within the Documents folder (Mac OS) or My
  Documents folder (Windows OS). All files associated with this book
  will reside inside this folder. The name of the project folder should
  be something meaningful, such as ``Applied\_Biostatics'' or the name
  of your class (for students in my Applied Biostatics class, this
  folder should be named ``BIO\_413'').
\item
  Within the project folder, create new folders named

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    ``notebooks'' -- this is where your R notebooks are stored
  \item
    ``R'' -- this is where additional R scripts are stored
  \item
    ``data'' -- this is where data that we download from public archives
    are stored
  \item
    ``output'' -- this is where you will store fake data generated in
    this class
  \item
    ``images'' -- this is where image files are stored
  \end{enumerate}
\item
  Open R Studio and click the menu item File \textgreater{} New
  Project\ldots{}
\item
  Choose ``Existing Directory'' and navigate to your project folder
\item
  Choose ``Create Project''
\item
  Check that a ``.Rproj'' file is in your project folder
\end{enumerate}

\section{R Notebooks}\label{r-notebooks}

A typical statistical modeling project will consist of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  importing data from Excel or text (.csv or .txt) files
\item
  cleaning data
\item
  analysis
\item
  model checking
\item
  generating plots
\item
  generating tables
\item
  writing text to describe the project, the methods, the analysis, and
  the interpretation of the results (plots and tables)
\end{enumerate}

The best practice for reproducible research is to have all of these
steps in your R Notebook. Too many research projects are not
reproducible because the data were cleaned in Excel, and then different
parts of the data were separately imported into a GUI statistics
software for analysis, and then output from the statistics software was
transcribed to Excel to make a table. And other parts of the analysis
are used to create a plot in some plotting software. And then the tables
and plots are pasted into Microsoft Word to create a report. Any change
at any step in this process will require the researcher to remember all
the downstream parts that are dependent on the change and to re-do an
analysis, or a table, or a plot, etc. etc.

The goal with an R Studio Notebook is to explicitly link all this so
that changes in earlier steps automatically flow into the later steps.
So, at the end of a project, a researcher can choose ``run all'' from
the menu and the data are read, cleaned, analyzed, ploted, tabled, and
put into a report with the text.

This means that you have to think of the organization of the R code that
your write in a Notebook. Your cannot simply append new code to the end
of a script if something earlier (or above) is dependent on it. You need
to go back up and insert the new code at some earlier (and meaningful)
point.

For example, an R chunk generates 100 random normal values and then
plots these with a histogram. This was the chunk that I wrote

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

When I ran the chunk, I got the error ``Error in rnorm(n) : object n not
found''. I was using the function \texttt{rnorm()} to generate values
but I hadn't assigned any value to \texttt{n} yet, so I got the error.
To get this to work properly, I could have just typed
\texttt{n\ \textless{}-\ 100} in the console and then re-run the script
but I want it to work properly on a fresh run of the chunk (after
quitting and re-opening R Studio) so I instead inserted
\texttt{n\ \textless{}-\ 100} at the start of the chunk, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\subsection{Create an R Notebook for this
Chapter}\label{create-an-r-notebook-for-this-chapter}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The top-left icon in R Studio is a little plus sign within a green
  circle. Click this and choose ``R Notebook'' from the pull-down menu.
\item
  Change the title of the notebook to ``Notebook\_01-organization''
\item
  Delete the default R Markdown text starting with ``This is an {[}R
  Markdown{]}\ldots{}''
\end{enumerate}

Now write some text documenting which packages you installed.

\subsection{\texorpdfstring{Create a ``load-packages''
chunk}{Create a load-packages chunk}}\label{create-a-load-packages-chunk}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Click on the ``Insert'' menu on the right hand side of the script (R
  Markdown) pane and choose ``R''. This will insert an R code chunk into
  your R markdown document.
\item
  The first R chunk of a notebook should be a setup chunk. Name the
  chunk ``setup''
\item
  load the libraries ggplot2 and data.table and click the chunk's run
  button (the green triangle to the right of the chunk)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

I added the chunk option ``message=FALSE''. Run your chunk with and
without this as an option.

\subsection{\texorpdfstring{Create a ``simple plot''
chunk}{Create a simple plot chunk}}\label{create-a-simple-plot-chunk}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Create a new chunk and label it ``simple plot''
\item
  insert the following R script and then click the chunk's run button.
  Do you get a plot?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/simple-plot-1.pdf}

\subsection{Create more R chunks and explore options and play with R
code}\label{create-more-r-chunks-and-explore-options-and-play-with-r-code}

\chapter{Data -- Reading, Wrangling, and
Writing}\label{data-reading-wrangling-and-writing}

Importing data into R can be a struggle for new R users and,
unfortunately, most online sources give easy but superficial methods
that don't follow best practices for increasing reproducibility or do
not allow flexible organization of files within a project. Some
examples:

(tl;dr -- go to next section if you just want to import a file and don't
want to understand this important background information!)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{df\ \textless{}-\ read.table(file="clipboard")} imports data
  copied to the clipboard, from an Excel/Sheets file or from an open
  text file. For this to be semi-reproducible, a comment specifying the
  filename, worksheet and range that was copied is necessary. More
  problematic (catastrophically so for reproducibility), is, how does a
  researcher know that they highlighted and copied the correct range in
  the Excel sheet?
\item
  \texttt{df\ \textless{}-\ read.csv(file.choose())} opens the familiar
  ``open file'' dialog box, which lets the user navigate to the file of
  choice. For this to be semi-reproducible, a comment specifying the
  filename to import is necessary. The catastrophic problem (for
  reproducibility) is, how does a researcher know which file was
  actually opened during an R session? The researcher might think they
  opened ``walker\_maine\_bee\_data\_clean\_androscoggin.csv'' but
  mistakenly opened ``walker\_maine\_bee\_data\_clean\_aroostook.csv''.
\item
  \texttt{df\ \textless{}-\ read.table(file="my\_data.txt")} and
  \texttt{df\ \textless{}-\ read\_excel(file="my\_data.xlsx")} are
  reproducible because the filename is explicitly specified. But, this
  method requires that ``my\_data'' is physically located in the same
  folder as the file containing the R script (the notebook .Rmd file in
  our case) and this violates the best practice of clean project
  organization with different folders for the different kinds of files
  (data, R scripts, images, manuscript text, etc.).
\item
  R Studio has an elegant import tool in the environment pane that opens
  a custom dialog box that allows the researcher to navigate to the
  file, and to specify what part of the file to import, such as the
  specific sheet and range for an Excel file. This has the same
  reproducibility issues as \#1 and \#2 but R Studio includes the
  equivalent script, which adds all relevant information for
  reproducility. One then simply copies and pastes this script into a
  code chunk and voila! The next time the script is run, the data can be
  imported from the script without using menus and dialog boxes. Except
  that..the script does not seem to take into account that the
  \textbf{working directory} of an R Markdown file is not the project
  folder but the folder containing the R Markdown file and so this
  two-step method fails. More personally, I'd prefer to run a chunk that
  quickly opens the data file instead of re-navigating through my file
  system and re-specifying the sheet and range every time I re-start the
  project in a new R session.
\end{enumerate}

There are at least three solutions to the issues raised above, all
requiring some understanding of \textbf{file paths} and directory
structure in an operating system. A file such as ``my\_data.xlsx'' has
an \textbf{absolute} file path, which is the full address of the file
(the filename is something like your house street number). The absolute
file path of ``my\_data.xlsx'' might be
``/Users/jwalker/Documents/applied-biostatistics/data/my\_data.xlsx''. A
\textbf{relative} file path is the file path from the \textbf{working
directory}. In an R Studio project, the working directory is the project
directory, which is the directory containing the .Rproj file. This will
be the working directory of the console. \emph{Importantly}, the working
directory of an R Markdown code chunk is the folder containing the saved
R Markdown file. An R Studio Notebook is an R Markdown file so the
working directory of a notebook code chunk is the folder containing the
saved notebook file. If a notebook file is located within the notebooks
folder, which is located within the project folder, then the relative
file path to ``my\_file.xlsx'' is ``../data/my\_file.xlsx''. The ``..''
tells the file OS to move ``up'' into the parent directory (which is the
project folder) and the ``data'' tells the file OS to move ``down'' into
the data folder. These are put together into a single address using
``/''. The beauty of relative paths is that they remain the same -- and
so do not break one's script -- if the project folder, and all of its
contents including the data folder and the notebooks folder, is moved to
another location on the hard drive (say into a new ``Research'' folder).
By contrast, the absolute file path changes, which breaks any old
script.

The three solutions are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a \textbf{relative path} to the file using something like
  \texttt{file\_path\ \textless{}-\ "../data/my\_data.xlsx"}. This
  should \emph{always} work but it fails on some computers. For example,
  if the project folder is on a Windows OS (but not Mac OS) desktop, the
  assigned relative address doesn't seem to look in the folder
  containing the file.
\item
  Create a setup chunk that reroutes the working directory to the
  project folder using the script
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use this in a chuck called "setup" to force the working directory to be}
\CommentTok{# at the level of the project file.}
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_knit}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{root.dir =}\NormalTok{ rprojroot}\OperatorTok{::}\KeywordTok{find_rstudio_root_file}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

For this to work, the chunk has to be named ``setup'', that is, the text
inside the curly brackets at the top of the chunk should be ``r setup''.
Then, with this chunk, the relative file path is
\texttt{file\_path\ \textless{}-\ "../data/my\_data.xlsx"} if
``my\_data.xlsx'' is immediately inside the data folder which is
imediately inside the project folder. This should work on any machine,
and should work even if a project folder is moved. However, it does not
work when writing a book with Bookdown a Bookdown project has multiple
.Rmd files and the chunk names cannot be repeated across files. There
can only be one chunk called ``setup''.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Use the function here(). The most robust solution seems to be using
  the function here() from the here package. The function works
  something like this
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_path <-}\StringTok{ "data"} \CommentTok{# path to data that are imported}
\NormalTok{file_name <-}\StringTok{ "my_data.xlsx"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, file_name) }\CommentTok{# paste together parts of the address}
\KeywordTok{read_excel}\NormalTok{(}\DataTypeTok{file=}\NormalTok{file_path)}
\end{Highlighting}
\end{Shaded}

\texttt{here()} creates an absolute path, but one that is created on the
fly, and will change (or should change) correctly if the project folder
is moved on the same machine or to another machine. And no chunk named
setup is required so this also works in every R markdown file when using
Bookdown.

\section{Create new notebook for this
chapter}\label{create-new-notebook-for-this-chapter}

Create a new notebook and save it to the ``notebooks'' folder of your
BIO\_413 project. \textbf{Important:} In an R Studio project, the
working directory is the project directory (the location of the .Rproj
file) -- this will be the working directory of the console. The working
directory of an R Markdown code chunk is the folder containing the saved
R Markdown file. An R Studio Notebook is an R Markdown file so the
working directory of an notebook code chunk is the folder containing the
saved notebook file. Importantly, the working directory of the code
chunk isn't ``moved'' to the folder containing the saved file
\emph{until the file is saved} so import/export scripts will no work
properly until the file is saved!

Annotate your notebook with notes! Update it as you learn more! We will
use data.table for importing text files in tab-delimited or
comma-separated formats and the readxl package for importing excel
files.

Start each notebook with a chunk named ``setup'' that loads packages and
sets the address for reading and writing files (note that since this
book is advocating the use of here() to set the file path, naming the
setup chunk ``setup'' is not imperative).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(here) }\CommentTok{# here() creates the absolute path to the file}
\KeywordTok{library}\NormalTok{(janitor) }\CommentTok{# clean_names to clean col labels of imported data}
\KeywordTok{library}\NormalTok{(readxl) }\CommentTok{# import excel}
\KeywordTok{library}\NormalTok{(data.table) }\CommentTok{# make data.frames data.tables}
\KeywordTok{library}\NormalTok{(ggplot2) }\CommentTok{# ggplot environment}
\KeywordTok{library}\NormalTok{(ggpubr) }\CommentTok{# publication ready plots}
\KeywordTok{library}\NormalTok{(emmeans) }\CommentTok{# get estimated marginal means and CIs, used for plot}

\NormalTok{here <-}\StringTok{ }\NormalTok{here}\OperatorTok{::}\NormalTok{here }\CommentTok{# plyr also defines a here function}

\CommentTok{# relative paths to project folders}
\NormalTok{data_path <-}\StringTok{ "data"} \CommentTok{# path to data that are imported}
\NormalTok{output_path <-}\StringTok{ "output"} \CommentTok{# path to data that are saved}
\NormalTok{image_path <-}\StringTok{ "images"} \CommentTok{# path to image folder}
\end{Highlighting}
\end{Shaded}

Note on the script above 1. Be kind to the future you by loading only
the packages necessary for the code in the R Markdown file that you are
working on. If your default is to load everything, the future you will
be confused why something was installed. 2. Be kind to the future you by
commenting on why a package is loaded; usually this is a specific
function from the package 3. \texttt{here\ \textless{}-\ here::here} is
my favorite script ever. What is it doing? One can read this as ``assign
the function here from the here package to the object here'' (this is
reading the script right to left). Why do this? It turns out the
multiple packages define a function called ``here''. If any of these
packages are loaded after the here package, then \texttt{here} from here
won't work -- it will be replaced by \texttt{here} from the more
recently loaded package. To make sure that \texttt{here} uses the
function from the here package, I simply reassign \texttt{here} from the
here package to the objeect ``here'' \emph{after} loading in all
packages.

\section{Importing data}\label{importing-data}

Throughout this book, we will download data from the
\href{https://datadryad.org}{Dryad Digital Repository}, which is a major
resource for increasing reproducibility in science. My own view is that
\emph{all data} should be archived on some public server (exceptions
include data that are proprietary or contain sensitive information --
such as human health measures).

\subsection{Excel file}\label{excel-file}

The Excel dataset is from an experiment on the growth response of zebra
finch chicks to an incubation call that presumably signals ``hot
environment'' to the embryos
(\href{http://science.sciencemag.org/content/353/6301/812}{Mariette,
M.M. and Buchanan, K.L., 2016. Prenatal acoustic communication programs
offspring for high posthatching temperatures in a songbird. Science,
353(6301), pp.812-814}). The source file is from the Dryad Repository
here:

\textbf{file name}: ``allDatasetsMarietteBuchanan2016.xls''

\textbf{source}: \url{https://datadryad.org//handle/10255/dryad.122315}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Copy the title of the Dryad page, which is ``Data from: Prenatal
  acoustic communication programs offspring for high post-hatching
  temperatures in a songbird''
\item
  Create a new folder within ``data'' and paste in the copied title as
  the folder name
\item
  Remove the colon from the name, so the folder name is ``Data from
  Prenatal acoustic communication programs offspring for high
  post-hatching temperatures in a songbird''
\item
  Download the .xls file into this folder
\end{enumerate}

A .xls file is an old (pre 2007) Microsoft Excel file type. It is a
binary file and can only be opened into a readable format with
specialized software. The more modern Excel file type is .xlsx, which
contains within it multiple xml components. An xml file is a text file,
and so contains readable content, but the content is xml code to display
something. In general, I am a big advocate of archiving stuff as text
files (manuscripts, data, scripts, blog posts) because these will
\emph{always} be readable by future software. Microsoft Excel is not
likely to die anytime soon and software that can read .xls and
especially .xlsx files (again, .xlsx files are text files) is even less
likely to disappear but we can feel even more confident if data are
archived as text files. That said, a single microsoft excel file with
multiple sheets is an efficient method for distributing data and the
readxl package provides excellent tools for reading different sheets of
a single .xls or .xlsx file.

The code below uses the function \texttt{read\_excel()} from the package
readxl. More about the amazing power of this package is the
\href{https://readxl.tidyverse.org}{tidyverse page} and
\href{http://r4ds.had.co.nz/data-import.html}{chapter 11} in the \emph{R
for Data Science} book.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "Data from Prenatal acoustic communication programs offspring for high post-hatching temperatures in a songbird"}
\NormalTok{fn <-}\StringTok{ "allDatasetsMarietteBuchanan2016.xls"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, fn)}
\CommentTok{#file_path <- paste(data_path, folder, fn, sep="/")}
\NormalTok{chick <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\StringTok{"nestlingMass"}\NormalTok{) }\CommentTok{# read the excel file}
\NormalTok{chick <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(chick) }\CommentTok{# convert to data.table}
\end{Highlighting}
\end{Shaded}

In this book, we will consistently uses this protocol for storing and
retrieving downloaded files. The first three lines in the script above
creates the directory path to the file. This path includes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data\_path -- the relative path into the folder ``data'' (relative to
  the location of the project file)
\item
  folder -- the name of the folder within ``data'' containing the file
\item
  filename -- the name of the file to read
\end{enumerate}

These are all put together into the absolute path using the function
\texttt{here()} from the here package. Take a look at the value of
file\_path to confirm. The
\texttt{read\_excel(file\_path,\ sheet="nestlingMass")} reads the
nestlingMass sheet only. The next
line:\texttt{chick\ \textless{}-\ data.table(chick)} converts the
data.frame into a data.table.

\subsubsection{Peak at the imported data.table to check that the file
was imported correctly and to learn about the
contents.}\label{peak-at-the-imported-data.table-to-check-that-the-file-was-imported-correctly-and-to-learn-about-the-contents.}

Insert the following after your import script and run:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{View}\NormalTok{(chick) }\CommentTok{# check -- are there headers? are there the correct number of columns?}
\end{Highlighting}
\end{Shaded}

The line \texttt{View(chick)} script opens a new tab containing a
spreadsheet-like display of the data frame \texttt{chick}. This is one
way to check that the data were imported correctly, to examine the
column labels to ensure they conform to best practices, and to simply
get to know the contents of the imported file. From the view of chick,
it may not be immediately apparent that the column names contain spaces.
This can be seen more easily using

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{colnames}\NormalTok{(chick)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "chick ID"                       "brood ID"                      
##  [3] "brood composition"              "sex"                           
##  [5] "rank in nest"                   "playback treatment"            
##  [7] "nest temperature above ambient" "max daily temp hatch day"      
##  [9] "mean max temp hatch to day2"    "mean max temp hatch to day10"  
## [11] "mean max temp hatch to day13"   "hatching mass"                 
## [13] "day1 mass"                      "day2 mass"                     
## [15] "day10 mass"                     "day13 mass"                    
## [17] "day13 tarsus"
\end{verbatim}

In general, it is bad practice to include spaces, parentheses, and
special characters such as \$ or \^{}, in the column names of a data
frame because these increase handling costs later on. The best practice
is to replace a blank with an underscore, for example
\texttt{brood\_id}. Some coders separate words with a period
(\texttt{brood.id}). Others mash words together into a single word like
this \texttt{broodid} but this should generally be avoided because the
result can be hard to read. Finally, some coders use Caps to designate
new words like this \texttt{BroodId}. This is easier to read than simple
concatenation but the underscore is the easiest to read.

The \texttt{janitor} package has a nice function to clean the column
names of a data frame including replacing spaces with an underscore and
stripping parentheses. The default clean includes changing any uppercase
letter to lower case. Many coders like to work with all lowercase
variable names to avoid having to hit the shift key.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chick <-}\StringTok{ }\KeywordTok{clean_names}\NormalTok{(chick)}
\end{Highlighting}
\end{Shaded}

Importantly, \textbf{resist the temptation to change the column names in
the data file}, which reduces reproducibility. Leave original data files
original. Always increase reproducibility!

\subsubsection{Best practices for creating data
files}\label{best-practices-for-creating-data-files}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \url{https://www.youtube.com/watch?time_continue=309\&v=Ry2xjTBtNFE}
  -- An excellent video introduction to best practices for organizing
  data in a spreadsheet that will subsequently be analyzed by statistics
  software.
\item
  Broman, K. W., \& Woo, K. H. (2017). Data organization in spreadsheets
  (No. e3183v1). \url{https://doi.org/10.7287/peerj.preprints.3183v1} --
  An excelllent review of best practices for organizing data in a
  spreadsheet.
\end{enumerate}

\subsubsection{Explore with plots}\label{explore-with-plots}

Just for fun, let's plot the data and reproduce Fig. 2A and B. We are
using the \texttt{qplot} function, which is from the ggplot2 package.
Two plots are made and only a subset of the rows are plotted in each (in
A, the subset in which playback\_treatment==``treat'' and, in B, the
subset in which playback\_treatment==``cont''). This book uses the
ggplot2 package extensively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nest_temperature_above_ambient, }\DataTypeTok{y=}\NormalTok{day13_mass, }\DataTypeTok{data=}\NormalTok{chick[playback_treatment}\OperatorTok{==}\StringTok{"treat"}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-plotchickdata-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nest_temperature_above_ambient, }\DataTypeTok{y=}\NormalTok{day13_mass, }\DataTypeTok{data=}\NormalTok{chick[playback_treatment}\OperatorTok{==}\StringTok{"cont"}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-plotchickdata-2.pdf}

\subsection{Text file}\label{text-file}

The example dataset comes from an experiment on the effect of
\href{http://science.sciencemag.org/content/early/2012/03/28/science.1215025}{neonicotinoid
pesticides on bumble bee colony growth}.

\textbf{file name}: ``Whitehorn, O'Connor, Wackers, Goulson (2012) Data
from `Neonicotinoid pesticide reduces bumblebee colony growth and queen
production'.csv.csv''

\textbf{source}:
\url{https://datadryad.org//resource/doi:10.5061/dryad.1805c973}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Copy the title of the Dryad page, which is ``Data from: Neonicotinoid
  pesticide reduces bumblebee colony growth and queen production''
\item
  Create a new folder within ``data'' and paste in the copied title as
  the folder name
\item
  Remove the colon from the name, so the folder name is ``Data from
  Neonicotinoid pesticide reduces bumblebee colony growth and queen
  production''
\item
  Download the .csv file into this folder
\end{enumerate}

A .csv file is a text file that is comma-delimted, which means that the
entries of a row are separated by commas. A text file is readable by any
text editor software and most other kinds of software. Datasets that are
stored as text files are typically saved as either .csv (where the
entries of a row are separated by commas) or .txt (where the entries are
separated by tabs). The base R way to read a .csv file is using
\texttt{read.csv}. The \texttt{read.table} function is more versatile,
as the delimiter can be specified. The function \texttt{fread()} from
the data.table package is fast, smart, and flexible. It is smart in the
sense that it guesses what the delimter is. Unfortunately, because of
spaces in the column labels for this file, fread guesses incorrectly
(another reason why spaces in column labels should be avoided). To
overcome this, the statement below specifies that the file contains a
``header'' (a line containing column labels)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "Data from Neonicotinoid pesticide reduces bumblebee colony growth and queen production"}
\NormalTok{filename <-}\StringTok{ "Whitehorn, O'Connor, Wackers, Goulson (2012) Data from 'Neonicotinoid pesticide reduces bumblebee colony growth and queen production'.csv.csv"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, filename)}
\NormalTok{bee <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(file_path, }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here, as with the import of the Excel file, the first three lines create
the directory path to the file. Peek at the file in the console. Again,
there are spaces in the column names. \textbf{Here I'll leave it to you
to change this}

Here is a reproduction of Fig 2 from the journal article.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bee[, treatment}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(treatment, }\KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Low"}\NormalTok{, }\StringTok{"High"}\NormalTok{))] }\CommentTok{# reorder factor levels}
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"new_queens"}\NormalTok{, }\DataTypeTok{add =} \StringTok{"mean_se"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-plot-bee-fig-2-1.pdf}

The plot suggests immediately some problems with the plot itself and the
associated analysis. First, the y-axis is counts, which means that
negative values are impossible. But the standard error bars look like
they use standard errors computed from a model that allows infinetly
large negative values, and the illustrated standard error bars imply
that negative values exist. So these error bars are misleading. Second,
it is good practice, especially if sample sizes are modest or small, to
``show the data'', which means, show the individual data points and not
just a summary of the distribution.

Here are three alternative plots for exploratory purposes. The first
simply ``shows the data'' but still uses the misleading standard error
bars. The second uses a box plot. The last plots the means and 95\%
confidence intervals modeled with a GLM (generalized linear model) to
account for the count data (the model used could be improved). Notice
that the bar length above the mean is longer than the bar length below
the mean (that is the interval is asymmetric about the mean). In order
to stay focussed on importing data, I leave explanation of these plots
and analysis to later chapters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"new_queens"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{, }\StringTok{"point"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-alternative-plots-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggboxplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"new_queens"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-alternative-plots-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(new_queens }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{family=}\KeywordTok{poisson}\NormalTok{())}
\NormalTok{means.glm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit.glm, }\DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{gg <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{data.frame}\NormalTok{(means.glm), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{y=}\NormalTok{rate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"gray"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{ymin=}\NormalTok{asymp.LCL, }\DataTypeTok{ymax=}\NormalTok{asymp.UCL), }\DataTypeTok{width=}\FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"New queens"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\NormalTok{gg}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-alternative-plots-3.pdf}

\section{Troubleshooting file import}\label{troubleshooting-file-import}

If you get an error that starts with ``Error: \texttt{path} does not
exist:'' then R is not ``seeing'' your specified file given the path
you've given it.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make sure you loaded the package \texttt{here} in a ``load-libraries''
  chunk and that you have run the chunk
\item
  Make sure you have assigned \texttt{data\_path\ \textless{}-\ "data"}
  in the setup chunk and have run the chunk.
\item
  Make sure your ``data'' folder is \emph{one level} inside your project
  folder. ``one level'' means it is not buried deeper inside other
  folders within the project folder.
\item
  Make sure your ``Data from \ldots{}'' folder is one level inside your
  ``data'' folder
\item
  Make sure your data file is one level inside the correct ``Data from''
  folder.
\item
  \textbf{Bug alert} Make sure you have the name of the ``Data from
  \ldots{}'' folder correct in your script. Do not type the name of the
  folder. Instead, go to the finder and highlight the folder containing
  the data file, copy the name, return to the R markdown script, type
  \texttt{folder\ \textless{}-\ ""} and then paste the clipboard (the
  name of the folder) in between the quote marks.
\item
  \textbf{Bug alert} Make sure the file name is correct in the script.
  As with the folder name, I go to the finder and copy the file name and
  paste it in place. In Windows use ctrl-a instead of ctrl-c to copy the
  full filename including the extension.
\end{enumerate}

More generally, Humans are very good at understanding misspelled and
OdDLy capitalized words but the R language (or any computer language) is
very literal. R is \textbf{case sensitive} (some programming languages
are not). ``Prenatal acoustic communication'', ``Prenatal Acoustic
Communication'', and ``prenatal acoustic communication'' are all
different values. Spelling AND capitalization have to be perfect, not
simply close. Spelling includes \emph{spaces}. A frequent bug is a file
name typed as ``Prenatal acoustic communication'' when the actual name
is ``Prenatal acoustic communication''. Can you spot the bug? The
original (what we need to copy) has two spaces between ``acoustic'' and
``communication'' while the incorrect copy has only one.

Spelling bugs are avoided by simply copy and pasting names of folders,
names of files, column names of data frames, and level names of factors,
which leads to a general rule of R scripting\ldots{}

\subsection{Rule number one in R scripting \{\#
rule1\}}\label{rule-number-one-in-r-scripting-rule1}

\textbf{Always copy and paste any text that will be inserted into
quotes}

Do not try to type it out. You have been warned.

\section{Data wrangling}\label{data-wrangling}

\subsection{Converting a single column with all combinations of a 2 x 2
factorial experiment into two columns, each containing the two levels of
a
factor}\label{converting-a-single-column-with-all-combinations-of-a-2-x-2-factorial-experiment-into-two-columns-each-containing-the-two-levels-of-a-factor}

\textbf{Article source}: Tauriello, D., Palomo-Ponce, S., Stork, D. et
al. TGFβ drives immune evasion in genetically reconstituted colon cancer
metastasis. Nature 554, 538--543 (2018) \url{doi:10.1038/nature25492}

\textbf{Data source}:
\url{https://www.nature.com/articles/nature25492\#Sec23}

filename: ``41586\_2018\_BFnature25492\_MOESM10\_ESM.xlsx''

sheet: ``Fig. 4h-tumours''

The analysis of the data in Fig. 4h specifies a single \(X\) variable
``Treatment'' with four levels (or groups): ``Con'', ``Gal'',
``aPD-L1'', and ``Gal+aPD-L1''. These levels indicate that the design is
actually \textbf{factorial} with two factors, each with two levels. The
first factor has levels ``no Gal'' and ``Gal''. The second factor has
levels ``no aPD-L1'', ``aPD-L1''. The single column Treatment
``flattens'' the 2 X 2 factorial design to a 4 x 1 design. In general,
we would want to analyze an experiment like this as factorial model,
because this allows us to make inferences about the \emph{interaction
effect} between the two factors. For these inferences, we need a
standard error, or a confidence interval, or a \emph{p}-value of the
estimate, which we can easily get from the factorial model. In order to
analyze the data with a factorial model, we need to create two new
columns -- one column is the factor variable containing the two levels
of Gal and one column is the factor variable containing the two levels
of aPD-L1.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gal_levels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"no Gal"}\NormalTok{, }\StringTok{"Gal"}\NormalTok{)}
\NormalTok{tumor[, gal }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(treatment }\OperatorTok{==}\StringTok{ "Gal"} \OperatorTok{|}\StringTok{ }\NormalTok{treatment }\OperatorTok{==}\StringTok{ "Gal+aPD-L1"}\NormalTok{,}
\NormalTok{                      gal_levels[}\DecValTok{2}\NormalTok{],}
\NormalTok{                      gal_levels[}\DecValTok{1}\NormalTok{])]}

\NormalTok{apd_levels <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"no aPD-L1"}\NormalTok{, }\StringTok{"aPD-L1"}\NormalTok{)}
\NormalTok{tumor[, apdl1 }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(treatment }\OperatorTok{==}\StringTok{ "aPD-L1"} \OperatorTok{|}\StringTok{ }\NormalTok{treatment }\OperatorTok{==}\StringTok{ "Gal+aPD-L1"}\NormalTok{,}
\NormalTok{                      apd_levels[}\DecValTok{2}\NormalTok{],}
\NormalTok{                      apd_levels[}\DecValTok{1}\NormalTok{])]}

\CommentTok{# re-order factor levels}
\NormalTok{tumor[, gal}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(gal, gal_levels)]}
\NormalTok{tumor[, apdl1}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(apdl1, apd_levels)]}
\end{Highlighting}
\end{Shaded}

A way to check the results to make sure that our conversion is correct
is to compute the sampel size for the 2 x 2 combinations, but include
the original treatment column in the \texttt{by} list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tumor[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(num_positive_per_mm), .(}\DataTypeTok{N=}\NormalTok{.N), by=.(treatment, gal, apdl1)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     treatment    gal     apdl1   N
## 1:        Con no Gal no aPD-L1 124
## 2:        Gal    Gal no aPD-L1  89
## 3:     aPD-L1 no Gal    aPD-L1 101
## 4: Gal+aPD-L1    Gal    aPD-L1  58
\end{verbatim}

That looks good.

\textbf{Bug alert} If you break Rule \#1, and type in the treatment
level ``Gal+aPD-L1'' as ``Gal + aPD-L1'', then you will get new columns
containing junk.

\begin{verbatim}
##     treatment    gal     apdl1   N
## 1:        Con no Gal no aPD-L1 124
## 2:        Gal    Gal no aPD-L1  89
## 3:     aPD-L1 no Gal    aPD-L1 101
## 4: Gal+aPD-L1 no Gal no aPD-L1  58
\end{verbatim}

Remember Rule \#1. Always copy and paste any text that will be inserted
into quotes. This is easily done here by typing
\texttt{unique(tumor\$treatment)} into the console. This function
returns the unique values of the column ``treatment'' of the data.table
``tumor''.

\begin{quote}
unique(tumor\$treatment) {[}1{]} ``Con'' ``Gal'' ``aPD-L1''
``Gal+aPD-L1''
\end{quote}

Now, copy the name of a level and paste into your code. Repeat until
done.

\subsection{Combining data}\label{combining-data}

\textbf{Source article} Bak, A.M., Vendelbo, M.H., Christensen, B.,
Viggers, R., Bibby, B.M., Rungby, J., Jørgensen, J.O.L., Møller, N. and
Jessen, N., 2018. Prolonged fasting-induced metabolic signatures in
human skeletal muscle of lean and obese men. PloS one, 13(9),
p.e0200817.

\textbf{Data source}
\url{https://datadryad.org/stash/dataset/doi:10.5061/dryad.6121hj7}

\textbf{file name}: datafiles.xlsx

The data are from a randomized \textbf{crossover} design where 18 men (9
lean and 9 obese) were measured for multiple metabolic markers at two
times: 1) in a post-absorptive state after 12 hours overnight fast, and
2) in a prolonged fasting state after 72 hours of fasting. In addition,
at each time point, metabolic markers were measured prior to and after
an insulin infusion. Here, we want to reproduce values in Table 2, which
are measures of mean blood insulin and metabolite levels after 12 hours
and 72 hours fasting in both the lean and obese groups.

A difficulty for the analyst is that the response data are in the
``Table 2'' sheet but the variable containing the assignment to ``lean''
or ``obese'' group is in the ``Table 1'' sheet. To analyze these
response, the two datasets need to be combined into a single data frame.
The important consideration when combining data is that \emph{like is
matched with like}. For the fasting dataset, ``like'' is the subject id,
and we have some data for each subject id in Table 1 and other data for
the same subject ids in Table 2. This means that we essentially want to
glue the columns of table 2 to the columns of table 1 in a way that
insures that the correct data for each subject id is on the same row.
This is a bit more complicated for these data because Table 1 contains
18 data rows, one for each subject id and Table 2 contains 36 data rows,
2 for each subject id, because each subject has data measured at 12
hours and at 72 hours.

\subsection{Subsetting data}\label{subsetting-data}

It is common to see researchers create multiple subsets of data for
further processing. This practice should be be discouraged because the
same variables will be in multiple data frames and it can be hard to
keep track of any processing of variables in the different datasets.
Instead, subset the data at the level of analysis.

There are many ways to subset data in R. Experienced users tend to
divide up into those using base R, those using the tidyverse packages,
or those using data.table. Learn one well. This book uses data.table.

In these examples, dt is the name of a data table (a data frame that has
been converted to a data table), not a function. Remember that all
functions are followed by parentheses.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{dt{[}sex=="female",{]}} includes all rows in which the column
  \emph{sex} contains the value ``female''
\item
  \texttt{dt{[}sex=="female"\ \&\ age\ \textgreater{}\ 18,\ {]}}
  includes all rows in which the column \texttt{sex} contains the value
  ``female'' AND the column \emph{age} contains values greater than 18
\item
  \texttt{dt{[}site=="wells\ marsh"\ \textbar{}\ site=="scarborough\ marsh",\ {]}}
  contains the rows in which the column ``site'' contains either the
  value ``wells marsh'' OR ``scarborough marsh''
\item
  \texttt{dt{[}(site=="wells\ marsh"\ \textbar{}\ site=="scarborough\ marsh")\ \&\ sex\ ==\ "female",\ {]}}
  contains the rows in which the column ``site'' contains either the
  value ``wells marsh'' OR ``scarborough marsh'' AND the column
  \emph{sex} contains the value ``female''.
\end{enumerate}

Here I fit a linear model of glucose \textasciitilde{} group to the ``12
hour fast'' subset of the fasting data that was just imported
(\emph{group} is the column containing either ``lean'' or ``obese'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(glucose_t_240_min_m_m }\OperatorTok{~}\StringTok{ }\NormalTok{group, }\DataTypeTok{data=}\NormalTok{fasting[intervention}\OperatorTok{==}\StringTok{"12 h fast"}\NormalTok{, ])}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)   4.8375  0.1201747 40.2538890 7.115319e-16
## groupobese    0.0250  0.1699527  0.1470997 8.851507e-01
\end{verbatim}

\subsection{missing data}\label{missing-data}

The function \texttt{na.omit} omits all rows with at least one missing
value. In general, be vary careful if using \texttt{na.omit} because you
may delete values that you should be analyzing. For example, here is a
fake data set, call it ``plant'', with two response variables, one of
which has a missing value.

treatment

root\_mass

shoot\_mass

cn

100.2

239.9

cn

205.4

cn

95.6

244.6

cn

149.1

252.1

tr

100.2

239.9

tr

118.0

205.4

tr

95.6

244.6

tr

149.1

252.1

This code ``cleans'' the data.table \emph{plant} by omitting rows with
any missing value

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plant <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(plant)}
\end{Highlighting}
\end{Shaded}

The problem is, any analysis of \(shoot\_mass\) will not contain the
value 244.6 because it was deleted from the data frame along with the
rest of the row containing the missing value.

Instead of deleting the entire row, retain all data but handle missing
values at the level of the analysis. This can be done using the subset
mechanism. The function \texttt{is.na} returns a boolean (a variable
with values TRUE or FALSE) for each element in a list. Consequently, the
function \texttt{!is.na} can be used to obtain the rows in which the
value of \(root_mass\) is ``not na''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_table <-}\StringTok{ }\NormalTok{plant[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(root_mass),}
\NormalTok{                       .(}\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(root_mass),}
                         \DataTypeTok{sd=}\KeywordTok{sd}\NormalTok{(root_mass),}
                         \DataTypeTok{se=}\KeywordTok{sd}\NormalTok{(root_mass)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(.N),}
                         \DataTypeTok{N=}\NormalTok{.N), }
\NormalTok{                       by=.(treatment)]}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(summary_table, }\DataTypeTok{digits=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

treatment

mean

sd

se

N

cn

115.0

29.6

17.1

3

tr

115.7

24.2

12.1

4

Many R functions automatically handle missing values. For example, the
\texttt{mean} and \texttt{sd} functions have NA handling methods. These
are used in the script below and generate the correct mean and sd. But
the computation of the standard error fails to compute the correct
answer because it uses the wrong N. Compare the results to the script
using \texttt{!is.na}. Without using \texttt{!is.na}, the sample size
for the control, \emph{for the variable root\_mass}, is incorrect--it is
simply the number of rows where \(Treatment\) is ``cn''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary_table <-}\StringTok{ }\NormalTok{plant[,}
\NormalTok{                       .(}\DataTypeTok{mean=}\KeywordTok{mean}\NormalTok{(root_mass, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{),}
                         \DataTypeTok{sd=}\KeywordTok{sd}\NormalTok{(root_mass, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{),}
                         \DataTypeTok{se=}\KeywordTok{sd}\NormalTok{(root_mass, }\DataTypeTok{na.rm=}\OtherTok{TRUE}\NormalTok{)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(.N),}
                         \DataTypeTok{N=}\NormalTok{.N), }
\NormalTok{                       by=.(treatment)]}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(summary_table, }\DataTypeTok{digits=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

treatment

mean

sd

se

N

cn

115.0

29.6

17.1

3

tr

115.7

24.2

12.1

4

Familiarize yourself with these and with the NA handling methods of
other functions, especially \texttt{lm} and \texttt{predict}.

\subsection{Reshaping data}\label{reshaping-data}

In general, your data should be in long format. What is long format?
Consider data from a simple experiment with a response variable and a
treatment variable with three levels. A common way to organize these
data in Microsoft Excel is in \textbf{wide format}.

\label{tab:unnamed-chunk-16}Root mass for three treatment levels in wide
format

control

extract

charcoal

10.18

4.34

9.57

10.40

3.74

8.59

8.61

4.58

9.69

11.04

4.12

8.07

9.24

2.44

7.63

Wide format is efficient for computations in a spreadsheet, such as
computing means and standard deviations of columns, and for plotting.
For most statistical analyses in R (and most statistics software), all
measures of a variable (such as root biomass in the example above)
should be in a single column with a second column identifying the
treatment level associated with each measure. This is called
\textbf{long format}.

\label{tab:unnamed-chunk-17}Root mass for three treatment levels in long
format

treatment

root\_mass

control

10.18

control

10.40

control

8.61

control

11.04

control

9.24

extract

4.34

extract

3.74

extract

4.58

extract

4.12

extract

2.44

charcoal

9.57

charcoal

8.59

charcoal

9.69

charcoal

8.07

charcoal

7.63

\subsubsection{Wide to long}\label{wide-to-long}

There are many functions to tidy data from wide to long. \texttt{melt()}
from the data.table package is especially useful. It is data.table's
version of melt from the reshape2 package.

The major arguments of data.table::melt are

\texttt{melt(data,\ id.vars,\ measure.vars,\ variable.name,\ value.name)}

melt takes the data in the columns listed in \texttt{measure.vars} and
stacks these into a single column named \texttt{value.name}. The names
of the columns in \texttt{measure.vars} are the values of the elements
in a new column named \texttt{variable.name}. The elements of any column
in \texttt{id.vars} are repeated \emph{p} times, where \emph{p} is the
number of columns that were stacked.

Article: Rolig, A. S., Mittge, E. K., Ganz, J., Troll, J. V., Melancon,
E., Wiles, T. J., \ldots{} Guillemin, K. (2017). The enteric nervous
system promotes intestinal health by constraining microbiota
composition. PLOS Biology, 15(2), e2000689.
\url{https://doi.org/10.1371/journal.pbio.2000689}

\textbf{Data source}
\url{https://doi.org/10.1371/journal.pbio.2000689.s008}

\textbf{file name}: ``journal.pbio.2000689.s008.xlsx''

Let's import and tidy the data for figure 2d. Look at the excel file and
the data in Fig. 2d. There is a single treament with four levels, but
the authors have organized the data in each level in separate columns
and used the column header as the level name.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "The enteric nervous system promotes intestinal health by constraining microbiota composition"}
\NormalTok{filename <-}\StringTok{ "journal.pbio.2000689.s008.xlsx"}

\CommentTok{# figure 2D data}
\NormalTok{sheet_i <-}\StringTok{ "Figure 2"}
\NormalTok{range_i <-}\StringTok{ "F2:I24"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, filename)}
\CommentTok{#file_path <- paste(data_path, folder, fn, sep="/")}
\NormalTok{dt_wide <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\NormalTok{sheet_i, }\DataTypeTok{range=}\NormalTok{range_i))}
\CommentTok{# clean names}
\NormalTok{dt_wide <-}\StringTok{ }\KeywordTok{clean_names}\NormalTok{(dt_wide)}
\CommentTok{# get rid of "_donor"}
\NormalTok{new_colnames <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"gf"}\NormalTok{, }\StringTok{"wt"}\NormalTok{, }\StringTok{"sox10"}\NormalTok{, }\StringTok{"iap_mo"}\NormalTok{)}
\KeywordTok{setnames}\NormalTok{(dt_wide, }\DataTypeTok{old=}\KeywordTok{colnames}\NormalTok{(dt_wide), }\DataTypeTok{new=}\NormalTok{new_colnames)}

\CommentTok{# wide to long}
\NormalTok{exp2d <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(dt_wide, }
              \DataTypeTok{measure.vars=}\KeywordTok{colnames}\NormalTok{(dt_wide), }
              \DataTypeTok{variable.name=}\StringTok{"donor"}\NormalTok{, }
              \DataTypeTok{value.name=}\StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Look at the wide and long data frames using View. For these data, the
number of measures within the different treatments differs, and as a
consequence, there are multiple cells with \texttt{NA} which indicates a
missing value. To delete the rows with the missing values, use

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete rows with no data}
\NormalTok{exp2d <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(exp2d)}
\end{Highlighting}
\end{Shaded}

For both analysis and plots, we want to compare values to the control
level, which is named ``wt'' for the exp2d data. That is, we want ``wt''
to be the \emph{reference} level. To achieve this, the levels of the
factor \emph{donor} need to be re-ordered.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{exp2d[, donor}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(donor, }\KeywordTok{c}\NormalTok{(}\StringTok{"wt"}\NormalTok{, }\StringTok{"gf"}\NormalTok{, }\StringTok{"sox10"}\NormalTok{, }\StringTok{"iap_mo"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

The example above is pretty easy, because the all columns in the
original data frame are melted (stacked). Here is an example in which
only a subset of columns are stacked. In addition, only a subset of the
remaining columns are retained in the long format data frame. The data
are from Panel A of supplement Fig. 8
(\url{https://journals.plos.org/plosbiology/article/file?type=supplementary\&id=info:doi/10.1371/journal.pbio.2003467.s019})
from

Kešnerová, L., Mars, R.A., Ellegaard, K.M., Troilo, M., Sauer, U. and
Engel, P., 2017. Disentangling metabolic functions of bacteria in the
honey bee gut. PLoS biology, 15(12), p.e2003467.

**data source*:**
\url{https://doi.org/10.1371/journal.pbio.2003467.s001}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "Data from Disentangling metabolic functions of bacteria in the honey bee gut"}
\NormalTok{filename <-}\StringTok{ "journal.pbio.2003467.s001.xlsx"}

\CommentTok{# figure 2D data}
\NormalTok{sheet_i <-}\StringTok{ "S8 Fig"}
\NormalTok{range_i <-}\StringTok{ "A2:H12"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, filename)}
\CommentTok{#file_path <- paste(data_path, folder, fn, sep="/")}
\NormalTok{dt_wide <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\NormalTok{sheet_i, }\DataTypeTok{range=}\NormalTok{range_i))}
\CommentTok{# clean names}
\NormalTok{dt_wide <-}\StringTok{ }\KeywordTok{clean_names}\NormalTok{(dt_wide)}

\CommentTok{# wide to long}
\NormalTok{stack_cols <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"replicate"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\NormalTok{exp_s8a <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(dt_wide,}
              \DataTypeTok{id.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"media"}\NormalTok{, }\StringTok{"time_h"}\NormalTok{), }\CommentTok{# strain col not adding any information}
              \DataTypeTok{measure.vars=}\NormalTok{stack_cols, }
              \DataTypeTok{variable.name=}\StringTok{"Replicate"}\NormalTok{, }
              \DataTypeTok{value.name=}\StringTok{"OD600"}\NormalTok{) }\CommentTok{# measure of absorbance at 600nm}
\end{Highlighting}
\end{Shaded}

\subsubsection{Stacking multiple sets of
columns}\label{stacking-multiple-sets-of-columns}

This example comes from my lab, where a student measured sprint speed in
each fish three times prior to treatment and three times following
treatment. The wide format data looked something like this

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{fd_wide <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{fish_ID=}\KeywordTok{paste0}\NormalTok{(}\StringTok{"fish"}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{),}
                      \DataTypeTok{treatment=}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{, }\StringTok{"tr"}\NormalTok{), }\DataTypeTok{each=}\DecValTok{2}\NormalTok{),}
                      \DataTypeTok{length=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{2}\NormalTok{),}
                      \DataTypeTok{pre_1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{),}
                      \DataTypeTok{pre_2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{),}
                      \DataTypeTok{pre_3=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{),}
                      \DataTypeTok{post_1=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{),}
                      \DataTypeTok{post_2=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{),}
                      \DataTypeTok{post_3=}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{                      )}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(fd_wide, }\DataTypeTok{digits=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

fish\_ID

treatment

length

pre\_1

pre\_2

pre\_3

post\_1

post\_2

post\_3

fish1

cn

10.7

51.6

52.9

46.9

49.9

54.6

53.1

fish2

cn

12.4

45.9

48.5

38.9

54.7

53.9

49.7

fish3

tr

10.3

52.4

57.6

55.6

54.1

50.4

49.2

fish4

tr

15.2

53.7

51.9

49.8

53.0

40.1

42.6

To analyze the response (post-treatment sprint) adjusted for
pre-treatment sprint, the three pre-treatment sprint measures need to be
stacked into a single column and the three post-treatment measures need
to be stacked into a single column. This is easy using
\texttt{data.table::melt}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pre_cols <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"pre"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{)}
\NormalTok{post_cols <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"post"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{)}
\NormalTok{fd <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(fd_wide,}
           \DataTypeTok{id.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"fish_ID"}\NormalTok{, }\StringTok{"treatment"}\NormalTok{, }\StringTok{"length"}\NormalTok{),}
           \DataTypeTok{measure.vars=}\KeywordTok{list}\NormalTok{(pre_cols, post_cols),}
           \DataTypeTok{variable.name=}\StringTok{"Order"}\NormalTok{,}
           \DataTypeTok{value.name=}\KeywordTok{c}\NormalTok{(}\StringTok{"sprint_pre"}\NormalTok{, }\StringTok{"sprint_post"}\NormalTok{))}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(fd, }\DataTypeTok{digits=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

fish\_ID

treatment

length

Order

sprint\_pre

sprint\_post

fish1

cn

10.7

1

51.6

49.9

fish2

cn

12.4

1

45.9

54.7

fish3

tr

10.3

1

52.4

54.1

fish4

tr

15.2

1

53.7

53.0

fish1

cn

10.7

2

52.9

54.6

fish2

cn

12.4

2

48.5

53.9

fish3

tr

10.3

2

57.6

50.4

fish4

tr

15.2

2

51.9

40.1

fish1

cn

10.7

3

46.9

53.1

fish2

cn

12.4

3

38.9

49.7

fish3

tr

10.3

3

55.6

49.2

fish4

tr

15.2

3

49.8

42.6

\subsection{Miscellaneous data
wrangling}\label{miscellaneous-data-wrangling}

\hypertarget{vole-data}{\subsubsection{Vole data}\label{vole-data}}

A interesting data set to introduce a linear model for estimating
treatment effects in an experiment are from

Selman, C., McLaren, J.S., Collins, A.R., Duthie, G.G. and Speakman,
J.R., 2013. Deleterious consequences of antioxidant supplementation on
lifespan in a wild-derived mammal. Biology letters, 9(4), p.20130432.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.31cc4/1}
\item
  File: ``RSBL-2013-0432 vole data.xlsx''
\item
  Sheet: ``COLD VOLES LIFESPAN''
\end{enumerate}

The vole data were archived in a format that requires some wrangling
before it can be analyzed. The ``lifespan (days)'' column contains the
lifespan for all three treatment levels, that is, the lifespan values
are ``stacked'' or in ``long'' format. What is unusual is that there is
no ``treatment'' column specifying the three treatment levels (control,
vitamin E, vitamin C). Instead there are control, vitamin E, and vitamin
C columns that contain a 1, if the lifespan value belongs to that
treatment, and a NULL value if not. On import, the NULL value is
replaced with NA, to indicate ``missing''. In order to analyze the data,
the three treatment assignment columns need to be combined into a single
``treatment'' column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "Deleterious consequences of antioxidant supplementation on lifespan in a wild-derived mammal"}
\NormalTok{filename <-}\StringTok{ "RSBL-2013-0432 vole data.xlsx"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, filename)}
\NormalTok{vole <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\StringTok{"COLD VOLES LIFESPAN"}\NormalTok{, }\DataTypeTok{range=}\StringTok{"a2:d98"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{clean_names}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{data.table}\NormalTok{()}

\CommentTok{# create treatment column}
\CommentTok{# in rows where control=1, set treatment value to "control"}
\NormalTok{vole[control}\OperatorTok{==}\DecValTok{1}\NormalTok{, treatment}\OperatorTok{:}\ErrorTok{=}\StringTok{"control"}\NormalTok{]}
\CommentTok{# in rows where vitamin_e=1, set treatment value to "vitamin_e"}
\NormalTok{vole[vitamin_e}\OperatorTok{==}\DecValTok{1}\NormalTok{, treatment}\OperatorTok{:}\ErrorTok{=}\StringTok{"vitamin_E"}\NormalTok{]}
\CommentTok{# in rows where vitamin_c=1, set treatment value to "vitamin_c"}
\NormalTok{vole[vitamin_c}\OperatorTok{==}\DecValTok{1}\NormalTok{, treatment}\OperatorTok{:}\ErrorTok{=}\StringTok{"vitamin_C"}\NormalTok{]}

\CommentTok{# change column "lifespan_days" to "lifespan"}
\KeywordTok{setnames}\NormalTok{(vole, }\DataTypeTok{old=}\StringTok{"lifespan_days"}\NormalTok{, }\StringTok{"lifespan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plot the data with a stripchart (often called a dot plot) to view the
lifespan as a function of treatment level

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"lifespan"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{vole,}
             \DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_ci"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/data-vole-plot-1.pdf}

\section{Saving data}\label{saving-data}

For many projects, it is uncommon to save data. I might save simulated
data if it takes a long time (tens of minutes to hours or even days) to
generate these and I simply want to work with the simulated data in the
future and not have to regenerate it. Or I might save processed data if
it takes a long time import and process and I want to analyze the
processed data in the future and not have to re-import and process it.

If the data will only be used in this or future R projects, the data can
be saved as an R object using \texttt{saveRDS()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{save_file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(output_path, }\StringTok{"vole.Rds"}\NormalTok{)}
\KeywordTok{saveRDS}\NormalTok{(}\DataTypeTok{object =}\NormalTok{ vole, }\DataTypeTok{file =}\NormalTok{ save_file_path)}

\CommentTok{# to read this use}
\NormalTok{vole <-}\StringTok{ }\KeywordTok{readRDS}\NormalTok{(save_file_path)}
\end{Highlighting}
\end{Shaded}

Reading a large .Rds file is very fast compared to reading the same data
stored as a text file. However, if the data need to be imported into
some other software, such as a spreadsheet, then save the data as a text
file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the data to output folder}

\CommentTok{# tab delimited}
\NormalTok{save_file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(output_path, }\StringTok{"vole.txt"}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(vole, save_file_path, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{)}

\CommentTok{# comma delimited}
\NormalTok{save_file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(output_path, }\StringTok{"vole.csv"}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(vole, save_file_path, }\DataTypeTok{sep=}\StringTok{","}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Look at your project directory to make sure the file is where it should
be! We used \texttt{write.table()} to create a tab-delimited text file
using \texttt{sep="\textbackslash{}t"} to specify tabs to separate the
row elements. ``\t'' is the standard character string for a tab. Check
in your output folder and open the file in a text editor.

\chapter*{Part II: Some Fundamentals of Statistical
Modeling}\label{part-ii-some-fundamentals-of-statistical-modeling}
\addcontentsline{toc}{chapter}{Part II: Some Fundamentals of Statistical
Modeling}

\chapter{An Introduction to Statistical
Modeling}\label{an-introduction-to-statistical-modeling}

This chapter introduces statistical modeling using the \textbf{linear
model}. All students are familiar with the idea of a linear model from
learning the equation of a line, which is

\begin{equation}
Y = mX + b
\label{eq:line}
\end{equation}

where \(m\) is the slope of the line and \(b\) is the \(Y\)-intercept.
It is useful to think of equation \eqref{eq:line} as a function that maps
values of \(X\) to values of \(Y\). Using this function, if we input
some value of \(X\), we always get the same value of Y as the output.

A linear model is a function, like that in equation \eqref{eq:line}, that
is fit to a set of data, often to model a process that generated the
data or something like the data. The line in Figure \ref{fig:line}A is
just that, a line, but the line in Figure \ref{fig:line}B is a linear
model fit to the data in Figure \ref{fig:line}B.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/line-1.pdf}
\caption{\label{fig:line}A line vs.~a linear model. (A) the line
\(y=-3.48X + 105.7\) is drawn. (B) A linear model fit to the data. The
model coefficients are numerically equal to the slope and intercept of
the line in A.}
\end{figure}

\section{Two specifications of a linear
model}\label{two-specifications-of-a-linear-model}

\subsection{\texorpdfstring{The ``error draw''
specification}{The error draw specification}}\label{the-error-draw-specification}

A linear model is commonly specified using

\begin{align}
Y &= \beta_0 + \beta_1 X + \varepsilon\\
\label{eq:lm}
\end{align}

This specification of a linear model has two parts: the \textbf{linear
predictor} \(Y = \beta_0 + \beta_1 X\) and the \textbf{error}
\(\varepsilon\). The linear predictor part looks like the equation for a
line except that 1) \(\beta_0\) is used for the intercept and
\(\beta_1\) for the slope and 2) the intercept term precedes the slope
term. This re-labeling and re-arrangement make the notation for a linear
model more flexible for more complicated linear models. For example
\(Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon\) is a model
where \(Y\) is a function of two \(X\) variables.

As with the equation for a line, the linear predictor part of a linear
model is a function that maps a specific value of \(X\) to a value of
\(Y\). This mapped value is the \textbf{expected value} given a specific
input value of \(X\). This is often written as \(\mathrm{E}[Y|X]\),
which is read as ``the expected value of \(Y\) given \(X\)'', where
``given X'' means a specific value of X. Importantly,
\(\mathrm{E}[Y|X]\) is the \textbf{conditional mean}, which is the
\emph{modeled} value of \(Y\) for all observations in which \(X\) takes
some specific value \(x\).

Introductory textbooks almost always introduce linear models using
equation \eqref{eq:lm} above. The key part of the model that is missing
from the specification above is a second line

\begin{equation}
\varepsilon \sim N(0, \sigma^2)
\end{equation}

which is read as ``epsilon is distributed as Normal with mean zero and
variance sigma squared''. This line explicitly specifies the
distribution of the error part. The error part of a linear model is a
random ``draw'' from a normal distribution with mean zero and variance
\(\sigma^2\). Think of this as adding some random value to the expected
value.

\subsection{\texorpdfstring{The ``conditional draw''
specification}{The conditional draw specification}}\label{the-conditional-draw-specification}

A second way of specifying a linear model is

\begin{align}
y_i &\sim N(\mu_i, \sigma^2)\\
\mathrm{E}(Y|X) &= \mu\\
\mu_i &= \beta_0 + \beta_1 x_i
\label{eq:lm-spec2}
\end{align}

The first line states that the response variable \(Y\) is a random
variable independently drawn from a normal distribution with mean
\(\mu\) and variance \(\sigma^2\). This first line is the
\textbf{stochastic} part of the statistical model. The second line
simply states that \(\mu\) (the greek letter ``mu'') from the first line
is the conditional mean or conditional expectation. The third line
states how \(\mu_i\) is generated given that \(X=x_i\). This is the
linear predictor, which is the \textbf{systematic} (or deterministic)
part of the statistical model. It is systematic because the same value
of \(x_i\) will always generate the same \(\mu_i\).

\subsection{Comparing the two ways of specifying the linear
model}\label{comparing-the-two-ways-of-specifying-the-linear-model}

These two ways of specifying the model encourage slightly different ways
of thinking about how the data (the response varible \(Y\)) were
generated. The error draw specification ``generates'' data by randomly
drawing some error \(\varepsilon_i\) from the specified distribution and
adding this to \(x_i\). The conditional draw specification ``generates''
data by constructing what \(y_i\) ``should be'' given \(x_i\) (the
conditional expection), and then drawing a random variable from a
distribution with this expectation. This random draw is \(y_i\) and not
the ``error''. For the error draw generation, we need only one hat of
random numbers, but for the conditional draw generation, we need a hat
for each value of \(x_i\).

The conditional draw specification explicitly defines all parameters,
including the parameters of the linear predictor (\(\beta_0\) and
\(\beta_1\)), the conditional mean \(\mu\) and the variance
\(\sigma^2\). The error draw specification only defines the parameters
of the linear predictor, and often these are referrred to as ``the
parameters'' in the sense that there are not other parameters. The error
draw specification is most useful for thinking about model checking a
fit linear model. The random draw specification is more generally useful
in that it is easily generalized to more complex models, including
hierarchical models, generalized linear models, and Bayesian models. In
fact, \emph{thinking about a model as a predictor plus error can lead to
the misconception that in a generalized linear models, the error has the
distribution (binomial, poisson, etc.) modeled}.

Although a linear model (or statistical model more generally) is a model
of a data-generating process, linear models are not typically used to
actually generate any data. Instead, when we use a linear model to
understand something about a real dataset, we think of our data as one
realization of a process that generates data like ours. A linear model
is a model of that process. That said, it is incredibly useful to use
linear models to create fake datasets for at least two reasons: to probe
our understanding of statistical modeling generally and, more
specifically, to check that a model actually creates data like that in
the real dataset that we are analyzing.

\section{\texorpdfstring{What do we call the \(X\) and \(Y\)
variables?}{What do we call the X and Y variables?}}\label{what-do-we-call-the-x-and-y-variables}

The inputs to a linear model (the \(X\) variables) have many names
including ``independent variables,'' ``predictor variables,'',
``explanatory variables,'' ``treatment variables,'' and ``covariates''.
The output of a linear model (the \(Y\) variable or variables if the
model is multivariate) is the ``dependent variable,'' ``response,'' or
``outcome.'' The \(\beta\) in the linear model are model
\textbf{parameters} and if a parameter is multiplied by an \(X\)
variable then it is also a \textbf{coefficient} (for example,
\(\beta_1\) in model \eqref{eq:lm} is a coefficient). The coefficients of
the \(X\) in a linear model (\(\beta_1\) in model \eqref{eq:lm}) are often
called ``the effects'' (so \(\beta_1\) is the effect of \(X_1\)).

\section{Statistical models are used for prediction, explanation, and
description}\label{statistical-models-are-used-for-prediction-explanation-and-description}

Researchers typically use statistical models to understand relationships
between one or more \(Y\) variables and one or more \(X\) variables.
These relationships include

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Descriptive modeling. Sometimes a researcher merely wants to describe
  the relationship between \(Y\) and a set of \(X\) variables, perhaps
  to discover patterns. For example, the arrival of a spring migrant
  bird (\(Y\)) as a function of sex (\(X_1\)) and age (\(X_2\)) might
  show that males and younger individuals arrive earlier. Importantly,
  if another \(X\) variable is added to the model (or one dropped), the
  coefficients, and therefore, the precise description, will change.
  That is, the interpretation of a coefficient as a descriptor is
  \emph{conditional} on the other covariates (\(X\) variables) in the
  model. In a descriptive model, there is no implication of causal
  effects and the goal is not prediction. Nevertheless, it is very hard
  for humans to discuss a descriptive model without using causal
  language, which probably means that it is hard for us to think of
  these models as \emph{mere description}. Like natural history,
  descriptive models are useful as patterns in want of an explanation,
  using more explicit causal models including experiments.
\item
  Predictive modeling. Predictive modeling is very common in applied
  research. For example, fisheries researchers might model the
  relationship between population density and habitat variables to
  predict which subset of ponds in a region are most suitable for brook
  trout (\emph{Salvelinus fontinalis}) reintroduction. The goal is to
  build a model with minimal prediction error, which is the error
  between predicted and actual values for a future sample. In predictive
  modeling, the \(X\) (``predictor'') variables are largely instrumental
  -- how these are related to \(Y\) is not a goal of the modeling,
  although sometimes an investigator may be interested in the relative
  importance among the \(X\) for predicting \(Y\) (for example,
  collecting the data may be time consuming, or expensive, or
  enviromentally destructive, so know which subset of \(X\) are most
  important for predicting \(Y\) is a useful strategy).
\item
  Explanatory (causal) modeling. Very often, researchers are explicitly
  interested in \emph{how} the \(X\) variables are causally related to
  \(Y\). The fisheries researchers that want to reintroduce trout may
  want to develop and manage a set of ponds to maintain healthy trout
  populations. This active management requires intervention to change
  habitat traits in a direction, and with a magnitude, to cause the
  desired response. This model is predictive -- a specific change in
  \(X\) predicts a specific response in \(Y\) -- because the
  coefficients of the model provide knowledge on how the system
  functions -- how changes in the inputs \emph{cause} change in the
  output. Causal interpretation of model coefficients requires a set of
  strong assumptions about the \(X\) variables in the model. These
  assumptions are typically met in \textbf{experimental designs} but not
  \textbf{observational designs}.
\end{enumerate}

With observational designs, biologists are often not very explicit about
which of these is the goal of the modeling and use a combination of
descriptive, predictive, and causal language to describe and discuss
results. Many papers read as if the researchers intend explanatory
inference but because of norms within the biology community, mask this
intention with ``predictive'' language. Here, I advocate embracing
explicit, explanatory modeling by being very transparent about the
model's goal and assumptions.

\section{Modeling strategy}\label{modeling-strategy}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{choose a model}. Statistical modeling includes a diverse array
  of models, yet almost all methods used by researchers in biology, and
  all models in this book, are generalizations of the linear model
  specified in \eqref{eq:lm-spec2}.
\item
  \textbf{fit the model}, in order to estimate the model parameters and
  the uncertainty in these estimates.
\item
  \textbf{check the model}, which means to use a series of diagnostic
  plots and computations of model output to check that the data
  reasonably approximate the chosen model.
\item
  \textbf{inference from the model}, which means to use the fit
  parameters to learn, with uncertainty, about the system, or to predict
  future observations, with uncertainty.
\item
  \textbf{plot the model}, which means to plot the estimated parameters
  (or other results dervived from the estimates) with their uncertainty.
\end{enumerate}

In order to use a statistical model to describe, predict, or explain, we
need to fit a model to data in order to estimate the parameters. A
linear model fit to some data is

\begin{align}
\hat{y}_i &= b_0 + b_1 x_i + e_i\\
\label{eq:yhat}
\end{align}

\(\hat{y}_i\) (``y hat'') is the \textbf{predicted value} of individual
\(i\), \(b_0\) and \(b_1\) are the coefficients of the model fit (though
technically \(b_0\) is not a coefficient), and \(e_i\) is the residual.
Sometimes \(\hat{y}_i\) is simply called ``the prediction''.

If our goal is inference -- to infer something about the ``population''
from the sample using the fit model, then \(\hat{y}_i\) is the
\textbf{point estimate} of the parameter \(\mu_i\), the coefficients
\(b_0\) and \(b_1\) are point estimates of the parameters \(\beta_0\)
and \(\beta_1\), and the standard deviation of the \(e_i\) is an
estimate of \(\sigma\). ``Population'' is in quotes because it is a very
abstract concept. Throughout this book, Greek letters refer to a
theoretical parameter and Roman letters refer to point estimates.

Throughout this text, I recommend reporting and interpreting
\textbf{interval estimates} of the point estimate. A \textbf{confidence
interval} is a type of interval estimate. A confidence interval of a
parameter is a measure of the uncertainty in the estimate. A 95\%
confidence interval has a 95\% probability (in the sense of long-run
frequency) of containing the parameter This probability is a property of
the population of intervals that could be computed using the same
sampling and measuring procedure. It is not correct, without further
assumptions, to state that there is a 95\% probability that the
parameter lies within the interval. Perhaps a more useful interpretation
is that the interval is a \textbf{compatability interval} in that it
contains the range of estimates that are compatible with the data, in
the sense that a \(t\)-test would not reject the null hypothesis of a
difference between the estimate and any value within the interval (this
interpretation does not imply anything about the true value).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/coldVoles-1.pdf}
\caption{\label{fig:coldVoles}HarrellPlot of vole data.}
\end{figure}

For the model fit to the data in Figure \ref{fig:line}B, the coefficient
of \(X\) is the slope of the line. Perhaps surprisingly, we can fit a
model like equation \eqref{eq:lm} to data in which the \(X\) variable is
categorical. A simple example is the experiment of antioxidants
(vitamins C and E) on lifespan in Voles (Fig. \ref{fig:coldVoles}). In
this experiment, the \(X\) variable is categorical, with three
\textbf{levels}: ``Control'', ``Vitamin\_E'' and ``Vitamin\_C''.
Categorical \(X\) variables are often called \textbf{factors}. The trick
to using a statistical model with categorical \(X\) is to recode the
factor levels into numbers -- how this is done is explained in Chapter
xxx. When the \(X\) variable is categorical, the coefficients of the
\(X\) are \emph{differences in group means}. The linear model fit to the
vole data has two coefficients, one for Vitamin E and one for vitamin C.
The estimate and uncertainty of the these two coefficients are shown in
the top part of Figure \ref{fig:coldVoles}. The bottom part shows the
raw data, as well as the group (factor level) means and the uncertainty
in the estimate of these means.

\section{A mean is the simplest
model}\label{a-mean-is-the-simplest-model}

The simplest possible model that can be fit to the data is

\begin{equation}
\mathrm{E}[Y] = b_0
\label{eq:unconditional}
\end{equation}

which is simply the mean of \(Y\), or, more specifically, the
\textbf{unconditional mean} of \(Y\), since its value is not conditional
on any value of \(X\).

\section{Assumptions for inference with a statistical
model}\label{assumptions-for-inference-with-a-statistical-model}

\textbf{Inference} refers to using the fit model to generalize from the
sample to the population, which assumes that the response is drawn from
some specified probability distribution (Normal, or Poisson, or
Bernouli, etc.). Throughout this text, I emphasize reporting and
interpreting point estimates and confidence intervals. Another kind of
inference is a \textbf{significance test}, which is the computation of
the probability of ``seeing the data'' or something more extreme than
the data, given a specified null hypothesis. A significance test results
in a \textbf{p-value}, which can be reported with the point estimate and
confidence interval. Somewhat related to a significance test is a
hypothesis test, or what is now often perjoratively called a
\textbf{Null-Hypothesis Signficance Test} (NHST), in which the
\(p\)-value from a significance test is compared to a pre-specified
error rate called \(\alpha\). NHST may be useful for some very limited
kinds of science but, in general, is not useful for most biological
research and, instead, leads to large misconceptions. A general rule of
thumb is, do not compare a reported \(p\)-value to \(\alpha\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The data were generated by a process that is ``linear in the
  parameters'', which means that the different components of the model
  are added together. This additive part of the model containing the
  parameters is the linear predictor in specifications \eqref{eq:lm} and
  \eqref{eq:lm-spec2} above. For example, a cubic polynomial model
\end{enumerate}

\begin{equation}
\mathrm{E}(Y|X) = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3
\end{equation}

is a linear model, even though the function is non-linear, because the
different components are added. Because a linear predictor is additive,
it can be compactly defined using matrix algebra

\begin{equation}
\mathrm{E}(Y|X) = \mathbf{X}\boldsymbol{\beta}
\end{equation}

where \(mathbf{X}\) is the \textbf{model matrix} and
\(\boldsymbol{\beta}\) is the vector of parameters. We discuss these
more in chapter xxx.

A \textbf{Generalized Linear Model} (GLM) has the form
\(g(\mu_i) = \eta_i\) where \(\eta\) (the Greek letter ``eta'') is the
linear predictor

\begin{equation}
\eta = \mathbf{X}\boldsymbol{\beta} 
\end{equation}

GLMs are extensions of linear models. There are non-linear models that
are not linear in the parameters, that is, the predictor is not a simple
dot product of the model matrix and a vector of parameters. For example,
the Michaelis-Menten model is a non-linear model

\begin{equation}
\mathrm{E}(Y|X)  = \frac{\beta_1 X}{\beta_2 + X}
\end{equation}

that is non-linear in the parameters because the parts are not added
together. This text covers linear models and generalized linear models,
but not non-linear models that are also non-linear in the parameters.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The draws from the probability distribution are \textbf{independent}.
  Independence implies \textbf{uncorrelated} \(Y\) conditional on the
  \(X\), that is, for any two \(Y\) with the same value of \(X\), we
  cannot predict the value of one given the value of the other. For
  example, in the vole data above, uncorrelated implies that we cannot
  predict the lifespan of one vole within the Vitamin E treatment given
  the lifespan of another vole in the Vitamin E treatment. For linear
  models, this assumption is often stated as ``independent errors'' (the
  \(\varepsilon\) in model \eqref{eq:lm}) instead of independent
  observations.
\end{enumerate}

There are lots of reasons that conditional responses might be
correlated. In the vole example, perhaps the voles were housed in
batches of 5 individuals, and slight differences in the environment
among the housing containers, caused all the voles in some containers to
have shorter lifespans than expected given their treatment assigment and
all voles in other containers to have longer lifespans than expected
given their treatment assigment. More generally, if there are measures
both within and among experimental units (field sites or humans or rats)
then we'd expect the measures within the same unit to err from the model
in the same direction. Multiple measures within experimental units (a
site or individual) creates ``clustered'' observations. Lack of
independence or clustered observations can be modeled using models with
\textbf{random effects}. These models go by many names including linear
mixed models (common in Ecology), hierarchical models, multilevel
models, and random effects models. A linear mixed model is a variation
of model \eqref{eq:lm}. This text introduces linear mixed models in
chapter xxx.

Measures that are taken from sites that are closer together or measures
taken closer in time or measures from more closely related biological
species will tend to have more similar values than measures taken from
sites that are further apart or from times that are further apart or
from species that are less closely related. Space and time and phylogeny
create \textbf{spatial and temporal and phylogenetic autocorrelation}.
Correlated error due to space or time or phylogeny can be modeled with
\textbf{Generalized Least Squares} (GLS) models. A GLS model is a
variation of model \eqref{eq:lm}.

\section{Specific assumptions for inference with a linear
model}\label{specific-assumptions-for-inference-with-a-linear-model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Constant variance} or \textbf{homoskedasticity}. The most
  common way of thinking about this is the error term \(\varepsilon\)
  has constant variance, which is a short way of saying that random
  draws of \(\varepsilon\) in model \eqref{eq:lm} are all from the same
  (or \textbf{identical}) distribution. This is explicitly stated in the
  second line of model specification \eqref{eq:lm}. If we were to think
  about this using model specification \eqref{eq:lm-spec2}, then
  homoskedasticity means that \(\sigma\) in \(N(\mu, \sigma)\) is
  constant for all observations (or that the \emph{conditional}
  probability distributions are identical, where \emph{conditional}
  would mean adjusted for \(\mu\))
\end{enumerate}

Many biological processes generate data in which the error is a function
of the mean. For example, measures of biological variables that grow,
such as lengths of body parts or population size, have variances that
``grow'' with the mean. Or, measures of counts, such as the number of
cells damaged by toxin, the number of eggs in a nest, or the number of
mRNA transcripts per cell have variances that are a function of the
mean. Heteroskedastic error can be modeled with \textbf{Generalized
Least Squares}, a generalization of the linear model, and with
\textbf{Generalized Linear Models} (GLM), which are ``extensions'' of
the classical linear model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Normal or \textbf{Gaussian} probability distribution. As above, the
  most common way of thinking about this is the error term
  \(\varepsilon\) is Normal. Using model specification
  \eqref{eq:lm-spec2}, we'd say the conditional probablity distribution of
  the response is normal. A normal probability distribution implies that
  1) the response is continuous and 2) the conditional probability is
  symmetric around \(mu_i\). If the conditional probability distribution
  has a long left or right tail it is \textbf{skewed} left or right.
  Counts (number of cells, number of eggs, number of mRNA transcripts)
  and binary responses (sucessful escape or sucessful infestation of
  host) are not continuous and often often have asymmetric probablity
  distributions that are skewed to the right and while sometimes both
  can be reasonably modeled using a linear model they are more often
  modeled using generalized linear models, which, again, is an extension
  of the linear model in equation \eqref{eq:lm-spec2}.
\end{enumerate}

A common misconception is that inference from a linear model assumes
that the \emph{unconditional response} (\(Y\)) is normally distributed.
Both the ``linear model'' and ``statistical model'' ways of specifying
the model show precisely why this conception is wrong. Model \eqref{eq:lm}
states explicitly that it is the error that has the normal distribution
-- the distribution of \(Y\) is a mix of the distribution of \(X\) and
the error. Model \eqref{eq:lm-spec2} states that the conditional outcome
has a normal distribution, that is, the distribution after adjusting for
variation in \(X\).

\section{\texorpdfstring{``Statistical model'' or ``regression
model''?}{Statistical model or regression model?}}\label{statistical-model-or-regression-model}

Statistical modeling terminology can be confusing. The \(X\) variables
in a statistical model may be quantitative (continuous or integers) or
categorical (names or qualitative amounts) or some mix of the two.
Linear models with all quantitative independent variables are often
called ``regression models.'' Linear models with all categorical
independent variables are often called ``ANOVA models.'' Linear models
with a mix of quantitative and categorical variables are often called
``ANCOVA models'' if the focus is on one of the categorical \(X\) or
``regression models'' if there tend to be many independent variables.
Other patterns occur. For example ``ANCOVA models'' often include
interaction effects but ``regression models'' rarely do. To avoid
thinking of statistical analysis as ``regression vs.~ANOVA'' (the type
of thinking encouraged by many textbooks in biostatistics), I will most
often use the term ``statistical model'' for general usage, and use a
more specific term only to emphasize something about the model in that
particluar context.

\section{GLM vs.~GLM vs.~GLS}\label{glm-vs.glm-vs.gls}

Linear models are sometimes called ``general linear models'' with the
abbreviation GLM. This is unfortunate because the abbreviation GLM
usually refers to \textbf{generalized linear models}. Regardless, don't
confuse either version of GLM with GLS, which is the abbreviation of
\textbf{generalized least squares}. GLS generalizes the linear model to
allow for heteroskedastic and/or correlated error (using the ``linear
model'' way of thinking about model specification)

\chapter{Variability and Uncertainty (Standard Deviations, Standard
Errors, Confidence
Intervals)}\label{variability-and-uncertainty-standard-deviations-standard-errors-confidence-intervals}

\textbf{Uncertainty} is the stuff of science. A major goal of statistics
is measuring uncertainty. What do we mean by uncertainty? Uncertainty is
the error in estimating a parameter, such as the mean of a sample, or
the difference in means between two experimental treatments, or the
predicted response given a certain change in conditions. Uncertainty is
measured with a \textbf{variance} or its square root, which is a
\textbf{standard deviation}. The standard deviation of a statistic is
also (and more commonly) called a \textbf{standard error}.

Uncertainty emerges because of variability. In any introductory
statistics class, students are introduced to two measures of
variability, the ``standard deviation'' and the ``standard error.''
These terms are absolutely fundamental to statistics -- they are the
start of everything else. Yet, many biology researchers confuse these
terms and certainly, introductory students do too.

When a research biologist uses the term ``standard deviation,'' they are
probably referring to the sample standard deviation which is a measure
of the variability of a sample. When a research biologist uses the term
``standard error,'' they are probably referring to the standard error of
a mean, but it could be the standard error of another statistics, such
as a difference between means or a regression slope. An important point
to remember and understand is that all standard errors \emph{are}
standard deviations. This will make more sense soon.

\section{The sample standard deviation vs.~the standard error of the
mean}\label{the-sample-standard-deviation-vs.the-standard-error-of-the-mean}

\subsection{Sample standard deviation}\label{sample-standard-deviation}

The sample standard deviation is a measure of the variability of a
sample. For example, were we to look at a histological section of
skeletal muscle we would see that the diameter of the fibers (the muscle
cells) is variable. We could use imaging software to measure the
diameter of a sample of 100 cells and get a \textbf{distribution} like
this

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/histogram-1.pdf}

The mean of this sample is 69.4µm and the standard deviation is 2.8 µm.
The standard deviation is the square root of the variance, and so
computed by

\begin{equation}
s_y = \sqrt{\frac{\sum_{i=1}^n{(y_i - \overline{y})^2}}{n-1}}
\label{eq:variance}
\end{equation}

Memorize this equation. To understand the logic of this measure of
variability, note that \(y_i - \overline{y}\) is the \textbf{deviation}
of the \(i\)th value from the sample mean, so the numerator is the sum
of squared deviations. The numerator is a sum over \(n\) items and the
denominator is \(n-1\) so the variance is (almost!) an averaged squared
deviation. More variable samples will have bigger deviations and,
therefore, bigger average squared deviations. Since the standard
deviation is the square root of the variance, a standard deviation is
the square root of an average squared deviation. This makes it similar
in value to the averaged deviation (or average of the absolute values of
the deviations since the average deviation is, by definition of a mean,
zero).

\subsubsection{Notes on the variance and standard
deviation}\label{notes-on-the-variance-and-standard-deviation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Variances are additive but standard deviations are not. This means
  that the variance of the sum of two independent (uncorrelated) random
  variables is simply the sum of the variances of each of the variables.
  This is important for many statistical analyses.
\item
  The units of variance are the square of the original units, which is
  awkward for interpretation. The units of a standard deviation is the
  same as that of the original variable, and so is much easier to
  interpet.
\item
  For variables that are approximately normally distributed, we can map
  the standard deviation to the quantiles of the distribution. For
  example, 68\% of the values are within one standard deviation of the
  mean, 95\% of the values are within two standard deviations, and 99\%
  of the values are within three standard deviations.
\end{enumerate}

\subsection{Standard error of the
mean}\label{standard-error-of-the-mean}

A standard error of a statistic is a measure of the precision of the
statistic. The standard error of the mean is a measure of the precision
of the estimate of the mean. The standard error of a difference in means
is a measure of the precision of the estimate of the difference in
means. The smaller the standard error, the more precise the estimate.
The standard error of the mean (SEM) is computed as

\begin{equation}
SEM = \frac{s_y}{\sqrt{n}}
\label{eq:se}
\end{equation}

The SEM is often denoted \(s_{\bar{y}}\) to indicate that it is a
standard deviation of the mean (\(\bar{y}\)).

\subsubsection{The standard error of the mean can be thought of as a
standard deviation of an infinitely long column of re-sampled
means}\label{the-standard-error-of-the-mean-can-be-thought-of-as-a-standard-deviation-of-an-infinitely-long-column-of-re-sampled-means}

In what sense is a standard error a standard deviation? This is kinda
weird. If we sample 100 cells in the slide of muscle tissue and compute
the mean diameter, how can the mean have a standard deviation? There is
only one value!

To understand how the SEM is a standard deviation, imagine that we
sample \(n\) values from \(N(\mu, \sigma^2)\) (a normal distribution
with mean \(\mu\) and variance \(\sigma^2\). The mean of our sample is
an estimate of \(\mu\) the standard deviation of sample is an estimate
of \(\sigma\)) an infinite number of times and each time, we write down
the mean of the new sample. The standard deviation of this infinitely
long column of means is the standard error of the mean. Our observed SEM
is an estimate of this true value because our observed standard
deviation is an estimate of \(\sigma\).

\subsubsection{A standard deviation can be computed for any statistic --
these are all standard
errors.}\label{a-standard-deviation-can-be-computed-for-any-statistic-these-are-all-standard-errors.}

The SEM is only one kind of standard error. A standard deviation can be
computed for any statistic -- these are all standard errors. For some
statistics, such as the mean, the standard error can be computed
directly using an equation, such as that for the SEM (equation
\eqref{eq:se}). For other statistics, a computer intensive method known as
the \textbf{bootstrap} is necessary to compute a standard error. We will
return to the bootstrap in Section \ref{bootstrap}.

\subsubsection{Notes on standard errors}\label{notes-on-standard-errors}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The units of a standard error are the units of the measured variable.
\item
  A standard error is proportional to sample variability (the sample
  standard deviation, \(s_y\)) and inversely proportional to sample size
  (\(n\)). Sample variability is a function of both natural variation
  (there really is variation in diameter among fibers in the quadriceps
  muscle) and measurement error (imaging software with higher resolution
  can measure a diameter with less error). Since the SEM is a measure of
  the precision of estimating a mean, this means this precision will
  increase (or the SEM will decrease) if 1) an investigator uses methods
  that reduce measurement error and 2) an investigator computes the mean
  from a larger sample.
\item
  This last point (the SEM decreases with sample size) seems obvious
  when looking at equation \eqref{eq:se}, since \(n\) is in the
  denominator. Of course \(n\) is also in the denominator of equation
  \eqref{eq:variance} for the sample standard deviation but the standard
  deviation does not decrease as sample size increases. First this
  wouldn't make any sense -- variability is variability. A sample of
  10,000 cell diameters should be no more variable than a sample of 100
  cell diameters (think about if you agree with this or not). Second,
  this should also be obvious from equation \eqref{eq:variance}. The
  standard deviation is the square root of an average and averages don't
  increase with the number of things summed since both the the numerator
  (a sum) and denominator increase with \(n\).
\end{enumerate}

\section{Using Google Sheets to generate fake data to explore the
standard
error}\label{using-google-sheets-to-generate-fake-data-to-explore-the-standard-error}

In statistics we are interested in estimated parameters of a
\textbf{population} using measures from a \textbf{sample}. The goal in
this section is to use Google Sheets (or Microsoft Excel) to use fake
data to discover the behavior of sampling and to gain some intuition
about uncertainty using standard errors.

\subsection{Steps}\label{steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open Google Sheets
\item
  In cell A1 type ``mu''. mu is the greek letter \(\mu\) and is very
  common notation for the poplation value (the TRUE value!) of the mean
  of some hypothetical measure. In cell B1, insert some number as the
  value of \(\mu\). Any number! It can be negative or positive.
\item
  In cell A2 type ``sigma''. sigma is the greek letter \(\sigma\).
  \(\sigma^2\) is very common (universal!) notation for the population
  (TRUE) variance of some measure or parameter. Notice that the true
  (population) values of the mean and variance are greek letters. This
  is pretty standard in statistics. In cell B2, insert some positive
  number (standard deviations are the positive square roots of the
  variance).
\item
  In cell A8 type the number 1
\item
  In cell A9 insert the equation ``=A8 + 1''. What is this equation
  doing? It is adding the number 1 to to the value in the cell above, so
  the resulting value should be 2.
\item
  In Cell B8, insert the equation ``=normsinv(rand())*\$B\$2 + \$B\$1``.
  The first part of the equation creates a random normal variable with
  mean 0 and standard deviation 1. multiplication and addition transform
  this to a random normal variable with mean \(\mu\) and standard
  deviation \(\sigma\) (the values you set in cells B1 and B2).
\item
  copy cell B8 and paste into cell B9. Now Higlight cells A9:B9 and copy
  the equations down to row 107. You now have 100 random variables
  sampled from a infinite population with mean \(\mu\) and standard
  deviation \(\sigma\).
\item
  In cell A4 write ``mean 10''. In cell B4 insert the equation
  ``=average(B8:B17)''. The resulting value is the \textbf{sample mean}
  of the first 10 random variables you created. Is the mean close to
  \(\mu\)?
\item
  In cell A5 write ``sd 10''. In cell B5 insert the equation
  ``stdev(B8:B17)''. The result is the \textbf{sample standard
  deviation} of the first 10 random variables. Is this close to
  \(\sigma\)?
\item
  In cell A6 write ``mean 100''. In cell B6 insert the equation
  ``=average(B8:B107)''. The resulting value is the \textbf{sample mean}
  of the all 100 random variables you created. Is this mean closer to
  \(\mu\) than mean 10?
\item
  In cell A7 write ``sd 100''. In cell B7 insert the equation
  ``=stdev(B8:B107)''. The resulting value is the \textbf{sample
  standard deviation} of the all 100 random variables you created. Is
  this SD closer to \(\sigma\) than sd 10?
\end{enumerate}

The sample standard deviation is a measure of the variability of the
sample. The more spread out the sample (the further each value is from
the mean), the bigger the sample standard deviation. The sample standard
deviation is most often simply known as ``The'' standard deviation,
which is a bit misleading since there are many kinds of standard
deviations!

Remember that your computed mean and standard deviations are estimates
computed from a sample. They are estimates of the true values \(\mu\)
and \(\sigma\). Explore the behavior of the sample mean and standard
deviation by re-calculating the spreadsheet. In Excel, a spreadsheet is
re-calculated by simultaneously pressing the command and equal key. In
Google, command-R recalculates but is painfully slow. Instead, if using
Google Sheets, just type the number 1 into a blank cell, and the sheet
recalculates quickly. Do it again. And again.

Each time you re-calculate, a new set of random numbers are generated
and the new means and standard deviations are computed. Compare mean 10
and mean 100 each re-calculation. Notice that these estimates are
variable. They change with each re-calculation. How variable is mean 10
compared to mean 100? The variability of the estimate of the mean is a
measure of \textbf{uncertainty} in the estimate. Are we more uncertain
with mean 10 or with mean 100? This variability is measured by a
standard deviation. This \textbf{standard deviation of the mean} is also
called the \textbf{standard error of the mean}. Many researchers are
loose with terms and use ``The'' standard error to mean the standard
error of the mean, even though there are many kinds of standard errors.
In general, ``standard error''" is abbreviated as ``SE.'' Sometimes
``standard error of the mean'' is specifically abbreviated to ``SEM.''

The standard error of the mean is a measure of the precision in
estimating the mean. The smaller the value the more precise the
estimate. The standard error of the mean \emph{is} a standard deviation
of the mean. This is kinda weird. If we sample a population one time and
compute a mean, how can the mean have a standard deviation? There is
only one value! And we compute this value using the sample standard
deviation: \(SEM = \frac{SD}{\sqrt{N}}\). To understand how the SEM is a
standard deviation, Imagine recalculating the spread sheet an infinite
number of times and each time, you write down the newly computed mean.
The standard error of the mean is the standard deviation of this
infinitely long column of means.

\section{Using R to generate fake data to explore the standard
error}\label{using-r-to-generate-fake-data-to-explore-the-standard-error}

note that I use ``standard deviation'' to refer to the sample standard
deviation and ``standard error'' to refer to the standard error of the
mean (again, we can compute standard errors as a standard deviation of
any kind of estimate)

\subsection{part I}\label{part-i}

In the exercise above, you used Google Sheets to generate \(p\) columns
of fake data. Each column had \(n\) elements, so the matrix of fake data
was \(n \times m\) (it is standard in most fields to specify a matrix as
rows by columns). This is \emph{much} easier to do in R and how much
grows exponentially as the size of the matrix grows.

To start, we just generate a \(n \times p\) matrix of normal random
numbers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R script to gain some intuition about standard deviation (sd) and standard error (se)}
\CommentTok{# you will probably need to install ggplot2 using library(ggplot2) }
\NormalTok{n <-}\StringTok{ }\DecValTok{6} \CommentTok{# sample size}
\NormalTok{p <-}\StringTok{ }\DecValTok{100} \CommentTok{# number of columns of fake data to generate}
\NormalTok{fake_data <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n, }\DataTypeTok{ncol=}\NormalTok{p) }\CommentTok{# create a matrix}
\end{Highlighting}
\end{Shaded}

the 3rd line is the cool thing about R. In one line I'm creating a
dataset with \(n\) rows and \(p\) columns. Each column is a sample of
the standard normal distribution which by definition has mean zero and
standard deviation of 1. But, and this is important, any sample from
this distribution will not have exactly mean zero and standard deviation
of 1, because it's a sample, the mean and standard deviation will have
some small errror from the truth. The line has two parts to it: first
I'm using the function ``rnorm'' (for random normal) to create a vector
of n*m random, normal deviates (draws from the random normal
distribution) and then I'm organizing these into a matrix (using the
function ``matrix'')

To compute the vector of means, standard deviations, and standard errors
for each column of \texttt{fake\_data}, use the \texttt{apply()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(fake_data,}\DecValTok{2}\NormalTok{,mean) }\CommentTok{# the apply function is super useful}
\NormalTok{sds <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(fake_data,}\DecValTok{2}\NormalTok{,sd)}
\NormalTok{sems <-}\StringTok{ }\NormalTok{sds}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\texttt{apply()} is a workhorse in many R scripts and is often used in R
scripts in place of a for-loop (see below) because it takes fewer lines
of code.

The SEM is the standard deviation of the mean, so let's see if the
standard deviation of the means is close to the true standard error. We
sampled from a normal distribution with SD=1 so the true standard is

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4082483
\end{verbatim}

and the standard deviation of the \(p\) means is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3731974
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  how close is \texttt{sd(means)} to the true SE?
\item
  change p above to 1000. Now how close is sd(means) to the true SE?
\item
  change p above to 10,000. Now how close is sd(means) to the true SE?
\end{enumerate}

\subsection{part II - means}\label{part-ii---means}

This is a visualization of the spread, or variability, of the sampled
means

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-27-1.pdf}

Compute the mean of the means

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.039961
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remember that the true mean is zero. How close, in general, are the
  sampled means to the true mean. How variable are the means? How is
  this quantified?
\item
  change n to 100, then replot. Are the means, in general, closer to the
  true mean? How variable are the means now?
\item
  Is the mean estimated with \(n=100\) closer to the truth, in general,
  then the mean estimated with \(n=6\)?
\item
  Redo with \(n=10000\)
\end{enumerate}

\subsection{part III - how do SD and SE change as sample size (n)
increases?}\label{part-iii---how-do-sd-and-se-change-as-sample-size-n-increases}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.017144
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  what is the mean of the standard deviations when n=6 (set p=1000)
\item
  what is the mean of the standard deviations when n=100 (set p=1000)
\item
  when n = 1000? (set p=1000)
\item
  when n = 10000? (set p=1000)
\item
  how does the mean of the standard deviations change as n increases
  (does it get smaller? or stay about the same size)
\item
  repeat the above with SEM
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4152472
\end{verbatim}

Congratulations, you have just done a Monte Carlo simulation!

\subsection{Part IV -- Generating fake data with
for-loops}\label{part-iv-generating-fake-data-with-for-loops}

A \textbf{for-loop} is used to iterate a computation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{6} \CommentTok{# sample size}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5} \CommentTok{# number of iterations of loop (equivalent to p)}
\NormalTok{means <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\NormalTok{sds <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\NormalTok{sems <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{}
\NormalTok{  y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n) }\CommentTok{# mean=0 and sd=1 are default so not necessary to specify}
\NormalTok{  means[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y)}
\NormalTok{  sds[i] <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(y)}
\NormalTok{  sems[i] <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(y)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\NormalTok{\}}
\KeywordTok{sd}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4090702
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3883867
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do \texttt{sd(means)} and \texttt{mean(sems)} converge to as
  \texttt{n\_iter} is increased from 100 to 1000 to 10,000?
\item
  Do they converge to the same number?
\item
  Should they?
\item
  What is the correct number?
\end{enumerate}

Question number 4 is asking what is E(SEM), the ``expected standard
error of the mean''. There is a very easy formula to compute this. What
is it?

\section{Bootstrapped standard errors}\label{bootstrap}

The bootstrap is certainly one of the most valuable tools invented in
modern statistics. But, it's not only a useful tool for applied
statistics, it's a useful tool for understanding statistics. Playing
with a parametric bootstrap will almost certainly induce an ``aha, so
that's what statisticians mean by \ldots{}'' moment.

To understand the bootstrap, let's review a standard error. A
\emph{parametric} standard error of a mean is the \emph{expected}
standard deviation of an infinite number of means. A standard error of
any statistic is the \emph{expected} standard deviation of that
statistic. I highlight \emph{expected} to emphasize that parametric
standard errors assume a certain distribution (not necessarily a Normal
distribution, although the equation for the SEM in Equation \eqref{eq:se}
assumes a normal distribution if the standard deviation is computed as
in Equation \eqref{eq:sd}).

A bootstrapped standard error of a statistic is the \textbf{empirical}
standard deviation of the statistic from a finite number of
\emph{samples}. The basic algorithm for a bootstrap is (here ``the
statistic'' is the mean of the sample)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sample \(n\) values from a probability distribution
\item
  compute the mean
\item
  repeat step 1 and 2 many times
\item
  for a bootstrapped standard error, compute the standard deviation of
  the set of means saved from each iteration of steps 1 and 2.
\end{enumerate}

The probability distribution can come from two sources:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \textbf{parametric bootstrap} uses samples from a parametric
  probability distribution, such as a Normal distribution or a poisson
  distribution (remember, these are ``parametric'' because the
  distribution is completely described by a set of parameters). A good
  question is why bother? In general, one would use a parametric
  bootstrap for a statistic for which there is no formula for the
  standard error, but the underlying data come from a parametric
  probability distribution.
\item
  A \textbf{non-parametric} bootstrap uses \emph{resamples from the
  data}. The data are resampled \emph{with replacement}. ``Resample with
  replacement'' means to sample \(n\) times from the full set of
  observed values. If we were to do this manually, we would i) write
  down each value of the original sample on its own piece of paper and
  throw all pieces into a hat. ii) pick a paper from the hat, add its
  value to sample \(i\), and return the paper to the hat. iii) repeat
  step ii \(n\) times, where \(n\) is the original sample size. The new
  sample contains some values multiple times (papers that were picked
  out of the hat more than once) and is missing some values (papers that
  were not picked out in any of the \(n\) picks). A good question is,
  why bother? A non-parametric bootstrap assumes no specific
  \emph{parametric} probability distribution but it does assume the
  distributio of the observed sample is a good approximation of the true
  population distribution (in which case, the probability of picking a
  certain value is a good approximation to the true probability).
\end{enumerate}

\subsection{An example of bootstrapped standard errors using vole
data}\label{an-example-of-bootstrapped-standard-errors-using-vole-data}

Let's use the vole data to explore the bootstrap and ``resampling''. The
data are archived at Dryad Repository. Use the script in Section
\ref{vole-data} to wrangle the data into a usable format.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  URL: \url{https://datadryad.org//resource/doi:10.5061/dryad.31cc4}
\item
  file: RSBL-2013-0432 vole data.xlsx
\item
  sheet: COLD VOLES LIFESPAN
\end{enumerate}

The data are the measured lifespans of the short-tailed field vole
(\emph{Microtus agrestis}) under three different experimental
treatments: vitamin E supplementation, vitamin C supplementation, and
control (no vitamin supplementation). Vitamins C and E are antioxidants,
which are thought to be protective of basic cell function since they
bind to the cell-damaging reactive oxygen species that result from cell
metabolism.

Let's compute the standard error of the mean of the control group
lifespan using both a parametric and a nonparametric bootstrap. To
implement the algorithm above using easy-to-understand code, I'll first
extract the set of lifespan values for the control group and assign it
to its own variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{control_voles <-}\StringTok{ }\NormalTok{vole[treatment}\OperatorTok{==}\StringTok{"control"}\NormalTok{, lifespan]}
\end{Highlighting}
\end{Shaded}

\texttt{{[}treatment=="control",\ {]}} indexes the rows (that is,
returns the row numbers) that satisfy the condtion
\texttt{treatment\ =\ "control"}. Or, put another way, it selects the
\textbf{subset} of rows that contain the value ``control'' in the column
``treatment''. \texttt{{[},\ lifespan{]}} indexes the column labeled
``lifespan''. Combined, these two indices extract the values of the
column ``lifespan'' in the subset of rows that contain the value
``control'' in the column ``treatment''. The resulting vector of values
is assigned to the variable ``control\_voles''.

\subsubsection{parametric bootstrap}\label{parametric-bootstrap}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# we'll use these as parameters for parametric bootstrap}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(control_voles)}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(control_voles)}
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(control_voles)}

\NormalTok{n_iter <-}\StringTok{ }\DecValTok{1000} \CommentTok{# number of bootstrap iterations, or p}
\NormalTok{means <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter) }\CommentTok{# we will save the means each iteration to this}

\ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{ }\CommentTok{# this line sets up the number of iterations, p}
\NormalTok{  fake_sample <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma)}
\NormalTok{  means[iter] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(fake_sample)}
\NormalTok{\}}
\NormalTok{(se_para_boot <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(means))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 30.49765
\end{verbatim}

\subsubsection{non-parametric bootstrap}\label{non-parametric-bootstrap}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{1000} \CommentTok{# number of bootstrap iterations, or p}
\NormalTok{means <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter) }\CommentTok{# we will save the means each iteration to this}
\NormalTok{inc <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{n }\CommentTok{# inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean}
\ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{ }\CommentTok{# this line sets up the number of iterations, p}
\NormalTok{  means[iter] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(control_voles[inc]) }\CommentTok{# inc is the set of rows to include in the computation of the mean.}
\NormalTok{  inc <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{n, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# re-sample for the next iteration}
\NormalTok{\}}
\NormalTok{(se_np_boot <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(means))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 32.47356
\end{verbatim}

The parametric bootstrapped SEM is 30.5. The non-parametric bootstrapped
SEM is 32.47. Run these several times to get a sense how much variation
there is in the bootstrapped estimate of the SEM given the number of
iterations. Compute the \textbf{parametric} standard error using
equation \eqref{eq:se} and compare to the bootstrapped values.

\section{Confidence Interval}\label{confidence-interval}

Here I introduce a \textbf{confidence interval} of a sample mean but the
concept is easily generalized to any parameter. The mean of the Control
voles is 503.4 and the SE of the mean is 31.61. The SE is used to
construct the lower and upper boundary of a ``1 - \(\alpha\)''
confidence interval using
\texttt{lower\ \textless{}-\ mean(x)\ +\ qt(alpha/2,\ df\ =\ n-1)*se(x)}
and
\texttt{upper\ \textless{}-\ mean(x)\ +\ qt(1-(alpha/2),\ df\ =\ n-1)*se(x)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(lower <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(control_voles) }\OperatorTok{+}\StringTok{ }\KeywordTok{qt}\NormalTok{(}\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{df=}\NormalTok{(n}\OperatorTok{-}\DecValTok{1}\NormalTok{))}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(control_voles)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 440.0464
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(upper <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(control_voles) }\OperatorTok{+}\StringTok{ }\KeywordTok{qt}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\FloatTok{0.05}\OperatorTok{/}\DecValTok{2}\NormalTok{, }\DataTypeTok{df=}\NormalTok{(n}\OperatorTok{-}\DecValTok{1}\NormalTok{))}\OperatorTok{*}\KeywordTok{sd}\NormalTok{(control_voles)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 566.7393
\end{verbatim}

The function \texttt{qt} maps a probability to a \emph{t}-value -- this
is the opposite of a \emph{t} test, which maps a \emph{t}-value to a
probability. Sending \(\alpha/2\) and \(1 - \alpha/2\) to \texttt{qt}
returns the bounds of the confidence intereval on a standardized scale.
Multiplying these bounds by the standard error of the control vole
lifespan pops the bounds onto the scale of the control vole lifespans.

We can check our manual computation with the linear model

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(}\KeywordTok{lm}\NormalTok{(control_voles }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                2.5 %   97.5 %
## (Intercept) 440.0464 566.7393
\end{verbatim}

\subsection{Interpretation of a confidence
interval}\label{interpretation-of-a-confidence-interval}

Okay, so what \emph{is} a confidence interval? A confidence interval of
the mean is a measure of the uncertainty in the estimate of the mean. A
95\% confidence interval has a 95\% probability (in the sense of
long-run frequency) of containing the true mean. It is not correct to
state that ``there is a 95\% probability that the true mean lies within
the interval''. These sound the same but they are two different
probabilities. The first (correct interpretation) is a probability of a
procedure -- if we re-do this procedure (sample data, compute the mean,
and compute a 95\% CI), 95\% of these CIs will contain the true mean.
The second (incorrect interpretation) is a probability that a parameter
(\(\mu\), the true mean) lies within some range. The second (incorrect)
interepretation of the CI is correct only if we also assume that
\emph{any} value of the mean is equally probable (Greenland xxx), an
assumption that is absurd for almost any data.

Perhaps a more useful interpretation of a confidence interval is, a
confidence interval contains the range of true means that are compatible
with the data, in the sense that a \(t\)-test would not reject the null
hypothesis of a difference between the estimate and any value within the
interval (this interpretation does not imply anything about the true
value) (Greenland xxx). The ``compatibility'' interpretation is very
useful because it implies that values outside of the interval are less
compatible with the data.

Let's look at the confidence intervals of all three vole groups in light
of the ``compatibility'' interpretation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vole_ci <-}\StringTok{ }\NormalTok{vole[, .(}\DataTypeTok{lifespan =} \KeywordTok{mean}\NormalTok{(lifespan),}
                    \DataTypeTok{lo =} \KeywordTok{mean}\NormalTok{(lifespan) }\OperatorTok{+}\StringTok{ }\KeywordTok{sd}\NormalTok{(lifespan)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(.N)}\OperatorTok{*}\KeywordTok{qt}\NormalTok{(.}\DecValTok{025}\NormalTok{, (.N}\OperatorTok{-}\DecValTok{1}\NormalTok{)),}
                    \DataTypeTok{up =} \KeywordTok{mean}\NormalTok{(lifespan) }\OperatorTok{+}\StringTok{ }\KeywordTok{sd}\NormalTok{(lifespan)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(.N)}\OperatorTok{*}\KeywordTok{qt}\NormalTok{(.}\DecValTok{975}\NormalTok{, (.N}\OperatorTok{-}\DecValTok{1}\NormalTok{)),}
                    \DataTypeTok{N =}\NormalTok{ .N),}
\NormalTok{                by =}\StringTok{ }\NormalTok{.(treatment)]}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{vole_ci, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{y=}\NormalTok{lifespan)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{ymin=}\NormalTok{lo, }\DataTypeTok{ymax=}\NormalTok{up), }
                \DataTypeTok{width=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-35-1.pdf}

\chapter{Covariance and Correlation}\label{covariance-and-correlation}

Variance is one of two major concepts in statistical modeling. The
second is \textbf{covariance}, which arises when two variables measured
on the same unit vary together. ``Vary together'' means that if we
measured leg length and arm length on each individual in a sample of
humans, we'd expect individuals with long arms to also have long legs
while those with short arms to have short legs. ``Measured on the same
unit'' means that we measure both leg length and arm length in each
individual of a sample -- we cannot compute a covariance if we measure
legs in one sample and arms in a second. Covariance can be positive or
negative. It is positive when the tendency is for both values to be
large or both values to be small. It is negative when the tendency is
for one value to be small when the other is large. Positive and negative
covariance are easily visualized with a scatterplot
\ref{fig:covariance-scatterplot}.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/covariance-scatterplot-1.pdf}
\caption{\label{fig:covariance-scatterplot}Scatterplot illustrating two
variables with (A) positive covariance and (B) negative covariance}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Covariance
\end{enumerate}

\begin{equation}
\mathrm{COV}[X, Y] = \sum_{i=1}^n{\frac{(x_i - \bar{x})(y_i - \bar{y})}{n-1}}
\label{eq:cov}
\end{equation}

Compare this to the equation for the variance. In \eqref{eq:cov}, the
numerator in the sum is the product of two different deviations (one for
each variable) instead of the product of a deviation with itself.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Correlation
\item
  Regression
\end{enumerate}

\chapter{P-values}\label{p-values}

(A general perception of a ``replication crisis'' may thus reflect
failure to recognize that statistical tests not only test hypotheses,
but countless assumptions and the entire environment in which research
takes place. Because of all the uncertain and unknown assumptions that
underpin statistical inferences, we should treat inferential statistics
as highly unstable local descriptions of relations between assumptions
and data, rather than as providing generalizable inferences about
hypotheses or models. And that means we should treat statistical results
as being much more incomplete and uncertain than is currently the
norm.)\footnote{Amrhein, V., Trafimow, D. and Greenland, S., 2019.
  Inferential statistics as descriptive statistics: There is no
  replication crisis if we don't expect replication. The American
  Statistician, 73(sup1), pp.262-270.}

\section{\texorpdfstring{\(p\)-values}{p-values}}\label{p-values-1}

Let's use the vole data to re-fresh our learning of the meaning of the
\emph{p}-value. The archived format of these data requires some
wrangling -- a method for this is in \protect\hyperlink{vole-data}{Vole
data}.

A typical analysis would compare the means of the two supplement
treatment levels with the control treatment level and use a
\emph{p}-value as evidence of ``an effect''.

contrast

estimate

SE

df

lower.CL

upper.CL

t.ratio

p.value

vitamin\_C - control

-115.1

54

93

-223.2

-6.9

-2.1

0.037

vitamin\_E - control

-89.9

52

93

-194.1

14.3

-1.7

0.090

vitamin\_E - vitamin\_C

25.2

65

93

-103.8

154.1

0.4

0.699

The table above gives the SE, \(t\) and \emph{p}-value for each pairwise
contrast (difference in means) among the three treatment levels. A
typical report (one with several misconceptions) might read

``We found a significant effect of Vitamin C (\(t=\) -2.1, \(p=\) 0.037)
on lifespan, but no effect of vitamin E (\(t=\) -1.7, \(p=\) 0.09) on
lifespan.''

A \(p\) value \emph{is a continuous measure of evidence against the
null}. As long as the data approximate the assumptions of the null
hypothesis pretty well, a very small \emph{p}-value, such as 0.0005, is
pretty good evidence against the null hypothesis -- but does not mean
``an effect exists''. To show an effect exists, we should have small
\emph{p}-values in multiple replicates \emph{and} we should rigorously
probe the hypothesis with different experiments that challenge the
hypothesis in different ways. A small \(p\) is evidence for a research
program to move forward with replication and probing. A big
\emph{p}-value, say 0.22 or 0.76, is pretty weak evidence against the
null, but does not mean ``there is no effect.'' If an experiment is well
designed, a big \(p\) could suggest abandoning any hypotheses that
predict biologically consequential effects. Unfortunately, a big \(p\)
could also reflect a weak experimental design. Between small and big
\(p\) values, such as 0.004 or 0.11, problems arise. These intermediate
\emph{p}-values beg for replication. A major problem of inference using
\(p\) values is that there is no sharp boundaries in how to interpret
p-values. Biologists, encouraged by both the literature and textbooks,
typically use \(p < 0.05\) as a sharp boundary to declare that an effect
exists or not. This ``dichotomania'' is discussed below.

Okay. so what \emph{is} a \emph{p}-value? When we do a \emph{t}-test, we
get a \emph{p}-value. There are several ways to think about this
probability. The most compact way is \(P(data | null)\), which is
literally read as the probability of the data given the null (or
``conditional'' on the null), but is really short for \emph{the
probability of the data, or something more extreme than the data, given
that the null hypothesis is true}. The ``probability of the data'' is
kinda vague. More specifically, we mean the probability of some
statistic about the data such as the difference in means between group A
and group B or the \emph{t}-value associated with this difference. So, a
bit more formally, the probability returned in a \emph{t}-test is
\(\mathrm{prob}(t \ge t_{obs} | H_0)\). This is the long run frequency
of observing a \emph{t}-value as big or bigger than the observed
\emph{t}-value (the one you actually got with your data) if the null is
true. Let's parse this into ``long run frequency of observing a
\emph{t}-value as big or bigger than the observed \emph{t}-value'' and
``null is true''.

A thought experiment: You open a google sheet and insert 12 standard,
normal random deviates (so the true mean is zero and the true variance
is one) in Column A, rows 1-12. You arbitrarily assign the first six
values (rows 1-6) to treatment A and the second six values (rows 7-12)
to treatment B. You use the space immediately below these data to
compute the mean of treatment A, the mean of treatment B, the difference
in means (A - B), and a \emph{t}-value. Unfortunately, google sheets
doesn't have a \emph{t}-value function so you'd have to compute this
yourself. Or not, since this is a thought experiment. Now ``fill right''
or copy and paste these functions into 999 new columns. You now have
1000 \emph{t}-tests. The expected value of the difference in means is
zero (why?) but the actual values will form a normal distribution about
zero. Most will be close to zero (either in the negative or positive
direction) but some will be further from zero. The expected
\emph{t}-value will also be zero (why?) and the distribution of these
1000 \(t\) values will look normal but the tails are a little fuller.
This row of \(t\) values is a null distribution, because in generating
the data we used the exact same formula for the values assigned to A and
the values assigned to B. Now think of a \emph{t}-value in your head,
say 0.72 (remember that \(t\) values will largely range from about -3 to
+3 although the theoretical range is \(-\infty\) to \(+\infty\). What is
the probability of observing a \(t\) of 0.72 \emph{or bigger} if the
null is true? Look at the row of \emph{t}-values! Count the number of
\(t \ge 0.72\) and then divide by the total number of \emph{t}-values in
the row (1000) and you have a probability computed as a frequency. But
remember the frequentist definition is the long run frequency, or the
expected frequency at the limit (when you've generated not 1000 or even
1,000,000 but an infinite number of columns and \emph{t}-values).

Some asides to the thought experiment: First, why ``as big or bigger''
and not just the probability of the value itself? The reason is that the
probability of finding the exact \(t\) is 1/infinity, which doesn't do
us much good. So instead we compute the probability of finding \(t\) as
big, or bigger, than our observed \(t\). Second, the \emph{t}-test
probability described above is a ``one-tail probability''. Because a
difference can be both in the positive direction and the negative
direction, we usually want to count all the \(t \ge 0.72\) and the
\(t \le -0.72\) and then add these two counts to compute the frequency
of \emph{as extreme or more extreme} values. This is called a
``two-tailed probability'' because we find extremes at both tails of the
distribution. Third, we don't really count \(t \ge 0.72\) but take
advantage of the beautiful mathematical properties of the theoretical
\(t\) distribution, which allows us to compute the frequentist
probability (expected long range frequency) given the \emph{t}-value and
the degrees of freedom using the \emph{t}-distribution.

Now what do I mean with the phrase ``null is true''? Most people equate
``null is true'' with ``no difference in means'' but the phrase entails
much more than this. Effectively, the phrase means that the
\emph{p}-value is based on modeling the real data with a theoretical
sample in which all the points were randomly sampled from the same
distribution and that the assignment of the individual points to
treatment was random. This model means the theoretical sample has three
properties: First, random assignment to treatment after sampling from
the same distribution means that the expected means are the same, or put
differently, the expected difference in means between the assigned
groups is zero. Second, random assignment to treatment after sampling
from the same distribution \emph{also} means that the expected variances
of the two groups are equal. And third, random sampling means that the
values of each point are independent -- we cannot predict the value of
one point knowing information about any other point. \textbf{Here is
what is super important about this}: if we get a really low
\emph{p}-value, any one of these consequences may be untrue about our
data, for example it could be that the true means of the two treatment
groups really are different, or it could mean it is the variances that
differ between the two groups, or it could mean that the data (or
technically, the errors) are not independent of each other. This is why
we need certain assumptions to make a \emph{p}-value meaningful for
empirical data. By assuming independent error and homogenous (equal)
variances in our two samples, a low \(p\) value is evidence of unequal
means.

\section{Creating a null
distribution.}\label{creating-a-null-distribution.}

Let's repeat: A pretty good definition of a \emph{p}-value is: the
long-run frequency of observing a test-statistic as large or larger than
the observed statistic, if the null were true. A more succinct way to
state this is

\begin{equation}
p = \mathrm{prob}(t \ge t_o | H_o)
\end{equation}

where \(t\) is a hypothetically sampled \emph{t}-value from a null
distribution, \(t_o\) is the observed \emph{t}-value, and \(H_o\) is the
null hypothesis. Part of the null hypothesis is the expected value of
the parameter estimated is usually (but not always) zero -- this can be
called the nil null. For example, if there is no vitamin E effect on
lifespan, then the expected difference between the means of the control
and vitamin E treatment levels is zero. Or,

\begin{equation}
\mathrm{E}(\bar{vitamin_E} - \bar{control} | H_o) = 0.0
\end{equation}

let's plot the data and look at the group means. Below is a strip chart
of the vole data with superimposed treatment level means, using the
function \texttt{ggstripchart} from the ggpubr package (can you make
this?). I'm going to refer to this kind of chart as a ``dot plot'',
which is what most biology researchers call this type of chart.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/inference-strip-chart-1.pdf}

\subsection{the Null Distribution}\label{the-null-distribution}

The mean lifespan in the vitamin\_E treatment is 89.9 days shorter than
the mean lifespan in the control treatment. And, the mean lifespan in
the vitamin\_E treatment is 115.1 days shorter than the mean lifespan in
the control treatment. These are the measured effects, or the
\textbf{observed differences in means}. How confident are we in these
effects? Certainly, if the researchers did the experiment with
\emph{two} control treatment groups, they would measure some difference
in their means simply because of finite sampling (more specifically, the
many, many random effects that contribute to lifespan will differ
between the two control groups). So let's reframe the question: are the
observed differences unusually large compared to a distribution of
differences that would occur if there were no effect? That is, if the
``null were true''. To answer this, we compare our observed difference
to this \textbf{null distribution}. This comparison gives the
probability (a long-run frequency) of ``sampling'' a random difference
from the null distribution of differences that is as large, or larger,
than the observed difference.

What is a null distribution? It is the distribution of a statistic (such
as a difference in means, or better, a \emph{t}-value) if the null were
true. Here, I am generating a null distribution that is relevant to the
cold vole data. See if you can understand the script before reading the
explanation below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seed <-}\StringTok{ }\DecValTok{1}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5} \CommentTok{# number of iterations}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan]) }
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan])}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{((vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{,]))}
\NormalTok{sample1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples (each size n)}
\NormalTok{sample2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples}
\NormalTok{null_dis <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, mean) }\OperatorTok{-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample1, }\DecValTok{2}\NormalTok{, mean)}
\KeywordTok{qplot}\NormalTok{(null_dis)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/null-distribution-1.pdf}
\caption{\label{fig:null-distribution}Null distribution for an infinitely
large data set that looks curiously like the lifespans of the cold-rear
voles from the control treatment.}
\end{figure}

What have we done above? We've simulated an infinitely large population
of voles that have a distribution of lifespans similar to that of the
cold-reared voles assigned to the control group. The mean \(\mu\) and
standard deviation \(\sigma\) of the simulated lifespan are equal to the
observed mean and standard deviation of the lifespans of the control
voles. Then, the script:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  randomly sample 56 values from this population of simulated lifespans
  and assign to sample1. We sample 56 values because that is the sample
  size of our control in the experiment.
\item
  randomly sample 56 values from this population of simulated lifespans
  and assign to sample2.
\item
  compute the difference \(\bar{Y}_{sample2} - \bar{Y}_{sample1}\).
\item
  repeat 1-3 100,000 times, each time saving the difference in means.
\item
  plot the distribution of the 100,000 differences using a histogram
\end{enumerate}

The distribution of the differences is a null distribution. Notice that
the mode of the null distribution is at zero, and the mean (-0.11584) is
close to zero (if we had set \(n\) to infinity, the mean would be
precisely zero). \emph{The expected difference between the means of two
random samples from the same population is, of course, zero}. Don't
gloss over this statement if that is not obvious. The tails extend out
to a little more than +100 and -100. What this means is that it would be
rare to randomly sample two sets of data from the same population with
mean \(\mu\) and standard deviation \(\sigma\) and find a difference of,
say, -257. In fact, in the 100,000 runs, there were no difference as
large as \textbar{}-257\textbar{} (the absolute value of -257). The
minimum and maximum differences sampled over the 100,000 iterations was
-187 days and 201 days.

How do our observed differences compare? Let's focus on vitamin E. The
vitamin\_E effect is -89.9 days. There are 2110 sampled differences less
than the observed value and 2126 greater than the absolute value of the
observed value. Together this is 4236 so the frequency of differences
from the simulated null distribution that as larger or larger than the
observed difference is 0.042 (this compuation includes the observed
value in both the numerator and denominator).

\subsection{t-tests}\label{t-tests}

A \emph{t}-test is a test of differences between two values. These could
be

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the difference between the means of two samples (a ``two-sample''
  \emph{t}-test)
\item
  the difference between a mean of a sample and some pre-specified value
  (a ``one-sample'' \emph{t}-test)
\item
  the difference between a coefficient from a linear model and a value
  (often zero)
\end{enumerate}

A \emph{t}-test compares an observed \emph{t}-value to a
\emph{t}-distribution. The null distribution introduced above was a
distribution of mean differences. This isn't generally useful, since the
distribution of expected mean differences under the null will be unique
to every study. A \emph{t}-distribution is a distribution of
\emph{t}-values under the null (statistical jargon for ``given the null
is true''), where a \emph{t}-value is a difference standardized by its
standard error. For a two-sample \emph{t}-test, this is

\begin{equation}
t = \frac{\bar{X}_1 - \bar{X}_2}{SE_{\bar{X}_1 - \bar{X}_2}}
\end{equation}

The numerator is \textbf{the effect} while the denominator is the
precision of the estimate. Like many test statistics, a \emph{t}-value
is a signal-to-noise ratio -- the effect is the signal and the SE of the
difference is the noise.

A \emph{t} distribution looks like a standard, normal distribution,
except the tails are heavy, meaning there are more large-ish values than
the normal. Like the standard normal distribution, large \emph{t}-values
are unlikely under the null and, therefore, a large \emph{t} has a low
probability -- or \emph{p}-value -- under the null. Looking at the
equation for the two-sample \emph{t}-test above, it is easy to see that
three features of an experiment are associated with large \emph{t} and
small \emph{p}-values: 1) big effect size (the numerator of the
equation), 2) small sample standard deviations (which results in small
standard errors of the difference, the denominator of eq xxx), and 3)
large sample size (which results in small standard errors of the
difference). As a quick-and-dirty generalization, absolute t-values
greater than 2 are unlikely if the null is true.

The difference between the mean of the vitamin\_E treatment and the
control treatment is -89.9. A two-sample \emph{t}-test of this
difference is 0.1078146

The \emph{p}-value comes from comparing the observed \(t\) to a null
\(t\) distribution and ``counting'' the values that are bigger than the
observed \(t\). These are counted in both tails, because \(p\) is the
probability of a \(t\) more extreme than the observed value, and \(t\)
can be more extreme in the negative direction and in the positive
direction. We can simulate this with a finite, instead of infinite, null
distribution using the t-distribution instead of the distribution of
mean differences, as above. I show the script, but don't just cut and
paste the code. Spend time thinking about what the each line does.
Explore it by copying parts and pasting into console.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{4} \CommentTok{# number of iterations}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan]) }
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan])}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{((vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{,]))}
\NormalTok{sample1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples}
\NormalTok{sample2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples}

\CommentTok{#way no. 1 - compute the t-tests manually}
\NormalTok{mean_diffs <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, mean) }\OperatorTok{-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample1, }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# what is the apply function returning?}
\NormalTok{se_mean_diffs <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, sd)}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{n }\OperatorTok{+}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample1, }\DecValTok{2}\NormalTok{, sd)}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{n)}
\NormalTok{t_dis <-}\StringTok{ }\NormalTok{mean_diffs}\OperatorTok{/}\NormalTok{se_mean_diffs}

\CommentTok{#way no.2 - compute the t-tests using the base R function t.test}
\CommentTok{# rbind stacks sample1 under sample2. I used this order to be consistent with}
\CommentTok{# t.test function}
\NormalTok{t_dis2 <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(sample2, sample1), }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }
\NormalTok{  \{}\KeywordTok{t.test}\NormalTok{(x[}\DecValTok{1}\OperatorTok{:}\NormalTok{n], x[(n}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{(}\DecValTok{2}\OperatorTok{*}\NormalTok{n)], }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}\OperatorTok{$}\NormalTok{statistic\})}
\CommentTok{# confirm that t_dis = t_dis2}

\CommentTok{# plot the null distribution of t-values}
\KeywordTok{qplot}\NormalTok{(t_dis)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/null-distribution-t-1.pdf}
\caption{\label{fig:null-distribution-t}Null distribution of t-values. The
simulation generated 10,000 t-tests with a true null.}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# what is the p-value?}
\CommentTok{# the p-value is the number of t-values in t_dis that are as large}
\CommentTok{# or larger than the observed t. Large, negative t-values}
\CommentTok{# are as unlikely under the null as large, positive t-values.}
\CommentTok{# To account for this, we want to use absolute values in our counts}
\CommentTok{# this is a "two-tail test"}

\CommentTok{# first compute the observed t-value}
\NormalTok{t_obs <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(}\DataTypeTok{x=}\NormalTok{vole[treatment}\OperatorTok{==}\StringTok{'vitamin_E'}\NormalTok{, lifespan],}
       \DataTypeTok{y=}\NormalTok{vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan],}
       \DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}\OperatorTok{$}\NormalTok{statistic}
\CommentTok{# now count the number of t-values in t_dis as big or bigger than this}
\CommentTok{# include the observed value as one of these (so add 1 to the count)}
\NormalTok{count <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(t_dis) }\OperatorTok{>=}\StringTok{ }\KeywordTok{abs}\NormalTok{(t_obs)) }\OperatorTok{+}\StringTok{ }\DecValTok{1}

\CommentTok{# the p-value is the frequency of t_dis >= t_obs, so divide}
\CommentTok{# count by the total number of t-values in the distribution.}
\CommentTok{# Again add one since the observed value counts as a sample}
\NormalTok{(p_vitamin_E <-}\StringTok{ }\NormalTok{count}\OperatorTok{/}\NormalTok{(n_iter }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1014899
\end{verbatim}

Hey that looks pretty good! A \(p\) value can be computed by counting
the number of simulated \emph{t}-values, \emph{including the observed
value}, that are more extreme (in either the positive or negative
direction) than the observed \(t\). Including the observed \(t\), there
are 1015 values that are more extreme than that observed. An approximate
measure of \(p\) is this count divided by 100,001 (why is 1 added to the
denominator?), which is 0.1014899. This simulation-based \emph{p}-value
is very (very!) close to that computed from the observed \emph{t}-test.

\subsection{P-values from the perspective of
permutation}\label{p-values-from-the-perspective-of-permutation}

A very intuitive way to think about \emph{p}-values is with a
permutation test. Consider two of the treatment levels in the vole data,
say vitamin E and the vitamin C (I'm bored with the control!). Think
about the structure of the dataset: there are two columns,
``Treatment'', which contains the assigned treatment, and ``Lifespan''.
The values in the Treatment column were randomly assigned prior to the
start of the experiment. If there is an effect of treatment on lifespan,
then assginment matters -- the values in the lifespan column for the
vitamin E rows will be more or less, on average, than the values in the
lifepan column for the vitamin C rows. Or, the lifespan values are what
they are \emph{because} of the values in the treatment column.

Now let's leave the values in the treatment column be, and just randomly
re-arrange or permute the lifespan values. What is the new expected
diference in lifespan between the two treatments? Zero, of course! That
is, because the lifespans were randomly re-arranged, they cannot be
caused by treatment assignment!

A permutation is a random re-arrangement of values in a column. Consider
the many thousands of permutations of the values in the lifespan column.
A difference in means can be computed from each of these permuations and
a distribution of differences can be generated. Is the observed
difference extreme relative to the other values in this distribution?
This is a permutation test -- it compares an observed statistic to a
distributin of the statistic computed over many thousands of
permutations.

\section{Statistical modeling instead of hypothesis
testing}\label{statistical-modeling-instead-of-hypothesis-testing}

This chapter is an introduction to a \emph{p}-value by way of
\emph{t}-tests. I advocate that you analyze \emph{t}-test like questions
using statistical modeling instead of null hypothesis significance
testing. The reason is that we learn much more from an estimate of the
effect and a CI than from a \(t\) and \emph{p}-value. But, it is also
good to know that a \(t\) test is a special case of a linear model, and
you can get that \(t\) and \(p\) using a statistical modeling approach
should your boss want them (and you cannot convince them otherwise).
Let's explore this.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the emmeans package, compute the effects (differences in means)
  of vitamin E and vitamin C on lifespan, relative to the control, with
  their 95\% CI and the \(t\) and \(p\) values for the cold-reared vole
  data.
\item
  Compute a separate \emph{t}-test of vitamin-E vs.~control and vitamin
  C vs.~control.
\end{enumerate}

Are the \(t\) and \(p\) values the same? No! The reason is that the
statistical model had three groups and the SE of the difference was
computed from the sample standard deviation of all three groups. Each
t-test computes the SE of the difference from only the two groups being
compared. In general, the SE computed from all three groups is better
because it uses more information. This is one reason to prefer the
linear model instead of the separate t-tests.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  To convince yourself that a \emph{t}-test is a special case as of a
  linear model, compute the effects of the vitamin E treatment (relative
  to control) \textbf{but exclude the vitamin C data from the model
  fit}. Now compare the \(t\) and \(p\) values with the \emph{t}-test.
  These should be the same.
\item
  Now use the default t.test function by deleting ``var.equal=TRUE''
  from the function. Are \(t\) and \(p\) still equal to those from the
  statistical model? No! the reason is because the default t.test
  function uses a modification of the t-test called ``Welsch's t-test''.
  This test allows for heterogenity of variances. Several sources argue
  that one should always uses Welsch's test since it simplifies to the
  classical t-test when the sample variances are equal. This is true,
  but only relevant if you're into \emph{t}-tests. And, we can model
  heterogenous variances using a statistical model. We'll do this in a
  later chapter.
\item
  Use the function \texttt{pairwise.t.test} to compute all pairwise
  t.tests among the three treatment levels. Is the \emph{p}-value for
  the vitamin\_E - control contrast the same as that if using t.test
  (with var.equal=TRUE) or the statistical model with vitamin\_C data
  excluded? No! The reason is that pairwise.t.test adjusts the p-values
  for multiple testing as a default.
\end{enumerate}

Pro tip: Before you use a new R function like t.test or pairwise.t.test,
it is really advisable to read the help page and look at the defaults
for the parameters! Researchers publish errors because they failed to
look closely at what the R function was doing and they think the
function is doing something else. Ooops!

\section{frequentist probability and the interpretation of
p-values}\label{frequentist-probability-and-the-interpretation-of-p-values}

\subsection{Background}\label{background}

There are at least three different meanings of \textbf{probability}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{subjective probability} is the probability that an individual
  assigns to an event based on prior knowledge and the kinds of
  information considered reliable evidence. For example, if I asked a
  sample of students, what is the probability that a 30c homeopathic
  medicine could clear a \emph{Streptococcus} infection from your
  respiratory system, their answers would differ because of variation in
  their knowledge of basic science, including chemistry and physics,
  their knowledge of what homeopathic medicines are, and how they weight
  different kinds of evidence.
\item
  \textbf{classical probability} is simply one divided by the number of
  possible unique events. For example, with a six-sided die, there are
  six possible unique events. The probability of rolling a 2 is
  \(\frac{1}{6}\) and the probability of rolling an odd number is
  \(\frac{1}{2}\).
\item
  \textbf{frequentist probability} is based on the concept of
  \textit{long run frequency}. If I roll a die 10 times, the frequency
  of rolling a 2 will be approximately \(\frac{1}{6}\). If I roll the
  die 100 times, the frequency of rolling a two will be closer, but to
  \(\frac{1}{6}\). If I roll the die 1000 times, the frequency of
  rolling the die will be even closer to \(\frac{1}{6}\). So the
  frequentist definition is the expected frequency given an infinite
  number of rolls. For events with continous outcomes, a frequentist
  probability is the long run frquency of \emph{observing the outcome or
  one more extreme}.
\end{enumerate}

\subsection{\texorpdfstring{This book covers frequentist approaches to
statistical modeling and when a probability arises, such as the
\emph{p}-value of a test statistic, this will be a frequentist
probability.}{This book covers frequentist approaches to statistical modeling and when a probability arises, such as the p-value of a test statistic, this will be a frequentist probability.}}\label{this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability.}

When we do a \emph{t}-test, we get a \emph{p}-value. There are several
ways to think about this probability. The most compact way is
\(P(data | null)\), which is literally read as the probability of the
data given the null (or ``conditional'' on the null), but is really
short for \emph{the probability of the data, or something more extreme
than the data, given that the null hypothesis is true}. The
``probability of the data'' is kinda vague. More specifically, we mean
the probability of some statistic about the data such as the difference
in means between group A and group B or the \emph{t}-value associated
with this difference. So, a bit more formally, the probability returned
in a \emph{t}-test is \(\mathrm{prob}(t \ge t_{obs} | H_0)\). This is
the long run frequency of observing a \emph{t}-value as big or bigger
than the observed \emph{t}-value (the one you actually got with your
data) if the null is true. Let's parse this into ``long run frequency of
observing a \emph{t}-value as big or bigger than the observed
\emph{t}-value'' and ``null is true''.

A thought experiment: You open a google sheet and insert 12 standard,
normal random deviates (so the true mean is zero and the true variance
is one) in Column A, rows 1-12. You arbitrarily assign the first six
values (rows 1-6) to treatment A and the second six values (rows 7-12)
to treatment B. You use the space immediately below these data to
compute the mean of treatment A, the mean of treatment B, the difference
in means (A - B), and a \emph{t}-value. Unfortunately, google sheets
doesn't have a \emph{t}-value function so you'd have to compute this
yourself. Or not, since this is a thought experiment. Now ``fill right''
or copy and paste these functions into 999 new columns. You now have
1000 \(t\) tests. The expected value of the difference in means is zero
(why?) but the actual values will form a normal distribution about zero.
Most will be close to zero (either in the negative or positive
direction) but some will be further from zero. The expected
\emph{t}-value will also be zero (why?) and the distribution of these
1000 \(t\) values will look normal but the tails are a little fuller.
This row of \(t\) values is a null distribution, because in generating
the data we used the exact same formula for the values assigned to A and
the values assigned to B. Now think of a \emph{t}-value in your head,
say 0.72 (remember that \(t\) values will largely range from about -3 to
+3 although the theoretical range is \(-\infty\) to \(+\infty\). What is
the probability of observing a \(t\) of 0.72 \emph{or bigger} if the
null is true? Look at the row of \emph{t}-values! Count the number of
\(t \ge 0.72\) and then divide by the total number of \emph{t}-values in
the row (1000) and you have a probability computed as a frequency. But
remember the frequentist definition is the long run frequency, or the
expected frequency at the limit (when you've generated not 1000 or even
1,000,000 but an infinite number of columns and \emph{t}-values).

Some asides to the thought experiment: First, why ``as big or bigger''
and not just the probability of the value itself? The reason is that the
probability of finding the exact \(t\) is 1/infinity, which doesn't do
us much good. So instead we compute the probability of finding \(t\) as
big, or bigger, than our observed \(t\). Second, the \emph{t}-test
probability described above is a ``one-tail probability''. Because a
difference can be both in the positive direction and the negative
direction, we usually want to count all the \(t \ge 0.72\) and the
\(t \le -0.72\) and then add these two counts to compute the frequency
of \emph{as extreme or more extreme} values. This is called a
``two-tailed probability'' because we find extremes at both tails of the
distribution. Third, we don't really count \(t \ge 0.72\) but take
advantage of the beautiful mathematical properties of the theoretical
\(t\) distribution, which allows us to compute the frequentist
probability (expected long range frequency) given the \emph{t}-value and
the degrees of freedom using the \emph{t}-distribution.

Now what do I mean with the phrase ``null is true''? Most people equate
``null is true'' with ``no difference in means'' but the phrase entails
much more than this. Effectively, the phrase means that the
\emph{p}-value is based on modeling the real data with a theoretical
sample in which all the points were randomly sampled from the same
distribution and that the assignment of the individual points to
treatment was random. This model means the theoretical sample has three
properties: First, random assignment to treatment after sampling from
the same distribution means that the expected means are the same, or put
differently, the expected difference in means between the assigned
groups is zero. Second, random assignment to treatment after sampling
from the same distribution \emph{also} means that the expected variances
of the two groups are equal. And third, random sampling means that the
values of each point are independent -- we cannot predict the value of
one point knowing information about any other point. \textbf{Here is
what is super important about this}: if we get a really low
\emph{p}-value, any one of these consequences may be untrue about our
data, for example it could be that the true means of the two treatment
groups really are different, or it could mean it is the variances that
differ between the two groups, or it could mean that the data (or
technically, the errors) are not independent of each other. This is why
we need certain assumptions to make a \emph{p}-value meaningful for
empirical data. By assuming independent error and homogenous (equal)
variances in our two samples, a low \(p\) value is evidence of unequal
means.

\subsection{\texorpdfstring{Two interpretations of the
\emph{p}-value}{Two interpretations of the p-value}}\label{two-interpretations-of-the-p-value}

Since we want to be working scientists who want to use \emph{p}-values
as a tool, we need to know how to interpret (or use) the \emph{p}-value
to make reasonable inferences and how to avoid mis-interpreting the
\emph{p}-value and making unreasonable or even incorrect inferences.
Ronald Fisher, the inventor of frequentist statistics, developed an
interpretation of the \emph{p}-value that is probably most useful for
academic and applied research programs. Neyman and Pearson
(Neyman-Pearson) gave the \emph{p}-value a different interpretation, one
that is probably most useful for industrial quality control. Today's
biology researchers use an interpretation that is an odd hybrid of the
two, which often leads to silly inference. Regardless, understanding the
distinction between Fisher and Neyman-Pearson will inform how we write
up our results in a manuscript. I'll describe these in the context of
the two-sample \emph{t}-test.

\subsubsection{Fisher's interpretation}\label{fishers-interpretation}

Fisher was working in the context of an agricultural field station, the
goal of which is to discover better agricultural practices. Does this
new fertilizer work better than our old fertilizer? This is the context
of much of modern biosciences and clinical medicine. Fisher thought of
\(p\) as evidence against the null; the smaller the \(p\) the better the
evidence that the means differ, which, in an experimental context,
implies a treatment effect. If an experiment results in a large
\emph{p}-value, we can move on and test other fertilizers. If an
experiment results in a small \emph{p}-value, we want to pursue this new
fertilizer more. Do more experiments! Fisher never thought of a single
experiment as definitive. The decision to move on or pursue is only
partly informed by the \emph{p}-value and Fisher offered no rule about
what \emph{p}-value lies on the threshold of this decision. When
pressed, Fisher might say that \(p=0.05\) is a reasonable threshold.

\subsubsection{Neyman-Pearson
interpretation}\label{neyman-pearson-interpretation}

Neyman-Pearson thought of \(p\) as the necessary and sufficient
information to make a decision between accepting the null (or at least
not rejecting the null) or rejecting the null and accepting an
alternative hypothesis. This decision balances two sorts of errors: Type
I (false positives), which they called \(\alpha\), and Type II (false
negatives), which they called \(\beta\). A false positive means the null
was rejected but there really is no effect. A false negative means that
the null was not rejected but there actually is an effect. \(\alpha\) is
set by the experimenter and is the long-term frequency (or ``rate'') of
false positives \textbf{when the null is true} that the experimenters
are willing to accept. This is easily understood in the context of
manufacturing. I've just made a batch of beer that I now need to ship. I
sample 10 cans and test the quality against a norm. If \(p < \alpha\),
we reject the null in favor of the alternative -- something may be wrong
with the batch, it differs from the norm. We throw the beer away. If
\(p > \alpha\), we do not reject the null, nor the beer! We ship it.

After setting \(\alpha\), the experimenter designs the experiment to
achieve an acceptable rate of \(\beta\). Since \(\beta\) is the false
negative rate then \(1-\beta\) is the rate of not making a false
negative error, that is, the rate of rejecting the null when there
really is an effect. This is called the \textbf{power} of the
experiment. An experiment with high power will have a low probability of
a Type II error. An experiment with low power will have a high
probability of a Type II error. Power is partly determined by sample
size, the bigger the sample the smaller the \emph{p}-value, all other
things equal (think about why in the context of the formula for the
\emph{t}-value). Power is also a function of \(\alpha\). If we set a low
\(\alpha\) (say, \(\alpha=0.01\)), the test is conservative. We are more
likely to fail to reject the null even if the null is false. This is the
balance. We want to make sure that we test our batch of beer using
enough cans to find a bad batch if it exists, but we don't want to test
too many cans because this is a waste of money. An experimenter sets
\(\alpha\), computes the sample size needed to achieve a certain level
of power (\(1-\beta\)), and then does the experiment.

In Fisher's interpretation, there is no \(\alpha\), no \(\beta\), no
alternative hypothesis, and no sharp decision rule. Instead, in Fisher,
\(p\) is a continuous measure of evidence against the null and its value
is interpreted subjectively by an informed and knowledgeable expert
using additional information to make decisions. Neyman-Pearson rejected
Fisher's conception of \(p\) as evidence against the null and used \(p\)
as a tool to make a decision that maintains long-term type I error rates
at alpha given a certain power. In Neyman-Pearson, \(p\) is compared to
a threshold, \(\alpha\) and this alone makes the decision. In
Neyman-Pearson, \(p\) is not treated as continuous information.
\(p=0.00000001\) is no more evidence to use to reject the null than
\(p=0.049\).

\subsection{NHST}\label{nhst}

Modern researchers interpret \(p\) using a combination of Fisher and
Neyman-Pearson concepts in what has become known as Null Hypothesis
Significance Testing (NHST). Similar to Neyman-Pearson, a \emph{p}-value
is compared to \(\alpha\) but similar to Fisher, many researchers, and
many textbooks and statistics software (including base R) trichtomize a
statistically significant \(p\) into ``significance levels'' (three
asterisks for \(p < 0.001\), two asterisks for \(0.001 < p < 0.01\), and
one asterisk for \(0.01 < p < 0.05\)) but many researchers also
casuallly partition non-significant \(p\) values into ``marginally
signifiant'' (or similar) and ``not significant''.

\subsection{\texorpdfstring{Some major misconceptions of the
\(p\)-value}{Some major misconceptions of the p-value}}\label{some-major-misconceptions-of-the-p-value}

Setting the type I error rate \(\alpha\) to 0.05 is so pervasive that
I'm going to simply use ``0.05'' instead of ``alpha'' in discussing
misconceptions.

\subsubsection{\texorpdfstring{Misconception: \(p\) is the probability
that the null is true \emph{and} \(1-p\) is probability that the
alternative is
true}{Misconception: p is the probability that the null is true and 1-p is probability that the alternative is true}}\label{misconception-p-is-the-probability-that-the-null-is-true-and-1-p-is-probability-that-the-alternative-is-true}

Many researchers believe that if \(p > 0.05\) then ``there is no
effect.'' A frequentist hypothesis test cannot show that an effect
doesn't exist, only that the null has a low probablity of producing a
test statistic as extreme or more extreme than the observed effect.

Many researchers believe that if \(p < 0.05\) then ``there is an
effect.'' Again, a frequentist hypothesis test cannot show that an
effect exists, only that the null has a low probablity of producing a
test statistic as extreme or more extreme than the observed effect.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The statement ``There is no effect of predators on feeding behavior''
  is not a valid conclusion of a frequentist hypothesis test.
\item
  The statement ``We found no effect of predators on feeding behavior''
  is misleading because a frequentist hypothesis test can neither find
  an effect nor find no effect.
\end{enumerate}

The two errors above are gross misconceptions that are pervasive in the
biology literature. A more subtle issue is the belief that a low
\emph{p}-value shows that the researcher's explanatory hypothesis is
correct. For example, researchers believe the result ``the prey fish fed
14.2 (95\% CI: 9.2, 19.2) minutes shorter in the presence of the
predator fish'' confirms their hypothesis that prey modulate feeding
duration as a function of their ability to assess the risk of predation.
Some alternative explanations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The predator fish also competes with the prey fish for the prey fish's
  food and with less food the prey fish spends less time feeding because
  it gives up when food density drops below some amount.
\item
  The predator fish is introduced to the prey tank by hand and odorant
  molecules from the researcher's hands are detected by the prey and the
  prey reduces feeding duration because of these odorants.
\end{enumerate}

Importantly, no single experiment confirms an explanatory hypothesis.
Instead, alternative explanations require multiple experiments with
different controls to ``rigrously probe'' the preferred hypothesis.

\subsubsection{\texorpdfstring{Misconception: a \(p\)-value is
repeatable}{Misconception: a p-value is repeatable}}\label{misconception-a-p-value-is-repeatable}

Many researchers believe that a \emph{p}-value is a precise measure --
that if the experiment were replicated, a similar \(p\) would result.
This belief requires at least two misconceptions. First, if the null
were true, then \emph{any} \emph{p}-value is equally likely.
\(p=0.00137\) is just as likely as \(p=0.492\). In other words, if the
null were true, the \emph{p}-value is not replicable at all! Second, the
\(p\) value is highly dependent on the sample, and can be highly
variable among replications, but there is no true \emph{p}-value, so
there can be no estimate or standard error. Let's explore these.

\textbf{What is the distribution of \(p\)-values under the null?} I
often ask students, ``if the null were true, what is the most likely
\emph{p}-value?'' or ``if the null were true, what kind of
\emph{p}-values would we expect, that is what is the expected
distribution''. A common answer is \(p=0.5\) is the most likely value
and something like a normal curve, except the tails abruptly stop at 0
and 1, is the expected distribution.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/uniform-1.pdf}

\textbf{The incredible inconsistency of the \(p\)-value}

How replicable is the conclusion of an experiment if the \emph{p}-value
for a \emph{t}-test is 0.03? If our conclusion is based on \(p < 0.05\),
then the conclusion is not very replicable. The simulation below shows
the results of 15 replicates of an experiment with true power of 40\%.
There are five ``significant'' results (one less than expected) but
several replicates have very high \emph{p}-values.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/dance-fig-1.pdf}
\caption{\label{fig:dance-fig}Variability of \emph{p}-values when the power
is 0.4}
\end{figure}

\subsubsection{Misconception: 0.05 is the lifetime rate of false
discoveries}\label{misconception-0.05-is-the-lifetime-rate-of-false-discoveries}

An important and widespread misconception is that if a researcher
consistently uses \(\alpha=0.05\), then the frequency of incorrectly
concluding an effect exists, or ``discovering'' an effect, over the
lifetime of the researcher, will be 5\%. This is incorrect. \(\alpha\)
is the rate of false positive if the null hypothesis is true, so our
lifetime ``false discovery'' rate could only be 5\% if everything we
ever tested has no true effect! More generally, the \textbf{false
discovery rate} is the frequency of false positives divided by the
frequency of positives (the sum of false and true positives). This
differs from the the Type I error rate, which is the frequency of false
positives divided by the frequency of tests \emph{in which the null is
true}.

Imagine we test

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  1000 null hypotheses over a lifetime
\item
  60\% are true nulls, this means there are 600 true nulls and 400 true
  effects
\item
  alpha is 5\%. This means we expect to find \(p \le 0.05\) 30 times
  (\(0.05 \times 600\)) when the null is true
\item
  power is 25\%. This means we expect to find \(p \le 0.05\) 100 times
  (\(0.25 \times 400\)) when the null is false
\item
  We have made \(30 + 100=130\) ``discoveries'' (all experiments with
  \(p \le 0.05\)), but
\item
  30 of the 130, or 23\%, are ``false discoveries''. This is the false
  discovery rate.
\end{enumerate}

Think about this. If the null is never true, you cannot have a false
discovery--every \(p \le 0.05\) is a true discovery (the false discovery
rate is 0\%). And if the null is always true, every \(p < 0.05\) is a
false discovery (the false discovery rate is 100\%).

\subsubsection{\texorpdfstring{Misconception: a low \(p\)-value
indicates an important
effect}{Misconception: a low p-value indicates an important effect}}\label{misconception-a-low-p-value-indicates-an-important-effect}

Many researchers write results as if they believe that a small
\emph{p}-value means the effect is big or important. This may
misconception may arise because of the ubiquitous use of ``significant''
to indicate a small p-value and ``very'' or ``extremely'' or ``wicked''
significant to indicate a really small p-value. Regardless, this is a
misconception. A small p-value will usually result when there is high
power (but can occur even if power is low) and power is a function of
effect size, variability (the standard deviation), and sample size. A
small \(p\) could result from a large effect size but can also result
with a small effect size if the sample size is big enough.

This is easy to simulate (see script below). Let's model the effect of
the genotype of a gene on height

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{rho <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{4}
\NormalTok{genotype <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"+/+"}\NormalTok{, }\StringTok{"+/-"}\NormalTok{, }\StringTok{"-/-"}\NormalTok{)}
\NormalTok{Sigma <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{Sigma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{Sigma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{rho}
\NormalTok{X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\DataTypeTok{mean=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{sigma=}\NormalTok{Sigma)}
\KeywordTok{colnames}\NormalTok{(X) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X1"}\NormalTok{, }\StringTok{"X2"}\NormalTok{)}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\FloatTok{0.05}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\NormalTok{X}\OperatorTok{%*%}\NormalTok{beta }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{coefficients}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                Estimate Std. Error   t value     Pr(>|t|)
## (Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01
## XX1         0.044304824 0.01154709 3.8368830 1.253725e-04
## XX2         0.048228101 0.01170855 4.1190490 3.835033e-05
\end{verbatim}

\subsubsection{\texorpdfstring{Misconception: a low \(p\)-value
indicates high model fit or high predictive
capacity}{Misconception: a low p-value indicates high model fit or high predictive capacity}}\label{misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity}

On page 606, of Lock et al ``Statistics: Unlocking the Power of Data'',
the authors state in item D ``The p-value from the ANOVA table is 0.000
so the model as a whole is effective at predicting grade point
averages.'' This is incorrect. A p-value is not a measure of the
predictive capacity of a model because the p-value is a function of the
signal, noise (unmodeled error), and \emph{sample size} while predictive
capacity is a function of just the signal:noise ratio. If the
signal:noise ratio is tiny, the predictive capacity is small but the
p-value can be tiny if the sample size is large. This is easy to
simulate (see script below). The whole-model p-value is exceptionally
small (0.00001002) but the relative predictive ability, measured by the
\(R^2\), is near zero (0.002).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{rho <-}\StringTok{ }\FloatTok{0.5}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{4}
\NormalTok{Sigma <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{Sigma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] <-}\StringTok{ }\NormalTok{Sigma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{rho}
\NormalTok{X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\DataTypeTok{mean=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{sigma=}\NormalTok{Sigma)}
\KeywordTok{colnames}\NormalTok{(X) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"X1"}\NormalTok{, }\StringTok{"X2"}\NormalTok{)}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\OperatorTok{-}\FloatTok{0.05}\NormalTok{)}
\NormalTok{y <-}\StringTok{ }\NormalTok{X}\OperatorTok{%*%}\NormalTok{beta }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{X)}
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6449 -0.6857  0.0148  0.6756  3.6510 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  0.007473   0.010079   0.741 0.458466    
## XX1          0.044305   0.011547   3.837 0.000125 ***
## XX2         -0.051772   0.011709  -4.422  9.9e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.008 on 9997 degrees of freedom
## Multiple R-squared:  0.0023, Adjusted R-squared:  0.002101 
## F-statistic: 11.52 on 2 and 9997 DF,  p-value: 1.002e-05
\end{verbatim}

\paragraph{\texorpdfstring{What the \(p\)-value does not
mean}{What the p-value does not mean}}\label{what-the-p-value-does-not-mean}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(p\) is not the probability of the null being true. More formally,
  this probability is \(Prob(null | data)\) but our \emph{p}-value is
  \(P(data | null)\). These are not the same. \(P(null | data)\) is the
  probability of the null being true given the data. \(P(data | null)\)
  is the probability of our data, or something more extreme than our
  data, conditional on a true null.
\item
  \(1-p\) is not the probability of the alternative
\item
  \(p\) is not a measure of effect size.
\item
  \(p\) in one experiment is not the same level of evidence against the
  null as in another experiment
\item
  \(p\) is not a great indicator of which is more likely, H0 or H1.
\item
  If one treatment level has \(p < 0.05\) and another treatment level
  has \(p > 0.05\), this is not evidence that the treatment levels have
  different effects on the outcome.
\end{enumerate}

\subsection{Recommendations}\label{recommendations}

\textbf{If you are working on basic science research} simply report the
exact \emph{p}-value, along with a CI. If \(p < 0.05\) (or some other
\(\alpha\)) do not report this as ``significant'' -- in fact, avoid the
word ``significant''. In the english language, ``significant'' implies
big or important. Small \emph{p}-values can result even with trivially
small effects if \(n\) is big or sample variation is small. If \(p\) is
smaller than say 0.001, then this is pretty good evidence that the data
is not a fluke of sampling. But if \(p\) is closer to 0.01 or 0.05, this
is only weak evidence of a fluke because of the sampling variability of
\(p\).

\textbf{If a decision needs to be made} then a \(p\) value is a useful
tool, but is only relevant compared to a decision rule with
well-reasoned values of \(\alpha\) and \(\beta\) -- exact values of
\(p\) are not very meaningful.

\section{Problems}\label{problems}

Problem 1 -- simulate the distribution of \(p\) under the null. There
are many ways to do this but a straightforard approach is to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a \(2n \times m\) matrix of random normal deviates with mean 0
  and sd 1
\item
  Do a \emph{t}-test on each column, with the first \(n\) values
  assigned to one group and the remaining \(n\) values assigned to the
  second group. Save the \emph{p}-value from each.
\item
  Plot a histogram of the \emph{p}-values.
\item
  What is the distribution? What is the most likely value of \(p\)?
\end{enumerate}

Problem 2 -- simulate power. Again, many ways to do this but following
up on Problem 1. 1. Create a \(2n \times m\) matrix of random normal
deviates with mean 0 and sd 1 2. Add an effect to the first \(n\) values
of each column. Things to think about a. what is a good effect size to
add? The effect/sd ratio, known as Cohen's d, is a relative (or
standardized) measure of effect size. Cohen suggest 0.2, 0.5, and 0.8 as
small, medium, and large standardized effects. b. should the same effect
be added to each individual? Yes! It is the random component that
captures the individual variation in the response. 3. Do a \emph{t}-test
on each column of the matrix, using the first \(n\) values in group 1
and the remaining \(n\) values in group 2. Save the p-values for each.
4. Compute the power, the relative frequency \(p \le 0.05\). 5. Repeat
with different values of \(n\), effect size, and sd, but only vary one
at a time. How does power vary with these three parameters?

Problem 3 -- write a script for a permutation test of the vitamin E and
vitamin C levels of the vole data. Compare this to the \emph{t}-test.

Problem 4 -- grad students only. Simulate the false discovery rate.
Explore the parameters: the frequency of true nulls and the power.

\chapter{Creating Fake Data}\label{creating-fake-data}

Fake data are generated by sampling from one of R's random sampling
functions. These functions sample from different distributions including

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  uniform -- function \texttt{runif(n,\ min=0,\ max=1)}, which samples n
  continous values between \texttt{min} and \texttt{max}.
\item
  normal (Gaussian) -- function \texttt{rnorm(n,\ mean=0,\ sd=1)}, which
  samples n continous values from a distribution with the specified mean
  and standard deviation. The default is the ``standard'' normal
  distribution.
\item
  poisson -- function \texttt{rpois(n,\ lambda)}, which samples
  \texttt{n} counts from a distribution with mean and variance equal to
  \texttt{lambda}.
\item
  negative binomial -- \texttt{rnegbin(n,\ mu=n,\ theta)}, which samples
  \texttt{n} counts with mean \texttt{mu} and variance
  \texttt{mu\ +\ mu\^{}2/theta}.
\end{enumerate}

\subsection{Continuous X (fake observational
data)}\label{continuous-x-fake-observational-data}

A very simple simulation of observational design (the \(X\) are not at
``controlled'' levels)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{25}
\CommentTok{# the paramters}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\DecValTok{25} \CommentTok{# the true intercept}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\FloatTok{3.4} \CommentTok{# the true slope}
\NormalTok{sigma <-}\StringTok{ }\DecValTok{2} \CommentTok{# the true standard deviation}

\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{y <-}\StringTok{ }\NormalTok{beta_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{1}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma)}
\KeywordTok{qplot}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/continuous-X-1.pdf}

How well does a model fit to the data recover the true parameters?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x)}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{coefficients}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit)), }\DataTypeTok{digits=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

25.8

0.34

75.8

0

x

4.2

0.40

10.4

0

The coefficient of \(x\) is the ``Estimate''. How close is the estimate?
Run the simulation several times to look at the variation in the
estimate -- this will give you a sense of the uncertainty. Increase
\(n\) and explore this uncertainty. Increase all the way up to
\(n=10^5\). Commenting out the qplot line will make this exploration
easier.

\subsection{Categorical X (fake experimental
data)}\label{categorical-x-fake-experimental-data}

Similar to above but the \(X\) are at controlled levels and so this
simulates an experimental design

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{5} \CommentTok{# the sample size per treatment level}

\NormalTok{fake_data <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{Treatment=}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"control"}\NormalTok{, }\StringTok{"treated"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n))}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\FloatTok{10.5} \CommentTok{# mean of untreated}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\FloatTok{2.1} \CommentTok{# difference in means (treated - untreated)}
\NormalTok{sigma <-}\StringTok{ }\DecValTok{3} \CommentTok{# the error standard deviation}
\CommentTok{# the Y variable ("Response") is a function of treatment. We use some matrix}
\CommentTok{# algebra to get this done.}
\CommentTok{# Turn the Treatment assignment into a model matrix. Take a peak at X!}
\NormalTok{X <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, fake_data)}
\CommentTok{# to make the math easier the coefficients are collected into a vector}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta_}\DecValTok{0}\NormalTok{, beta_}\DecValTok{1}\NormalTok{)}
\CommentTok{# you will see the formula Y=Xb many times. Here it is coded in R}
\NormalTok{fake_data[, Response}\OperatorTok{:}\ErrorTok{=}\NormalTok{X}\OperatorTok{%*%}\NormalTok{beta }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma)]}
\CommentTok{# plot it with a strip chart (often called a "dot plot")}
\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{fake_data, }\DataTypeTok{x=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Response"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/categorical-X-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit using base R linear model function}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Response }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data=}\NormalTok{fake_data)}
\CommentTok{# display a pretty table of the coefficients}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{coefficients}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit)), }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

11.626

1.097

10.601

0.000

Treatmenttreated

2.100

1.551

1.354

0.213

Check that the intercept is close to beta\_0 and the coefficient for
Treatment is close to beta\_1. This coefficient is the difference in
means between the treatment levels. It is the simulated effect. Again,
change \(n\). Good values are \(n=20\) and \(n=100\). Again, comment out
the plot line to make exploration more efficient.

\subsection{Correlated X (fake observational
data)}\label{correlated-x-fake-observational-data}

\subsubsection{Generating correlated X
variables}\label{generating-correlated-x-variables}

It's useful to think about how correlated data are generated because
often we want to generate fake\_data with an expected correlation. Let's
say we want to generate two X variables that have an expected
correlation of 0.6. To generate this, we take advantage of the fact that
two variables, X1 and X2, are correlated if they share a ``common
cause'' -- a variable Z that effects (or ``causes'') both X1 and X2. If
the expected variances of X1, X2, and Z are all 1, then the expected
correlation between X1 and X2 is the product of the causal effect from Z
to each X. The easiest way to implement this is to simply make the
effect from Z to both X equal to \(\sqrt(0.6)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{3}
\NormalTok{z <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n) }\CommentTok{# the common cause, with sigma = 1}
\NormalTok{rho <-}\StringTok{ }\FloatTok{0.6} \CommentTok{# the true correlation between X1 and X2}
\NormalTok{beta_z <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(rho) }\CommentTok{# the easiest way to get effects of z on X1 and X2 that generates rho}
\NormalTok{sigma_x <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{rho) }\CommentTok{# we will make the variance of X1 and X2 = 1, so the "explained" variance in X is beta_z^2 = rho so this is the sqrt of the unexplained variance}
\NormalTok{x1 <-}\StringTok{ }\NormalTok{beta_z}\OperatorTok{*}\NormalTok{z }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma_x)}
\NormalTok{x2 <-}\StringTok{ }\NormalTok{beta_z}\OperatorTok{*}\NormalTok{z }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma_x)}
\CommentTok{# check}
\KeywordTok{cov}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{X1=}\NormalTok{x1, }\DataTypeTok{X2=}\NormalTok{x2)) }\CommentTok{# is the diagonal close to 1, 1?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           X1        X2
## X1 1.0523598 0.5978408
## X2 0.5978408 0.9713292
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(x1, x2) }\CommentTok{# is the value close to rho?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5913168
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(x1, x2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-37-1.pdf}

Now create fake \(Y\) that is a function of both \(X_1\) and \(X_2\).
Create ``standardized'' fake data, where \(\sigma_Y = 1\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\FloatTok{3.2}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\FloatTok{0.7}
\NormalTok{beta_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\OperatorTok{-}\FloatTok{0.3}
\NormalTok{explained_sigma <-}\StringTok{ }\NormalTok{beta_}\DecValTok{1}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{2}\OperatorTok{^}\DecValTok{2} \OperatorTok{+}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{beta_}\DecValTok{1}\OperatorTok{*}\NormalTok{beta_}\DecValTok{2}\OperatorTok{*}\NormalTok{rho }\CommentTok{# Wright's rules! Compare to Trig!}
\NormalTok{sigma_Y.X <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{explained_sigma) }\CommentTok{# sqrt unexplained variance}
\NormalTok{y <-}\StringTok{ }\NormalTok{beta_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{1}\OperatorTok{*}\NormalTok{x1 }\OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{2}\OperatorTok{*}\NormalTok{x2 }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma_Y.X)}

\CommentTok{# check}
\KeywordTok{var}\NormalTok{(y) }\CommentTok{# should be close to 1 as n gets bigger}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9791502
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# check parameters}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x1 }\OperatorTok{+}\StringTok{ }\NormalTok{x2))) }\CommentTok{# should be near parameters as n gets bigger}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)  3.1887712 0.02536483 125.716255 0.000000e+00
## x1           0.6885565 0.03064195  22.471043 8.422461e-91
## x2          -0.3037457 0.03189446  -9.523465 1.223898e-20
\end{verbatim}

Note that the variance of \(Y\) is the variance of the explained part
due to X1 and X2 and the unexplained part and if the expected variance
of \(Y=1\) then this sets an upper limit for the explained part. This
means that

\begin{equation}
\beta_1^2 + \beta_2^2 + 2 \beta_1 \beta_2 \rho < 1
\end{equation}

which means the magnitude of \(\beta_1\) and \(\beta_2\) should
generally be less than 1.

\subsubsection{Creating mulitple X variables using the package
mvtnorm}\label{creating-mulitple-x-variables-using-the-package-mvtnorm}

The package mvtnorm provides a function to generate multivariate data
(multiple columns) with a specified vector of means (the means of each
column) and covariance matrix among the means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rcov1 <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p)\{}
  \CommentTok{# p is the number of columns or number of variables}
\NormalTok{  pp <-}\StringTok{ }\NormalTok{p}\OperatorTok{*}\NormalTok{(p}\OperatorTok{-}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\DecValTok{2} \CommentTok{# number of elements in lower tri}
\NormalTok{  max_r <-}\StringTok{ }\FloatTok{0.7}
\NormalTok{  r <-}\StringTok{ }\KeywordTok{rexp}\NormalTok{(pp)}
\NormalTok{  r <-}\StringTok{ }\NormalTok{r}\OperatorTok{*}\NormalTok{max_r}\OperatorTok{/}\KeywordTok{max}\NormalTok{(r)}
  
  \CommentTok{# create correlation matrix}
\NormalTok{  R <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{p, }\DataTypeTok{ncol=}\NormalTok{p)}
\NormalTok{  R[}\KeywordTok{lower.tri}\NormalTok{(R, }\DataTypeTok{diag=}\OtherTok{FALSE}\NormalTok{)] <-}\StringTok{ }\NormalTok{r}
\NormalTok{  R <-}\StringTok{ }\KeywordTok{t}\NormalTok{(R)}
\NormalTok{  R[}\KeywordTok{lower.tri}\NormalTok{(R, }\DataTypeTok{diag=}\OtherTok{FALSE}\NormalTok{)] <-}\StringTok{ }\NormalTok{r}
  
  \CommentTok{# convert to covariance matrix}
\NormalTok{  L <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{rexp}\NormalTok{(p))) }\CommentTok{# standard deviations}
\NormalTok{  S <-}\StringTok{ }\NormalTok{L}\OperatorTok{%*%}\NormalTok{R}\OperatorTok{%*%}\NormalTok{L}
  
  \CommentTok{# check -- these should be the same}
  \CommentTok{# R}
  \CommentTok{# cov2cor(S)}
  \KeywordTok{return}\NormalTok{(S)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now let's use mvtnorm to generate fake correlated X

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p <-}\StringTok{ }\DecValTok{5} \CommentTok{# number of X variables}
\NormalTok{S <-}\StringTok{ }\KeywordTok{rcov1}\NormalTok{(p)}

\CommentTok{# make the fake X}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(p, }\DataTypeTok{min=}\DecValTok{10}\NormalTok{, }\DataTypeTok{max=}\DecValTok{100}\NormalTok{) }\CommentTok{# vecctor of p means}
\NormalTok{X <-}\StringTok{ }\KeywordTok{rmvnorm}\NormalTok{(n, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sigma=}\NormalTok{S)}

\CommentTok{# how close? (check the cor as this is easier to scan)}
\KeywordTok{cov2cor}\NormalTok{(S)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]       [,2]       [,3]       [,4]       [,5]
## [1,] 1.0000000 0.15511020 0.11863165 0.48788949 0.14545246
## [2,] 0.1551102 1.00000000 0.70000000 0.03514487 0.04345391
## [3,] 0.1186317 0.70000000 1.00000000 0.05346376 0.54083807
## [4,] 0.4878895 0.03514487 0.05346376 1.00000000 0.44504979
## [5,] 0.1454525 0.04345391 0.54083807 0.44504979 1.00000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]       [,2]       [,3]       [,4]       [,5]
## [1,] 1.0000000 0.15618693 0.12146387 0.48987841 0.14569745
## [2,] 0.1561869 1.00000000 0.69709529 0.03879653 0.03961509
## [3,] 0.1214639 0.69709529 1.00000000 0.05598629 0.54138395
## [4,] 0.4898784 0.03879653 0.05598629 1.00000000 0.44214983
## [5,] 0.1456974 0.03961509 0.54138395 0.44214983 1.00000000
\end{verbatim}

\subsubsection{The rcov1 algorithm is
naive}\label{the-rcov1-algorithm-is-naive}

A problem with generating a fake covariance matrix as above is that it
is likely to be \textbf{singular} as \(p\) get's bigger. A singular
covariance matrix is one where there are fewer orthogonal axes of
variation then there are variables. Imagine a multidimensional
scatterplot of a data set with the fake covariance matrix. If we zoom
around this multidimensional space, we will come across a ``view'' in
which all the points are compressed along a single line -- that is there
is no variation on the axis orthogonal to this line of points. This is
bad, as it means we cannot fit a linear model using least squares
(because the inverse of the covariance matrix doesn't exist).

Let's explore this. If the a covariance matrix is singular, then at
least one eigenvalue of the matrix is negative (eigenvalues is a
multivariate term beyond the scope of this text but, effectively, these
are the variances of the orthogonal axes referred to above). Here I
compute the frequency of covariance matrices with at least one negative
eigenvalue as \(p\) increases

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{niter <-}\StringTok{ }\DecValTok{1000}
\NormalTok{p_vec <-}\StringTok{ }\DecValTok{3}\OperatorTok{:}\DecValTok{10}
\NormalTok{counts <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\KeywordTok{length}\NormalTok{(p_vec))}
\NormalTok{i <-}\StringTok{ }\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(p }\ControlFlowTok{in} \DecValTok{3}\OperatorTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{  i <-}\StringTok{ }\NormalTok{i}\OperatorTok{+}\DecValTok{1}
  \ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{niter)\{}
\NormalTok{    counts[i] <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(}\KeywordTok{eigen}\NormalTok{(}\KeywordTok{rcov1}\NormalTok{(p))}\OperatorTok{$}\NormalTok{values[p] }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{, counts[i]}\OperatorTok{+}\DecValTok{1}\NormalTok{, counts[i])}
\NormalTok{  \}}
\NormalTok{\}}
\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{p=}\NormalTok{p_vec, }\DataTypeTok{freq=}\NormalTok{counts}\OperatorTok{/}\NormalTok{niter)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     p  freq
## 1:  3 0.000
## 2:  4 0.025
## 3:  5 0.066
## 4:  6 0.135
## 5:  7 0.190
## 6:  8 0.225
## 7:  9 0.275
## 8: 10 0.368
\end{verbatim}

\subsubsection{Generating multiple columns of X variables with a
non-singular covariance
matrix}\label{generating-multiple-columns-of-x-variables-with-a-non-singular-covariance-matrix}

This section uses some ideas from matrix algebra. The goal is to create
a \(n \times p\) matrix of \(X\) variables that have some random
covariance structure that is full-rank (not singular, or no negative
eigenvalues). The algorithm starts with a \(p \times p\) random
eigenvector matrix \(\mathbf{E}\) and a \(p \times p\) random eigenvalue
matrix \(\mathbf{L}\) and then computes the random covariance matrix
using \(\mathbf{E}\mathbf{L}\mathbf{E}^\top\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Generate a random \(p \times p\) random eigenvector matrix from a
  covariance matrix of \(p \times p\) matrix of random normal variables.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fake.eigenvectors <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p)\{}
\NormalTok{    a <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(p}\OperatorTok{*}\NormalTok{p), p, p) }\CommentTok{# only orthogonal if p is infinity so need to orthogonalize it}
\NormalTok{    a <-}\StringTok{ }\KeywordTok{t}\NormalTok{(a)}\OperatorTok{%*%}\NormalTok{a }\CommentTok{# this is the sum-of-squares-and-cross-product-matrix}
\NormalTok{    E <-}\StringTok{ }\KeywordTok{eigen}\NormalTok{(a)}\OperatorTok{$}\NormalTok{vectors }\CommentTok{# decompose to truly orthogonal columns}
    \KeywordTok{return}\NormalTok{(E)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Generate \(p\) random eigenvalues in descending order and that sum to
  1. There are several ways to create this sequence. Here are two:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# force the eigenvalues to descend at a constant rate}
\NormalTok{fake.eigenvalues <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p, }\DataTypeTok{m=}\NormalTok{p, }\DataTypeTok{start=}\DecValTok{1}\NormalTok{, }\DataTypeTok{rate=}\DecValTok{1}\NormalTok{)\{}
  \CommentTok{# m is the number of positive eigenvalues}
  \CommentTok{# start and rate control the decline in the eigenvalue}
\NormalTok{  s <-}\StringTok{ }\NormalTok{start}\OperatorTok{/}\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{m)}\OperatorTok{^}\NormalTok{rate}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{c}\NormalTok{(s, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p}\OperatorTok{-}\NormalTok{m)) }\CommentTok{# add zero eigenvalues}
\NormalTok{  L <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(s}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(s)}\OperatorTok{*}\NormalTok{m) }\CommentTok{# rescale so that sum(s)=m and put into matrix,}
  \CommentTok{# which would occur if all the traits are variance standardized}
  \KeywordTok{return}\NormalTok{(L)}
\NormalTok{\}}

\CommentTok{# random descent}
\NormalTok{fake.eigenvalues.exp <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p, }\DataTypeTok{m=}\NormalTok{p, }\DataTypeTok{rate=}\DecValTok{1}\NormalTok{)\{}
  \CommentTok{# exponential distribution of eigenvalues}
  \CommentTok{# m is the number of positive eigenvalues}
  \CommentTok{# start and rate control the decline in the eigenvalue}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{rexp}\NormalTok{(m, rate)}
\NormalTok{  s <-}\StringTok{ }\NormalTok{s[}\KeywordTok{order}\NormalTok{(s, }\DataTypeTok{decreasing=}\OtherTok{TRUE}\NormalTok{)] }\CommentTok{# re-order into descending order}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{c}\NormalTok{(s, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, p}\OperatorTok{-}\NormalTok{m)) }\CommentTok{# add zero eigenvalues}
\NormalTok{  L <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(s}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(s)}\OperatorTok{*}\NormalTok{m) }\CommentTok{# rescale so that sum(s)=m and put into matrix,}
  \CommentTok{# which would occur if all the traits are variance standardized}
  \KeywordTok{return}\NormalTok{(L)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Generate the random covariance matrix
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fake.cov.matrix <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(p)\{}
    \CommentTok{# p is the size of the matrix (number of cols and rows)}
\NormalTok{    E <-}\StringTok{ }\KeywordTok{fake.eigenvectors}\NormalTok{(p)}
\NormalTok{    L <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{fake.eigenvalues}\NormalTok{(p))}
\NormalTok{    S <-}\StringTok{ }\NormalTok{E}\OperatorTok{%*%}\NormalTok{L}\OperatorTok{%*%}\KeywordTok{t}\NormalTok{(E)}
    \KeywordTok{return}\NormalTok{(S)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Generate the random \(X\) variables using
  \(\mathbf{X} = \mathbf{X}' (\mathbf{E}\sqrt{\mathbf{L}})^\top\)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# two functions to compute the random data}
\NormalTok{fake.X <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(n,p,E,L)\{}
  \CommentTok{# n is number of observations}
  \CommentTok{# p is number of variables}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p),}\DataTypeTok{nrow=}\NormalTok{n,}\DataTypeTok{ncol=}\NormalTok{p) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(E}\OperatorTok{%*%}\KeywordTok{sqrt}\NormalTok{(L))}
    \KeywordTok{return}\NormalTok{(X)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

An example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5}
\NormalTok{p <-}\StringTok{ }\DecValTok{5}
\NormalTok{E <-}\StringTok{ }\KeywordTok{fake.eigenvectors}\NormalTok{(p)}
\NormalTok{L <-}\StringTok{ }\KeywordTok{fake.eigenvalues}\NormalTok{(p, }\DataTypeTok{start=}\DecValTok{1}\NormalTok{, }\DataTypeTok{rate=}\DecValTok{1}\NormalTok{)}
\NormalTok{X <-}\StringTok{ }\KeywordTok{fake.X}\NormalTok{(n, p, E, L)}
\KeywordTok{colnames}\NormalTok{(X) <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\NormalTok{p)}
\NormalTok{E}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]        [,2]        [,3]        [,4]        [,5]
## [1,]  0.4762049  0.76679336  0.02189702 -0.37886958 -0.20306443
## [2,] -0.3226816  0.26514622 -0.21147219  0.50680583 -0.72387940
## [3,]  0.6343127 -0.07414265  0.40381446  0.65434666  0.03024185
## [4,]  0.2450305 -0.53523178  0.22900376 -0.40906173 -0.65856872
## [5,] -0.4546568  0.22305882  0.85982044 -0.06406746 -0.01166667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{E <-}\StringTok{ }\KeywordTok{eigen}\NormalTok{(}\KeywordTok{cov}\NormalTok{(X))}\OperatorTok{$}\NormalTok{vectors}
\NormalTok{scores <-}\StringTok{ }\NormalTok{X}\OperatorTok{%*%}\NormalTok{E}
\KeywordTok{colnames}\NormalTok{(scores) <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"pc"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\NormalTok{p)}
\KeywordTok{cor}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(X, scores))[}\DecValTok{1}\OperatorTok{:}\NormalTok{p, (p}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{(p}\OperatorTok{*}\DecValTok{2}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           pc1         pc2          pc3        pc4          pc5
## X1  0.6381822  0.71536319  0.009665104  0.2581645 -0.119317350
## X2 -0.5701649  0.32632936 -0.213505507 -0.4289443 -0.582102493
## X3  0.8417769 -0.06005797  0.312645510 -0.4358132  0.011149679
## X4  0.4130194 -0.64271492  0.232581875  0.3517727 -0.488358607
## X5 -0.6579685  0.23893819  0.712352851  0.0501983 -0.004418134
\end{verbatim}

\chapter*{Part III: Introduction to Linear
Models}\label{part-iii-introduction-to-linear-models}
\addcontentsline{toc}{chapter}{Part III: Introduction to Linear Models}

\chapter{\texorpdfstring{A linear model with a single, continuous
\emph{X}}{A linear model with a single, continuous X}}\label{a-linear-model-with-a-single-continuous-x}

\section{\texorpdfstring{A linear model with a single, continuous
\emph{X} is classical
``regression''}{A linear model with a single, continuous X is classical regression}}\label{a-linear-model-with-a-single-continuous-x-is-classical-regression}

To introduce modeling with a single continuous \(X\) variable, I'll use
data from

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.b3h4q}
\item
  File: ``FCM data dryad.csv''
\end{enumerate}

The data are from \citet{Dantzer_xxx}, who showed that North American
red squirrel (\emph{Tamiasciurus hudsonicus}) mothers from Yukon, Alaska
produce faster growing pups in years with increased squirrel density.
Remarkably, they even showed that perceived (but not actual) density
results in faster growing pups. To begin to investigate how pregnant
mothers control the future growth rate of pups, the researchers measured
the relationship between local squirrel density and the amount of fecal
cortisol metabolites from pregnant mothers. Cortisol is a hormone that
is secreted as part of stress response. The researchers were interested
in cortisol because it had previously been shownt that, in mammals,
blood cortisol levels in pregnant mothers have numerous effects on
offspring long past birth. If increased squirrel density causes
increased blood cortisol levels then we would expect to find a positive
relationship between \(Density\) and

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/squirrel-1.pdf}
\caption{\label{fig:squirrel}A scatterplot of Fecal cortisol matabolites and
squirrel density.}
\end{figure}

Figure \ref{fig:squirrel} is a \textbf{scatterplot} of the data with the
amount of cortisol metabolites in the feces on the \(Y\) axis and local
squirrel density on the \(X\) axis. The line through the data is a
graphical representation of a linear model fit to the data and the gray
cloud around the line is a graphical representation of the uncertainty
in the model. The researchers wanted to model the ``effect'' of squirrel
density on the amount of cortisol metabolites in the feces of the
pregnant mothers. Graphically, this effect is the slope of the line in
Figure \ref{fig:squirrel}.

The model fit to the data is

\begin{equation}
FCM_i = \beta_0 + \beta_1 Density_i + \varepsilon_i
\label{eq:fcm-model}
\end{equation}

which contains both the linear predictor and the error. For inference,
for example, computing standard errors of the coefficients, We need to
model the error. Here, we use the simplest model of error which is ``IID
\(N(0, \sigma)\)''. This means, the modeled error is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent -- individual error values are independent of other
  values.
\item
  Identical -- individual error can be thought of as a sample from a
  single \textbf{random distribution} (the same for each individual
  value). For this model, this distribution is
\item
  \(N(0, \sigma)\) -- the modeled distribution is ``Normal'' or
  ``Gaussian'', with a mean of zero and a standard deviation of
  \(\sigma\).
\end{enumerate}

The predictor part of the model is

\begin{equation}
\textrm{E}[FCM|Density] = \beta_0 + \beta_1 Density
\label{eq:regression}
\end{equation}

In words, model \eqref{eq:regression} reads ``the expected value of
\(FCM\) conditional on density is beta-knot plus beta-one times
density''. An \textbf{expected value} is a long run average -- if we
were to sample lots and lots of red squirrel populations with
\(Density=x\) (where \(x\) is a specific value), we'd expect the average
\(FCM\) across these samples to be \(\beta_0 + \beta_1 x\).

\begin{quote}
Let's unpack this. \(\textrm{E}[Y]\) is the \textbf{expectation} or
\textbf{expected value} of \(Y\). An expection is the long-run average
of \(Y\) if we were to run an experiment or re-sample a population many
times. The sample mean of \(Y\) is an estimate of \(\textrm{E}[Y]\).
\(\textrm{E}[Y|X]\) is a conditional expectation of \(Y\) -- it is the
expectation given additional conditions. Using the red squirrel example,
these conditions are a specific value of \(Density\). If \(FCM\) is
linearly related to \(Density\) (the right-hand side of equation
\eqref{eq:regression}) then the expected value of \(FCM\) given a local
density of 2.8 squirrels differs from the expected value of \(FCM\)
given a local density of 1.4 squirrels (the units of \(Density\) are
squirrels per 150 meter radius of the individual female's midden).
\end{quote}

In model \eqref{eq:regression}, there is a single \(X\) variable
(\(FCM\)). While the \(X\) variables are often called the ``dependent''
variables, in this model \(FCM\) does not ``depend'' on the independent
variable \(Density\) in any causal sense -- meaning if I were to
intervene and set \(Density\) to some value \(x\), I would expect
\(FCM\) to equal \(\beta_0 + \beta_1 x\). Rather, \(FCM\) only
``depends'' on \(Density\) in a probablistic sense -- if \(Density = x\)
then the most probable value of \(FCM\) is \(\beta_0 + \beta_1 x\). With
some strong assumptions model \eqref{eq:regression} can be turned into a
model of causal dependency, which is the focus of chapter xxx.

\(\beta_0\) and \(\beta_1\) are the \textbf{parameters} of model
\eqref{eq:regression}. Specifically \(\beta_0\) is the model
\textbf{intercept} and \(\beta_1\) is the modeled \textbf{effect} of
\(Density\). Again, the effect (\(\beta_1\)) has a probabilistic, and
not causal, interpretation. This interpretation is

\begin{equation}
\beta_1 = \textrm{E}[FCM|Density=x+1] - \textrm{E}[FCM|Density=x] 
\label{eq:beta1}
\end{equation}

Or, in words, ``beta-1 is the expected value of FCM when density equals
x + 1 minus the expected value of FCM when the density equals x.''
\(\beta_1\) is simply the difference in expected values given a one unit
difference in \(Density\). A very short way to state this is
``\(\beta_1\) is a difference in conditional means''.

\subsection{Using a linear model to estimate explanatory
effects}\label{using-a-linear-model-to-estimate-explanatory-effects}

The goal of the statistical model here is to estimate \(\beta_1\) -- the
probabalistic effect of \(Density\) on \(FCM\). This estimate, and a
measure of the uncertainty of this estimate, are in the table of
coefficients of the fit model

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

736.0

331.9

2.2

0.0281

Density

671.1

178.9

3.8

0.0002

where the entries in the column ``Estimate'' are estimates of the
parameters \(\beta_0\) and \(\beta_1\) in model \eqref{eq:regression}. The
entries in the column ``Std. Error'' are the standard errors (SE) of the
estimates, which are measures of the uncertainty of the estimates.

The parameter estimates in the table above are the coefficients of the
fitted model

\begin{equation}
FCM_i = b_0 + b_1 Density_i + e_i
\label{eq:fcmi}
\end{equation}

where the subscript \emph{i} refers to the \emph{i}th individual. The
coefficients \(b_0\) and \(b_1\) are the y-intercept and the slope of
the line in Figure \ref{fig:squirrel}. The coefficient for \(Density\)
(\(b_1\)) is 671.1, and (given the definition of the parameter
\(\beta_1\) in equation \eqref{eq:beta1}) we expect squirrel mothers with
a local density of 2 squirrels within a 150 m radius of her midden to
average 671.1 more units of FCM (ng of fecal cortical metabolites per
gram dry food) than mother squirrels with a local density of only 1
squirrel within a 150 m radius of her midden.

\subsubsection{Probabilistic vs.~causal
conditioning}\label{probabilistic-vs.causal-conditioning}

Remember that this coefficient is estimating a probabilistic parameter.
Consequently, the coefficient \(b_1\) is simply a descriptor of a
pattern of relationship between local density and fecal cortisol
metabolites - no causal effect is implied. With the strong assumptions
explained in chapter xxx, however, \(b_1\) can estimate a causal effect.

\subsection{Using a linear model for
prediction}\label{using-a-linear-model-for-prediction}

Model \eqref{eq:fcmi} gives the measured value of \emph{FCM} for each
squirrel. The equation includes the linear predictor
(\(b_0 + b_1 Density_i\)) and the \textbf{residual} from the predictor
(\(e_i\)). The predictor part is called ``predictor'' because it is the
equation for predicting the value of an individual's \(FCM\) given that
individual's value of \(Density\):

\begin{equation}
\widehat{FCM} = b_0 + b_1 Density
\label{eq:fcmhat}
\end{equation}

where \(\widehat{FCM}\) is read as ``FCM hat'' and is the
\textbf{predicted value} or simply ``prediction''. Very often, we use
the predictor part (equation \eqref{eq:fcmhat}) to predict unknown or
future values given different modeled inputs (the \(X\)).

\subsection{Reporting results}\label{reporting-results}

The authors of the squirrel fcm data published a figure and table
similar to fig. xxx and table above but used a slightly more complex
linear model. Here is how the author's reported the results:

\begin{quote}
Across 6 years (2006 to 2011), we found a positive relationship between
local density and concentrations of fecal cortisol metabolites {[}FCM;
t\(_155\) = 3.63, P = 0.0002 (table S4 and Fig. 3A){]}.
\end{quote}

I would advocate reporting the estimate and a confidence interval
instead of \(t\) and \(p\). For example ``Across 6 years (2006 to 2011),
the probabilistic effect of local density on fecal cortisol metabolites
is 671.1 (95\% CI: 317.7, 1024.5). If a \(p\)-value is report \emph{in
addition} to the effect and CI, always report the exact \emph{p}-value,
which emphasizes the continuous nature of evidence against the null, and
not something like''\(p < 0.05\)``, which artificially dichotomizes the
evidence against the null.

\section{Working in R}\label{working-in-r}

\subsection{\texorpdfstring{Exploring the bivariate relationship between
\emph{Y} and
\emph{X}}{Exploring the bivariate relationship between Y and X}}\label{exploring-the-bivariate-relationship-between-y-and-x}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import the ``FCM data dryad.csv'' data from the Dryad repository as
  the data.table \texttt{fcm}
\item
  How are different words in the column labels demarcated? Is this good
  practice?
\end{enumerate}

Here we want to fit a model of \texttt{FCM.ng.g.dry} as a function of
\texttt{Raw.Squirrel.Density}. The authors used prior knowledge to
expect a positive relationship between these two variables. Use qplot to
generate a scatterplot of \(FCM\) against \(Density\)

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Is there a trend? If so, does the trend look linear or non-linear?
\item
  Does the residual variation (the deviation from the trend on the \(Y\)
  axis) look homogenous along the \(X\)-axis?
\item
  Are there any obvious outliers?
\end{enumerate}

\subsection{Fitting the linear model}\label{fitting-the-linear-model}

We will fit a linear model to the data using the \texttt{lm} function,
which is very general and will be our workhorse throughout the class.
The minimal input to the function is a model formula and the name of the
data.frame (remember, a data.table is a data.frame). A formula is of the
form \texttt{Y\ \textasciitilde{}\ X}. All of the output we assign to
the object \texttt{fit}.

Let's fit the linear model to the data using density as the predictor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(FCM.ng.g.dry }\OperatorTok{~}\StringTok{ }\NormalTok{Raw.Squirrel.Density, }\DataTypeTok{data=}\NormalTok{fcm)}
\end{Highlighting}
\end{Shaded}

R will look for the specified \(Y\) and \(X\) variables in the column
names of \texttt{fcm}. If these are not found, R will return an error,
for example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(FCM_ng_g_dry }\OperatorTok{~}\StringTok{ }\NormalTok{Raw_Squirrel_Density, }\DataTypeTok{data=}\NormalTok{fcm)}
\end{Highlighting}
\end{Shaded}

will return the error ``Error in eval(predvars, data, env) : object
`FCM\_ng\_g\_dry' not found''. This means your spelling and
capitalization have to be exact!

\subsection{\texorpdfstring{Getting to know the linear model: the
\texttt{summary}
function}{Getting to know the linear model: the summary function}}\label{getting-to-know-the-linear-model-the-summary-function}

The \texttt{lm} function returns an \texttt{lm} object, which we've
assigned to the name \texttt{fit}. \texttt{fit} contains lots of
information about our fit of the linear model to the data. Most of the
information that we want for most purposes can be retrieved with the
\texttt{summary} function, which is a general-purpose R command the
works with many R objects.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = FCM.ng.g.dry ~ Raw.Squirrel.Density, data = fcm)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2107.5 -1108.3  -434.9   511.8  8186.8 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(>|t|)    
## (Intercept)             736.0      331.9   2.217 0.028078 *  
## Raw.Squirrel.Density    671.1      178.9   3.752 0.000248 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1732 on 154 degrees of freedom
##   (7 observations deleted due to missingness)
## Multiple R-squared:  0.08374,    Adjusted R-squared:  0.07779 
## F-statistic: 14.07 on 1 and 154 DF,  p-value: 0.0002484
\end{verbatim}

What is here:

\textbf{Call}. This is the model that was fit

\textbf{Residuals}. This is a summary of the distribution of the
residuals. From this one can get a sense of the distribution (for
inference, the model assumes a normal distribution with mean zero). More
useful ways to examine this distribution will be introduced later in
this chapter.

\textbf{Coefficients table}. This contains the linear model coefficients
and their standard error and associated \(t\) and \(p\) values.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The column of values under ``Estimate'' are the coefficients of the
  fitted model (equation \eqref{eq:fcmi}). Here, 735.9604344 is the
  intercept (\(b_0\)) and 671.1379749 is the effect of \(Density\)
  (\(b_1\)).
\item
  The column of values under ``Std. Error'' are the standard errors of
  the coefficients.
\item
  The column of values under ``t value'' are the \emph{t-statistics} for
  each coefficient. A \(t\)-value is a \textbf{signal to noise ratio}.
  The coefficient \(b_1\) is the ``signal'' and the SE is the noise. Get
  used to thinking about this ratio. Any \(t\) less than 2 is indicative
  of too much noise to say much about the signal. A \(t\) between 2 and
  3 means the noise is large enough to suggest an effect. A \(t\)
  greater than 3 is pretty good evidence of an effect.
\item
  The column of values under ``Pr(\textgreater{}\textbar{}t\textbar{})''
  is the \(p\)-value, which is the exact probability associated with a
  particular \(t\). What is the \(p\)-value a test of? The \(p\)-value
  tests the hypothesis ``how probable are the data if the coefficient is
  zero?''. Formally \(P = \mathrm{freq(t' \ge t|H_o)}\), where \(t'\) is
  the hypothetical t-value, t is the observed \(t\)-value, and \(H_o\)
  is the null hypothesis. We will return to \(p\)-values in Chapter xxx.
\end{enumerate}

\textbf{Signif. codes}. I am surprised that base R returns this. These
are useless because the concept of ``levels of significance'' is
muddled, as will be discussed in Chapter xxx.

Beneath the Signif. codes are some model statistics which are useful

\textbf{Residual standard error} This is \(\sqrt{\sum{e_i^2}/(n-2)}\),
where \(e_i\) are the residuals in the fitted model. ``degrees of
freedom'' is the number of \(e_i\) that are ``allowed to vary'' after
fitting the parameters, so is the total sample size (\(n\)) minus the
number of parameters fit. The fit model has two fit parameters (\(b_0\)
and \(b_1\) so the df is \(n-2\). Note that this is the denominator in
the residual standard error equation.

\textbf{Multiple R-squared}. This is an important but imperfect summary
measure of the whole model that effectively measures how much of the
total variance in the response variable ``is explained by'' the model.
Its value lies between zero and 1. \textbf{It's a good measure to report
in a manuscript}.

\textbf{F-statistic and p-value}. These are statistics for the whole
model (not the individual coefficients) and I just don't find these very
useful.

Note that the \(p\)-value for the coefficient for Raw.Squirrel.Density
is very small and we could conclude that the data are not consistant
with a model of no slope. But did we need a formal hypothesis test for
this? We haven't learned much if we have only learned that the slope is
``not likely to be exactly zero''. What we want to know is not \emph{if}
there is a relationship between \(FCM\) and \(Density\), which is
imperfectly answered with a \(p\)-value, but \emph{the sign and
magnitude} of the relationship and the uncertainty in this estimate. For
this, we don't need the \(p\)-value. Instead, we want to interpret the
coefficient to its SE directly (for a quick-and-dirty interpretation) or
the confidence interval of the effect (for a more formal
interpretation). Please read this paragraph again. We will come back to
it over and over.

\subsection{display: An alternative to
summary}\label{display-an-alternative-to-summary}

Much of what we want to know about a model fit is returned by the
\texttt{display} function from the \texttt{arm} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{display}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lm(formula = FCM.ng.g.dry ~ Raw.Squirrel.Density, data = fcm)
##                      coef.est coef.se
## (Intercept)          735.96   331.94 
## Raw.Squirrel.Density 671.14   178.90 
## ---
## n = 156, k = 2
## residual sd = 1732.02, R-Squared = 0.08
\end{verbatim}

The \texttt{display} function does not give a \(t\)-value or a
\(p\)-value of the coefficients because the authors of the arm package
do not think \(p\)-values are very valuable. We don't need a \(t\)
because one can mentally compute the approximate ratio of the
coefficient to its SE and get a sense of the signal to noise, and that's
all the authors of the display function think we need.

\subsection{Confidence intervals}\label{confidence-intervals}

Confidence intervals for the coefficients of the model are obtained by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                          2.5 %   97.5 %
## (Intercept)           80.21785 1391.703
## Raw.Squirrel.Density 317.73057 1024.545
\end{verbatim}

\texttt{confint} returns by default the 95\% confidence interval (CI) of
all parameters. The most useful way of thinking about the meaning of a
CI is

\textbf{A confidence interval contains the range of parameter values
that are consistent with the data, in the sense that a \(t\)-test would
not reject the null hypothesis of a difference between the estimate and
any value within the interval}

A more textbook way of defining a CI is: A 95\% CI of a parameter has a
95\% probability of including the true value of the parameter. It does
not mean that there is a 95\% probability that the true value lies in
the interval. This is a subtle but important difference. Here is a way
of thinking about the proper meaning of the textbook definition: we
don't know the true value of \(\beta_1\) but we can 1) repeat the
experiment or sampling, 2) re-estimate \(\beta_1\), and 3) re-compute a
95\% CI. If we do 1-3 many times, 95\% of the CIs will include
\(\beta_1\) within the interval.

Confidence intervals are often interpreted like \(p\)-values. That is,
the researcher looks to see if the CI overlaps with zero and if it does,
concludes there is ``no effect''. First, this conclusion is not correct
-- \textbf{the inability to find sufficient evidence for an effect does
not mean there is no effect, it simply means there is insufficient
evidence to conclude there is an effect}!

Second, what we want to use the CI for is to guide us about how big or
small the effect might reasonably be, given the data. Again, A CI is a
measure of parameter values that are ``consistent'' with the data. If
our biological interpretations at the small-end and at the big-end of
the interval's range radically differ, then we don't have enough
\emph{precision} in our analysis to reach an unambiguous conclusion.
Remember this.

\subsection{How good is our model?}\label{how-good-is-our-model}

How well does variation in \(Density\) ``explain'' variation in \(FCM\)?
The answer to this is in the \(R^2\) value, which is given in
\texttt{display(fit)} and in \texttt{summary(fit)} and accessed directly
with

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08373756
\end{verbatim}

\(R^2\) is the fraction of the total variance of \(Y\) explained by the
model, or more specifically, the linear predictor. It will vary from
zero (the model explains nothing) to one (the model explains
everything). If \(R^2=0\) the response is completely unpredictable by
the predictors. We can think of the values of the response as white
noise or all error. This doesn't mean that the values are ``not caused''
or ``random'' or not predicted by some other variable. It only means the
values are random with respect to the \(X\) variable(s) in the model. If
\(R^2=1\) we can \emph{exactly} predict the response from the \(X\)
variables in the model. So the bigger the \(R^2\), the better the model
in the sense that the response is more predicatable. \textbf{Super
importantly}, ``explains'' is in a probabilistic and not causal sense.
We will explore this concept much more in future worksheets.

\subsection{Model checking}

\texttt{plot} is a very useful base R function for ``model checking'' or
``model diagnostics'' to see if our model fit is acceptable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-1.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-2.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-3.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-4.pdf}

Compare the four diagnostic plots using the guidelines from here
\url{http://data.library.virginia.edu/diagnostic-plots/}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Look at the plots you just made. What is a residual? What is a fitted
  value?
\end{enumerate}

\subsection{exploring a lm object}\label{exploring-a-lm-object}

\texttt{fit} contains much information but simply typing \texttt{fit}
into the console gives us only the model and the coefficients.
\texttt{names} is a super important R function. It gives us the names of
all the parts of some R object. \texttt{fit} is an lm object.
\texttt{names(fit)} gives us all the parts contained in an lm object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "coefficients"  "residuals"     "effects"       "rank"         
##  [5] "fitted.values" "assign"        "qr"            "df.residual"  
##  [9] "na.action"     "xlevels"       "call"          "terms"        
## [13] "model"
\end{verbatim}

You can see any of these parts using the dollar sign

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  What does \texttt{fit\$residuals} return? Answer using equation
  \eqref{eq:fcmi}
\item
  What does \texttt{fit\$fitted.values} return? Answer using equation
  @ref(eq:fcmi
\end{enumerate}

You can use qplot to make a plot similar to the first plot of
\texttt{plot(fit)}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(fit}\OperatorTok{$}\NormalTok{fitted.values, fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{geom=}\KeywordTok{c}\NormalTok{(}\StringTok{'point'}\NormalTok{, }\StringTok{'smooth'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'loess' and formula 'y ~ x'
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-fitted-1.pdf}

\section{Problems}\label{problems-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the chick data from Chapter 3. Compare the effects of
  nest\_temperature\_above\_ambient on day13\_mass by fitting two
  separate linear models 1) one using only the control group and one
  using the treated group. The grouping variable is playback\_treatment.
  These models were plotted in Chapter 3 so \texttt{lm} will return the
  linear model behind these plots.
\end{enumerate}

Report the results using the two effect estimates and a 95\% confidence
interval (we will learn in a later chapter a more sophisticated way of
comparing the effects between the groups)

\textbf{file name}: ``allDatasetsMarietteBuchanan2016.xls''

\textbf{source}: \url{https://datadryad.org//handle/10255/dryad.122315}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (Grad students only) -- find a dataset using Dryad that has data that
  can be fit by a simple linear model with a single continuous \(X\)
  (its okay if the authors fit the data with a more complex model). Fit
  the data and report the results with a plot and text.
\end{enumerate}

\chapter{\texorpdfstring{A linear model with a single, categorical
\emph{X}}{A linear model with a single, categorical X}}\label{a-linear-model-with-a-single-categorical-x}

\section{\texorpdfstring{A linear model with a single, categorical
\emph{X} estimates the effects of \emph{X} on the
response.}{A linear model with a single, categorical X estimates the effects of X on the response.}}\label{a-linear-model-with-a-single-categorical-x-estimates-the-effects-of-x-on-the-response.}

To introduce modeling with a single, categorical \(X\) variable, I'll
use the \protect\hyperlink{vole-data}{Vole data} from Chapter 2. Normal
cellular metabolism creates reactive oxygen species (ROS) that can
disrupt cell function and potentially cause cell damage. Anti-oxidants
are molecules that bind ROS, inhibiting their ability to disrupt cell
activity. A working hypothesis for many years is that supplemental
anti-oxidants should improve cell function and, scaling up, whole-animal
function (such as lifespan). The vole data explores this with
supplemental Vitamins C and E, which are anti-oxidants, in the diet of
the short-tailed field vole (\emph{Microtus agrestis}).

The goal of the study is to measure the effect of anti-oxidants on
lifespan. The researchers randomly assigned the voles to one of thre
treatment levels: ``control'', ``vitamin E'', and ``vitamin C''. The
variable \(treatment\), is a single, categorical \(X\) variable.
Categorical variables are often called \textbf{factors} and the
treatment levels are often called \textbf{factor levels}. ``Levels'' is
a strange usage of this word; a less formal name for levels is
``groups''. There are no units to a categorical \(X\) variable (even
though a certain amount of each anti-oxidant was supplemented). The
response (\(Y\)) is \(lifespan\) measured in days.

The linear model with a categorical \(X\) variable with three levels is
not immediately obvious, and so I don't present the model until after
showing the table of model coefficients. The verbal model is

\begin{equation}
lifespan \sim treatment
\end{equation}

which can be read as ``lifespan as a function of treatment''.

\subsection{Table of model
coefficients}\label{table-of-model-coefficients}

The \emph{table of coefficients} from a linear model fit to some data is
critically important for understanding a linear model and interpreting
results. Read this section carefully. The coefficient table for a linear
model fit to the vole data is

\label{tab:vole-table}Coefficient table of fit linear model of vole data.

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

503.4

27.4

18.4

0.000

treatmentvitamin\_C

-115.1

54.5

-2.1

0.037

treatmentvitamin\_E

-89.9

52.5

-1.7

0.090

The table has three values in the column ``Estimate''. The first
estimate, that for ``(intercept)'' is the mean response in the reference
level. Here, the reference level is the ``control'' group. The second
estimate, that for ``treatmentvitamin\_C'' is the difference between the
mean of the vitamin C group and the the mean of the reference (control)
group. The \emph{direction} of this difference is important; it is
\(\bar{Y}_{vitamin\_c} - \bar{Y}_{control}\), that is the non-reference
level minus the reference level. The third estimate, that for
``treatmentvitamin\_E'' is just like the second estimate, except for the
vitamin E group. That is, it is
\(\bar{Y}_{vitamin\_e} - \bar{Y}_{control}\). The 2nd and 3rd values in
the ``Estimate'' columns are the ``effects'' in the model. These effects
are ``what happens'' when we add a treatment, such as vitamin E
supplementation. When we add the vitamin E supplement, we find the
lifespan changes by -89.9 days, relative to the control.

So typically with categorical \(X\), when we speak of an \emph{effect}
we mean a difference in means, or a \textbf{contrast}.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/vole-mean-plot-1.pdf}
\caption{\label{fig:vole-mean-plot}What the coefficients of a linear model
with a single categorical X mean. The means of the three treatment
levels for the vole data are shown with the filled circles. The length
of the double-headed arrows are differences in means. The intercept
(\(b_0\)) is the mean of the reference treatment level. The coefficients
(\(b_1\) and \(b_2\)) are the differences between the treatment level's
mean and the reference mean. As with a linear model with a continuous X,
the coefficients are effects.}
\end{figure}

\subsection{The linear model}\label{the-linear-model}

We can see an immediate difference between the coefficient table for a
linear model fit to a single, categorical \(X\) and that for a single,
continuous \(X\). For the latter, there is a single coefficient for
\(X\). For the former, there is a coefficient for each level of the
categorical \(X\) \emph{except} the ``reference'' level.

The linear model for a single, categorical \(X\) with three factor
levels is

\begin{equation}
Y = \beta_0 + \beta_1 X_{group2} + \beta_2 X_{group3} + \varepsilon
\end{equation}

where \(group2\) and \(group3\) refer to the two non-reference groups.

For the vole data, ``control'' is the reference, so the model is

\begin{equation}
lifespan = \beta_0 + \beta_1 vitamin\_C + \beta_2 vitamin\_E + \varepsilon
\end{equation}

\textbf{The ``estimates'' in the coefficient table are the estimates of
the parameters in this linear model. These estimates are the
coefficients of the fit model,}

\begin{equation}
lifespan_i = b_0 + b_1 vitamin\_C_i + b_2 vitamin\_E_i + e_i
\label{eq:model-cat-x}
\end{equation}

Given the interpretation of the estimates above, \(b_0\) is the mean of
the control group, \(b_1\) is the difference in means between the
vitamin C and control groups, and \(b_2\) is the difference in means
between the vitamin E and control groups. These estimates and their
meaning are illustrated in Figure \ref{fig:vole-mean-plot}. Take a while
to memorize the bold-faced sentence above equation \eqref{eq:model-cat-x}
and to understand this plot. Be able to ``visualize'' the meaning of the
coefficients of a linear model in this way.

\subsubsection{A linear model with a categorical X is a regression model
with the treatment levels re-coded as
numbers}\label{a-linear-model-with-a-categorical-x-is-a-regression-model-with-the-treatment-levels-re-coded-as-numbers}

Model \eqref{eq:model-cat-x} is a regression model. A regression model
requires that the \(X\) variables be numeric, so how can this model be a
regression model? What are the numeric values of \(vitamin\_C\) and
\(vitamin\_E\)? The answer is very clever: \(vitamin\_C\) and
\(vitamin\_E\) are recoded into \textbf{dummy variables} that contain a
one, if response \(i\) is from that treatment level, and zero otherwise.
This is called dummy coding or treatment coding. The \texttt{lm}
function creates these dummy variables under the table, in something
called the \textbf{model matrix}, which we'll cover in another chapter.
You won't see these columns in your data. But if you did, they would
look like this

lifespan

treatment

treatmentvitamin\_E

treatmentvitamin\_C

621

control

0

0

865

control

0

0

583

vitamin\_E

1

0

561

vitamin\_E

1

0

315

vitamin\_C

0

1

157

vitamin\_C

0

1

R names the dummy variables by combining the names of the factor and the
name of the level within the factor. So the \(X\) variables that R
creates in the model matrix for the fit linear model in model
\eqref{eq:model-cat-x} are \(treatmentvitamin\_C\) and
\(treatmentvitamin\_E\). You can see these names as terms in the
coefficient table of the fit model. Using these dummy variable names, a
better way of writing out the fit model than the notation in model
\eqref{eq:model-cat-x} is

\begin{equation}
lifespan_i = b_0 + b_1 treatvitamin\_C_i + b_2 treatvitamin\_E_i + e_i
\label{eq:model-cat-x-better}
\end{equation}

There are alternative coding methods. Dummy coding is the default in R
and it makes since when thinking about experimental data. Note that the
method of coding can make a difference in an ANOVA table, and many
published papers using R have published incorrect interpretations of
ANOVA table outputs. This is both getting ahead of ourselves and
somewhat moot, because I don't advocate publishing ANOVA tables.

\subsubsection{\texorpdfstring{Some math to convince you that the
``intercept'' of a linear model with a categorical \(X\) is the mean of
the reference group \emph{and} the intercept of a
line}{Some math to convince you that the intercept of a linear model with a categorical X is the mean of the reference group and the intercept of a line}}\label{some-math-to-convince-you-that-the-intercept-of-a-linear-model-with-a-categorical-x-is-the-mean-of-the-reference-group-and-the-intercept-of-a-line}

The mean lifespan given a specific value of \(treatmentvitamin\_C\) and
\(treatmentvitamin\_E\) is

\begin{equation}
\mathrm{E}[lifespan] = b_0 + b_1 treatvitamin\_C + b_2 treatvitamin\_E
\label{eq:model-cat-x-better-parametric}
\end{equation}

Recall from your ``Y = mX + b'' days that the intercept of a line is the
value of Y when X is set to zero. This is the same for a regression
model: the intercept of a regression model is the expected value when
all \(X\)-variables are set to zero. Setting the two dummy variables to
zero, model \eqref{eq:model-cat-x-better-parametric} reduces to

\begin{equation}
\mathrm{E}[lifespan|treatmentvitamin\_C=0, treatmentvitamin\_E=0] = b_0
\end{equation}

which can be read as ``the mean lifespan when treatmentvitamin\_C and
treatmentvitaminE are both set to zero is equal to the intercept''.
Since the rows in which both \(treatmentvitamin\_C\) and
\(treatmentvitamin\_E\) are zero are the rows for the control group, the
intercept of the model is the mean of the control group. Consequently,

\begin{equation}
b_0 = \overline{lifespan}_{control}
\end{equation}

\subsubsection{\texorpdfstring{Some math to convince you that the
coefficient of a dummy variable in a linear model with a categorial
\(X\) is a difference in means \emph{and} the slope of a
line.}{Some math to convince you that the coefficient of a dummy variable in a linear model with a categorial X is a difference in means and the slope of a line.}}\label{some-math-to-convince-you-that-the-coefficient-of-a-dummy-variable-in-a-linear-model-with-a-categorial-x-is-a-difference-in-means-and-the-slope-of-a-line.}

The slope of a line is the difference in \(Y\) given a one unit increase
in \(X\). In model \eqref{eq:model-cat-x-better-parametric} we have two
slopes, one for \(treatmentvitamin\_C\) and one for
\(treatmentvitamin\_E\). If we increase \(treatmentvitamin\_C\) from
zero to one, but keep \(treatmentvitamin\_E = 0\), model
\eqref{eq:model-cat-x-better-parametric} reduces to

\begin{equation}
\mathrm{E}[lifespan|treatmentvitamin\_C=1, treatmentvitamin\_E=0] = b_0 + b_1
\end{equation}

Since the rows in which \(treatmentvitamin\_C = 1\) and
\(treatmentvitamin\_E = 0\) are the rows for the vitamin C group,
\(b_0 + b_1\) is the mean of the vitamin C group, or

\begin{equation}
b_0 + b_1 = \overline{lifespan}_{vitamin\_C}
\end{equation}

Solving for \(b_1\)

\begin{equation}
b_1 = \overline{lifespan}_{vitamin\_C} - b_0
\end{equation}

and since \(b_0 = \overline{lifespan}_{control}\),

\begin{equation}
b_1 = \overline{lifespan}_{vitamin\_C} - \overline{lifespan}_{control}
\end{equation}

The coefficient of \(treatmentvitamin\_C\) is a difference in means but
a slope is a ratio of differences, \(\frac{Y_2 - Y_1}{X_2 - X_1}\). How
can \(b_1\) be a difference \emph{and} a ratio? A more satisfactory way
to think about \(b_1\) is to remember that we set the \(X\) variable
\(treatmentvitamin\_C\) to one to calculate
\(\overline{lifespan}_{vitamin\_C}\) and to zero to calculate
\(\overline{lifespan}_{control}\), so

\begin{equation}
b_1 = \frac{\overline{lifespan}_{vitamin\_C} - \overline{lifespan}_{control}}{1-0}
\end{equation}

One more potential confusion to clarify: \(b_1\) is a difference in mean
lifespand and has the units of \(Lifespan\), which is days. But the
units of a regression coefficient are the units of \(Y\) divided by the
units of \(X\). In a regression model with a categorical \(X\), the
constructed dummy variable is \emph{unitless} and the coefficient simply
has the units of \(Y\).

\subsection{Reporting results}\label{reporting-results-1}

What should be reported for the analyis of effects of anti-oxidant
supplements on vole lifespan? Best practice includes reporting the raw
data with a summary distribution and treatment effects with CIs. ``Raw
data'' means the individual lifespans as a function of treatment level.

\subsubsection{Harrell Plot of the data}\label{harrell-plot-of-the-data}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/harrellplot-1.pdf}
\caption{\label{fig:harrellplot}HarrellPlot of the raw data, distribution,
and effects of the vole lifespan data.}
\end{figure}

The raw data, the distributions within treatment level, and the effects
(difference in means) of treatment can be combined into a single plot
that I call a Harrell plot (Figure \ref{fig:voles}). Notice that the
\emph{x}-axis and \emph{y} axes are flipped so that \(lifespan\) is on
the \emph{x}-axis. It is still the ``response'' or ``Y'' variable! The
Harrell plot contains two parts

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The bottom contains a \textbf{strip chart} (often called a ``dot
  plot'') of the raw response measures grouped by factor level.
  Superimposed over the strip chart is a \textbf{box plot} summarizing
  the distribution of each factor level. The line in the center of a box
  is the median \(lifespan\) for that group, the left and right edges of
  the box are the 25\% and 75\% quantiles of \(lifespan\) for that grop,
  and the lines extending to the left and right of the box are the
  ``whiskers'', which are the smallest and largest value within
  \(1.5 IQR\) (inter-quartile range, which is the interval bounded by
  box).
\item
  The top is a \textbf{forest plot} of the effects and the 95\% CI of
  the effects. For categorical \(X\), the effects could be model
  coefficients or treatment \textbf{contrasts}, which are differences in
  means between treatment levels. Model coefficients are a subset of
  possible treatment contrasts.
\end{enumerate}

The Harrell plot above shows the effects as model coefficients, which
(again!) are differences between the mean of the response in a specific
treatment level and the mean of the response in a reference level. Here
the reference level is the control group.

\subsubsection{In-text reporting}\label{in-text-reporting}

``The mean lifespan of cold-reared voles supplemented with vitamin E was
89.9 days shorter than the mean lifespan for the control group (95\% CI:
-194.1, 14.3). The mean lifespan of cold-reared voles supplmented with
vitamin C was 115.1 days shorter than the mean lifespan for the control
group (95\% CI: -223.2, -6.9).

\subsubsection{Correct interpretation of the Confidence Interval is
key}\label{correct-interpretation-of-the-confidence-interval-is-key}

Remember, that the CI contains the range of parameter values that are
consistent with the data (in the sense that a t-test wouldn't reject the
hypothesis test). This means that a true value at the low end or the
high end of the CI is consistent with the data. Your technical
report/manuscript should discuss the consequences of this. For example,
A small, increase in lifespan is consistant with the Vitamin E but not
Vitamin C supplementation, if we use the 95\% CI as a pretty good range
for inferring ``consistent with''. Both a 223 day and a 7 day decrease
in lifespan are consistant with the Vitamin C effect. 223 days seems
like a huge effect, especially for a short lived vole. 7 days is
certainly a much smaller effect, but this doesn't mean that it doesn't
have important ecological, behavioral, or fitness consequences.

\section{Comparing the results of a linear model to classical hypothesis
tests}\label{comparing-the-results-of-a-linear-model-to-classical-hypothesis-tests}

\subsection{t-tests are special cases of a linear
model}\label{t-tests-are-special-cases-of-a-linear-model}

There isn't ``a'' \emph{t}-test but several flavors of \emph{t}-test
including

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Student's \emph{t}-test. The classical ``two-sample'' test for
  comparing the means between two groups
\item
  Welch's \emph{t}-test. A modification of Student's test, which relaxes
  the assumption of equal variance between the groups.
\item
  paired \emph{t}-test. A version of the test when values in the two
  groups are ``paired'', for example, measuring weight in ten mice
  before treatment (at ``baseline''), measuring weight in the same ten
  mice after treatment, then comparing the mean post-treatment to mean
  pre-treatment weight.
\end{enumerate}

All of these are special cases of the linear model. Welch's and paired
t-test are swept within a linear model in later chapters. Here, I focus
on Student's \emph{t}-test.

First, let's review \emph{t}-values, which were introduced in Chapter 5
on \emph{p}-values. A reminder, a \emph{t}-value is a ratio of signal to
noise, where the signal is an estimate of some parameter and the noise
is the standard error of the estimate. The parameter of interest here is
the difference in means between treatment and control, so \emph{t} is

\begin{equation}
t = \frac{\bar{y}_t - \bar{y}_c}{s_{\bar{y}_t - \bar{y}_c}}
\label{eq:lm-t-value}
\end{equation}

Note that the numerator and denominator in equation \eqref{eq:lm-t-value}
are in the coefficient table of a linear model with categorical \(X\) --
the numerator is the estimate of the effect of a treatment and the
denominator is the standard error of this estimate.

To explore these equalities, let's use data from

\textbf{article} Bak, A.M., Vendelbo, M.H., Christensen, B., Viggers,
R., Bibby, B.M., Rungby, J., Jørgensen, J.O.L., Møller, N. and Jessen,
N., 2018. Prolonged fasting-induced metabolic signatures in human
skeletal muscle of lean and obese men. PloS one, 13(9), p.e0200817.

\textbf{data source}
\url{https://datadryad.org/stash/dataset/doi:10.5061/dryad.6121hj7}

The data are from a randomized \textbf{crossover} design where 18 men (9
lean and 9 obese) were measured for multiple metabolic markers at two
times: 1) in a post-absorptive state after 12 hours overnight fast, and
2) in a prolonged fasting state after 72 hours of fasting. In addition,
at each time point, metabolic markers were measured prior to and after
an insulin infusion.

\subsubsection{\texorpdfstring{A student t-test is equivalant to the
t-value and p-value in a coefficient table of a linear model \emph{if}
there are only two levels in the treatment
factor}{A student t-test is equivalant to the t-value and p-value in a coefficient table of a linear model if there are only two levels in the treatment factor}}\label{a-student-t-test-is-equivalant-to-the-t-value-and-p-value-in-a-coefficient-table-of-a-linear-model-if-there-are-only-two-levels-in-the-treatment-factor}

Here I compare pre-insulin infusion blood levels of free fatty acids
(ffa) between obese and lean subjects at 12h. The data are in Table 2
and the response is the column ``ffa\_t\_210\_min\_m\_m''. The assigment
of lean or obese is in Table 1, which needs to be merged with Table 2 in
order to subset the lean subjects.

Coefficient table from the linear model

\begin{verbatim}
##             Estimate Std. Error  t value     Pr(>|t|)
## (Intercept) 0.417625 0.04433893 9.418923 1.950636e-07
## groupobese  0.105625 0.06270472 1.684482 1.142414e-01
\end{verbatim}

The \emph{t}-value and \emph{p}-value of the effect of obesity on
free-fatty acids \emph{is} a t-test. The numerator of \emph{t} is the
difference in free-fatty acids between obese and lean subjects (the
``Estimate'' in the coefficient table). The denominator of \emph{t} if
the standard error of this estimate (The ``Std. Error'' in the
coefficient table).

To confirm that that the the \emph{t}and \emph{p}-values of the effect
of obesity on free-fatty acids \emph{is} a t-test, let's compare the
coefficient table to the output of a t-test.

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  ffa_t_210_min_m_m by group
## t = -1.6845, df = 14, p-value = 0.1142
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.24011325  0.02886325
## sample estimates:
##  mean in group lean mean in group obese 
##            0.417625            0.523250
\end{verbatim}

The \emph{t}-value in the t.test output is the same as the
\emph{t}-value of the effect of obesity (``groupobese'') in the
coefficient table of the linear model, except it has the opposite sign.
This sign is arbitrary and simply reflects which mean is subtracted from
which. The \emph{p}-value for both is the same.

\subsubsection{\texorpdfstring{A student t-test is \emph{not} equivalant
to the t-value and p-value in a coefficient table of a linear model
\emph{if} there are more than two levels in the treatment factor. This
is a feture of a linear model, not a
bug.}{A student t-test is not equivalant to the t-value and p-value in a coefficient table of a linear model if there are more than two levels in the treatment factor. This is a feture of a linear model, not a bug.}}\label{a-student-t-test-is-not-equivalant-to-the-t-value-and-p-value-in-a-coefficient-table-of-a-linear-model-if-there-are-more-than-two-levels-in-the-treatment-factor.-this-is-a-feture-of-a-linear-model-not-a-bug.}

Let's return to the vole data. The \emph{t} and \emph{p} values of the
effects of vitamin C and vitamin E in the coefficient table of the
linear model of \(lifespan \sim treatment\) are

Level

t

p

vitamin\_C

-2.113

0.037

vitamin\_E

-1.713

0.090

while the \emph{t}-tests between the two supplement levels and the
control are

Level

t

p

vitamin\_C

-1.981

0.051

vitamin\_E

-1.628

0.108

The \emph{t}-test statistics differ from those of the linear model
because the two use \emph{different} standard errors in the denominator
of \emph{t}. Both denominators are computed from a \textbf{pooled
variance}, which estimates the population variance using a weighted
average of the variances of each of the groups in the model. The linear
model contains all three levels (groups) of \(treatment\) and,
consequently, the pooled variance is computed from the variances of all
three groups. The \emph{t}-test uses the pooled variance averaged over
only the two levels compared.

If the linear model uses a pooled variance over all three levels, this
raises the question of why the standard error of the vitamin C and
vitamin E effects differs (see the full table above). The reason is the
vitamin C and vitamin E groups have different sample sizes, so while the
standard errors in the table are computed using a common variance, they
are computed using different \(n\).

\subsubsection{Feature not a bug}\label{feature-not-a-bug}

\subsubsection{Use the linear model, not a
t-test.}\label{use-the-linear-model-not-a-t-test.}

\subsection{ANOVA is a special case of a linear
model}\label{anova-is-a-special-case-of-a-linear-model}

\section{Working in R}\label{working-in-r-1}

Import the vole data from the Dryad repository using the information
above and in Chapter 2 section \protect\hyperlink{vole-data}{Vole data}.

\subsection{Fitting the model}\label{fitting-the-model}

As with a single, continuous \(X\), we fit the model using the
\texttt{lm} function and with the model formula of the form
\texttt{y\ \textasciitilde{}\ x}. Note that the R formula can use the
single categorical variable \(treatment\). The code underneath lm will
note that \(treatment\) is a factor with three levels and will
automatically create the two dummy variables noted above in the linear
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lifespan }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{vole)}
\end{Highlighting}
\end{Shaded}

All of the same scripts to access the information in \texttt{fit} that
we used with the continuous \(X\) analysis are the same. For example,
the base R \texttt{summary} function gives the same information as in
the continuous \(X\) example. Other useful functions on the lm object
(``fit'') are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{coefficients(fit)} and \texttt{coefficients(summary(fit))}.
  Note the difference between these. The first is useful if we just want
  to extract the coefficient. The second if we want the addtional
  information. These can both be shortened using \texttt{coef} in place
  of \texttt{coefficients}.
\end{enumerate}

Let's look at the coefficient table

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                      Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)         503.39286   27.40978 18.365445 1.078296e-32
## treatmentvitamin_C -115.07707   54.45772 -2.113145 3.726632e-02
## treatmentvitamin_E  -89.91667   52.48574 -1.713164 9.001428e-02
\end{verbatim}

The reference level is ``control'' -- we know this because there are
estimates of the effects for the other two levels.

\subsection{Changing the reference
level}\label{changing-the-reference-level}

R assigns the order of the levels of a factor alphabetically, so the
order of the levels of treatment are ``control'', ``vitamin\_C'',
``vitamin\_E''. The first of these is the reference level. Remember the
intercept is the mean of the reference group and the remaining estimates
are the differences in means from this reference. If we want to make
some other level the reference, we can change the order of the factor
levels using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vole[, treatment}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(treatment, }
                         \DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"vitamin_C"}\NormalTok{, }\StringTok{"vitamin_E"}\NormalTok{, }\StringTok{"control"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

The order of the levels in the levels argument sets the new order for
any further analysis. Refit the model to see how this re-ordering
changes the coefficients

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lifespan }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{vole)}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                    Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)        388.3158   47.05685 8.2520573 1.005619e-12
## treatmentvitamin_E  25.1604   64.94462 0.3874132 6.993356e-01
## treatmentcontrol   115.0771   54.45772 2.1131453 3.726632e-02
\end{verbatim}

Understand why the values of these coefficients differ from those in the
coefficient table above.

Here, I'm returning the factors back to the original order.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# put factors back to original}
\NormalTok{vole[, treatment}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(treatment, }
                         \DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"control"}\NormalTok{, }\StringTok{"vitamin_C"}\NormalTok{, }\StringTok{"vitamin_E"}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

\subsection{An introduction to
contrasts}\label{an-introduction-to-contrasts}

We often want to compare more than just the non-reference levels to the
reference level. For example, we might want to compare the effects of
the vitamin E supplementation to vitamin C supplementation. Or, we might
want to combine (or ``pool'') vitamin C and vitamin E levels effects
into a single ``anti-oxidant'' level and compare to the control. These
comparisons of means are called linear \textbf{contrasts}. The emmeans
package is a good package for obtaining contrasts for both simple linear
models computed with \texttt{lm} and for more complicated statistical
models. If you haven't already, download the emmeans package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.em <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit, }\DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{)}
\NormalTok{fit.em}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  treatment emmean   SE df lower.CL upper.CL
##  control      503 27.4 93      449      558
##  vitamin_C    388 47.1 93      295      482
##  vitamin_E    413 44.8 93      325      502
## 
## Confidence level used: 0.95
\end{verbatim}

The \texttt{emmeans()} function returns various estimated means,
depending on what is specified with the \texttt{spec=} parameter. Here
the grouping variable ``treatment'' is specified, so the means returned
are estimates of \(\mathrm{E}(lifespan | treatment)\), the modeled means
for each level of treatment. For this simple analysis, the modeled means
are simply the group means. Note that the default value returned is a
table with the standard error and 95\% confidence limits of the
estimates.

Let's use the emmeans object to get the contrasts for all combinations
of treatment levels.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(fit.em, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast              estimate   SE df lower.CL upper.CL t.ratio p.value
##  vitamin_C - control     -115.1 54.5 93     -223    -6.93 -2.113  0.0373 
##  vitamin_E - control      -89.9 52.5 93     -194    14.31 -1.713  0.0900 
##  vitamin_E - vitamin_C     25.2 64.9 93     -104   154.13  0.387  0.6993 
## 
## Confidence level used: 0.95
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  method=``revpairwise''. \texttt{contrast} can create different
  combinations of differences between means. Here I've specified all
  pairwise differences (the ``rev'' reverses the order of the
  subtraction). Notice that the statistics (estimate, SE, etc) are equal
  to the same statistics for \(b_1\) and \(b_2\) of the linear model. I
  said earlier that these coefficients are contrasts!
\item
  adjust=``none''. In classical frequentist hypothesis testing, the
  p-value of a contrast in what are called ``post-hoc tests'' is
  adjusted to reflect ``multiple testing'' (more than one p-value is
  being computed). This adjustment is almost standard in biology, but
  the practice is hugely controversial. The concept of multiple testing
  is important, and we will return to this in a future chapter, but here
  I have chosen to show the unadjusted p-value. The reason is that I
  want the unadjusted confidence interval and the adjustment would
  adjust these as well. If deleted \texttt{adjust="none"} from the
  script, the contrast function would default to the \textbf{Tukey HSD}
  (Honestly Significant Difference) test. There are literally dozens and
  dozens of post-hoc tests, which largely reflects the misplaced
  emphasis on ``better'' \(p\)-values rather than parameter estimates
  and their uncertainty.
\item
  infer=c(TRUE, TRUE). This parameter controls what kind of inference to
  put in the table. The first value specifies the inclusion of the CI
  (emmeans uses ``CL'' for confidence limit), the second value specifies
  the inclusion of \(t\) and \(p\)-values.
\end{enumerate}

\subsection{Harrell plot}\label{harrell-plot}

\subsubsection{Installing the harrellplot
package}\label{installing-the-harrellplot-package}

The harrellplot package is available on github but not a cran repository
and, therefore, takes a little more work to install. To install a
package from a github repository, 1. load library(devtools) -- this may
need to be installed first using the R Studio Tools \textgreater{}
Install Packages\ldots{} tool 2. install harrellplot from github. In the
console, type

\texttt{install\_github("middleprofessor/harrellplot")}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  load the harrellplot package
\item
  harrellplot requires other packages including broom, Hmisc, car, lme4,
  and lmerTest. If you haven't installed these do. load these with the
  library() function at the start of your notebook.
\end{enumerate}

\subsubsection{Using harrellplot to make a nice, publishable plot of
treatment
effects}\label{using-harrellplot-to-make-a-nice-publishable-plot-of-treatment-effects}

In the console type \texttt{?harrellplot} to see the many parameters.
Unlike ggplot2, variable names need to be specified with quotes in the
harrellplot function. The harrellplot function is a list with several
elements.

Here is the default plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vole.harrellplot <-}\StringTok{ }\KeywordTok{harrellplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"lifespan"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{vole)}
\NormalTok{vole.harrellplot}\OperatorTok{$}\NormalTok{gg }\CommentTok{# gg is the plot object}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-53-1.pdf}

\chapter{Model Checking}\label{model-checking-1}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# a function to transform a vector into quantiles}
\CommentTok{# not if the data are 1:n then the output is "rankits"}
\NormalTok{quantilize_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}
  \CommentTok{# this is the ppoints(x) function}
\NormalTok{  m <-}\StringTok{ }\KeywordTok{length}\NormalTok{(x)}
\NormalTok{  s <-}\StringTok{ }\KeywordTok{trunc}\NormalTok{(}\KeywordTok{rank}\NormalTok{(x))}
\NormalTok{  a <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(m }\OperatorTok{<=}\StringTok{ }\DecValTok{10}\NormalTok{, }\DecValTok{3}\OperatorTok{/}\DecValTok{8}\NormalTok{, }\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\NormalTok{  q <-}\StringTok{ }\NormalTok{(s}\OperatorTok{-}\NormalTok{a)}\OperatorTok{/}\NormalTok{(m }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{a) }\OperatorTok{-}\StringTok{ }\NormalTok{a)}
  \KeywordTok{return}\NormalTok{(q)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\section{Do coefficients make numeric
sense?}\label{do-coefficients-make-numeric-sense}

\section{All statistical analyses should be followed by model
checking}\label{all-statistical-analyses-should-be-followed-by-model-checking}

We us a linear model (or statistical model more generally) to infer
effects or predict future outcomes. Our inference is uncertain. Given
some modeling assumptions, we can quantify this uncertainty with
standard errors, and from these standard errors we can compute
confidence intervals and \emph{p}-values. It is good practice to use a
series of \textbf{diagnostic plots}, diagnostic statistics, and
simulation to check how well the data approximate the fit model and
model assumptions. \textbf{Model checking} is used to both check our
subjective confidence in the modeled estimates and uncertainty and to
provide empirical evidence for subjective decision making in the
analysis workflow.

\textbf{NHST blues} -- Researchers are often encouraged by textbooks,
colleagues, or the literature to test the assumptions of a \emph{t}-test
or ANOVA with formal hypothesis tests of distributions such as a
Shaprio-Wilks test of normality or a Levine test of homogeneity. In this
strategy, an alternative to the \emph{t}-test/ANOVA is used if the
distribution test's p-value is less than some cut-off (such as 0.05).
Common alternatives include 1) transformations of the response to either
make it more normal or the variances more homogenous, 2) implementation
of alternative tests such as a Mann-Whitney-Wilcoxon (MWW) test for
non-normal data or a Welch \emph{t}-test/ANOVA for heterogenous
variances. The logic of a test of normality or homogeneity before a
\emph{t}-test/ANOVA isn't consistent with frequentist thinking because
the failure to reject a null hypothesis does not mean the null
hypothesis is true. We shouldn't conclude that a sample is ``normal'' or
that the variances are ``homogenous'' because a distributional test's
\emph{p}-value \textgreater{} 0.05. But, maybe we should of the
distributional pre-test as an ``objective'' model check? The logic of
this objective decision rule suffers from several issues.
\textbf{First}, the subsequent \emph{p}-value of the \emph{t}test/ANOVA
test is not valid because this \emph{p}-value is the long-run frequency
of a test-statistic as large or larger than the observed statistic
conditional on the null -- not conditional on the subset of nulls with
\(p > 0.05\) in the distribution test. \textbf{Second}, real data are
only approximately normal; with small \(n\), it will be hard to reject
the null of a normal distribution because of low power, but, as \(n\)
increses, a normality test will reject any real dataset. \textbf{Third},
and most importantly, our analysis should follow the logic of our goals.
If our goal is the estimation of effects, we cannot get meaningful
estimates from a non-parametric test (with a few exceptions) or a
transformed response, as these methods are entirely about computing a
``correct'' \emph{p}-value. Good alternatives to classic non-parametric
tests and transformations are bootstrap estimates of confidence limits,
permutation tests, and generalized linear models.

\section{Linear model assumptions}\label{linear-model-assumptions}

Assumptions of a linear model concern the distribution of the ``random
draw'' in the underlying statistical model. Again, in the random error
specification of a linear model

\begin{align}
Y &= \beta_0 + \beta_1 X + \varepsilon\\
\varepsilon &\sim N(0, \sigma)
\label{eq:model-checking-spec1}
\end{align}

the random draw (the ``error'') is from a normal distribution with mean
zero and standard deviation \(\sigma\). In the random conditional
response specification

\begin{align}
y_i &\sim N(\mu_i, \sigma)\\
\mathrm{E}(Y|X) &= \mu\\
\mu_i &= \beta_0 + \beta_1 x_i
\label{eq:model-checking-spec2}
\end{align}

the random draw is a value drawn from a normal distribution with mean
\(\mu_i = \beta_0 + \beta_1 x_i\) and variance \(\sigma^2\). Any
inference about the parameter \(\beta_1\) (such as confidence intervals
or hypothesis tests) assumes that the these distributions are IID Normal
where IID is \textbf{independent and identically distributed} and Normal
refers to the Normal (or Gaussian) distribution.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent means that the error for one case cannot be predicted from
  the error of any other case. This lack of independence creates
  \emph{correlated error}. There are lots or reasons that errors might
  be correlated. If individuals are measured both within and among
  cages, or tanks, or plots, or field sites, then we'd expect the
  measures within the same unit (cage, tank, plot, site) to err from the
  model in the same direction because of environmental features shared
  by individuals within the unit but not by individuals in other units.
  Multiple measures within experimental units create ``clusters'' of
  error. Lack of independence or clustered error can be modeled using
  models with \textbf{random effects}. These models go by many names
  including linear mixed models (common in Ecology), hierarchical
  models, multilevel models, and random effects models. A linear mixed
  model is a variation of model \eqref{eq:model-checking-spec1}.
\end{enumerate}

\textbf{tl;dr} -- Measures taken within the same individual over time
(\emph{repeated measures}) are correlated and are common in all areas of
biology. In ecology and evolutionary studies, measures that are taken
from sites that are closer together or measures taken closer in time or
measures from more closely related biological species will tend to have
more similar error than measures taken from sites that are further apart
or from times that are further apart or from species that are less
closely related. Space and time and phylogeny create \textbf{spatial,
temporal, and phylogenetic autocorrelation}. Correlated error due to
space or time or phylogeny can be modeled with \textbf{Generalized Least
Squares} (GLS) models. A GLS model is a variation of model
\eqref{eq:model-checking-spec1}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Identical means that the errors are ``drawn'' from the same
  distribution. Since the model is a linear model, this distribution is
  a Normal distribution with mean zero and variance \(\sigma^2\). A
  consequence of ``identical'' is that the error variance is
  \textbf{homoskedastic}, or constant, or independent of \(X\). If the
  error variance differs among the \(X\) then the errors are
  \textbf{heteroskedastic}. Many biological processes generate data in
  which the error is a function of the mean. For example, measures of
  biological variables that grow, such as lengths of body parts or
  population size, have variances that ``grow'' with the mean. Or,
  measures of counts, such as the number of cells damaged by toxin, the
  number of eggs in a nest, or the number of mRNA transcripts per cell
  have variances that are a function of the mean. Both growth and count
  measures can sometimes be reasonably modeled using a linear model but
  more often, they are better modeled using a \textbf{generalized linear
  model} (GLM), which is an extension of a linear model. Heteroskedasitc
  error arising for other reasons, both biological and experimental, can
  be modeled with Generalized Least Squares (GLS) or with linear mixed
  models..
\item
  Normal (Gaussian) error means that 1) the response is continuous and
  2) the probability of sampling an individual measuring 0.5 units below
  the population mean is the same as the probability of sampling an
  individual measuring 0.5 units above the population mean. Counts
  (number of cells, number of eggs, number of mRNA transcripts) and
  binary responses (sucessful escape or sucessful infestation of host)
  are not continous and often often have asymmetric probablity
  distributions that are skewed to the right and while sometimes both
  can be reasonably modeled using a linear model they are more often
  modeled using a \textbf{generalized linear model} (GLM), which, again,
  is an extension of the linear model in equation
  \eqref{eq:model-checking-spec1}.
\end{enumerate}

\section{Diagnostic plots use the residuals from the model
fit}\label{diagnostic-plots-use-the-residuals-from-the-model-fit}

\subsection{Residuals}\label{residuals}

A residual of a statistical model is \(y_i - \hat{y}_i\). Remember that
\(\hat{y}_i\) is the predicted value of \(Y\) when \(X\) has the value
\(x_i\) (compaactly written as \(X=x_i\)). And remember that
\(\hat{y}_i\) is the estimate of \(\mu_i\). For linear models (but not
generalized linear models), the residuals of the fit model are estimates
of the \(\varepsilon\) in equation \eqref{eq:model-checking-spec1}. This
\emph{is not} true for generalized linear models because GLMs are not
specified using \eqref{eq:model-checking-spec1}.

\textbf{Alert} A common misconception is that inference from a linear
model assumes that the \emph{response} (the measured \(Y\)) is IID
Normal. This is wrong. Either specification of the linear model shows
precisely why this conception is wrong. Model
\eqref{eq:model-checking-spec1} explicitly shows that it is the error that
has the normal distribution -- the distribution of \(Y\) is a mix of the
distribution of \(X\) and that of the error. A more general way of
thinking about the assumed distribution uses the specification in model
\eqref{eq:model-checking-spec2}, which shows that it is the
\emph{conditional} response that is assumed to be IID normal. Remember,
a conditional response (\(y_i\)) is a random draw from the infinite set
of responses at a given value of \(X\).

Let's look at the distribution of residuals versus the response for a
hypothetical experiment with a single, categorical \(X\) variable (the
experimental factor) with two levels (``Cn'' for control and ``Tr'' for
treatment). The true parameters are \(\beta_0 = 10\) (the true mean for
the control group, or \(\mu_{0}\)), \(\beta_1=4\) (the difference
between the true mean for the treatment minus the true mean for the
control, or \(\mu_1 - \mu_0\)), and \(\sigma = 2\) (the error standard
deviation).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/model-check-histogram, model-check-residuals1-1.pdf}
\caption{(\#fig:model-check-histogram, model-check-residuals1)Histogram
of the (A) response, showing with modes near the true means of each
group and (B) residuals, with a mode for both groups at zero.}
\end{figure}

The plot above shows a histogram of the response (A) and residuals (B).
In the plot of the response, the mode (the highest bar, or bin with the
most cases) includes true mean for each group. And, as expected given
\(\beta_1=4\), the modes of the two groups are 4 units apart. It should
be easy to see from this plot that the response does not have a normal
distribution. Instead, it is distincly bimodal. But the distribution of
the response \emph{within} each level looks like these are drawn from a
normal distribution -- and it should. In the plot of the residuals, the
values of both groups are shifted so that the mean of each group is at
zero. The consequence of the shift is that the combined set of residuals
does look like it is drawn from a Normal distribution.

The two plots suggest two different approaches for model checking.
First, we could examine the responses within each level of the
experimental factor. Or, second, we could examine the residuals of the
fit model, ignoring that the residuals come from multiple groups. The
first is inefficient because it requires as many checks as there are
levels in the factor. The second requires a single check.

\textbf{Alert} Some textbooks that recommend formal hypothesis tests of
normality recommend the inefficient, multiple testing on each group
separately. This isn't wrong, it's just more work than it needs to be
and also suffers from ``multiple testing''.

\subsection{A Normal Q-Q plot is used to check
normality}\label{a-normal-q-q-plot-is-used-to-check-normality}

A Normal Q-Q plot of the residuals can be used to check how closely the
residuals approximate a normal distribution. A Normal Q-Q plot is a
scatterplot of

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{sample quantiles} on the \emph{y} axis. The sample quantiles
  is the vector of \(N\) residuals in rank order, from smallest (most
  negative) to largest (most positive). Sometimes this vector is
  standardized (doing this makes not difference to the interpretation of
  the Q-Q plot).
\item
  \textbf{standard normal quantiles} on the \emph{x} axis. This is the
  vector of standard normal quantiles given \(N\) elements in the
  vector.
\end{enumerate}

\textbf{Stats 101} A quantile is the value of a distribution that is
greater than \(p\) percent of the values in the distribution. The 2.5\%
quantile of a uniform distribution from 0 to 1 is 0.025. The 2.5\%
quantile of a standard normal distribution is -1.96 (remember that 95\%
of the values in a standard normal distribution are between -1.96 and
1.96). The 50\% quantile of a uniform distribution is 0.5 and the 50\%
quantile of a standard normal distribution is 0.0 (this is the median of
the distribtion -- 50\% of the values are smaller and 50\% of the values
are larger).

\textbf{Stats 201} A Q-Q plot more generally is a scatter plot of two
vectors of quantiles either of which can come from a sample or a
theoretical distribution. In the GLM chapter, the text will introduce
Q-Q plots of residual quantiles transformed to have an expected uniform
distribution. These are plotted against theoretical uniform quantiles
from 0 to 1.

If the sampled distribution closely approximates a normal distribution,
the scatter should fall along a line from the bottom, left to the top,
right of the plot. The interpretation of a normal Q-Q plot is enhanced
with a line of ``expected values'' of the sample quantiles if the sample
residuals are drawn from a normal distribution. The closer the sample
quantiles are to the line, the more closely the residuals approximate a
normal distribution. Because of sampling, the sampled values always
deviate from the line, especially at the ends. If the sample was drawn
from a normal distribution, these deviations should be small if the
sample size is big, but can be more pronounced with a small sample size.
This makes it hard to have much confidence in the ``normality'' of a
small sample.

Let's have a look at a Normal Q-Q plot of the residuals of the fake data
generated above.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/model-check-normal-qq-1.pdf}

Rules of a Q-Q plot

At the small end of the distribution (bottom-left), the sample values
are a bit more negative than expected, which means the left tail is a
bit extended. At the large end (upper-right), the sample values are, a
bit less positive than expected, which means the right tail is a bit
shortened. Should we fit a different model given these deviations? To
answer this, we look look at the shaded area, which represents the range
of expected deviations from expectation (the line) given the sample
size. Clearly the deviations are within this range.

Now let's look at simulated samples drawn from non-normal distributions
to identify their characteristic deviations.

\subsubsection{Right skewed}\label{right-skewed}

Many biological measures are from a distribution with long, right tails
(right skewed). Examples include many count variables (number of eggs in
a clutch, number of cells colonized by a parasite), and measures of
time, weight, or length. What is common to all of these is unconstrained
upper bounday but a constrained lower boundary at or above zero (A nest
might have zero but eggs. The weight of a fat depot must be greater than
zero but the weight of a specific species of fish in a trawl catch might
be zero).

A long right tail of conditional responses creates a characteristic
positive deviation of the largest quantiles in a Normal Q-Q plot of the
residuals from a linear model. Positive deviations at the upper end
indicate larger values than expected given a normal distribution. This
is the signature of the residuals of a linear model fit to right skewed
data.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/model-check-lognormal-1.pdf}

A continous response with a right skewed distribution can be modeled
with a generalized linear model using a lognormal or gamma distribution.
A count response can be modeled with a generalized linear model using a
Poisson, quasi-Poisson, or negative binomial distribution (Chapter xxx).

\subsubsection{Excess zeroes}\label{excess-zeroes}

Count data often have an excess of zeroes (for example a lake with no
nests or a host with no parasites), resulting in positive deviations
(closer to zero) at the lower end of the quantile range.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# zero inflated}
\NormalTok{mu_pois <-}\StringTok{ }\FloatTok{2.7}
\NormalTok{beta_pois <-}\StringTok{ }\DecValTok{2}
\NormalTok{p_zero <-}\StringTok{ }\FloatTok{0.3}

\NormalTok{fd.zipois <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}
  \DataTypeTok{Treatment=}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Cn"}\NormalTok{, }\StringTok{"Tr"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n),}
  \DataTypeTok{Response=}\KeywordTok{c}\NormalTok{(SpiecEasi}\OperatorTok{::}\KeywordTok{rzipois}\NormalTok{(n, }\DataTypeTok{lambda =}\NormalTok{ mu_pois, }\DataTypeTok{pstr0 =}\NormalTok{ p_zero), SpiecEasi}\OperatorTok{::}\KeywordTok{rzipois}\NormalTok{(n, }\DataTypeTok{lambda =}\NormalTok{ (mu_pois}\OperatorTok{+}\NormalTok{beta_pois), }\DataTypeTok{pstr0 =}\NormalTok{ p_zero))}
\NormalTok{)}
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Response }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data=}\NormalTok{fd.zipois)}
\NormalTok{fd.zipois[, Residual}\OperatorTok{:}\ErrorTok{=}\KeywordTok{residuals}\NormalTok{(m1)]}
\NormalTok{gg1.zipois <-}\StringTok{ }\KeywordTok{gghistogram}\NormalTok{(}\DataTypeTok{data=}\NormalTok{fd.zipois, }\DataTypeTok{x =} \StringTok{"Response"}\NormalTok{,}
                          \DataTypeTok{color=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{fill=}\StringTok{"Treatment"}\NormalTok{,}
                          \DataTypeTok{add =} \StringTok{"mean"}\NormalTok{, }\DataTypeTok{rug =} \OtherTok{FALSE}\NormalTok{,}
                          \DataTypeTok{bins=}\DecValTok{9}\NormalTok{,}
                          \DataTypeTok{palette =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{)}
\NormalTok{)}

\NormalTok{gg3.zipois <-}\StringTok{ }\KeywordTok{ggqqplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{fd.zipois, }\DataTypeTok{x =} \StringTok{"Residual"}\NormalTok{, }\DataTypeTok{title=}\StringTok{"zero-inflated poisson"}\NormalTok{)}

\NormalTok{gg3.zipois}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-54-1.pdf}

\subsubsection{Constrained lower and upper
bounds}\label{constrained-lower-and-upper-bounds}

Proportions are constrained to values between zero and one. A proportion
can have a distribution that approximates a normal distribution if the
mean is near 0.5 and the standard deviation is small. But, more
generally, proportions can have distributions with diverse shapes.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/model-check-proportions-1.pdf}

\subsubsection{Binary responses}\label{binary-responses}

\subsection{Outliers - an outlier is a point that is highly unexpected
given the modeled
distribution.}\label{outliers---an-outlier-is-a-point-that-is-highly-unexpected-given-the-modeled-distribution.}

\section{Model checking
homoskedasticity}\label{model-checking-homoskedasticity}

\section{Model checking independence - hapiness adverse
example.}\label{model-checking-independence---hapiness-adverse-example.}

\section{Using R}\label{using-r}

\chapter{Model Fitting and Model Fit
(OLS)}\label{model-fitting-and-model-fit-ols}

\section{Least Squares Estimation and the Decomposition of
Variance}\label{least-squares-estimation-and-the-decomposition-of-variance}

The linear models in the last chapter and for much of this book are fit
to data using a method called ``ordinary least squares'' (OLS). This
chapter explores the meaning of OLS and related statistics, including
\(R^2\), as well as some alternative methods for bivariate regression.

\section{OLS regression}\label{ols-regression}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fake-data-ma-1.pdf}

The fake data illustrated in the scatterplot above (Figure
\ref{fig:fake-data-ma}) were modeled to look something like the squirrel
fecal cortisol metabolite data in the previous chapter. If a typical
student is asked to draw a regression line through the scatter, they
typically draw a line similar to that in Figure \ref{fig:fake-data-ma}.
This line is not the OLS regression line but the major axis of an elipse
that encloses the scatter of points--that students invariably draw this
line suggests that the brain interprets the major axis of an elliptical
scatter of points as a trend (This major axis line is an alternative
method for estimating a slope and is known as standard major-axis
regression. More about this at the end of this chapter.)

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fake-data-ols-1.pdf}

The OLS regression line is the red line in Figure
\ref{fig:fake-data-ols} -- the standard major axis line is left for
comparison). The OLS regression line

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  passes through the bivariate mean (\(\bar{x}\), \(\bar{y}\)) of the
  scatter, and
\item
  minimizes the sum of the squared deviations from each point to it's
  modeled value \(\sum{(y_i - \hat{y}_i)^2}\)
\end{enumerate}

There are an infinite number of lines that pass through the bivariate
mean (think of anchoring a line at the bivariate mean and spinning it).
The OLS line is the line that minimizes the squared (vertical)
deviations (``least squares'').

For a bivariate regression, the slope (coefficient \(b_1\) of \(X\)) of
the OLS model fit is computed by

\begin{equation}
b_1 = \frac{\mathrm{COV}(X, Y)}{\mathrm{VAR}(X)}
\end{equation}

This equation is worth memorizing. We will generalize this into a more
flexible equation in a few chapters.

\section{\texorpdfstring{How well does the model fit the data? \(R^2\)
and ``variance
explained''}{How well does the model fit the data? R\^{}2 and variance explained}}\label{how-well-does-the-model-fit-the-data-r2-and-variance-explained}

Let's switch to real data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.056r5}
\item
  File: ``Diet-shift data.xls''
\end{enumerate}

Fish require arachidonic acid (ARA) and other highyly unsaturated fatty
acids in their diet and embryo and yolk-stage larvae obtain these from
yolk. Fuiman and Faulk (xxx) designed an experiment to investigate if
red drum (\emph{Sciaenops ocellatus}) mothers provision the yolk with
ARA from recent dietary intake or from stored sources in somatic
tissues. The data below are from experiment 8. The \emph{x}-axis is the
days since a diet shift to more and less ARA (\(days\)) and the
\emph{y}-axis is the ARA content of the eggs (\(ARA\)).

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/egg-data-1.pdf}

The statistic \(R^2\) is a measure of the fit of a model to data. The
\(R^2\) for the fit of the egg data is 0.42. \(R^2\) is the fraction of
two variances \(\frac{\mathrm{VAR}(Model)}{\mathrm{VAR}(Y)}\), or, the
fraction of the variance of \(Y\) ``explained by the model.'' The value
of \(R^2\) ranges from zero (the fit cannot be any worse) to one (the
fit is ``pefect'').

To understand \(R^2\), and its computation, a bit more, let's look at
three kinds of deviations.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ols-deviations-1.pdf}
\caption{\label{fig:ols-deviations}Three kinds of deviations from a fit
model. A. Deviations of the measured values from the mean. These are in
the numerator of the equation of the sample variance. The dashed line is
the mean ARA content. B. Deviations of the measured values from the
modeled values. The sum of these deviations squared is what is minimized
in an OLS fit. C. Deviations of the modeled values from the mean ARA
content. The measured values are in gray, the modeled values in black}
\end{figure}

Figure \ref{fig:ols-deviations}A shows the deviations from the measured
values to the mean value (dashed line). These are the deviations in the
numerator of the equation to compute the variance of \(ARA_EGG_MG\).
Figure \ref{fig:ols-deviations}B shows the deviations of the measured
values from the modeled values. The sum of these deviations squared is
what is minimized by the OLS fit. The bigger these deviations are, the
worse the model fit. Figure \ref{fig:ols-deviations}C shows the
deviations of the modeled values to the mean value. The bigger these
deviations are, the better the model fit.

The sums of the squares of these deviations (or ``sums of squares'')
have names:

\begin{equation}
\mathrm{SS(total)} = \sum{(y_i - \bar{y})^2}
\end{equation}

\begin{equation}
\mathrm{SS(error)} = \sum{(y_i - \hat{y_i})^2}
\end{equation}

\begin{equation}
\mathrm{SS(model)} = \sum{(\hat{y_i} - \bar{y})^2}
\end{equation}

Again, \(\mathrm{SS(total)}\) is the numerator of the equation for the
sample variance. It is called ``s-s-total'' because
\(\mathrm{SS(total)} = \mathrm{SS(model)} + \mathrm{SS(error)}\). That
is, the total sums of squares can be \textbf{decomposed} into two
\textbf{components}: the modeled sums of squares and the error sums of
squares. Given these components, it's easy to understand \(R^2\)

\begin{equation}
R^2 = \frac{SS(model)}{SS(total)}
\end{equation}

\(R^2\) is the fraction of the total sums of squares that is due to (or
``explained by'') the model sums of squares. Above I said that \(R^2\)
is the fraction of \emph{variance} explained by the model. Equation xxx
is a ratio of variance, but the \((n-1)^{-1}\) in both the numerator and
the denominator cancel out. Finally, many sources give the equation for
\(R^2\) as

\begin{equation}
R^2 = 1- \frac{SS(error)}{SS(total)}
\end{equation}

which is an obvious alternative given the decomposition. I prefer the
former equation because it emphasizes the model fit instead of model
ill-fit.

\chapter{Best Practices -- Issues in
Inference}\label{best-practices-issues-in-inference}

\section{Power}\label{power}

\subsection{\texorpdfstring{``Types'' of
Error}{Types of Error}}\label{types-of-error}

I, II, S, M

\section{multiple testing}\label{multiple-testing}

\textbf{Multiple testing} is the practice of adjusting \emph{p}-values
(and less commonly confidence intervals) to account for the expected
increase in the frequency of Type I error when there are multiple tests
(typically Null Hypothesis Significance Tests). Multiple testing tends
to arise in two types of situations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multiple pairwise contrasts among treatment levels (or combinations of
  levels) are estimated.
\item
  The effects of a treatment on multiple responses are estimated. This
  can arise if

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    there are multiple ways of measuring the consequences of something
    -- for example, an injurious treatment on plant health might effect
    root biomass, shoot biomass, leaf number, leaf area, etc.
  \item
    one is exploring the consequences of an effect on many, many
    outcomes -- for example, the expression levels of 10,000 genes
    between normal and obese mice.
  \end{enumerate}
\end{enumerate}

Despite the ubiquitous presence of multiple testing in elementary
biostatistics textbooks, in the applied biology literature, and in
journal guidelines, the practice of adjusting \emph{p}-values for
multiple tests is highly controversial among statisticians. My thoughts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In situations like (1) above, I advocate that researchers \textbf{do
  not adjust p-values for multiple tests}. In general, its a best
  practice to only estimate contrasts for which you care about because
  of some \emph{a priori} model of how the system works. If you compare
  all pairwise contrasts of an experiment with many treatment levels
  and/or combinations, expect to find some false discoveries.
\item
  In situations like (2a) above, I advocate that researchers \textbf{do
  not adjust p-values for multiple tests}.
\item
  In situations like (2b) above, adjusting for the \textbf{False
  Discovery Rate} is an interesting approach. But, recognize that tests
  with small \emph{p}-values are \emph{highly provisional} discoveries
  of a patterns only and not a discovery of the causal sequelae of the
  treatment. For that, one needs to do the hard work of designing
  experiments that rigorously probe a working, mechanistic model of the
  system.
\end{enumerate}

Finally, recognize that anytime there are multiple tests, Type M errors
will arise due to the vagaries of sampling. This means that in a
rank-ordered list of the effects, those at the top have measured effects
that are probably bigger than the true effect. An alternative to
adjusted \emph{p}-values is a \textbf{penalized regression} model that
shrinks effects toward the mean effect.

\subsection{Some background}\label{some-background}

\subsubsection{Family-wise error rate}\label{family-wise-error-rate}

The logic of multiple testing goes something like this: the more tests
that a researcher does, the higher the probability that a false positive
(Type I error) will occur, therefore a researcher should should adjust
\emph{p}-values so that the Type I error over the set (or ``family'') of
tests is 5\%. This adjusted Type I error rate is the ``family-wise error
rate''.

If a researcher carries out multiple tests \emph{of data in which the
null hypothesis is true}, what is the probability of finding at least
one Type I error? This is easy to compute. If the frequency of Type I
error for a single test is \(\alpha\), then the probability of no Type I
error is \(1 - \alpha\). For two tests, the probability of no Type I
error in either test is the product of the probability for each test, or
\((1 - \alpha)^2\). By the same logic, for \(m\) tests, the probabilty
of no type I error in any of the tests is \((1 - \alpha)^m\). The
probability of at least one type one error, across the \(m\) tests,
then, is \(1 - (1 - \alpha)^m\). A table of these probabilities for
different \(m\) is given below. If the null is true in all tests, then
at least one Type I error is more likely than not if there are 14 tests,
and close to certain if there more than 50 tests. Don't skip over this
paragraph -- the logic is important even if I don't advocate adjusting
for multiple tests.

\label{tab:best-type1-table}Probability of at least one type I error within
the set of multiple tests, for data in which the null hypothesis is
true. The Type I error rate for a single test is 0.05. The number of
tests is m. The probability is p.

m

p

1

0.05

3

0.14

6

0.26

10

0.40

50

0.92

100

0.99

\subsubsection{False discovery rate}\label{false-discovery-rate}

If a researcher carries out thousands of tests to ``discover'' new
facts, and uses \(p < 0.05\) as evidence of discovery, then what is the
frequency of \textbf{false discoveries}?

\subsubsection{p-value filter I -- Inflated
effects}\label{p-value-filter-i-inflated-effects}

If a researcher caries out many tests, and ranks the effects by
magnitude or \emph{p}-value, then the effect sizes of the largest
effects will be inflated. Before explaining why, let's simulate this
using an experiment of allelopathic effects of the invasive garlic
mustard (\emph{Alliaria petiolata}) on gene expression in the native
American ginseng (\emph{Panax quinquefolius}). In the treated group, we
have ten pots, each with an American ginseng plant grown in a container
with a mustard plant. In the control group, we have ten pots, each with
an American ginseng plant grown in a container with another American
ginseng. I've simulated the response of 10,000 genes. The treatment has
a true effect in 10\% of the 10,000 genes but most effects are very
small.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{4}\NormalTok{)}
\NormalTok{p <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{4} \CommentTok{# number of genes}
\NormalTok{pt <-}\StringTok{ }\FloatTok{0.1}\OperatorTok{*}\NormalTok{p }\CommentTok{# number of genes with true response to treatment}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}

\CommentTok{# sample the gene effects from an exponential distribution}
\NormalTok{theta <-}\StringTok{ }\NormalTok{.}\DecValTok{3}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rexp}\NormalTok{(pt, }\DataTypeTok{rate=}\DecValTok{1}\OperatorTok{/}\NormalTok{theta),}
          \KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, (p}\OperatorTok{-}\NormalTok{pt))) }\CommentTok{# the set of 10,000 effects}

\CommentTok{# sample the variance of the expression level with a gamma, and set a minimum}
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{rgamma}\NormalTok{(p, }\DataTypeTok{shape=}\DecValTok{2}\NormalTok{, }\DataTypeTok{scale=}\DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{+}\StringTok{ }\FloatTok{0.58}
\CommentTok{# quantile(sigma, c(0.001, 0.1, 0.5, 0.9, 0.999))}

\NormalTok{Y1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\KeywordTok{rep}\NormalTok{(sigma, }\DataTypeTok{each=}\NormalTok{n)), }\DataTypeTok{nrow=}\NormalTok{n)}
\NormalTok{Y2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p, }\DataTypeTok{mean=}\KeywordTok{rep}\NormalTok{(beta, }\DataTypeTok{each=}\NormalTok{n), }\DataTypeTok{sd=}\KeywordTok{rep}\NormalTok{(sigma, }\DataTypeTok{each=}\NormalTok{n)), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# check}
\CommentTok{# use n <- 10^4 to check}
\CommentTok{# apply(y2, 2, mean)[1:5]}
\CommentTok{# b[1:5]}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{,}\StringTok{"tr"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n)}
\NormalTok{bhat <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(p)}
\NormalTok{p.value <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(p)}
\NormalTok{sigma_hat <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(p)}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{p)\{}
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{c}\NormalTok{(Y1[,j], Y2[, j]) }\OperatorTok{~}\StringTok{ }\NormalTok{x)}
\NormalTok{  bhat[j] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit))[}\StringTok{"xtr"}\NormalTok{, }\StringTok{"Estimate"}\NormalTok{]}
\NormalTok{  p.value[j] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit))[}\StringTok{"xtr"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  sigma_hat[j] <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(fit}\OperatorTok{$}\NormalTok{residuals}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\NormalTok{fit}\OperatorTok{$}\NormalTok{df.residual)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/inflation-histogram-1.pdf}
\caption{\label{fig:inflation-histogram}A histogram of the distribution of
the 10,000 effects}
\end{figure}

\label{tab:unnamed-chunk-56}The top 10 genes ranked by p-value. Rank is the
rank of the true effect, from large to small.

effect

estimate

sigma

sd

p.value

relative true effect

rank

2.23

2.67

0.81

0.55

0.0000000

1.00

1

1.59

1.92

0.62

0.57

0.0000005

0.71

7

1.46

1.86

0.84

0.71

0.0000159

0.65

10

1.47

1.78

0.83

0.74

0.0000409

0.66

9

0.00

1.95

1.10

0.85

0.0000717

0.00

0.48

1.26

0.67

0.56

0.0000816

0.21

212

0.97

1.32

0.69

0.60

0.0001004

0.43

45

0.54

1.68

1.06

0.78

0.0001321

0.24

173

0.00

2.05

1.39

0.96

0.0001488

0.00

0.43

-1.78

1.33

0.84

0.0001733

0.19

244

The table above lists the top 10 genes ranked by \emph{p}-value, using
the logic that the genes with the smallest \emph{p} values are the genes
that we should pursue with further experiments to understand the system.
Some points

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Six of the top ten genes with biggest true effects are \emph{not} on
  this list. And, in the list are three genes with true effects that
  have relatively low ranks based on true effect size (column "rank")
  \emph{and} two genes that have no true effect at all. Also in this
  list is one gene with an estimated effect (-1.78) that is
  \emph{opposite} in sign of the true effect (but look at the
  \emph{p}-value!)
\item
  The estimate of the effect size for all top-ten genes are inflated.
  The average estimate for these 10 genes is 1.47 while the average true
  effect for these 10 genes is 0.92 (the estimate ).
\item
  The sample standard deviation (sd) for all top-ten genes is less than
  the true standard deviation (sigma), in some cases substantially.
\end{enumerate}

The consequence of an inflated estimate of the effect and a deflated
estimate of the variance is a large \emph{t} (not shown) and small
\emph{p}. What is going on is an individual gene's estimated effect and
standard deviation are functions of 1) the true value and 2) a random
sampling component. The random component will be symmetric, some effects
will be overestimated and some underestimated. When we rank the genes by
the estimate of the effect or \emph{t} or \emph{p}, some of the genes
that have ``risen to the top'' will be there because of a large,
positive, sampling (random) component of the effect and/or a large,
negative, sampling component of the variance. Thus some genes' high rank
is artificial in the sense that it is high because of a random fluke. If
the experiment were re-done, these genes at the top because of a large,
random component would (probably) fall back to a position closer to
their expected rank (regression to the mean again).

In the example here, all genes at the top have inflated estimates of the
effect because of the positive, random component. This inflation effect
is a function of the signal to noise ratio, which is controled by theta
and sigma in the simulation. If theta is increased (try theta=1), or if
sigma is decreased, the signal to noise ratio increases (try it and look
at the histogram of the new distribution of effects) and both the 1)
inflation and the 2) rise to the top phenomenon decrease.

\subsubsection{p-hacking}\label{p-hacking}

\subsection{Multiple testing -- working in
R}\label{multiple-testing-working-in-r}

\subsubsection{Tukey HSD adjustment of all pairwise
comparisons}\label{tukey-hsd-adjustment-of-all-pairwise-comparisons}

The \texttt{adjust} argument in \texttt{emmeans::contrast()} controls
the method for \emph{p}-value adjustment. The default is ``tukey''.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``none'' -- no adjustment, in general my preference.
\item
  ``tukey'' -- Tukey's HSD, the default
\item
  ``bonferroni'' -- the standard bonferroni, which is conservative
\item
  ``fdr'' -- the false discovery rate
\item
  ``mvt'' -- based on the multivariate \emph{t} distribution and using
  covariance structure of the variables
\end{enumerate}

The data are those from Fig. 2D of ``Data from The enteric nervous
system promotes intestinal health by constraining microbiota
composition''. There is a single factor with four treatment levels. The
response is neutrophil count.

No adjustment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{donor, }\DataTypeTok{data=}\NormalTok{exp2d)}
\NormalTok{m1.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\StringTok{"donor"}\NormalTok{)}
\NormalTok{m1.pairs.none <-}\StringTok{ }\KeywordTok{contrast}\NormalTok{(m1.emm, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(m1.pairs.none, }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast       estimate   SE df lower.CL upper.CL t.ratio p.value
##  gf - wt          -1.502 1.48 58    -4.47     1.47 -1.013  0.3153 
##  sox10 - wt        4.679 1.23 58     2.23     7.13  3.817  0.0003 
##  sox10 - gf        6.182 1.45 58     3.29     9.08  4.276  0.0001 
##  iap_mo - wt      -0.384 1.53 58    -3.45     2.68 -0.251  0.8025 
##  iap_mo - gf       1.118 1.71 58    -2.31     4.54  0.654  0.5159 
##  iap_mo - sox10   -5.064 1.49 58    -8.05    -2.07 -3.391  0.0013 
## 
## Confidence level used: 0.95
\end{verbatim}

Tukey HSD:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.pairs.tukey <-}\StringTok{ }\KeywordTok{contrast}\NormalTok{(m1.emm, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"tukey"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(m1.pairs.tukey, }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast       estimate   SE df lower.CL upper.CL t.ratio p.value
##  gf - wt          -1.502 1.48 58    -5.43     2.42 -1.013  0.7426 
##  sox10 - wt        4.679 1.23 58     1.44     7.92  3.817  0.0018 
##  sox10 - gf        6.182 1.45 58     2.36    10.01  4.276  0.0004 
##  iap_mo - wt      -0.384 1.53 58    -4.43     3.66 -0.251  0.9944 
##  iap_mo - gf       1.118 1.71 58    -3.41     5.64  0.654  0.9138 
##  iap_mo - sox10   -5.064 1.49 58    -9.01    -1.11 -3.391  0.0067 
## 
## Confidence level used: 0.95 
## Conf-level adjustment: tukey method for comparing a family of 4 estimates 
## P value adjustment: tukey method for comparing a family of 4 estimates
\end{verbatim}

\subsection{False Discovery Rate}\label{false-discovery-rate-1}

\section{difference in p is not
different}\label{difference-in-p-is-not-different}

\section{Inference when data are not
Normal}\label{inference-when-data-are-not-normal}

No real data are normal, although many are pretty good approximations of
a normal distribution.

I'll come back to this point, but first, let's back up. Inference in
statistical models (standard errors, confidence intervals,
\emph{p}-values) are a function of the modeled distributions of the
parameters (for linear models, this parameter is the conditional (or
error) variance \(\sigma^2\)); if the data do not approximate the
modeled distribution, then inferential statistics might be to liberal
(standard errors are too small, confidence intervals are too narrow,
Type I error is more than nominal) or to conservative (standard errors
are too large, confidence intervals are too wide, Type I error is less
than nominal).

Linear models assume that ``the data'' (specifically, the conditional
response, or, equivalently, the residuals from the model) approximate a
Normal distribution. Chapter xxx showed how to qualitatively assess how
well residuals approximate a Normal distribution using a Q-Q plot. If
the researcher concludes that the data poorly approximate a normal
distribution because of outliers, the researcher can use robust methods
to estimate the parameters. If the approximation is poor because the
residuals suggest a skewed distribution or one with heavy or light
tails, the researcher can choose among several strategies

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  continue to use the linear model; inference can be fairly robust to
  non-normal data, especially when the sample size is not small.
\item
  use a generalized linear model (GLM), which is appropriate if the
  conditional response approximates any of the distributions that can be
  modeled using GLM (Chapter xxx)
\item
  use bootstrap for confidence intervals and permutation test for
  \emph{p}-values
\item
  transform the data in a way that makes the conditional response more
  closely approximate a normal distribution.
\item
  use a classic non-parametric test, which are methods that do not
  assume a particular distribution
\end{enumerate}

This list is roughly in the order of how I would advise researchers,
although the order of 1-3 is pretty arbitrary. I would rarely advise a
researcher to use (4) and never advise (5). Probably the most common
strategies in the biology literature are (4) and (5). The first is also
common but probably more from lack of recognition of issues or because a
``test of normality'' failed to reject that the data are ``not normal''.

On this last point, do not use the \emph{p}-value from a ``test for
normality'' (such as a Shapiro-Wilk test) to decide between using the
linear model (or t-test or ANOVA) and an alternative such as a
generalized linear model (or transformation or non-parametric test). No
real data is normal. Tests of normality will tend to ``not reject''
normality (p \textgreater{} 0.05) when the sample size is small and
``reject'' normality (p \textless{} 0.05) when the sample size is very
large. But again, a ``not rejected'' hypothesis test does not mean the
null (in this case, the data are normal) is true. More importantly,
where the test for normality tends to fail to reject (encouraging a
researcher to use parametric statistics) is where parametric inference
performs the worst (because of small \emph{n}) and where the test for
normality tends to reject (encouraging a researcher to use
non-parametric statistics) is where the parametric inference performs
the best (because of large sample size) (Lumley xxx).

\subsection{Working in R}\label{working-in-r-2}

The data for demonstrating different strategies are from Fig. 4A of
``Data from The enteric nervous system promotes intestinal health by
constraining microbiota composition''. There is a single factor with two
treatment levels. The response is neutrophil count.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/best-import-non-normal-counts-1.pdf}
\caption{\label{fig:best-import-non-normal-counts}Distribution of the counts
in the wildtype (WT) and sox10 knockout (sox10-) groups. Both groups
show a strong right skew, which is common with count data.}
\end{figure}

A linear model to estimate the treatment effect and 95\% confidence
interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fig4a)}
\NormalTok{m1_emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(m1_emm, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast   estimate   SE  df lower.CL upper.CL t.ratio p.value
##  sox10 - wt     5.16 1.75 174      1.7     8.62 2.947   0.0037 
## 
## Confidence level used: 0.95
\end{verbatim}

\subsection{Bootstrap Confidence
Intervals}\label{bootstrap-confidence-intervals}

A bootstrap confidence interval is computed from the distribution of a
statistic from many sets of re-sampled data. The basic algorithm is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  compute the statistic for the observed data, assign this to
  \(\theta_1\)
\item
  resample \(n\) rows of the data, with replacement. ``with
  replacement'' means to sample from the entire set of data and not the
  set that has yet to be sampled. \(n\) is the original sample size; by
  resampling \(n\) rows with replacement, some rows will be sampled more
  than once, and some rows will not be sampled at all.
\item
  compute the statistic for the resampled data, assign these to
  \(\theta_{2..m}\)
\item
  repeat 2 and 3 \(m-1\) times
\item
  Given the distribution of \(m\) estimates, compute the lower interval
  as the \(\frac{\alpha}{2}\)th percentile and the upper interval as the
  \(1 - \frac{\alpha}{2}\)th percentile. For 95\% confidence intervals,
  these are the 2.5th and 97.5th percentiles.
\end{enumerate}

Let's apply this algorithm to the data from fig4A neutrophil count data
in the coefficient table above. The focal statistic in these data is the
difference in the mean count for the sox10 and wild type groups (the
parameter for \(treatment\) in the linear model). The script below,
which computes the 95\% confidence intervals of this difference,
resamples within \textbf{strata}, that is, within each group; it does
this to preserve the original sample size within each group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{5000}
\NormalTok{b1 <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(}\DecValTok{5000}\NormalTok{)}
\NormalTok{inc <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(fig4a) }\CommentTok{# the rows for the first iteration are all rows, so this is the observed effect}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{}
  \CommentTok{# inc creates the index of rows to resample preserving the sample size specific to each group}
\NormalTok{  b1[i] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{lm}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fig4a[inc, ]))[}\StringTok{"treatmentsox10"}\NormalTok{]}
\NormalTok{  inc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{which}\NormalTok{(fig4a[, treatment] }\OperatorTok{==}\StringTok{ "wt"}\NormalTok{), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{),}
           \KeywordTok{sample}\NormalTok{(}\KeywordTok{which}\NormalTok{(fig4a[, treatment] }\OperatorTok{==}\StringTok{ "sox10"}\NormalTok{), }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{\}}
\NormalTok{ci <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(b1, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\KeywordTok{c}\NormalTok{(}\DataTypeTok{contrast =}\NormalTok{ b1[}\DecValTok{1}\NormalTok{], ci[}\DecValTok{1}\NormalTok{], ci[}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## contrast     2.5%    97.5% 
## 5.163215 2.077892 8.264316
\end{verbatim}

The intervals calculated in step 5 are \textbf{percentile intervals}. A
histogram of the the re-sampled differences helps to visualize the
bootstrap (this is a pedagogical tool, not something you would want to
publish).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/best-bootstrap-histogram-1.pdf}
\caption{\label{fig:best-bootstrap-histogram}Distribution of the 5000
resampled estimates of the difference in means between the sox10 and wt
treatment levels. The dashed lines are located at the 2.5th and 97.5th
percentiles of the distribution.}
\end{figure}

\subsubsection{Some R packages for bootstrap confidence
intervals}\label{some-r-packages-for-bootstrap-confidence-intervals}

Percentile intervals are known to be biased, meaning the intervals are
shifted. The \texttt{boot} package computes a bias-corrected interval in
addition to a percentile interval. \texttt{boot} is a very powerful
bootstrap package but requires the researcher to write functions to
compute the parameter of interest. \texttt{simpleboot} provides
functions for common analysis that does this for you (in R speak, we say
that \texttt{simpleboot} is a ``wrapper'' to \texttt{boot}). The
function \texttt{simpleboot::two.boot} computes a \texttt{boot}-like
object that returns, among other values, the distribution of \(m\)
statistics. The \texttt{simpleboot} object is then be fed to
\texttt{boot::boot.ci} to get bias-corrected intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bs_diff <-}\StringTok{ }\KeywordTok{two.boot}\NormalTok{(fig4a[treatment}\OperatorTok{==}\StringTok{"sox10"}\NormalTok{, count],}
\NormalTok{                    fig4a[treatment}\OperatorTok{==}\StringTok{"wt"}\NormalTok{, count],}
\NormalTok{                    mean, }
                    \DataTypeTok{R=}\DecValTok{5000}\NormalTok{)}
\KeywordTok{boot.ci}\NormalTok{(bs_diff, }\DataTypeTok{type=}\StringTok{"bca"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 5000 bootstrap replicates
## 
## CALL : 
## boot.ci(boot.out = bs_diff, type = "bca")
## 
## Intervals : 
## Level       BCa          
## 95%   ( 2.087,  8.410 )  
## Calculations and Intervals on Original Scale
\end{verbatim}

\subsection{Permutation test}\label{permutation-test}

A permutation test effectively computes the probability that a random
assignment of a response to a particular value of \emph{X} generates a
test statistic as large or larger than the observed statistic. If this
probability is small, then this ``random assignment'' is unlikely. From
this we infer that the actual assignment matters, which implies a
treatment effect.

The basic algorithm is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  compute the test statistic for the observed data, assign this to
  \(\theta_1\)
\item
  permute the response
\item
  compute the test statistic for the permuted data, assign these to
  \(\theta_{2..m}\)
\item
  repeat 2 and 3 \(m-1\) times
\item
  compute \(p\) as
\end{enumerate}

\begin{equation}
p_{perm} = \frac{N_{\theta_i \ge \theta_{1}}}{m}
\end{equation}

This is easily done with a \textbf{for loop} in which the observed
statistic is the first value in the vector of statistics. If this is
done, the minimum value in the numerator for the computation of
\(p_{perm}\) is 1, which insures that \(p_{perm}\) is not zero.

The test statistic depends on the analysis. For the simple comparison of
means, a simple test statistic is the difference in means. This is the
numerator of the test statistic in a \emph{t}-test. The test has more
power if the test-statistic is scaled (Manley xxx), so a better test
statistic would be \emph{t}, which scales the difference by its standard
error.

Here, I implement this algorithm. The test is two-tailed, so the
absolute difference is recorded. The first value computed is the
observed absolute difference.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{n_permutations <-}\StringTok{ }\DecValTok{5000}
\NormalTok{d <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_permutations)}

\CommentTok{# create a new column which will contain the permuted response}
\CommentTok{# for the first iteration, this will be the observed order}
\NormalTok{fig4a[, count_perm }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\NormalTok{count]}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_permutations)\{}
\NormalTok{  d[i] <-}\StringTok{ }\KeywordTok{abs}\NormalTok{(}\KeywordTok{t.test}\NormalTok{(count_perm }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data =}\NormalTok{ fig4a)}\OperatorTok{$}\NormalTok{statistic)}
  
  \CommentTok{# permute the count_perm column for the next iteration}
\NormalTok{  fig4a[, count_perm }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{sample}\NormalTok{(count)]}
\NormalTok{\}}
\NormalTok{p <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(d }\OperatorTok{>=}\StringTok{ }\NormalTok{d[}\DecValTok{1}\NormalTok{])}\OperatorTok{/}\NormalTok{n_permutations}
\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002
\end{verbatim}

\subsubsection{Some R packages with permutation
tests.}\label{some-r-packages-with-permutation-tests.}

\texttt{lmPerm::lmp} generates permutation p-values for parameters of
any kind of linear model. The test statistic is the sum of squares of
the term scaled by the residual sum of squares of the model.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lmp}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{perm=}\StringTok{"Prob"}\NormalTok{, }\DataTypeTok{Ca=}\FloatTok{0.01}\NormalTok{, }
                 \DataTypeTok{data=}\NormalTok{fig4a)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Settings:  unique SS "
\end{verbatim}

\begin{verbatim}
##              Estimate Iter Pr(Prob)
## (Intercept) 13.694815 5000   0.0042
## treatment1  -2.581608 5000   0.0042
\end{verbatim}

\subsection{Non-parametric tests}\label{non-parametric-tests}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In general, the role of a non-parametric test is a better-behaved
  \emph{p}-value, that is, one whose Type I error is well controlled. As
  such, non-parametric tests are more about Null-Hypothesis Statistical
  Testing and less (or not at all) about Estimation.
\item
  In general, classic non-parametric tests are only available for fairly
  simple experimental designs. Classic non-parametric tests include

  \begin{itemize}
  \tightlist
  \item
    Independent sample (Student's) \emph{t} test: Mann-Whitney-Wilcoxan
  \item
    Paired \emph{t} test: Wilcoxan signed-rank test
  \end{itemize}
\end{enumerate}

One rarely sees non-parametric tests for more complex designs that
include covariates, or multiple factors, but for these, one could 1)
convert the response to ranks and fit the usual linear model, or 2)
implement a permutation test that properly preserves
\textbf{exchangeability}.

Permutation tests control Type I error and are powerful. That said, I
would recommend a permutation test as a supplment to, and not
replacement of, inference from a generalized linear model.

A non-parametric (Mann-Whitney-Wilcoxon) test of the fake data generated
above

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{wilcox.test}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fig4a)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  count by treatment
## W = 2275, p-value = 0.001495
## alternative hypothesis: true location shift is not equal to 0
\end{verbatim}

\subsection{Log transformations}\label{log-transformations}

Many response variables within biology, including count data, and almost
anything that grows, are right skewed and have variances that increase
with the mean. A log transform of a response variable with this kind of
distribution will tend to make the residuals more approximately normal
and the variance less dependent of the mean. At least two issues arise

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  if the response is count data, and the data include counts of zero,
  then a fudge factor has to be added to the response since log(0)
  doesn't exist. The typical fudge factor is to add 1 to \emph{all}
  values, but this is arbitrary and results do depend on the magnitude
  of this fudge factor.
\item
  the estimates are on a log scale and do not have the units of the
  response. The estimates can be back-transformed by taking the exponent
  of a coefficient or contrast but this itself produces problems. For
  example, the backtransformed mean of the log-transformed response is
  not the mean on the origianl scale (the arithmetic mean) but the
  \textbf{geometric mean}. Geometric means are smaller than arithmetic
  means, appreciably so if the data are heavily skewed. Do we want our
  understanding of a system to be based on geometric means?
\end{enumerate}

\subsubsection{Working in R -- log
transformations}\label{working-in-r-log-transformations}

If we fit a linear model to a log-transformed response then the
resulting coefficients and predictions are on the \textbf{log scale}. To
make interpretation of the analysies easier, we probably want to
\textbf{back-transform} the coefficients or the predictions to the
original scale of the response, which is called the \textbf{response
scale}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(count }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fig4a)}
\NormalTok{(m2_emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m2,}
                  \DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{,}
                  \DataTypeTok{type =} \StringTok{"response"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  treatment response    SE  df lower.CL upper.CL
##  wt            8.22 0.965 174      6.5     10.3
##  sox10        12.59 0.934 174     10.9     14.6
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log(mu + 1) scale
\end{verbatim}

The emmeans package is amazing. Using the argument
\texttt{type\ =\ "response"} not only backtransforms the means to the
response scale but also substracts the 1 that was added to all values in
the model.

What about the effect of treatment on count?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(m2_emm, }
                 \DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{,}
                 \DataTypeTok{type =} \StringTok{"response"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast   ratio    SE  df lower.CL upper.CL t.ratio p.value
##  sox10 / wt  1.47 0.185 174     1.15     1.89 3.100   0.0023 
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale 
## Tests are performed on the log scale
\end{verbatim}

It isn't necessary to backtransform the estimated marginal means prior
to computing the contrasts as this can be done in the contrast function
itself. Here, the \texttt{type\ =\ "response"} argument in the contrast
function is redundant since this was done in the computation of the
means. But it is transparent so I want it there.

\textbf{Don't skip this paragraph} Look at the value in the ``contrast''
column -- it is ``sox10 / wt'' and not ``sox10 - wt''. The
backtransformed effect is a ratio instead of a difference. \textbf{A
difference on the log scale is a ratio on the response scale} because of
this equality

\begin{equation}
\mathrm{exp}(\mu_2-\mu_1) = \frac{\mathrm{exp}(\mu_2)}{\mathrm{exp}(\mu_1)})
\end{equation}

The interpretation is: If \(b^*\) is the backtransformed effect, then,
given a one unit increase in \(X\), the expected value of the response
increases \(b^*\times\). For a categorical \(X\), this means the
backtransformed effect is the ratio of backtransformed means -- its what
you have to multiply the mean of the reference by to get the mean of the
treated group. And, because it is the response that is log-transformed,
these means are not arithemetic means but geometric means. Here, this is
complicated by the model -- the response is not a simple log
transformation but log(response + 1). It is easy enough to get the
geometric mean of the treated group -- multiply the backtransformed
intercept by the backtransformed coefficient and then subtract 1 -- but
because of this subtraction of 1, the interpretation of the
backtransformed effect is awkward at best (recall that I told you that a
linear model of a log transformed response, and especially the log of
the response plus one, leads to difficulty in interpreting the effects).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# backtransformed control mean -- a geometric mean}
\NormalTok{mu_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(m2)[}\DecValTok{1}\NormalTok{])}

\CommentTok{# backtransformed effect}
\NormalTok{b1_star <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(m2)[}\DecValTok{2}\NormalTok{])}

\CommentTok{# product minus 1}
\NormalTok{mu_}\DecValTok{1}\OperatorTok{*}\NormalTok{b1_star }\OperatorTok{-}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## (Intercept) 
##    12.59357
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# geometric mean of treatment group}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(fig4a[treatment}\OperatorTok{==}\StringTok{"sox10"}\NormalTok{, count])}
\KeywordTok{exp}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{log}\NormalTok{(fig4a[treatment}\OperatorTok{==}\StringTok{"sox10"}\NormalTok{, count}\OperatorTok{+}\DecValTok{1}\NormalTok{])))}\OperatorTok{-}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.59357
\end{verbatim}

Back-transformed effect

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fig4a)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{exp}\NormalTok{(}\KeywordTok{coef}\NormalTok{(m2)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    (Intercept) treatmentsox10 
##       9.219770       1.474394
\end{verbatim}

\subsection{Performance of parametric tests and
alternatives}\label{performance-of-parametric-tests-and-alternatives}

\subsubsection{Type I error}\label{type-i-error}

If we are going to compute a \(p\)-value, we want it to be uniformly
distributed ``under the null''. A simple way to check this is to compute
Type I error. If we set \(\alpha = 0.05\), then we'd expect 5\% of tests
of an experiment with no effect to have \(p < 0.05\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first create a matrix with a bunch of data sets, each in its own column}
\NormalTok{n <-}\StringTok{ }\DecValTok{10}
\NormalTok{n_sets <-}\StringTok{ }\DecValTok{4000}
\NormalTok{fake_matrix <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnegbin}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_sets, }\DataTypeTok{mu=}\DecValTok{10}\NormalTok{, }\DataTypeTok{theta=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n),}
                   \KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnegbin}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_sets, }\DataTypeTok{mu=}\DecValTok{10}\NormalTok{, }\DataTypeTok{theta=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n))}
\NormalTok{treatment <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{, }\StringTok{"tr"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n)}

\NormalTok{tests <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"lm"}\NormalTok{, }\StringTok{"log_lm"}\NormalTok{,}\StringTok{"mww"}\NormalTok{, }\StringTok{"perm"}\NormalTok{)}
\NormalTok{res_matrix <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_sets, }\DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(tests))}
\KeywordTok{colnames}\NormalTok{(res_matrix) <-}\StringTok{ }\NormalTok{tests}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_sets)\{}
\NormalTok{  res_matrix[j, }\StringTok{"lm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment}
\NormalTok{                                 )))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  res_matrix[j, }\StringTok{"log_lm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(fake_matrix[,j] }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{treatment}
\NormalTok{                                 )))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  res_matrix[j, }\StringTok{"mww"}\NormalTok{] <-}\StringTok{ }\KeywordTok{wilcox.test}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment,}
                                      \DataTypeTok{exact=}\OtherTok{FALSE}\NormalTok{)}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{  res_matrix[j, }\StringTok{"perm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lmp}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment,}
                                 \DataTypeTok{perm=}\StringTok{"Prob"}\NormalTok{, }\DataTypeTok{Ca=}\FloatTok{0.01}\NormalTok{)))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(Prob)"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(res_matrix, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}\NormalTok{)}\OperatorTok{/}\NormalTok{n_sets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      lm  log_lm     mww    perm 
## 0.04150 0.05250 0.04350 0.04675
\end{verbatim}

Type I error is computed for the linear model, the linear model with a
log transformed responpse, Mann-Whitney-Wilcoxon, and permutation tests.
All four tests are slightly conservative for data that look like that
modeled. The computed Type I error of the permutation test is closest to
the nominal value of 0.05.

\subsubsection{Power}\label{power-1}

Power is the probability of a test to reject the null hypothesis if the
null hypothesis is false (that is, if an effect exists)

\begin{equation}
\mathrm{Power} = \mathrm{Prob}(p < \alpha | mathrm{effect} \neq 0)
\end{equation}

If all we care about is a \(p-value\) then we want to use a test that is
most powerful. But, while power is defined using \(\alpha\), we
\emph{can} care about power even if we don't consider \(\alpha\) to be a
very useful concept because increased power also increases the precision
of an estimate (that is, narrows confidence intervals).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# first create a matrix with a bunch of data sets, each in its own column}
\NormalTok{n <-}\StringTok{ }\DecValTok{5}
\NormalTok{n_sets <-}\StringTok{ }\DecValTok{4000}
\NormalTok{fake_matrix <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnegbin}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_sets, }\DataTypeTok{mu=}\DecValTok{10}\NormalTok{, }\DataTypeTok{theta=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n),}
                   \KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnegbin}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_sets, }\DataTypeTok{mu=}\DecValTok{20}\NormalTok{, }\DataTypeTok{theta=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n))}
\NormalTok{treatment <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{, }\StringTok{"tr"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n)}

\NormalTok{tests <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"lm"}\NormalTok{, }\StringTok{"log_lm"}\NormalTok{,}\StringTok{"mww"}\NormalTok{, }\StringTok{"perm"}\NormalTok{)}
\NormalTok{res_matrix <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_sets, }\DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(tests))}
\KeywordTok{colnames}\NormalTok{(res_matrix) <-}\StringTok{ }\NormalTok{tests}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_sets)\{}
\NormalTok{  res_matrix[j, }\StringTok{"lm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment}
\NormalTok{                                 )))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  res_matrix[j, }\StringTok{"log_lm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(fake_matrix[,j] }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{~}\StringTok{ }\NormalTok{treatment}
\NormalTok{                                 )))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  res_matrix[j, }\StringTok{"mww"}\NormalTok{] <-}\StringTok{ }\KeywordTok{wilcox.test}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment,}
                                      \DataTypeTok{exact=}\OtherTok{FALSE}\NormalTok{)}\OperatorTok{$}\NormalTok{p.value}
\NormalTok{  res_matrix[j, }\StringTok{"perm"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lmp}\NormalTok{(fake_matrix[,j] }\OperatorTok{~}\StringTok{ }\NormalTok{treatment,}
                                 \DataTypeTok{perm=}\StringTok{"Prob"}\NormalTok{, }\DataTypeTok{Ca=}\FloatTok{0.01}\NormalTok{)))[}\DecValTok{2}\NormalTok{, }\StringTok{"Pr(Prob)"}\NormalTok{]}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(res_matrix, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x }\OperatorTok{<}\StringTok{ }\FloatTok{0.05}\NormalTok{)}\OperatorTok{/}\NormalTok{n_sets)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      lm  log_lm     mww    perm 
## 0.09200 0.12525 0.08375 0.10600
\end{verbatim}

As above, Power is computed for the linear model, linear model with a
log-transformed response, Mann-Whitney-Wilcoxan, and permutation, by
simulating a ``low power'' experiment. The effect is huge (twice as many
cells) but the power is low because the sample size is small
(\(n = 5\)). At this sample size, and for this model of fake data, all
tests have low power. The power of the log-transformed response is the
largest. A problem is, this is not a test of the means but of the log
transformed mean plus 1. The power of the permutation test is about 25\%
larger than that of the linear model and Mann-Whitney-Wilcoxan test. An
advantage of this test is that it is a p-value of the mean. A good
complement to this p-value would be bootstraped confidence intervals.
Repeat this simulation using \(n=40\) do see how the relative power
among the three change in a simulation of an experiment with more power.

\section{max vs.~mean}\label{max-vs.mean}

\section{pre-post, normalization}\label{pre-post-normalization}

\chapter{Plotting Models}\label{plotting-models}

\emph{So, along the lines of Sarah Susanka's ``Not So Big House,''
Kolbert asks the group, ``What would a Pretty Good House look like?''}
-- Michael Maines\footnote{``The Pretty Good House - Finding the right
  balance between construction cost and energy performance''.
  \url{https://www.greenbuildingadvisor.com/article/the-pretty-good-house}}

When it comes to plotting, many researchers mindlessly generate plots
that are easily generated by the software and look like the typical
plots published in the field. The resulting plot is comforting because
it is familiar, not because it effectively communicates what a good plot
should communicate -- the model results.

Plots should be the focus of both the reader and researcher. Instead of
mindless plotting, a researcher should ask a series of questions of
every plot

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the point of each element in a plot?
\item
  Are these the points that I most want to communicate?
\item
  Are there better practices for communicating these points?
\item
  Are the points that I want to communicate that are not covered by
  these elements?
\end{enumerate}

The answer to these questions should inform what is and what is not
plotted. The result is a pretty good plot. The idea of a pretty good
plot is borrowed from the ``pretty good house'' concept that grew out of
a collaborative group of builders and architects in Northern New
England. The ``pretty good house'' combines best practices for building
an earth friendly, high performance home at a reasonable cost. There is
no pretty good house governing body that awards certificates of
achievement but, instead, a set of metrics and a collection of building
practices that can achieve these.

A typical pretty good plot contains some combination of

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modeled effects with confidence intervals. ``Effects'' are the
  coefficients of a model, or contrasts constructed from the model, such
  as pairwise differences between the means of the levels of a factor.
  Inferences are typically made from the estimated effects
\item
  Modeled means and standard errors or confidence intervals.
\item
  Raw data points or a summary distribution of these.
\end{enumerate}

\section{Pretty good plots show the model and the
data}\label{pretty-good-plots-show-the-model-and-the-data}

The data to introduce best practices in plotting come from ``The enteric
nervous system promotes intestinal health by constraining microbiota
composition''\footnote{Rolig, A.S., Mittge, E.K., Ganz, J., Troll, J.V.,
  Melancon, E., Wiles, T.J., Alligood, K., Stephens, W.Z., Eisen, J.S.
  and Guillemin, K., 2017. The enteric nervous system promotes
  intestinal health by constraining microbiota composition. PLoS
  biology, 15(2), p.e2000689}. The researchers found that zebrafish with
a \emph{sox10} mutation lacked an enteric nervous system and developed a
microbiota-dependent inflammation. The paper includes several
experiments to probe the hypothesis that the ENS regulates microbial
community composition and, in turn, inflammatory status. The data here
are from Fig. 2 of the paper, which reports the results of one of a set
of experiments to test the hypothesis that microbiota from \emph{sox10}
mutants (that induce inflammation) are necessary and sufficient to
induce inflammation in wildtype guts. In this experiment, homogenized
intestines and their microbial community from four different donor
groups were added to the flasks housing the zebrafish. The response
variable is neutrophil count. Neutrophils are a white blood cell that
increase in number during inflammation. The four treatment levels are
the different donors of intestinal microbes: wt (wild type), gf (germ
free, so no microbes are transferred), iap\_mo (a control ``for the
possibility that nonbacterial factors such as host pro-inflammatory
cytokines rather than microbial derived factors cause transmissible
intestinal inflammation''), and sox10.

\subsection{Pretty good plot component 1: Modeled effects
plot}\label{pretty-good-plot-component-1-modeled-effects-plot}

Biologists infer the biological consequences of a treatment by
interpreting the magnitude and sign of treatment ``effects'', such as
the differences in means among treatment levels. Why then do we mostly
plot treatment level means, where effects can only be inferred
\emph{indirectly}, by mentally computing differences in means? Instead,
our primary plots should be effects plots, which \emph{directly}
communicate treatment effects, and the uncertainty in the estimates of
these effects.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plots-effect-1.pdf}
\caption{\label{fig:plots-effect}Effects Plot}
\end{figure}

The y-axis contains all pairwise comparisons among the four treatment
levels. The x-axis is the response, which is the ratio of the means of
the two groups in the comparison. For example, the top comparison shows
that guts in fish exposed to sox10 donors have 2.7X more neutrophils per
length of gut than guts in fish exposed to wild type donors. The bars
are 95\% confidence intervals, with is the range of effects that are
compatible with the observed data at the 95\% level (confidence
intervals are disscussed in depth in chapter xxx.). The small end of the
interval for the sox10/wt comparison is 1.31, meaning that effects as
small as 31\% increased neutrophil count are compatible with the data.
It is up to the research community to decide if 2.7X or 1.31X are
physiologically meaningful effects. \emph{p}-values from the hypothesis
tests are included.

\subsection{Pretty good plot component 2: Modeled mean and CI
plot}\label{pretty-good-plot-component-2-modeled-mean-and-ci-plot}

Often the means of the treatment levels are meaningful, for example, if
neutrophils per length of gut is a standard measure then researchers
working in this area will be familiar with usual and unusal values. The
data used in Fig \ref{fig:plots-effect} are used to plot means and
confidence intervals of the mean using a \textbf{bar chart}, which is a
pretty good chart type for measures such as counts in which negative
values are prohibited and zero is meaningful.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plots-means-1.pdf}
\caption{\label{fig:plots-means}Mean and error plot}
\end{figure}

Fig. \ref{fig:plots-means} plots the \emph{modeled} means, represented
by the tops of the bars, the modeled 95\% confidence intervals of each
mean, represented by the error bars, and the \emph{p}-values for all
pairwise comparisons. What do I mean by \emph{modeled} means and error
intervals?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Modeled means and error intervals are estimated from the statistical
  model. Many published plots are of raw means and error intervals,
  meaning that the mean and error for each treatment level is computed
  only using the response measures in that treatment level.
\item
  A modeled mean will often be equal to the raw mean, but this will not
  always be the case, for example if there are covariates in the model
  (Chapter xxx).
\item
  Modeled error intervals are never the same as the raw error intervals,
  and are commonly conspicuously different. Almost always, we should
  plot modeled means and error intervals, since these represent the
  statistics that are relevant to inference.
\end{enumerate}

Fig. \ref{fig:plots-means} also plots the raw count data as ``jittered''
black dots. ``Showing the data'' is a pretty good feature of a plot
because it allows the reader to get a sense of the underlying sample
size and distribution including outliers, which can be used to mentally
model check the published statistical analysis. For example, the
jittered dots in Fig. \ref{fig:plots-means} suggest a
\textbf{heterogeneity} of variances; specifically, the treatment level
with the largest mean has a conspicuously higher variance. This pattern
violates the assumptions of a general linear model and should raise a
red flag to a reader if the researchers used a general linear model to
analyze the data.

What a mean-and-error plot fails to show, at least directly, are the
effects. To infer the effects from the plot, a reader must perform
mental math -- either compute the difference or the ratio between pairs
of means. This mental math is easy enough if the comparisons are between
individual treatment levels but much harder if the comparisons are
between pooled sets of treatment levels, for example in a factorial
experimental design. The mental math that is excessively difficult is
the reconstruction of some kind of error interval of the contrasts, for
example the 95\% confidence intervals in Fig. \ref{plots-effect} and it
is this interval that is necessary for a researcher to infer the range
of biological consequences that are compatible with the experiment. The
inclusion of the \emph{p}-values for all pairwise comparisons gives the
significance level of these contrasts, but of the kinds of summary
results that we could present (contrasts, error intervals,
\emph{p}-values), the \emph{p}-values are the least informative.

\subsection{Combining Effects and Modeled mean and CI plots -- an
Effects and response
plot.}\label{combining-effects-and-modeled-mean-and-ci-plots-an-effects-and-response-plot.}

If one wants to show both effects and the data, then these can be
combined.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plots-harrellplot-1.pdf}
\caption{\label{fig:plots-harrellplot}A pretty good plot}
\end{figure}

If the means do not have any importance in understanding the results,
the effects plot can be combined with some kind of a plot summarizing
the distribution, such as a boxplot.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plots-harrelplot2-1.pdf}
\caption{\label{fig:plots-harrelplot2}Another pretty good plot}
\end{figure}

Regardless, the effects plot is the most important component as this is
the illustration of the story a researcher wants to tell.

\section{Some comments on plot
components}\label{some-comments-on-plot-components}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Alternatives to barplots make good plots for the supplement,
  not the main paper}. A prominent trend over the last few years has
  been the replacement of bar plots with plots that ``show the data'',
  such as jitter plots or dot plots, or that show summaries of the
  distribution, such as box plots or violin plots. These plot types were
  developed for exploratory data analysis, not to communicate the
  results of experiments. All of these plots fail to communicate the
  results of the statistical model and, because of this, are inferior to
  an effects plot, and even a mean-and-error plot, if the mean and error
  are the modeled values. Box/Violoin/Dot/Jitter plots are a useful
  supplement to an effects plot, either combined with the effects plot
  as above, or as a supplementary figure.
\item
  Standard error bars, computed from the raw data, can have absurd
  implications. For example, I sometimes see standard error bars cross
  \(y=0\) for a response that cannot be negative, such as a count. Even
  if the standard error bar doesn't cross zero, it is common to see
  standard error bars that imply (but do not explicitly show) 95\%
  confidence intervals that cross zero, again for responses that cannot
  be negative. A standard error bar or confidence interval that crosses
  zero implies that negative means are compatible with the data. This is
  an absurd implication for responses that cannot have negative values
  (or are ``bounded by'' zero). Explicit or implicit error bars that
  cross zero are especially common for count responses with small means.
  \emph{If} a researcher plots confidence intervals, these should be
  computed using a method that avoids absurd implications, such methods
  include the bootstrap and generalized linear models.
\item
  \textbf{Stars add minimal value}. Many researchers add star symbols to
  a plot indicating the level of significance of a particular paired
  comparison. An uncommon, but better, alternative would be to add the
  actual p-value (as above). Adding a p-value (or stars) does
  communicate model results, and so adds value to a mean-and-error or
  box/violin/jitter plot. However, much more value would be added by
  simply reporting an effects plot or a combined effects-and-response
  plot.
\end{enumerate}

\section{Working in R}\label{working-in-r-3}

A reasonable goal of any research project should be a script to generate
the final plots entirely within the R environment and not rely on
external drawing software to add finishing features.
\href{https://ggplot2.tidyverse.org}{ggplot2} is one of the major
plotting environments in R and the one that seems to have the strongest
following, especially among new R users. ggplot2 has the ability to
generate extremely personalized and finished plots. However, creating a
plot with multiple layers (bars, lines, error intervals, raw data
points, p-values, text annotations) can often require many hours of
googling.

\href{https://cran.r-project.org/web/packages/ggpubr/index.html}{ggpubr}
is an extension to ggplot2 (it calls ggplot2 functions under the hood)
and provides many canned functions for producing the kinds of ggplots
that are published in biological journals. With one line of script, a
researcher can generate a publishable plot that is as good or better
than many published plot.

\textbf{Here I show how to add custom (ggplot2) features to a ggpubr
plot}

Throughout this book, ggpubr is used to create a basic plot and then
additional features are added to the basic plot using ggplot2 functions.

\subsection{Unpooled SE bars and confidence
intervals}\label{unpooled-se-bars-and-confidence-intervals}

\texttt{ggplot2} and \texttt{ggpubr} default to unpooled error intervals
(standard error bars and confidence intervals).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg1 <-}\StringTok{ }\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ exp2d,}
                 \DataTypeTok{x =} \StringTok{"donor"}\NormalTok{, }
                 \DataTypeTok{y =} \StringTok{"count"}\NormalTok{, }
                 \DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{),}
                 \DataTypeTok{fill =} \StringTok{"steelblue"}
\NormalTok{)}
\NormalTok{gg2 <-}\StringTok{ }\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ exp2d,}
                 \DataTypeTok{x =} \StringTok{"donor"}\NormalTok{, }
                 \DataTypeTok{y =} \StringTok{"count"}\NormalTok{, }
                 \DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_ci"}\NormalTok{),}
                 \DataTypeTok{fill =} \StringTok{"steelblue"}
\NormalTok{)}
\KeywordTok{plot_grid}\NormalTok{(gg1, gg2, }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{, }\DataTypeTok{labels=}\StringTok{"AUTO"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plots-unpooled-1.pdf}
\caption{\label{fig:plots-unpooled}(A) Mean and 1 SE error bar. (B) Mean and
95\% CI.}
\end{figure}

\subsection{Adding bootstrap
intervals}\label{adding-bootstrap-intervals}

A bootstrap CI uses resamples of the data to estimate the interval and
is a better choice than the default CI for data such as counts and
proportions. The plot below uses ggpubr to create a stripchart of the
data and the color of the data points are ``de-emphasized'' -- in order
to emphasize the mean and CI -- by making them more transparent (using
the argument \texttt{alpha}). \texttt{alpha} is added before the
argument to add the mean in order to no de-emphasize the mean.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{gg.boot <-}\StringTok{ }\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{exp2d,}
                   \DataTypeTok{x =} \StringTok{"donor"}\NormalTok{, }
                   \DataTypeTok{y =} \StringTok{"count"}\NormalTok{, }
                   \DataTypeTok{alpha =} \FloatTok{0.4}\NormalTok{,}
                   \DataTypeTok{add =} \StringTok{"mean"}
\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{stat_summary}\NormalTok{(}\DataTypeTok{fun.data =} \StringTok{"mean_cl_boot"}\NormalTok{, }
               \DataTypeTok{geom =} \StringTok{"errorbar"}\NormalTok{, }
               \DataTypeTok{width =} \FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}


\NormalTok{gg.boot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plot-boot-plot-1.pdf}
\caption{\label{fig:plot-boot-plot}Sample means with bootstrapped 95\%
confidence intervals.}
\end{figure}

\subsection{Adding modeled means and error
intervals}\label{adding-modeled-means-and-error-intervals}

This section is extremely important for implementing the work flow
advocated in this text. The goal is to plot the modeled means with some
sort of error interval, typically a confidence interval, \emph{and} to
show the data or a summary of the data in a single plot. The procedure
is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  fit the model
\item
  use the fit model to estimate the modeled means and confidence limits
  using \texttt{emmeans} from the
  \href{https://cran.r-project.org/web/packages/emmeans/index.html}{emmeans
  package}.
\item
  use the \texttt{emmean} object to estimate the contrasts of interests
  using the \texttt{contrast} function from emmeans.
\item
  Use the objects from steps 2 and 3 to plot the modeled means
\end{enumerate}

\textbf{Step 1: Fit the model}. A negative binomial, generalized linear
model with log-link is fit to the count data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{glm.nb}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{donor, }\DataTypeTok{data=}\NormalTok{exp2d)}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Estimate Std. Error    z value     Pr(>|z|)
## (Intercept)  0.9873867  0.2229971  4.4278004 9.519895e-06
## donorgf     -0.8203326  0.4227008 -1.9406930 5.229553e-02
## donoriap_mo -0.1544775  0.3878578 -0.3982839 6.904209e-01
## donorsox10   1.0091672  0.2862047  3.5260325 4.218353e-04
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The estimates and SE are on the \textbf{link scale}, which means they
  are in log-transformed space (or ``log space''). Exponentiate these
  with exp(x) to \textbf{backstransform} these to the the
  \textbf{response scale} which is the scale of the measurement (number
  of neutrophils).
\end{itemize}

\textbf{Step 2: Estimate the modeled means and confidence levels}. The
second step is to pass the fit model object (m1) to \texttt{emmeans} to
estimate the modeled means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\StringTok{"donor"}\NormalTok{, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{m1.emm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  donor  response    SE  df asymp.LCL asymp.UCL
##  wt         2.68 0.599 Inf     1.734      4.16
##  gf         1.18 0.424 Inf     0.585      2.39
##  iap_mo     2.30 0.730 Inf     1.235      4.28
##  sox10      7.36 1.321 Inf     5.181     10.47
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale
\end{verbatim}

\begin{itemize}
\tightlist
\item
  We specify the means that we want to estimate with ``specs =''. Here,
  we want to estimate the means of the levels of \(donor\).
\item
  Because the linear predictor of the model is on the log scale, we use
  the ``type'' argument to specify that we want the means to be
  backtransformed to the \textbf{response scale}, which is the scale of
  the measurement (number of cells)
\item
  It can be useful to convert the emmeans table m1.emm to a data.table
  (or data.frame or tibble) using
  \texttt{m1.emm\ \textless{}-\ data.table(m1.emm)}. \textbf{Bug alert}
  If you do this, the object cannot be passed to the next step, the
  \texttt{contrast} function. So if you want the emmeans table as a
  data.table, assign it to a different name, for example
  \texttt{m1.emm\_dt\ \textless{}-\ data.table(m1.emm)}.
\end{itemize}

\textbf{Step 3: Compute the contrasts, with p-values and confidence
levels}. Contrasts among levels, or combinations of levels, are computed
by passing the emmeans object (m1.emm) to the \texttt{contrast}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.pairs <-}\StringTok{ }\KeywordTok{contrast}\NormalTok{(m1.emm, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summary}\NormalTok{(}\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m1.pairs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast       ratio    SE  df asymp.LCL asymp.UCL z.ratio p.value
##  gf / wt        0.440 0.186 Inf     0.192      1.01 -1.941  0.0523 
##  iap_mo / wt    0.857 0.332 Inf     0.401      1.83 -0.398  0.6904 
##  iap_mo / gf    1.946 0.933 Inf     0.761      4.98  1.389  0.1647 
##  sox10 / wt     2.743 0.785 Inf     1.566      4.81  3.526  0.0004 
##  sox10 / gf     6.231 2.501 Inf     2.837     13.68  4.558  <.0001 
##  sox10 / iap_mo 3.202 1.167 Inf     1.567      6.54  3.192  0.0014 
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale 
## Tests are performed on the log scale
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Here, we set ``method'' to ``revpairwise'' in order to compute
  contrasts among all pairs of levels of \(donor\). There are \(m = 4\)
  levels and so \(m(m-1)/2 = 6\) pairwise contrasts. ``revpairwise'' is
  used instead of ``pairwise'' because the former sets the direction of
  the contrasts that include the reference as non-reference level minus
  reference level.
\item
  I use the ``adjust'' argument to specify no \emph{p}-value adjustment
  for multiple tests.
\item
  the contrast object is then piped (\%\textgreater{}\%) to the summary
  function, where I pass to the argument ``infer'', that I want both the
  confidence intervals (the first TRUE) and \emph{p}-values (the second
  TRUE)
\item
  this step isn't necessary if we were plotting only modeled means and
  CIs but 1) we almost always want contrasts with a fit model and so
  that is done here as part of the uninterrupted work flow that this
  book advocates and 2) we do use the \emph{p}-values and CIs from this
  table (m1.pairs) in the final plot below.
\item
  \textbf{Bug alert} again, the emmeans table m1.emm must be passed to
  \texttt{contrast} as an emmeans object. If you have converted this
  object to a data.table, you will get an error. See the last note in
  Step 2.
\end{itemize}

\textbf{Step 4: Plot the modeled means and 95\% error intervals}.

The code below first creates the stripchart using the ggpubr function
and then adds the confidence intervals using \texttt{geom\_errorbar} and
means using \texttt{geom\_point}. The stripchart uses the data in the
exp2d data.table. The errorbar and mean use the values in m1.emm object
created by the \texttt{emmeans} function. The \texttt{geom\_errorbar}
and \texttt{geom\_point} functions require an ``aesthetic'' to tell
ggplot which column contains the y values of the points to plot (the
``x'' values are still in the column ``donor'', which is a column in
both the exp2d data.table and m1.emm). The name of the column containing
the ``y'' values in m1.emm is ``response''.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{gg.nb <-}\StringTok{ }\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{exp2d,}
                 \DataTypeTok{x=}\StringTok{"donor"}\NormalTok{, }
                 \DataTypeTok{y=}\StringTok{"count"}\NormalTok{,}
                 \DataTypeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Neutrophil count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{summary}\NormalTok{(m1.emm), }
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{response,}
                    \DataTypeTok{ymin=}\NormalTok{asymp.LCL, }
                    \DataTypeTok{ymax=}\NormalTok{asymp.UCL), }
                \DataTypeTok{width=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{summary}\NormalTok{(m1.emm), }
                \KeywordTok{aes}\NormalTok{(}\DataTypeTok{y=}\NormalTok{response), }
                \DataTypeTok{size=}\DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}

\NormalTok{gg.nb}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plot-nb-plot-1.pdf}
\caption{\label{fig:plot-nb-plot}Modeled means and 95\% confidence interval
computed from a negative binomial generalized linear model.}
\end{figure}

Some notes on the plot code

\begin{itemize}
\tightlist
\item
  A column name passed to a \texttt{ggpubr} function must be in quotes
  but a column name passed to a \texttt{ggplot2} function cannot be in
  quotes
\item
  \textbf{Bug alert}. The data passed to ggplot2 must be a data.frame.
  In order for the ggplot2 functions to use the m1.emm object, the
  object has to be passed as \texttt{summary(m1.emm)}.
\item
  \textbf{Bug alert}. Because the m1.emm table does not have a column
  named ``count'', which is the ``y'' column specified in
  \texttt{ggstripchart}, you must supply a new ``y'' column name to the
  \texttt{aes} function of \texttt{geom\_errorbar} and
  \texttt{geom\_point}. This is the name of the column in the emmeans
  table containing the modeled means. In m1.emm, this name is
  ``response'' but it can take different names in different emmeans
  tables, depending on the fit model.
\end{itemize}

\subsection{Adding p-values}\label{adding-p-values}

In this section, I show how to add \emph{p}-values to a ggpubr plot
using \texttt{stat\_compare\_means}. Because this function has only a
limited set of models that can be used to compute the \emph{p}-values, I
don't find it very useful and instead recommend adding custom
\emph{p}-values from the fit model (or from a permutation test) using
the method in the next section.

For this example, a ``t.test'' is used to compute the p-values. The mean
and error are the sample-based estimates because these, and not the
modeled estimates, are consistent with the t-test \emph{p}-values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compare_list <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"sox10"}\NormalTok{, }\StringTok{"iap_mo"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"sox10"}\NormalTok{, }\StringTok{"gf"}\NormalTok{), }\KeywordTok{c}\NormalTok{(}\StringTok{"sox10"}\NormalTok{, }\StringTok{"wt"}\NormalTok{))}

\NormalTok{gg.sample <-}\StringTok{ }\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{exp2d,}
          \DataTypeTok{x=}\StringTok{"donor"}\NormalTok{, }
          \DataTypeTok{y=}\StringTok{"count"}\NormalTok{,}
          \DataTypeTok{alpha =} \FloatTok{0.4}\NormalTok{,}
          \DataTypeTok{add=}\KeywordTok{c}\NormalTok{(}\StringTok{"mean_ci"}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{stat_compare_means}\NormalTok{(}\DataTypeTok{method =} \StringTok{"t.test"}\NormalTok{, }\DataTypeTok{comparisons=}\NormalTok{compare_list) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Neutrophil count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}

\NormalTok{gg.sample}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plot-p-values-1.pdf}
\caption{\label{fig:plot-p-values}t-test p-values for the plot of sample
means and CIs. The p-values were computed using ggpubr's function
stat\_compare\_means.}
\end{figure}

Notes on the code

\begin{itemize}
\item
  The pairs to compare with a \emph{p}-value are specified with
  \texttt{comparison\ =}. The order of the pairs in the list function
  determine the order plotted from bottom (lowest on the y-axis) to top
  (highest on the y-axis).
\item
  \textbf{It is important to know what exactly is being computed when
  analyzing data and reporting results} and ``t test'' is not sufficient
  to know this. The t-test could be the classic t-test or a Welch test.
  In this example, there are multiple comparisons and the standard error
  of the test statistic could be the pooled estimate from the linear
  model, or a pairwise estimate computed separately for each pair. And,
  given the multiple comparisons, the p-values could be adjusted or not.
  These kinds of questions can be checked with a function's help page.
  \texttt{?stat\_compare\_means} doesn't answer these questions but
  suggests \texttt{compare\_means}, which also doesn't answer these
  questions. The script below has checks to see what p-values the
  function is returning. Run it in your session by changing the value of
  check\_it to TRUE.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# checks on the p-value}
\CommentTok{# t-tests using SE pooled over all four groups}
\NormalTok{check_it <-}\StringTok{ }\OtherTok{FALSE}
\ControlFlowTok{if}\NormalTok{(check_it}\OperatorTok{==}\OtherTok{TRUE}\NormalTok{)\{}
\NormalTok{  m1.lm <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(count}\OperatorTok{~}\NormalTok{donor, }\DataTypeTok{data=}\NormalTok{exp2d)}
\NormalTok{  m1.lm.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1.lm, }\DataTypeTok{specs=}\StringTok{"donor"}\NormalTok{)}
  \KeywordTok{contrast}\NormalTok{(m1.lm.emm, }\DataTypeTok{method=}\StringTok{"trt.vs.ctrl"}\NormalTok{, }\DataTypeTok{ref=}\DecValTok{4}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{) }\CommentTok{# pooled SD}
  
  \KeywordTok{pairwise.t.test}\NormalTok{(exp2d}\OperatorTok{$}\NormalTok{count, exp2d}\OperatorTok{$}\NormalTok{donor, }\DataTypeTok{p.adjust.method=}\StringTok{"none"}\NormalTok{, }\DataTypeTok{pool.sd=}\OtherTok{FALSE}\NormalTok{) }\CommentTok{# non-pooled SD}
  \CommentTok{# compare}
  \KeywordTok{t.test}\NormalTok{(count}\OperatorTok{~}\NormalTok{donor, }\DataTypeTok{data=}\NormalTok{exp2d[donor}\OperatorTok{==}\StringTok{"wt"} \OperatorTok{|}\StringTok{ }\NormalTok{donor}\OperatorTok{==}\StringTok{"sox10"}\NormalTok{]) }\CommentTok{# matches, this is Welch t}
  \KeywordTok{t.test}\NormalTok{(count}\OperatorTok{~}\NormalTok{donor, }\DataTypeTok{data=}\NormalTok{exp2d[donor}\OperatorTok{==}\StringTok{"wt"} \OperatorTok{|}\StringTok{ }\NormalTok{donor}\OperatorTok{==}\StringTok{"sox10"}\NormalTok{], }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

So, the \emph{p}-values returned by
\texttt{stat\_compare\_means(method="t.test")} are computed from
independent (not pooled over the four groups) Welch t-tests.

\subsection{Adding custom p-values}\label{adding-custom-p-values}

If we want to add permutation \emph{p}-values to the plot with
bootstrapped CIs (\ref{fig:plot-boot-plot} or add \emph{p}-values from
the generalized linear model to the plot of modeled means and CIs
(\ref{fig:plot-nb-plot}, we need to use the function
\texttt{stat\_pvalue\_manual} from the ggpubr package. In order to
implement this, we need to add a step to the work flow path above

\textbf{Step 5: Add group columns and a column of formatted p-values to
the contrast table}

The \texttt{stat\_pvalue\_manual} function needs to read a data frame
with a columns labeled ``group1'' and ``group2'' that contain the pairs
of levels to compare with a plotted p-value and a column ``p''
containing the nicely formatted p-values to add to the plot. There is no
R function to create this table, but here is a script to add these to
the contrast object returned by the \texttt{contrast} function of
emmeans. In this example, I use m1.pairs from above and add the p-values
to the plot of modeled means and CIs (\ref{fig:plot-nb-plot}.

First, we need these functions. Run these two lines to define the
functions \texttt{odd} and \texttt{even}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{odd <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{%%}\DecValTok{2} \OperatorTok{!=}\StringTok{ }\DecValTok{0}
\NormalTok{even <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{%%}\DecValTok{2} \OperatorTok{==}\StringTok{ }\DecValTok{0}
\end{Highlighting}
\end{Shaded}

Second, we need to use these functions to add the columns. There are
several R packages that provide functions to format \emph{p}-values.
Here, I use the function \texttt{pvalString} from the
\href{https://www.rdocumentation.org/packages/lazyWeave/versions/3.0.2/topics/pvalString}{lazyWeave}
package. This script also uses \texttt{str\_split} from the package
\href{https://cran.r-project.org/web/packages/stringr/index.html}{stringr}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# convert m1.pairs to a data.table and assign to a new object, in order to}
\CommentTok{# keep a clean copy of m1.pairs}
\NormalTok{m1.pvalues <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(m1.pairs)}

\CommentTok{# if the linear model is from a glm with log link, use this}
\NormalTok{groups <-}\StringTok{ }\KeywordTok{unlist}\NormalTok{(}\KeywordTok{str_split}\NormalTok{(m1.pvalues}\OperatorTok{$}\NormalTok{contrast, }\StringTok{" / "}\NormalTok{))}
\CommentTok{# add the group1 and group 2 columns}
\NormalTok{m1.pvalues[, group1 }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\NormalTok{groups[}\KeywordTok{odd}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(groups))]]}
\NormalTok{m1.pvalues[, group2 }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\NormalTok{groups[}\KeywordTok{even}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(groups))]]}

\CommentTok{# create a column of nicely formatted p-values for display.}
\NormalTok{m1.pvalues[, p }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{pvalString}\NormalTok{(p.value)]}
\end{Highlighting}
\end{Shaded}

\textbf{Bug alert} notes on the script to build the p-value table, if
you don't want your code to fail.

\begin{itemize}
\tightlist
\item
  The script to extract the pair of group labels
  \texttt{str\_split(m1.pvalues\$contrast,\ "\ /\ "))} has to be written
  so that the characters within the quotes matches the characters
  separating the groups in the ``contrast'' column of the contrast table
  (here, m1.pairs). This will typically be either a space-minus-space or
  a space-slash-space. If the model fit is \texttt{lm} and the response
  is not transformed, then the correct code is
  \texttt{str\_split(m1.pvalues\$contrast,\ "\ -\ "))}. Regardless, look
  at the table to check.
\item
  In step 3 above, we took the contrast table object and passed it to
  the function \texttt{summary}, which converts the contrast table
  object to a data.frame. If we had skipped this step,
  \texttt{data.table(m1.pairs)} would fail. Instead, we'd have to use
  \texttt{data.table(summary(m1.pairs))}.
\end{itemize}

Now we can add the p-value to the ggplot object gg.nb created above.
This is the beauty of a ggplot object (including those created by
ggpubr), we can just keep adding stuff to it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg.nb <-}\StringTok{ }\NormalTok{gg.nb }\OperatorTok{+}
\StringTok{    }\KeywordTok{stat_pvalue_manual}\NormalTok{(m1.pvalues[}\DecValTok{4}\OperatorTok{:}\DecValTok{6}\NormalTok{,], }\CommentTok{# only show sox effects}
                           \DataTypeTok{label =} \StringTok{"p"}\NormalTok{, }
                           \DataTypeTok{y.position=}\KeywordTok{c}\NormalTok{(}\DecValTok{31}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{25}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}

\NormalTok{gg.nb}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plot-manual-p-values-1.pdf}
\caption{\label{fig:plot-manual-p-values}Effects and means plot. Top panel:
Effects (top panel) of treatments on neutrophil count. Bottom panel:
modeled means of treatment levels with 95\% confidence intervals.}
\end{figure}

Notes on adding manual \emph{p}-values to the plot:

\begin{itemize}
\tightlist
\item
  The pairs of groups to compare are specified by indexing the rows of
  m1.pvalues. Above, I limit the comparisons to those in rows 4-6. If I
  wanted to specify non-continous rows, I could use something like
  \texttt{m1.pvalues{[}c(1,3,5),{]}}, for example.
\item
  The most manual part of adding manual p-values is setting the position
  for the brackets using the ``position'' argument. The values in this
  argument are the y-coordinates of the brackets. This may take some
  trial-and-error to position the brackets satisfactorily.
\end{itemize}

\subsubsection{Modeled error intervals of the
effect}\label{modeled-error-intervals-of-the-effect}

For the plot of effects, we use table of contrasts m1.pairs as the data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg.effects <-}\StringTok{ }\KeywordTok{ggdotplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ m1.pairs,}
                        \DataTypeTok{x=}\StringTok{"contrast"}\NormalTok{, }
                        \DataTypeTok{y=}\StringTok{"ratio"}\NormalTok{, }
                        \DataTypeTok{color =} \StringTok{"steelblue"}\NormalTok{,}
                        \DataTypeTok{fill =} \StringTok{"steelblue"}\NormalTok{,}
                        \DataTypeTok{size=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{contrast, }
                    \DataTypeTok{ymin=}\NormalTok{asymp.LCL, }
                    \DataTypeTok{ymax=}\NormalTok{asymp.UCL),}
                \DataTypeTok{width=}\FloatTok{0.15}\NormalTok{, }
                \DataTypeTok{color=}\StringTok{"steelblue"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Effect ratio"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept=}\DecValTok{1}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }
\StringTok{  }\OtherTok{NULL}

\NormalTok{gg.effects}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-67-1.pdf}

\subsubsection{Combining effects and response
plots}\label{combining-effects-and-response-plots}

The ggplots are combined using \texttt{plot\_grid} from the package
\href{https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html}{cowplot}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg.effects <-}\StringTok{ }\NormalTok{gg.effects }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{position=}\StringTok{"right"}\NormalTok{)}
\KeywordTok{plot_grid}\NormalTok{(gg.effects, gg.nb, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{align =} \StringTok{"v"}\NormalTok{, }\DataTypeTok{rel_heights =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-68-1.pdf}

\subsection{Plotting two factors}\label{plotting-two-factors}

The data are from figure 6d. This solution requires computing either the
raw or modeled means and errors and adding these to a base ggpubr plot.
Many packages have summary statistics functions for means, standard
deviations, and standard errors. This is easily done by simply computing
the statistics using data.table functionality.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# compute raw statistics}
\CommentTok{# enclosing the line within parentheses prints the result to the console!}
\NormalTok{(exp6d.raw <-}\StringTok{ }\NormalTok{exp6d[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(count), .(}\DataTypeTok{count=}\KeywordTok{mean}\NormalTok{(count),}
                       \DataTypeTok{se=}\KeywordTok{sd}\NormalTok{(count)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(.N)),}
                   \DataTypeTok{by=}\NormalTok{.(treatment, strain)]}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     treatment strain    count       se
## 1:    control     wt 13.08333 2.310904
## 2:    control  sox10 45.61538 6.259903
## 3: transplant     wt 16.35714 2.259552
## 4: transplant  sox10 18.33333 4.536274
\end{verbatim}

Modeled means, standard errors, and confidence limits are conveniently
computed using the \texttt{emmeans} (``estimated marginal means'')
function from the emmeans package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# modeled statsistics}
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{glm.nb}\NormalTok{(count }\OperatorTok{~}\StringTok{ }\NormalTok{treatment}\OperatorTok{*}\NormalTok{strain, }\DataTypeTok{data=}\NormalTok{exp6d)}
\NormalTok{(m1.emm <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"treatment"}\NormalTok{, }\StringTok{"strain"}\NormalTok{), }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     treatment strain response       SE  df asymp.LCL asymp.UCL
## 1:    control     wt 13.08333 2.032161 Inf  9.649528  17.73907
## 2: transplant     wt 16.35714 2.289208 Inf 12.433129  21.51961
## 3:    control  sox10 45.61538 6.132974 Inf 35.048350  59.36837
## 4: transplant  sox10 18.33333 3.871911 Inf 12.119140  27.73391
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# change column "response" to "count" for the ggplot}
\KeywordTok{setnames}\NormalTok{(m1.emm, }\DataTypeTok{old=}\StringTok{"response"}\NormalTok{, }\DataTypeTok{new=}\StringTok{"count"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#pairs_i <- list(c("sox10", "iap_mo"), c("sox10", "gf"), c("sox10", "wt"))}
\NormalTok{pd =}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }
          \DataTypeTok{y=}\StringTok{"count"}\NormalTok{,}
          \DataTypeTok{data=}\NormalTok{exp6d,}
          \DataTypeTok{add=}\KeywordTok{c}\NormalTok{(}\StringTok{"mean"}\NormalTok{),}
          \DataTypeTok{color =} \StringTok{"black"}\NormalTok{,}
          \DataTypeTok{fill =} \StringTok{"strain"}\NormalTok{,}
          \DataTypeTok{palette =} \StringTok{"jco"}\NormalTok{,}
          \DataTypeTok{position =}\NormalTok{ pd,}
          \DataTypeTok{size=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{#stat_compare_means(method = "t.test", comparisons=pairs_i) +}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Neutrophil count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\CommentTok{# geom_dotplot(aes(fill=strain),}
\StringTok{  }\CommentTok{#              binaxis='y', stackdir='center', position=pd, show.legend=FALSE,}
\StringTok{  }\CommentTok{#              color="grey") +}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{strain), }\DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width=}\FloatTok{0.2}\NormalTok{), }\DataTypeTok{show.legend=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{ymin=}\NormalTok{asymp.LCL, }\DataTypeTok{ymax=}\NormalTok{asymp.UCL, }\DataTypeTok{group=}\NormalTok{strain),}
                \DataTypeTok{position=}\NormalTok{pd, }\DataTypeTok{width=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-71-1.pdf}

\subsection{Interaction plot}\label{interaction-plot}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#pairs_i <- list(c("sox10", "iap_mo"), c("sox10", "gf"), c("sox10", "wt"))}

\NormalTok{pd =}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\FloatTok{0.2}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{y=}\NormalTok{count, }\DataTypeTok{shape=}\NormalTok{strain, }\DataTypeTok{color=}\NormalTok{strain, }\DataTypeTok{group=}\NormalTok{strain)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position=}\NormalTok{pd, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{treatment, }\DataTypeTok{ymin=}\NormalTok{asymp.LCL, }\DataTypeTok{ymax=}\NormalTok{asymp.UCL, }\DataTypeTok{group=}\NormalTok{strain),}\DataTypeTok{position=}\NormalTok{pd, }\DataTypeTok{width=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{position=}\NormalTok{pd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Neutrophil count"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_jco}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_pubr}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-72-1.pdf}

\subsection{Plot components}\label{plot-components}

\subsubsection{Showing the data}\label{showing-the-data}

If there are only a few cases per group, there is little reason to
summarize the distribution. Instead plot the individual points using a
stripchart or a jitter plot

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sample 4 points from each group to make it a small n experiment}
\NormalTok{inc <-}\StringTok{ }\NormalTok{exp2d[, .(}\DataTypeTok{inc=}\KeywordTok{sample}\NormalTok{(}\KeywordTok{min}\NormalTok{(.I)}\OperatorTok{:}\KeywordTok{max}\NormalTok{(.I), }\DecValTok{4}\NormalTok{)), by=donor][, inc]}
\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{x =} \StringTok{"donor"}\NormalTok{,}
             \DataTypeTok{y =} \StringTok{"count"}\NormalTok{,}
             \DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{,}
             \DataTypeTok{add =} \StringTok{"mean"}\NormalTok{,}
             \DataTypeTok{data =}\NormalTok{ exp2d[inc,])}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-73-1.pdf}

With more points, a stripchart can be okay but with too many points the
distribution might be obscured. Reasonable alternatives are a box plot,
a violin plot, and a dotplot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg1 <-}\StringTok{ }\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{x =} \StringTok{"donor"}\NormalTok{,}
          \DataTypeTok{y =} \StringTok{"count"}\NormalTok{,}
          \DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{,}
          \DataTypeTok{data =}\NormalTok{ exp2d)}

\NormalTok{gg2 <-}\StringTok{ }\KeywordTok{ggboxplot}\NormalTok{(}\DataTypeTok{x =} \StringTok{"donor"}\NormalTok{,}
          \DataTypeTok{y =} \StringTok{"count"}\NormalTok{,}
          \DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{,}
          \DataTypeTok{data =}\NormalTok{ exp2d)}

\NormalTok{gg3 <-}\StringTok{ }\KeywordTok{ggviolin}\NormalTok{(}\DataTypeTok{x =} \StringTok{"donor"}\NormalTok{,}
          \DataTypeTok{y =} \StringTok{"count"}\NormalTok{,}
          \DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{,}
          \DataTypeTok{data =}\NormalTok{ exp2d)}
\NormalTok{gg4 <-}\StringTok{ }\KeywordTok{ggdotplot}\NormalTok{(}\DataTypeTok{x =} \StringTok{"donor"}\NormalTok{,}
          \DataTypeTok{y =} \StringTok{"count"}\NormalTok{,}
          \DataTypeTok{fill=}\StringTok{"steelblue"}\NormalTok{,}
          \DataTypeTok{data =}\NormalTok{ exp2d)}
\KeywordTok{plot_grid}\NormalTok{(gg1, gg2, gg3, gg4, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-74-1.pdf}

\chapter*{\texorpdfstring{Part IV: More than one \(X\) -- Multivariable
Models}{Part IV: More than one X -- Multivariable Models}}\label{part-iv-more-than-one-x-multivariable-models}
\addcontentsline{toc}{chapter}{Part IV: More than one \(X\) --
Multivariable Models}

\chapter{Adding covariates to a linear
model}\label{adding-covariates-to-a-linear-model}

In its most general sense, \textbf{Covariates} are simply the \(X\)
variables in a statistical model. With data from experiments,
``covariates'' more typically refers to \(X\) variables that are added
to a model to increase precision of the treatment effects. In
observational designs, covariates might be added to a model to 1)
increase predictive ability, 2) because the researcher is interested in
specific conditional effects, or 3) to eliminate confounding. These are
discussed in later chapters.

\hypertarget{adding-covariates-can-increases-the-precision-of-the-effect-of-interest}{\section{Adding
covariates can increases the precision of the effect of
interest}\label{adding-covariates-can-increases-the-precision-of-the-effect-of-interest}}

I use fake data to introduce the concept of \textbf{statistical
elimination} of a \textbf{covariate} in a statistical model. Here I am
modeling the effect of a new drug on blood LDL-C levels. LDL is a kind
of lipoprotein, which are particles in the blood that transport fats and
cholesterol to and from different tissues. LDL-C is cholesterol
associated with LDL particles. LDL-C is considered ``bad cholesterol''
because LDL is believed to transport cholesterol and other lipids to
arterial walls, which is the basis for atherosclerosis.

Twenty applied biostats students are recruited and are randomly assigned
to either the ``placebo'' treatment level or ``drug'' treatment level.
The response is blood LDL-C concentration. The drug manufacturer wants a
measure of the effect of the new drug on ldlc.

The plot below shows the LDL-C response in the placebo and drug groups,
including the group means and 95\% confidence intervals.

\begin{equation}
ldlc = \beta_0 + \beta_1 treatment + \varepsilon
\label{eq:cov-no-cov}
\end{equation}

where \(treatment\) is the dummy variable with \(placebo=0\) and
\(drug=1\).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/cov-harrelplot-1.pdf}
\caption{\label{fig:cov-harrelplot}The fake LDL-C experiment.}
\end{figure}

The coefficient table is

\begin{verbatim}
##               Estimate Std. Error t value Pr(>|t|)
## (Intercept)    113.069      1.899  59.548    0.000
## treatmentdrug   -1.947      2.685  -0.725    0.478
\end{verbatim}

The plot shows large overlap in LDL-C. There ``is no effect of the drug
(\(p = .478\))'' is an incorrect interpretation of the hypothesis test
of the estimate of \(\beta_1\). A correct interpretation is, the
estimated effect is -1.9 but everything from large, negative effects to
moderate positive effects are consistent with the data.

LDL-C is strongly correlated with age and there is a large range in age
among the Applied Bistats students. Consequently, age will contribute to
a large fraction of the variance in LDL-C. If so, this age-related
variance \emph{might} be masking the effect of the drug. Here is a plot
of LDL-C vs.~age, with treatment assignment color coded. Remember, these
are the exact same values of LDL-C as in figure \ref{fig:cov-harrelplot}
above.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ancova-plot2-1.pdf}
\caption{\label{fig:ancova-plot2}Linear regression of \(ldlc\) on dietary
\(fat\) fit to the fake LDL-C data. The points are color coded by
treatment.}
\end{figure}

The line is the bivariate regression fit to the data ignoring treatment
level.

\begin{equation}
ldlc = \beta_0 + \beta_1 age + \varepsilon
\label{eq:cov-age}
\end{equation}

While the points are color-coded by treatment level, \(treatment\) is
not in model \eqref{eq:cov-age}. The color-coding makes it clear that most
of the ``placebo'' data points are above the line, or have positive
residuals from the model, while the ``drug'' data points are below the
line, or have negative residuals from the model. A better way to think
about this pattern is that \textbf{at any specific level of age, the
LDL-C for drug is lower than the LDL-C for placebo}.

What is happening? Age is contributing to the variance of LDL-C, and the
noise in \(\varepsilon\) in model \eqref{eq:cov-no-cov}, and this added
noise makes it harder to measure the effect of the new drug relative to
placebo. Age is masking the effect. If we could somehow measure the
effect of the drug at a specific age, then we could get a more precise
estimate of the effect. But how to do this? Here are three possible
methods. The third is \emph{the only} one you should use but the second
is useful for understanding the third.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We could just analyze a subset of the data, that is, only the cases in
  which the value of age is nearly equal. This throws away perfectly
  good data and, consequently, greatly reduces the sample size and thus
  precision to estimate the effect.
\item
  We could use the residuals of the fitted model \eqref{eq:ancova-2} to
  estimate the effect of drug treatment (this is what we did by eye in
  figure \ref{fig:ancova-plot2}). Here is the new model
\end{enumerate}

\begin{equation}
ldlc.r = \beta_0 + \beta_1 treatment + \varepsilon
\label{eq:ancova-3}
\end{equation}

where \(ldlc.r\) is the set of residuals.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-76-1.pdf}

Now the estimate of the effect is -4.7 mg/dL blood and the SE is only
0.88. In this two-stage analysis (stage 1: fit ldlc \textasciitilde{}
age to get residuals, stage 2: fit residuals \textasciitilde{}
treatment), we have \emph{eliminated the effect of age} on the variance
of the response and, as a consequence, the estimate of the effect of the
drug is much more precise -- the effect of \(treatment\) has a smaller
standard error.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  A better method for this two-stage procedure that increases the
  precision of the estmate of the treatment effect by eliminating
  variance of a covariate (\(age\)) is to simply add the covariate to
  the original linear model.
\end{enumerate}

\begin{equation}
ldlc = \beta_0 + \beta_1 age + \beta_2 treatment + \varepsilon
\label{eq:cov-cov}
\end{equation}

which results in the Harrell Plot

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-77-1.pdf}

and the coefficient table

\label{tab:unnamed-chunk-78}Coefficients of the model that includes the
covariate age.

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

68.8

3.46

19.9

0.0e+00

age

1.6

0.12

13.0

0.0e+00

treatmentdrug

-5.1

0.87

-5.9

1.8e-05

In the linear model that includes the covariate \(age\) (model
\eqref{eq:cov-cov}), the SE of the treatment effect is 0.87. Compare this
to SE of the treatment effect in the model without the covariate (model
\eqref{eq:cov-no-cov}), which is 3.1X larger.

\section{Adding covariates can decrease prediction error in predictive
models}\label{adding-covariates-can-decrease-prediction-error-in-predictive-models}

\section{Adding covariates can reduce bias due to confounding in
explanatory
models}\label{adding-covariates-can-reduce-bias-due-to-confounding-in-explanatory-models}

\section{Best practices 1: A pre-treatment measure of the response
should be a covariate and not subtracted from the post-treatment measure
(regression to the
mean)}\label{best-practices-1-a-pre-treatment-measure-of-the-response-should-be-a-covariate-and-not-subtracted-from-the-post-treatment-measure-regression-to-the-mean}

It is common to measure the outcome variable (\(Y\)) both before and
after the experimental treatments are applied and then compare the
pre-post \emph{change} in \(Y\) in response to the treatment using a
\(t\)-test or ANOVA using this linear model

\begin{equation}
Y_{post}-Y_{pre} = \beta_0 + \beta_1 Treatment + \varepsilon
\label{eq:cov-change-score}
\end{equation}

\textbf{Don't do this}. Instead, add the pre-treatment measure into the
model as a covariate.

\begin{equation}
Y_{post} = \beta_0 + \beta_1 Y_{pre} + \beta_2 Treatment + \varepsilon
\label{eq:ancova-4}
\end{equation}

where \(Treatment\) is a dummy variable for a two-level factor. A
pre-treatment measure (\(Y_{pre}\)) is often called the \emph{baseline}
measure. The change in \(Y\) (\(\Delta Y = Y{post} - Y_{pre}\)) is
sometimes called a change score or gain score. If you really want to
estimate the treatment effect on the change from pre-treatment value to
post-treatment value, then use model \eqref{eq:ancova-4} with \(\Delta Y\)
as the response -- the \(p\)-value will be precisely the same (the
estimate and SE will differ of course because the response variable is
different).

The reason why a researcher should not model a change score
(\(\Delta Y\)) as a function of \(Treatment\) without \(Y_{pre}\) as a
covariate is a phenomenon called \textbf{regression to the mean}. To
explain regression to the mean, I use fake data simulated to model the
results from an important study on gut microbiomes. In this study, the
authors (Turnbaugh et al. xxx) showed that mice with feces from obese
(genotype \emph{ob/ob}) donors had higher weight gain than mice with
feces from lean (genotype \emph{+/+}) donors, presumably because of the
differences in microbial communities between the donor types (shown
elsewhere in their paper). To support the inference of a large
difference in weight change, they illustrated the percent change in each
treatment level in their Fig 3C, which is replicated here using
simulated data generated to match the original summary statistics
(Figure \ref{fig:ancova-mouseplot1}).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ancova-mouseplot1-1.pdf}
\caption{\label{fig:ancova-mouseplot1}Figure 3c of Turnbaugh \emph{et al}
2006. This figure was generated with simulated data matching the summary
statistics given in Turnbaugh \emph{et al} 2006}
\end{figure}

That looks like a big difference, with the mice from the obese-donor
treatment level gaining much more fat than the mice from the lean-donor
treatment level. Turnbaugh et al. used a simple t-test of this percent
change to test the effect of the \emph{ob/ob} treatment. The linear
model underneath this \(t\)-test is

\begin{equation}
percent\_change\_fat = \beta_0 + \beta_1 obese + \varepsilon
\end{equation}

where \(percent\_change\_fat\) is the percent change in fat from
baseline and \(obese\) is a dummy variable with \emph{ob/ob} \(= 1\).
The percent change in fat is
\(\frac{fat_{post} - fat_{pre}}{fat_{pre}} \times 100\), so is a
function of the change score \(\Delta_{fat} = fat_{post} - fat_{pre}\).

The model coefficients are

\begin{verbatim}
##                Estimate Std. Error  t value     Pr(>|t|)
## (Intercept)    25.24015   5.627515 4.485134 0.0003259533
## treatmentob/ob 21.92156   8.176589 2.681016 0.0157879742
\end{verbatim}

\begin{verbatim}
##                    2.5 %   97.5 %
## (Intercept)    13.367137 37.11317
## treatmentob/ob  4.670468 39.17266
\end{verbatim}

Or, the increase in fat in the obese-treated mice was 21.9\% (95\%CI:
4.7, 39.2\%, \(p=0.016\)) greater than the increase in lean-treated
mice. This result, if generally verified with replication and rigorous
probing, would have spectacular implications for human health.

\subsection{Regression to the mean in
words}\label{regression-to-the-mean-in-words}

Regression to the mean is the phenomenon that if an extreme value is
sampled, the next sample will likely be less extreme. This makes sense,
if you randomly sample a single human male and that individual is 6'10"
(about 4 standard deviations above the mean), the next human you
randomly sample will almost certainly be closer to the mean human male.
Or, if you randomly sample five human males and the mean height in the
group is 5'1" (about 3 standard deviations below the mean), the next
sample of five human males that you measure will almost certainly be
closer to the mean human male.

How does regression to the mean apply to the analysis of change scores
in a pre-post experiment, like the mouse fecal transplant study? In a
pre-post experiment, subjects are randomized to treatment group. The
response is measured at baseline and again at the conclusion of the
experiment. Despite random treatment assignment, the mean fat weight of
the \emph{ob/ob} group at baseline was 1.2 standard deviations smaller
than that of the \emph{+/+} group. If there is no treatment effect, what
is the expected difference at the end?

To answer this, we need to know how an individual's fat weight at the
end is related to its fat weight at baseline. An individual's final fat
is dependent on its initial fat if factors that contribute to the
measurement of fat are the same at baseline and the end. For example, if
an individual has relatively high metabolism both at baseline and at the
end, then that individual might have relatively low fat at baseline and
at the end. This dependence of final value on baseline value is
quantified by the correlation between the two measures. This correlation
is \(\rho\) (the greek letter rho). Factors that change over the
duration of the experiment, including random measurement error, cause
the correlation to be less than one. The two extremes of this
correlatioun, and the expected difference in fat weight at the end are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\rho=0\) -- if an individual's final fat is independent of its
  initial fat then we expect the difference at end to be zero.
\item
  \(\rho=1\) -- if an individuals's final fat is entirely dependent on
  its initial fat, then we'd expect the mean fat weight of the
  \emph{ob/ob} group to be 1.2 standard deviations smaller than that of
  the \emph{+/+} group, exactly as it was at baseline.
\end{enumerate}

Regression to the mean happens when \(\rho < 1\) and its consequences
increase as \(\rho\) goes to zero. What is meant by ``consequences''?

The fat weight of the \emph{ob/ob} group at baseline is 1.2 standard
deviations smaller than that of the \emph{+/+} group. If \(\rho=0\),
then we'd expect the difference between mean fat weight at the end of
the experiment to be zero. \emph{Given the starting differences in mean
weight}, to get to zero difference at the end, the \emph{ob/ob} mice
would have to gain more fat weight than the \emph{+/+} mice. Since the
expectation of the mean difference at the end is zero the expectation of
the change score \emph{must be bigger for the ob/ob mice than for the
+/+ mice}. That is the expectation of the \emph{difference} in change
score is conditional on (or ``a function of'') the difference in fat
weight at baseline.

\subsection{Regression to the mean in
pictures}\label{regression-to-the-mean-in-pictures}

Let's simulate this to pump our intuition about regression to the mean
and its consequences on pre-post experiments.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  randomly sample a normal distribution as the ``initial weight'' and
  randomly assign to treatment class
\item
  let the final weight have some correlation (\(\rho\)) with the initial
  weight. Some correlation should make sense -- we expect a mouse that
  has more fat than average at the start of the experiment to also have
  more fat than average at the end of the experiment. Run the experiment
  at different values of this correlation to see how it effects
  regression to the mean.
\item
  Do not add a treatment effect. We want to explore the behavior of the
  nill null hypothesis.
\end{enumerate}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ancova-sim1-1.pdf}
\caption{\label{fig:ancova-sim1}Effect of initial difference in weight on
the difference in change score. Increased initial difference in weight
results in an increased differences in change score between treatment
and control. Four different values of \emph{rho} (the correlation
between initial and final weights) were simulated. Only when
\emph{rho}=1 is there no influence of initial difference, because
whatever differences occur at baseline will be perfectly preserved in
the final measure. The X gives the values in the original Turnbaugh
data}
\end{figure}

What's happening in Figure \ref{fig:ancova-sim1}? Each point is a result
for a single, simulated experiment. In total, there are 1000 simulated
experiments for each of four values of \(\rho\). The \emph{x}-axis is
the difference between the means of the two treatment levels at baseline
(\emph{Initial difference}). The \emph{y}-axis is the difference in mean
change score between the two treatment levels -- that is the difference
in the means of \(\Delta Y\) from equation \eqref{eq:ancova-5}. This
difference in \(\Delta Y\) is the effect of the treatment the
researchers are interested in. The \emph{unconditional} expectation of
this difference is zero

\begin{equation}
\mathrm{E}(\Delta Y_{ob/ob} - \Delta Y_{+/+}) = 0
\end{equation}

but the change conditional on baseline is not zero

\begin{equation}
\mathrm{E}(\Delta Y_{ob/ob} - \Delta Y_{+/+}) \ne 0
\end{equation}

Instead, the conditional expectation is a function of the difference at
baseline. If the initial difference in weight happens to be unusually
large and negative, the expected difference in change score is unusually
positive. This non-zero expectation means that the estimate of the
treatment effect is \textbf{conditionally biased} for any model that
does not include the baseline fat weight as a covariate. And, from a
frequentist perspective, the Type I error for a test of a difference in
\(\Delta Y\) is strongly dependent on the initial difference in weight.

The big X in the plot indicates the difference at baseline and
difference in \(\Delta Y\) for the original fecal transplant study. The
difference in \(Delta Y\) is unusually positive (about .6\% of the
\(|\delta Y|\) are larger) but very close to the expected value given
the unusually large, negative difference at baseline. In other words,
the probability of the data, or more extreme than the data, is not 0.006
but something larger and perhaps, much larger (the computed value
depends on the observed \(\rho\). From, the plot, the X is very unusual
if \(\rho=1\), pretty unusual if \(\rho=0.66\), but pretty common if
\(\rho=0.33\) or if \(\rho=0\)).

\subsection{Do not use percent change, believing that percents account
for effects of initial
weights}\label{do-not-use-percent-change-believing-that-percents-account-for-effects-of-initial-weights}

Some researchers mistakenly believe that a \(t\)-test of percent change
automatically adjusts for effects in initial weight, since this initial
weight is in the denominator of the percent. This is wrong. The
dependency of the difference in change between treatments on the initial
difference between treatments is more severe if change is measured as a
percent, because the numerator (the change score) is expected to be
larger if the denominator is smaller (initial measure). Using the
simulated data from above, here is this dependency.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ancova-sim2-1.pdf}
\caption{\label{fig:ancova-sim2}Effect of initial difference in weight on
the difference in percent change. Increased initial difference in weight
results in an increased differences in Percent change between treatment
and control. Four different values of \emph{rho} (the correlation
between initial and final weights) were simulated. Note there is no
value of \emph{rho} where the difference in percent change is
independent of the initial difference. The X gives the values in the
original Turnbaugh data.}
\end{figure}

\subsection{\texorpdfstring{Do not ``test for balance'' of baseline
measures}{Do not test for balance of baseline measures}}\label{do-not-test-for-balance-of-baseline-measures}

A test of the null hypothesis of no difference in mean at baseline is a
``test for balance.'' Researchers frequently test for balance at
baseline and use the \emph{p}-value of the test to decide the next step:
1) if \(p > 0.05\), conclude that the pre-treatment means ``do not
differ'' and use something like a simple \emph{t} test of the
post-treatment means, 2) if \(p < 0.05\), then use the change score, or
the percent change, as the response in a simple \emph{t}-test, or 3) if
\(p < 0.05\), then use use a linear model with the pre-treatment value
as a covariate. Here, and in general, hypothesis tests used to decide
which of several ways to proceed do not make sense. First, a
null-hypothesis significance test cannot tell you that there is ``no
difference'' -- this is not what null-hypothesis tests do. Second, any
\(p\)-value after the initial test isn't strictly valid as it does not
take into account this decision step, but this is minor. Third,
\textbf{it doesn't matter}; there will always be some difference in the
actual means of the initial measures and, consequently, the conditional
expectation of the final measures, or change in measures, or percent
change will be dependent on this initial difference. So, if one has
initial measures, one should use an linear model that adjusts for
baseline measures to estimate the treatment effect in pre-post designs.
And, if one isn't planning on taking an initial measure, then maybe you
should, because the initial measure used in a linear model allows a
better estimate of the treatment effect, as discussed above in
\protect\hyperlink{adding-covariates-can-increases-the-precision-of-the-effect-of-interest}{Adding
covariates can increases the precision of the effect of interest}.

\section{Best practices 2: Use a covariate instead of normalizing a
response}\label{best-practices-2-use-a-covariate-instead-of-normalizing-a-response}

\chapter{\texorpdfstring{Two (or more) Categorical \(X\) -- Factorial
designs}{Two (or more) Categorical X -- Factorial designs}}\label{two-or-more-categorical-x-factorial-designs}

\section{Factorial experiments}\label{factorial-experiments}

A factorial experiment is one in which there are two or more categorical
\(X\) that are \textbf{crossed}, resulting in a group for all
combinations of the levels of each factor. Factorial experiments are
used to estimate the \textbf{interaction} between factors, which occurs
when the effect of the level of one factor depends on the levels of the
other factors. For example, a researcher wants to estimate the effect of
an environmental toxin on basal metabolic rate (BMR) in a fish and
designs an experiment with two factors: \(Treatment\) with levels
``control'' and ``toxin'' and \(Sex\), with levels ``male'' and
``female''. If the magnitude (and possibly sign) of the effect of the
toxin on BMR differs between males and females, there is an interaction
between \(Treatment\) and \(Sex\). Interactions are usually denoted with
a \(\times\) symbol: \(Treatment \times Sex\). Interactions are
ubiquitous, although sometimes they are small enough to ignore with
little to no loss of understading.

This chapter uses data from an experiment measuring the effect of
\(Temp\) and \(CO2\) on larval sea urchin metabolic rate (\(Resp\))
(there are other outcome measures in the study too). The units of
metabolic rate are pmol O2/hr/larva. There are two \(Temp\) levels (13C
and 18C) and two \(CO2\) levels (400 µAtm and 1100 µAtm) and the factors
are fully crossed, which makes this a \(2 \times 2\) (crossed or
factorial) design. There are \(n=6\) replicates for each combination of
the levels. A good way to visualize the treatment combinations in a
crossed design is with a \(m \times p\) table showing all combinations
of the \(m\) levels of factor 1 (\(Temp\)) against the \(p\) levels of
factor 2 (\(CO2\))

\includegraphics{images/2x2_table.png}

The upper left cell represents the combination of 13 C and 400 µAtm
level within the CO2 factor. The replicates in this cell were grown with
no added treatments, so this cell is the ``control'' for Temp and the
control for CO2, which we will use as the ``reference'' group for the
linear model. The replicates in the lower left cell were grown with an
added temperature treatment (in this case, a 5 C higher temperature).
The replicates in the upper right cell were grown with an added CO2
treatment (700 µATM higher CO2). And finally, the replicates in the
bottom right cell were grown with both the added temperature (+5 C) and
added CO2 (+700 µATM). Here, I use a ``+'' or ``-'' to designate the
addition (or not) of the treatment, so our \(2 \times 2\) treatment
levels are Temp-/CO2-, Temp+/CO2-, Temp-/CO2+ and Temp+/CO2+.

\subsection{Model coefficients: an interaction effect is what is
leftover after adding the treatment effects to the
control}\label{model-coefficients-an-interaction-effect-is-what-is-leftover-after-adding-the-treatment-effects-to-the-control}

A factorial design allows a researcher to estimate the interaction
between two factors. To clarify this, let's fit the factorial model and
look at the coefficient table. The systematic component of the factorial
model is

\begin{equation}
Resp = \beta_0 + \beta_1 Temp^+ + \beta_2 CO2^+ + \beta_3 Temp^+ CO2^+
\label{eq:factorial-full}
\end{equation}

Again, \(Temp^+\) and \(CO2^+\) are dummy variables. The model also
includes \(Temp^+ CO2^+\), which is a dummy variable for the interaction
between Temp and CO2. The value of this interaction dummy variable is
literally the product of the two main factor dummy variables (\(Temp^+\)
and \(CO2^+\)), which can be verified with the model matrix (which here,
is computed from the subset of the data that includeds only the first
two rows of each treatment combination)

(Intercept)

Temp+

CO2+

Temp+:CO2+

1

0

0

0

1

0

0

0

1

1

0

0

1

1

0

0

1

0

1

0

1

0

1

0

1

1

1

1

1

1

1

1

The coefficient table is

\label{tab:urchin-full-model}Coefficient table of the factorial model

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

8.23

0.73

11.3

0.000

Temp+

4.51

1.03

4.4

0.000

CO2+

-0.32

1.03

-0.3

0.761

Temp+:CO2+

-2.68

1.45

-1.9

0.079

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Intercept (\(b_0\)) is the mean (8.23) of the reference
  (Temp-/CO2-) group, and so the mean of the upper left cell in Table
  1).
\item
  The Temp+ coefficient (\(b_1\)) is the estimate of the added
  temperature effect relative to the reference, and so is the mean of
  the lower left cell minus the mean of the upper left cell
  (\(b_1=\bar{Y}_{Temp^+}-\bar{Y}_{Temp-/CO2-}\)). Another way of
  stating this is, it is the effect of Temp when CO2 is at its reference
  level.
\item
  The CO2+ coefficient (\(b_2\)) is the estimate of the added CO2 effect
  relative to the reference, and so is the mean of the upper right cell
  minus the mean of the upper left cell
  (\(b_2=\bar{Y}_{CO2^+}-\bar{Y}_{Temp-/CO2-}\)). Another way of stating
  this is, it is the effect of CO2 when Temp is at its reference level.
\item
  The Temp+:CO2+ coefficient (\(b_3\)) is the estimate of the
  \textbf{interaction effect}, which is the effect in addition to the
  Temp\(^+\) and CO2\(^+\) effects. If you added \(b_1\) and \(b_2\) to
  \(b_0\), you would get the mean of the Temp\(^+\)/CO2\(^+\) group
  \emph{if the effects were purely additive}. So the interaction effect
  is the difference between the mean of the bottom right cell and the
  sum of the coefficients of the other three cells
  (\(b_3 = \bar{Y}_{Temp^+CO2^+} - (b0 + b1 + b2)\)). An interaction is
  a \textbf{non-additive effect}. Think about this. Adding 5 C increases
  respiration by 4.51 units. Adding 700 µATM CO2 decreases respiration
  by .32 units. If these effects were purely additive, then adding both
  5 C and 700 µATM should result in a mean of 8.23 + 4.51 - .32 = 12.42
  units for the Temp\(^+\)/CO2\(^+\) group. What is the mean of this
  group?
\end{enumerate}

9.74! So the difference between the ``additive expectation'' and the
actual mean is \(9.74 - 12.42 = -2.68\), which is the interaction effect
(coefficient). A graphical interpretation of these coefficients are in
the figure of treatment means below (figure
\ref{factorial-what-are-coefficients-plot})

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/factorial-what-are-coefficients-plot-1.pdf}
\caption{\label{fig:factorial-what-are-coefficients-plot}Meaning of
coefficients in factorial model. b0 (dashed line) is the mean of the
reference. b1 (length of vector b1) is the mean of the Temp treatment
minus the mean of the reference. b2 (length of vector b2) is the mean of
the CO2 treatment minus the mean of the reference. b3 (length of vector
b3) is the mean of the Temp + CO2 treatment minus what this value would
be if there were no interaction (indicated by the open gold circle)}
\end{figure}

\subsection{What is the biological meaning of an interaction
effect?}\label{what-is-the-biological-meaning-of-an-interaction-effect}

I can dead lift 150 pounds and my friend Jake can deadlift 175 pounds.
Working together, we should be able to lift 325 pounds. What if
together, we could actually lift 400 pounds? If this were the case, this
would be an interaction with an effect equal to 75 pounds. Is this
biologically plausible? If so, what is the mechanism? Here is a possible
mechanism (although I am highly skeptical of it having a magnitude of 75
pounds): when lifting an object as part of a group, the central nervous
system allows increased motor unit recruitment, and so each person can
lift more weight than they could if lifting alone. A positive
interaction like this is called \emph{synergistic}. Always think about
the biological meaning of an interaction effect.

\subsection{The interpretation of the coefficients in a factorial model
is entirely dependent on the
reference\ldots{}}\label{the-interpretation-of-the-coefficients-in-a-factorial-model-is-entirely-dependent-on-the-reference}

at least using dummy coding of the factor variables, which is the
default in R. To see this, here is the coefficient table of the model
but assigning Temp+/CO2+ as the reference (by re-ordering levels in both
factors)

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

9.74

0.73

13.4

0.000

Temp-

-1.82

1.03

-1.8

0.091

CO2-

3.00

1.03

2.9

0.008

Temp-:CO2-

-2.68

1.45

-1.9

0.079

This dependence of the coefficients on the reference is a feature not a
bug. It is what we mean when we pose the questions ``Compared to larvae
raised at today's temperature, what is the effect of adding 5° Temp on
larval respiration?'', ``Compared to larvae raised at today's CO2, what
is the effect of adding 700 ppm CO2 on larval respiration?'', and
``Compared to larvae raised at today's temperature and CO2, what is the
effect of adding 5° Temp and 700 µAtm CO2 on larval respiration?'' If we
change the reference, we are asking different questions.

\subsection{Estimated marginal means}\label{estimated-marginal-means}

The modeled means (or predicted values) of the factorial model (Model
\eqref{eq:factorial-full}) fit to the urchin data are shown in the table
below. The values in the last column and row are the \textbf{marginal
means}, which are the means of the associated row or column. More
generally, \emph{marginal} refers to a statistic averaged across
multiple levels of another variable

\label{tab:factorial-means}Marginal means from the full factorial model

Temp

400 µAtm

1100 µAtm

mean

13 C

8.2333

7.9167

8.0750

18 C

12.7433

9.7417

11.2425

mean

10.4883

8.8292

The marginal means with their CIs are

Temp

emmean

SE

df

lower.CL

upper.CL

13

8.0750

0.5130468

20

7.004803

9.145197

18

11.2425

0.5130468

20

10.172303

12.312697

CO2

emmean

SE

df

lower.CL

upper.CL

400

10.488333

0.5130468

20

9.418136

11.558530

1100

8.829167

0.5130468

20

7.758970

9.899363

\subsection{In a factorial model, there are multiple effects of each
factor (simple
effects)}\label{in-a-factorial-model-there-are-multiple-effects-of-each-factor-simple-effects}

With a single factor, there was a single effect for each non-reference
level of the factor. For example, if the levels are ``control'',
``knockout'', and ``rescue'', the knockout effect is the contrast
between knockout and control and the rescue effect is the contrast
between rescue and control. In a factorial experiment with crossed A and
B factors, there are multiple effects of a non-reference level of factor
A -- one for each level of factor B. For the urchin experiment, there is
an effect of the 18 C level of Temp when CO2 is 400 µAtm and an effect
when CO2 is 1100 µAtm. Similarly, there is an effect of the 1100 level
of CO2 when Temp is 13 C and when Temp is 18 C. These effects, or
\textbf{contrasts} (differences in modeled means), are sometimes called
the \textbf{simple effects}. Another name could be the ``conditional''
effects, since the value of the effect is conditional on the level
factor B.

One way to visualize the simple effects is by using the \(2 \times 2\)
table of treatment combinations. The contrasts in the right-side column
are the simple effects of CO2 at each level of Temp. The contrasts in
the bottom row are the simple effects of Temp at each level of CO2. Note
that the first simple effect for each factor has a corresponding row in
the table of coefficients of the fit model above.

\label{tab:factorial-simple}Conditional (simple) effects of full factorial
model fit to urchin data

Temp

400 µAtm

1100 µAtm

simple

13 C

8.2333

7.9167

-0.3167

18 C

12.7433

9.7417

-3.0017

simple

4.5100

1.8250

The 95\% confidence intervals and \emph{p}-values of the simple effects
of the factorial model (Model \eqref{eq:factorial-full}) are given in the
table below.

CO2

Temp

Contrast

Estimate

Lower CI

Upper CI

t

p

400

.

18 - 13

4.5100

2.3696

6.6504

4.3953

0.0003

1100

.

18 - 13

1.8250

-0.3154

3.9654

1.7786

0.0905

.

13

1100 - 400

-0.3167

-2.4571

1.8237

-0.3086

0.7608

.

18

1100 - 400

-3.0017

-5.1421

-0.8613

-2.9253

0.0084

The first line is the effect of the 18 C level of Temp when CO2 is 400
µAtm. The 3rd line is the effect of the 1100 µAtm level of CO2 when Temp
is 13 C.

\subsection{Marginal effects}\label{marginal-effects}

The average of the simple effects for a factor are the \textbf{marginal
effects}, or the \textbf{main effects} in ANOVA terminology.

Temp

400 µAtm

1100 µAtm

simple

marginal

13 C

8.2333

7.9167

-0.3167

18 C

12.7433

9.7417

-3.0017

simple

4.5100

1.8250

3.1675

marginal

-1.6592

The 95\% confidence interval and \emph{p}-value of these marginal
effects are

Contrast

Estimate

Lower CI

Upper CI

t

p

18 - 13

3.1675

1.6540

4.6810

4.3656

0.0003

1100 - 400

-1.6592

-3.1727

-0.1457

-2.2867

0.0332

Marginal effects can be useful for summarizing a general trend, but,
like any average, might not be especially meaningful if there is large
heterogeneity of the simple effects, which occurs when the interaction
effect is large. The urchin example is a good example of marginal
effects that would be highly misleading to present without further
comment.

\subsection{The additive model}\label{the-additive-model}

If an interaction effect is small, then it can be useful to estimate the
effects of the two factors as if the interaction were equal to zero.

\begin{equation}
Resp = \beta_0 + \beta_1Temp^+ + \beta_2CO2^+
\end{equation}

This is a \textbf{reduced model} because one of the terms has been
removed from the model. This particular reduced model is often referred
to as the \textbf{additive model}, since it excludes the interaction
term, which is a \emph{product} of other terms. The model coefficients
of the additive model are given in the table below.

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

8.90

0.66

13.4

0.000

Temp+

3.17

0.77

4.1

0.000

CO2+

-1.66

0.77

-2.2

0.042

The conditional effects of the reduced model are

CO2

Temp

Contrast

Estimate

Lower CI

Upper CI

t

p

400

.

18 - 13

3.1675

1.5739

4.7611

4.1336

0.0005

1100

.

18 - 13

3.1675

1.5739

4.7611

4.1336

0.0005

.

13

1100 - 400

-1.6592

-3.2527

-0.0656

-2.1652

0.0420

.

18

1100 - 400

-1.6592

-3.2527

-0.0656

-2.1652

0.0420

The table shows that all conditional effects within a factor are the
same. This makes sense -- if the model fit is additive, the interaction
effect is set to zero and, consequently there cannot be differences in
conditional effects. Probably a better way of thinking about this is, it
doesn't make sense to compute or discuss conditional effects in an
additive model. Instead, an additive model automatically computes
marginal effects.

Contrast

Estimate

Lower CI

Upper CI

t

p

18 - 13

3.1675

1.5739

4.7611

4.1336

0.0005

1100 - 400

-1.6592

-3.2527

-0.0656

-2.1652

0.0420

Compare the table of marginal effects of the additive model to the table
of marginal effects of the full model. The estimates are the same but
the \emph{t}-values and \emph{p}-values differ because of different
degrees of freedom (the full model estimates one more parameter, the
interaction effect). The estimate is the same only if the design is
balanced, which means that each combination of treatment levels has the
same sample size \emph{n}.

\subsection{Reduce models for the right
reason}\label{reduce-models-for-the-right-reason}

Unless one factor truly has no effect, there will always be an
interaction. As stated above, interactions are ubiquitous. If an
interaction is small, it can make sense to drop the interaction term and
re-fit an additive model to estimate marginal effects in order to
present a simplified picture of what is going on, with the recognition
that these estimates are smoothing over the heterogenity in conditional
(simple) effects that truly exist.

Aided and abetted by statistics textbooks for biologists, there is a
long history of researchers dropping an interaction effect because the
interaction \(p>0.05\). Don't do this. It doesn't make any sense.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \(p\)-value is an arbitrary dichotomization of a continuous
  variable. Would it make sense to behave differently if the interaction
  were \(p=0.051\) vs. \(p=0.049\), given that these two p-values are
  effectively identical?
\item
  A \(p\)-value is not evidence that an effect is zero, or ``doesn't
  exist'', or even that an effect is ``trivially small''. This is
  because \(p\)-values are a function of measurement error, sampling
  error, and sample size, in addition to effect size.
\end{enumerate}

\subsection{What about models with more than two
factors?}\label{what-about-models-with-more-than-two-factors}

A factorial model can have more than two factors, for example, a model
with three factors (A, B, and C), each with two levels (which I'll
designate with a ``+''), is

\begin{equation}
Y = \beta_0 + \beta_1 A^+ + \beta_1 B^+ + \beta_3 C^+ + \beta_4 A^+ B^+ + \beta_5 A^+ C^+ + \beta_6 B^+ C^+ + \beta_7 A^+ B^+ C^+ + \varepsilon
\end{equation}

It is easy enough to get an ANOVA table with \(p\)-values for this model
but I don't recommend it because

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  If space and/or time and/or materials are limited then it typically
  makes more sense to prioritize the power to estimate standard errors
  by choosing one of the two-factor models and increasing sample size
\item
  Interaction effects in 2-factor models are hard enough to interpret. A
  3-way interaction is very, very tough to interpret. If all we did was
  table up \(F\)-ratios and \(p\)-values, this wouldn't matter. But it
  does matter.
\end{enumerate}

\section{Reporting results}\label{reporting-results-2}

\subsection{Text results}\label{text-results}

The effect of the increased temperature at the control CO2 level was 4.5
pmol O2/hr/larva (95\% CI: 2.4, 6.7; \(p < 0.001\)). The effect of
increased CO2 at the control temperature was -0.3 pmol O2/hr/larva (95\%
CI: -2.4, 1.8; \(p=.76\)). The interaction effect was -2.7 pmol
O2/hr/larva (95\% CI: -5.7, 0.3; \(p = 0.079\)). Because of the
relatively large interaction, the effect of temperature at the high
level of CO2 was less than half the effect at the low level of CO2
(estimate: 1.82; 95\% CI: -0.3, 4.0; \(p = 0.091\)) and the effect of
CO2 at the high level of Temp was 10 times greater than that at the low
level of Temp (estimate: -3.0; 95\% CI: -5.1, -.9; \(p = 0.0084\)).

The CI on the interaction includes both large negative values and
trivially small values, including zero, and, consequently, our data is
compatible with both scientific models (that is, we can neither support
nor reject the predictions of the scientific model using these results).

\section{Working in R}\label{working-in-r-4}

\subsection{Model formula}\label{model-formula}

A full-factorial model with two factors is specified in the model
formula as \texttt{y\ \textasciitilde{}\ A*B} where A is the first
factor, and B is the second factor. The * indicates to cross A and B. R
expands this formula to
\texttt{y\ \textasciitilde{}\ 1\ +\ A\ +\ B\ +\ A:B} where the colon
indicates an interaction (multiplicative) effect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp}\OperatorTok{*}\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin) }\CommentTok{# use urchin1 data with relabeled levels}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Estimate Std. Error    t value     Pr(>|t|)
## (Intercept)     8.2333333  0.7255577 11.3475922 3.626935e-10
## Temp18          4.5100000  1.0260936  4.3953106 2.792573e-04
## CO21100        -0.3166667  1.0260936 -0.3086138 7.608069e-01
## Temp18:CO21100 -2.6850000  1.4511155 -1.8503007 7.910035e-02
\end{verbatim}

The additive model is specified by the formula
\texttt{y\ \textasciitilde{}\ A\ +\ B}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp }\OperatorTok{+}\StringTok{ }\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin) }\CommentTok{# use urchin1 data with relabeled levels}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              Estimate Std. Error   t value     Pr(>|t|)
## (Intercept)  8.904583  0.6636207 13.418183 9.038657e-12
## Temp18       3.167500  0.7662831  4.133590 4.721000e-04
## CO21100     -1.659167  0.7662831 -2.165214 4.203445e-02
\end{verbatim}

\subsection{Modeled means}\label{modeled-means}

Modeled means are estimated using \texttt{emmeans::emmeans}. The means
for all combinations of Temp and CO2 are obtained with the specs
argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"Temp"}\NormalTok{, }\StringTok{"CO2"}\NormalTok{))}
\NormalTok{m1.emm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Temp CO2  emmean    SE df lower.CL upper.CL
##  13   400    8.23 0.726 20     6.72     9.75
##  18   400   12.74 0.726 20    11.23    14.26
##  13   1100   7.92 0.726 20     6.40     9.43
##  18   1100   9.74 0.726 20     8.23    11.26
## 
## Confidence level used: 0.95
\end{verbatim}

\subsection{Marginal means}\label{marginal-means}

The marginal means are

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm.temp <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"Temp"}\NormalTok{))}
\NormalTok{m1.emm.co2 <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"CO2"}\NormalTok{))}
\NormalTok{m1.emm.temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Temp emmean    SE df lower.CL upper.CL
##  13     8.07 0.513 20      7.0     9.15
##  18    11.24 0.513 20     10.2    12.31
## 
## Results are averaged over the levels of: CO2 
## Confidence level used: 0.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm.co2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  CO2  emmean    SE df lower.CL upper.CL
##  400   10.49 0.513 20     9.42     11.6
##  1100   8.83 0.513 20     7.76      9.9
## 
## Results are averaged over the levels of: Temp 
## Confidence level used: 0.95
\end{verbatim}

\subsection{Contrasts}\label{contrasts}

All six pairwise contrasts are computed using
\texttt{emmeans::contrast}. The adjust argument specifies the adjustment
for multiple testing. The method argument specifies the type of contrast
(pairwise and revpairwise give all pairwise contrasts. revpairwise
simply gives the reverse of pairwise)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.contrast <-}\StringTok{ }\KeywordTok{contrast}\NormalTok{(m1.emm, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{)}
\CommentTok{# add CIs}
\NormalTok{m1.contrast.ci <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(m1.contrast, }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m1.contrast.ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast          estimate   SE df lower.CL upper.CL t.ratio p.value
##  18,400 - 13,400      4.510 1.03 20    2.370    6.650  4.395  0.0003 
##  13,1100 - 13,400    -0.317 1.03 20   -2.457    1.824 -0.309  0.7608 
##  13,1100 - 18,400    -4.827 1.03 20   -6.967   -2.686 -4.704  0.0001 
##  18,1100 - 13,400     1.508 1.03 20   -0.632    3.649  1.470  0.1571 
##  18,1100 - 18,400    -3.002 1.03 20   -5.142   -0.861 -2.925  0.0084 
##  18,1100 - 13,1100    1.825 1.03 20   -0.315    3.965  1.779  0.0905 
## 
## Confidence level used: 0.95
\end{verbatim}

\subsection{Simple effects}\label{simple-effects}

The four conditional (simple) effects are a subset of the contrasts
above and are computed using the arguments \texttt{simple="each"} and
\texttt{combine=TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.effects <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(m1.emm,}
                 \DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{,}
                 \DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{,}
                 \DataTypeTok{simple =} \StringTok{"each"}\NormalTok{,}
                 \DataTypeTok{combine=}\OtherTok{TRUE}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m1.effects}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  CO2  Temp contrast   estimate   SE df lower.CL upper.CL t.ratio p.value
##  400  .    18 - 13       4.510 1.03 20    2.370    6.650  4.395  0.0003 
##  1100 .    18 - 13       1.825 1.03 20   -0.315    3.965  1.779  0.0905 
##  .    13   1100 - 400   -0.317 1.03 20   -2.457    1.824 -0.309  0.7608 
##  .    18   1100 - 400   -3.002 1.03 20   -5.142   -0.861 -2.925  0.0084 
## 
## Confidence level used: 0.95
\end{verbatim}

\subsection{Marginal effects}\label{marginal-effects-1}

The marginal effects of the factorial model are

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm.}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"Temp"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NOTE: Results may be misleading due to involvement in interactions
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.effects.}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(m1.emm.}\DecValTok{1}\NormalTok{,}
                 \DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{,}
                 \DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m1.effects.}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast estimate    SE df lower.CL upper.CL t.ratio p.value
##  18 - 13      3.17 0.726 20     1.65     4.68 4.366   0.0003 
## 
## Results are averaged over the levels of: CO2 
## Confidence level used: 0.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.emm.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"CO2"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NOTE: Results may be misleading due to involvement in interactions
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.effects.}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(m1.emm.}\DecValTok{2}\NormalTok{,}
                 \DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{,}
                 \DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\OtherTok{TRUE}\NormalTok{))}
\NormalTok{m1.effects.}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast   estimate    SE df lower.CL upper.CL t.ratio p.value
##  1100 - 400    -1.66 0.726 20    -3.17   -0.146 -2.287  0.0332 
## 
## Results are averaged over the levels of: Temp 
## Confidence level used: 0.95
\end{verbatim}

These can be combined into a single table using \texttt{rbind}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1.effects.marginal <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(}\KeywordTok{data.table}\NormalTok{(m1.effects.}\DecValTok{1}\NormalTok{), }\KeywordTok{data.table}\NormalTok{(m1.effects.}\DecValTok{2}\NormalTok{))}
\NormalTok{m1.effects.marginal}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      contrast  estimate        SE df  lower.CL   upper.CL   t.ratio
## 1:    18 - 13  3.167500 0.7255577 20  1.654013  4.6809869  4.365607
## 2: 1100 - 400 -1.659167 0.7255577 20 -3.172654 -0.1456798 -2.286747
##         p.value
## 1: 0.0002993051
## 2: 0.0332473272
\end{verbatim}

\subsection{Plotting results}\label{plotting-results}

\subsubsection{Bar plot with uniform coloring poorly communicate the
factorial
design}\label{bar-plot-with-uniform-coloring-poorly-communicate-the-factorial-design}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bar plot with uniform color}
\NormalTok{urchin[, xlabel }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"Temp:"}\NormalTok{,Temp,}\StringTok{"/"}\NormalTok{,}\StringTok{"CO2:"}\NormalTok{,CO2)]}
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"xlabel"}\NormalTok{,}
          \DataTypeTok{y=}\StringTok{"Resp"}\NormalTok{,}
          \DataTypeTok{data=}\NormalTok{urchin[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Resp),],}
          \DataTypeTok{add=}\KeywordTok{c}\NormalTok{(}\StringTok{"mean_ci"}\NormalTok{, }\StringTok{"jitter"}\NormalTok{),}
          \DataTypeTok{fill=}\NormalTok{(}\KeywordTok{pal_jco}\NormalTok{(}\StringTok{"default"}\NormalTok{)(}\DecValTok{4}\NormalTok{))[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Treatment"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-88-1.pdf}

\subsubsection{Plots that communicate the factorial
design}\label{plots-that-communicate-the-factorial-design}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# bar-plot with 2nd factor different color}
\NormalTok{pd <-}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}
\NormalTok{gg1 <-}\StringTok{ }\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"Temp"}\NormalTok{,}
          \DataTypeTok{y=}\StringTok{"Resp"}\NormalTok{,}
          \DataTypeTok{fill=}\StringTok{"CO2"}\NormalTok{,}
          \DataTypeTok{data=}\NormalTok{urchin[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Resp),],}
          \DataTypeTok{add=}\KeywordTok{c}\NormalTok{(}\StringTok{"mean_ci"}\NormalTok{),}
          \DataTypeTok{position=}\NormalTok{pd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill=}\NormalTok{CO2), }
             \DataTypeTok{color=}\StringTok{"black"}\NormalTok{, }
             \DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width=}\FloatTok{0.2}\NormalTok{), }
             \DataTypeTok{show.legend=}\OtherTok{FALSE}\NormalTok{, }
             \DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_jco}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}

\CommentTok{# "interaction" plot}
\NormalTok{m1.emm.dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1.emm))}
\NormalTok{pd =}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\FloatTok{0.7}\NormalTok{)}
\NormalTok{gg2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm.dt, }
                      \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Temp, }
                          \DataTypeTok{y=}\NormalTok{emmean, }
                          \DataTypeTok{shape=}\NormalTok{CO2, }
                          \DataTypeTok{color=}\NormalTok{CO2, }
                          \DataTypeTok{group=}\NormalTok{CO2)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{position=}\NormalTok{pd, }\DataTypeTok{size=}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Temp, }
                                 \DataTypeTok{ymin=}\NormalTok{lower.CL, }
                                 \DataTypeTok{ymax=}\NormalTok{upper.CL,}
                                 \DataTypeTok{group=}\NormalTok{CO2)}
\NormalTok{                , }\DataTypeTok{position=}\NormalTok{pd, }\DataTypeTok{width=}\FloatTok{0.1}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{position=}\NormalTok{pd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Resp"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_jco}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_pubr}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\CommentTok{#theme(legend.position="bottom") +}
\StringTok{  }\OtherTok{NULL}

\CommentTok{# interaction "jitter" plot}
\NormalTok{gg3 <-}\StringTok{ }\NormalTok{gg2 }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\NormalTok{urchin[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Resp),], }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Temp, }\DataTypeTok{y=}\NormalTok{Resp, }\DataTypeTok{fill=}\NormalTok{CO2),}
             \DataTypeTok{position=}\KeywordTok{position_jitterdodge}\NormalTok{(}\DataTypeTok{jitter.width=}\FloatTok{0.2}\NormalTok{)) }\OperatorTok{+}
\CommentTok{#             position=position_jitter(width=0.2)) +}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position=}\StringTok{"bottom"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\NormalTok{gg_response <-}\StringTok{ }\NormalTok{gg3 }\CommentTok{# used below}

\CommentTok{# box "interaction" plot}
\NormalTok{m1.emm.dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1.emm))}
\NormalTok{pd <-}\StringTok{ }\KeywordTok{position_dodge}\NormalTok{(}\FloatTok{0.8}\NormalTok{)}
\NormalTok{gg4 <-}\StringTok{ }\KeywordTok{ggboxplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"Temp"}\NormalTok{,}
          \DataTypeTok{y=}\StringTok{"Resp"}\NormalTok{,}
          \DataTypeTok{data=}\NormalTok{urchin[}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(Resp),],}
          \DataTypeTok{fill=}\StringTok{"CO2"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_fill_jco}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm.dt, }
             \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Temp, }\DataTypeTok{y=}\NormalTok{emmean, }\DataTypeTok{group=}\NormalTok{CO2),}
             \DataTypeTok{color=}\StringTok{"red"}\NormalTok{,}
             \DataTypeTok{position=}\NormalTok{pd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{data=}\NormalTok{m1.emm.dt, }
            \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Temp, }\DataTypeTok{y=}\NormalTok{emmean, }\DataTypeTok{group=}\NormalTok{CO2),}
            \DataTypeTok{position=}\NormalTok{pd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position=}\StringTok{"bottom"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}

\KeywordTok{plot_grid}\NormalTok{(gg1, gg2, gg3, gg4, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }\DataTypeTok{labels=}\StringTok{"AUTO"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/factorial-interaction-plots-1.pdf}
\caption{\label{fig:factorial-interaction-plots}Interaction plots. (B) is
the classic interaction plot, which is characterized by lines connecting
the groups that share the same Factor B level. This line allows one to
visual the effect of Factor A (the slope) at each level of Factor B.}
\end{figure}

A common way to plot the results of factorial models is with an
\textbf{interaction plot} (Figure
\ref{fig:factorial-interaction-plots}). In the interaction plot of the
urchin data, the \(X\)-axis contains the two \(Temp\) treatment levels
and the \(Y\)-axis is the outcome (\(Resp\)). The plot shows the four
cell means indicated by the circles (low CO2 levels) or triangles (high
CO2 levels). The solid lines connect the cell means \emph{across Temp
levels within CO2 levels}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The slope of a line is the effect of \(Temp\) on \(Resp\)
\item
  The relative \emph{elevation} of the two lines is the effect of
  \(CO2\) on \(Resp\)
\item
  The difference in slope \emph{or} the relative elevation at each level
  of \(Temp\) is the interaction effect
\end{enumerate}

Let's deconstruct this. The top (CO2-) line is the effect of \(Temp\) at
the control (400 µATM) value of \(CO2\). The slope of the bottom (CO2+)
line is the effect of \(Temp\) at the high (1100 µATM) value of \(CO2\).
\emph{These lines have different slopes}, or the slope \emph{is
conditional on} the level of CO2. This means that the effect of \(Temp\)
on respiration is \emph{conditional on the value of \(CO2\)}. Think
about this. This is what an interaction implies--conditional effects.

At the reference temperature (13 C), the CO2+ line is barely below the
CO2- line. But at the high temperature (18 C), the CO2+ line is far
below the CO2- line. That is, the relative elevation (the \(CO2\)
effect) is conditional on the level of \(Temp\). It will always be the
case that if the effect of Factor A is conditional on the levels of
Factor B, then the effect of Factor B will be conditional on the levels
of Factor A.

An interaction plot is an okay plot. It doesn't show the data, only a
minimal, descriptive summary (means and standard errors). If we are
interested in the interaction effect, it doesn't give us a very good
sense of the error in this effect. And \emph{that} is a problem because
with real data, two lines are never precisely parallel. Our
interpretation of the similarity of the slopes would probably mostly
reflect our pre-conceived scientific model.

\subsubsection{Effects plots}\label{effects-plots}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# need m1.emm and m1.effects from above}
\CommentTok{# convert to data.table}
\NormalTok{m1.coefs <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))}
\NormalTok{m1.ci <-}\StringTok{ }\KeywordTok{confint}\NormalTok{(m1)}
\NormalTok{m1.coefs.dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{Term=}\KeywordTok{row.names}\NormalTok{(m1.coefs), m1.coefs, m1.ci)}
\CommentTok{# convert labels to match those of m1.effects}
\KeywordTok{setnames}\NormalTok{(m1.coefs.dt, }
         \DataTypeTok{old=}\KeywordTok{c}\NormalTok{(}\StringTok{"Estimate"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{, }\StringTok{"2.5 %"}\NormalTok{, }\StringTok{"97.5 %"}\NormalTok{),}
         \DataTypeTok{new=}\KeywordTok{c}\NormalTok{(}\StringTok{"estimate"}\NormalTok{, }\StringTok{"SE"}\NormalTok{, }\StringTok{"p.value"}\NormalTok{, }\StringTok{"lower.CL"}\NormalTok{, }\StringTok{"upper.CL"}\NormalTok{))}
\NormalTok{m1.contrasts.dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(m1.effects)}
\CommentTok{# create a label for each contrast}
\NormalTok{m1.contrasts.dt[, Term}\OperatorTok{:}\ErrorTok{=}\KeywordTok{ifelse}\NormalTok{(CO2}\OperatorTok{!=}\StringTok{"."}\NormalTok{, }
                              \KeywordTok{paste0}\NormalTok{(CO2, }\StringTok{":"}\NormalTok{, contrast), }
                              \KeywordTok{paste0}\NormalTok{(Temp, }\StringTok{":"}\NormalTok{, contrast))]}
\NormalTok{m1.effects.dt <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(m1.coefs.dt[}\DecValTok{4}\NormalTok{,], m1.contrasts.dt, }\DataTypeTok{fill=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# effects plot}
\CommentTok{# get p-values}
\NormalTok{pval <-}\StringTok{ }\KeywordTok{as.character}\NormalTok{(}\KeywordTok{round}\NormalTok{(m1.effects.dt}\OperatorTok{$}\NormalTok{p.value, }\DecValTok{3}\NormalTok{))}
\NormalTok{pval[}\DecValTok{2}\NormalTok{] <-}\StringTok{ "0.0003"}
\NormalTok{gg_effects <-}\StringTok{ }\KeywordTok{ggdotplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"Term"}\NormalTok{, }
                 \DataTypeTok{y=}\StringTok{"estimate"}\NormalTok{, }
                 \DataTypeTok{data=}\NormalTok{m1.effects.dt, }
                 \DataTypeTok{color =}\NormalTok{ (}\KeywordTok{pal_jco}\NormalTok{(}\StringTok{"default"}\NormalTok{)(}\DecValTok{4}\NormalTok{))[}\DecValTok{1}\NormalTok{],}
                 \DataTypeTok{fill =}\NormalTok{ (}\KeywordTok{pal_jco}\NormalTok{(}\StringTok{"default"}\NormalTok{)(}\DecValTok{4}\NormalTok{))[}\DecValTok{1}\NormalTok{],}
                 \DataTypeTok{size=}\FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Term, }\DataTypeTok{ymin=}\NormalTok{lower.CL, }\DataTypeTok{ymax=}\NormalTok{upper.CL),}
                \DataTypeTok{width=}\FloatTok{0.15}\NormalTok{, }\DataTypeTok{color=}\NormalTok{(}\KeywordTok{pal_jco}\NormalTok{(}\StringTok{"default"}\NormalTok{)(}\DecValTok{4}\NormalTok{))[}\DecValTok{1}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Contrast"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_hline}\NormalTok{(}\DataTypeTok{yintercept=}\DecValTok{0}\NormalTok{, }\DataTypeTok{linetype =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x =} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, }\DataTypeTok{y =} \KeywordTok{rep}\NormalTok{(}\FloatTok{7.75}\NormalTok{, }\DecValTok{5}\NormalTok{), }\DataTypeTok{label =}\NormalTok{ pval) }\OperatorTok{+}
\StringTok{  }\KeywordTok{annotate}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DataTypeTok{x=}\FloatTok{5.4}\NormalTok{, }\DataTypeTok{y=}\FloatTok{7.75}\NormalTok{, }\DataTypeTok{label=}\StringTok{"p-value"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{expand_limits}\NormalTok{(}\DataTypeTok{y =} \FloatTok{8.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Contrast"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord_flip}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\OtherTok{NULL}

\NormalTok{gg_effects}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-89-1.pdf}

This effects plot shows the four simple effects, the single interaction
(Temp18:CO21100), and their 95\% confidence intervals. In the original
paper, the researchers were testing a scientific (not statistical!)
model that predicted no interaction between CO2 and Temp, and the
researchers argued that these data supported this model because of the
``not statistically significant'' \emph{p}-value for the interaction
effect. The data are consistent with this model (one end of the 95\% CI
for the interaction includes zero) but also support a model of a large,
negative interaction (the other end of the 95\% CI includes large
negative values). The data are too course (or the signal:noise ratio is
to small) to have much confidence in the size of the interaction effect.

\subsubsection{Harrell plots}\label{harrell-plots}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg_effects <-}\StringTok{ }\NormalTok{gg_effects }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_y_continuous}\NormalTok{(}\DataTypeTok{position=}\StringTok{"right"}\NormalTok{)}

\KeywordTok{plot_grid}\NormalTok{(gg_effects, gg_response, }\DataTypeTok{nrow=}\DecValTok{2}\NormalTok{, }
          \DataTypeTok{align =} \StringTok{"v"}\NormalTok{, }
          \DataTypeTok{rel_heights =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{1.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-90-1.pdf}

The effects and interaction plot are combined into a single plot.

\section{Problems}\label{problems-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Draw four \(2 \times 2\) tables and label the row and column headers
  using the levels of the urchin treatment. In the first table, insert
  the cell means. In the 2nd table, insert the equation for the
  coefficient. In the third table, solve the equations. And in the
  fourth column, insert the estimates from the table above. Are tables 3
  and 4 the same? If not, you've goofed somewhere.
\item
  Frew et al. (2017) showed that increasing atomospheric CO2 increases
  grub activity in the soil which in turn increases root damage to
  sugarcane. They used a 2 x 2 experiment to then show that silicon
  added to the soild decreased the damage to the roots by the grubs
  (silicon minerals are very hard and plants uptake silicon from the
  soil to mineralize tissues to protect against insect damage). There
  are lots of analyses in the paper -- try to reproduce Fig. 4b, but
  using an interaction plot.
\end{enumerate}

(The treatment assignments are in a different file than the experimental
results. Use the \texttt{merge} function to glue the two tables
together, keying on the common column ``plant'')

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{file name}: ``canegrub\_feedingtrial.csv''
\item
  \textbf{file name}: ``treatments.csv''
\item
  \textbf{source}:
  \url{https://datadryad.org/resource/doi:10.5061/dryad.r3s16}
\end{enumerate}

\includegraphics{images/frew_fig_4b.png}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Kardol et al investigated the effect of moss growth in response to
  rainfall and community structure. Analyze the effect of these two
  factors on biomass gain and generate a Harrell plot alternative to
  their bar plot in Fig. 3 (see below). What is striking about your plot
  compared to theirs?
\end{enumerate}

\textbf{Filename} ``Data file for Dryad.xlsx'' \textbf{sheet} ``Data''
**Source*:**
\url{https://datadryad.org/resource/doi:10.5061/dryad.66d5f}

\includegraphics{images/kardol_fig_3.png}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  (Grad students only) Generate a fake experiment! The experiment should
  have two factors each with two levels. Experiment with power by
  varying sample size and effect size.
\end{enumerate}

\chapter{ANOVA Tables}\label{anova-tables}

Treatment effects are most often analyzed using ANOVA, which is short
for ``Analysis of Variance''. This is somewhat of an odd name for a
method to test for treatments effects - what do differences in means
have to do with an analyis of variance? The name makes sense in light of
the decomposition of the total variance into a model variance and the
residual variance (chapter xxx). If there are differences among the
means, then the total variance is increased because of variation among
groups.

The engine underneath modern ANOVA is a linear model. If the model has a
single categorical factor, the ANOVA is \textbf{one-way}. If the model
has two categorical factors it is a two-way ANOVA. If the model has a
single categorical factor and one continuous factor it is an ANCOVA,
short for \textbf{analysis of covariance} (next chapter). More complex
experimental designs classically analyzed with ANOVA are nested,
split-plot, latin-square, and many others.

\section{Summary of usage}\label{summary-of-usage}

If you choose to report an ANOVA, also report the effects and their
uncertainty in some way, either the model coefficients or contrasts.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ANOVA generates a table with one row for each term in the linear
  model. A term is a factor or a covariate or an interaction. For a
  two-way factorial ANOVA, these terms are the two main effects and the
  interaction effect.
\item
  The ANOVA generates an \(F\) and \(p\)-value for the whole model and
  for each term in the ANOVA table.
\item
  The \(p\)-value of an interaction term is often used as a decision
  rule to interpret the main effects. If \(p \le 0.05\) then do not
  interpret the main effects but instead examine the condition
  (``simple'') effects. If \(p > 0.05\), then interpret the main
  effects. Regardless, this sort of decision rule is itself
  controversial, and for good reason.
\item
  If the main effects are to be interpreted, some statisticians advocate
  re-fitting the model without the interaction effect, others advocate
  interpreting the main effects with the interaction term in the model.
  This only matters if the design is unbalanced (see below).
\item
  Regardles of any decision, always plot the data using a Harrell plot
  or interaction plot to understand and communicate the magnitude and
  pattern of interaction.
\item
  For factors with more than two levels, the \(p\)-value is often used
  as a decision rule to dissect the factor with post-hoc tests, such as
  Tukey HSD.
\item
  A design is balanced if all the cells have the same number of
  replicates. A design is unbalanced if one or more of the cells has a
  different number of replicates. Unbalanced designs make it necessary
  to make decisions, none of which are perfect, and all of which are
  controversial. Some statisticians have even advocated randomly
  excluding data until the design is back in balance. Don't do this.
\item
  There are multiple ways to decompose the sum of squares. I highlight
  the major three: Type I (sequential), Type II (partial sequential),
  and Type III. Most statistics software and introductory statistics
  books default to Type III and, consequently, many researchers are
  unaware that Types I and II exist. R's default is Type I, and this can
  make a difference if the design is unbalanced. This is \emph{not} a
  rare error in publications.
\item
  Because R defaults to Type I sum of squares, the \(p\)-value of a
  factor depends on the order of the factors in the model if the design
  is unbalanced. This is a feature, not a bug.
\item
  ANOVA based on type II sum of squares do not depend on factor order if
  the design is unbalanced, but it does assume that the interaction is
  zero.
\item
  ANOVA based on type III sum of squares do not depend on order if the
  design is unbalanced and does not assume the interaction is zero.
\item
  If the design is balanced, Type I, II, and III sum of squares generate
  the same ANOVA table. And the ANOVA table of just the main effects is
  the same as the ANOVA table that includes the interaction term. None
  of this is true when the design is unbalanced, However, the decision
  to use type II or type III is very controversial.
\end{enumerate}

\section{Example: a one-way ANOVA using the vole
data}\label{example-a-one-way-anova-using-the-vole-data}

The vole data has a single factor (``treatment'') with three levels
(``control'', ``vitamin\_E'', ``vitamin\_C''). In statistics textbooks
that emphasize hypothesis testing, the ``Which test should I use''
flowchart would guide a researcher given this design to a single
classification, or one-way ANOVA, since a t-test can only compare two
levels but an ANOVA can compare more than two levels. There are better
ways to think about what ANOVA is doing, but okay.

Here is an ANOVA table of the vole data:

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

treatment

2

248446

124223.0

2.95

0.057

Residuals

93

3912751

42072.6

I'll explain all the parts of the ANOVA table later, but for now, focus
on the \(p\)-value, which is that most researchers want out of the
table. What null hypothesis does this \(p\)-value test? The p-value
gives the probability of the observed \(F\) or larger \(F\), if the null
were true. The null hypothesis models the data as if they were sampled
from a single, normal distribution and randomly assigned to different
groups. Thus the null hypotheis includes the equality of the means among
factor levels. In the vole data, the single treatment factor has three
levels and a small \(p\)-value could occur because of a difference in
means between the vitamin\_E treatment and control, or between the
vitamin\_C treatment and control, or between the two vitamin treatments.
The \(p\)-value or ANOVA table doesn't indicate what is different, only
that the observed \(F\) is unexpectedly large if the null were true. As
a consequence, researchers typically interpret a low \(p\)-value in an
ANOVA table as evidence of ``an effect'' of the term but have to use
additional tools to dissect this effect. The typical additional tools
are either \textbf{planned comparisons}, which are contrasts among a
subset of a priori identified treatment levels (or groups of levels) or
unplanned comparisons (``post-hoc'' tests) among all pairs of levels.

The \(p\)-value in the ANOVA table acts as a decision rule: if
\(p < 0.05\) then it is okay to further dissect the factor with planned
comparisons or post-hoc tests because the significant \(p\) ``protects''
the type I error of further comparisons. I'm not fond of using
\(p\)-values for these sorts of decision rules.

\section{Example: a two-way ANOVA using the urchin
data}\label{example-a-two-way-anova-using-the-urchin-data}

Let's use the urchin data from the previous chapter xxx to explore the
ANOVA table, which is what is typically reported. The experiment has two
factors (\(Temp\) and \(CO2\)), each with two levels. Here is the linear
model

\begin{equation}
Resp = \beta_0 + \beta_1 Temp + \beta_2 CO2 + \beta_3 TempCO2 + \varepsilon
\end{equation}

In order to understand factorial ANOVA (or any ANOVA with multiple
factors), it is useful to know the difference between
\textbf{conditional means} and \textbf{marginal means}

\begin{verbatim}
##          CO2-  CO2+ Temp-mm
## Temp-   8.233 7.917   8.075
## Temp+  12.743 9.742  11.243
## CO2-mm 10.488 8.829   9.659
\end{verbatim}

In the table above, the upper, left \(2 \times 2\) grid of cells are the
conditional means, which are the means of each group, where a group is a
specific combination of factor levels. The first two values of the third
row are the marginal means for CO2. The first (10.488) is the mean of
the two means when CO2=CO2-. This can be written as
\(\mathrm{E}(Resp|CO2-)\). The second (8.829) is the mean of the two
means when CO2=CO2+, or \(\mathrm{E}(Resp|CO2+)\). The first two
elements of the third column are the marginal means for Temp. These are
\(\mathrm{E}(Resp|Temp-)\) and \(\mathrm{E}(Resp|Temp+)\). The bottom
right value (9.659) is the grand mean.

A \textbf{conditional effect} is a difference between conditional means.
For example the conditional effect of \(Temp\) \emph{conditional on}
CO2=CO2- is \(12.743-8.233\). A \textbf{marginal effect} is a difference
in marginal means within a factor, for example the marginal effect of
\(Temp\) is \(11.243 - 8.075\).

Here is the ANOVA table of the urchin data

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

60.2

60.2

19.1

0.0003

CO2

1

16.5

16.5

5.2

0.0332

Temp:CO2

1

10.8

10.8

3.4

0.0791

Residuals

20

63.2

3.2

This ANOVA table uses what are called Type 3 sum of squares, which is
\emph{NOT} the default in R but is the default in many other statistics
software and is, therefore, the \emph{only} type of ANOVA that many
researchers know (and, many researchers are unaware that there are
multiple types of ANOVA table). Understanding these differences is
important, at least if one is reporting ANOVA tables. I'll return to the
importance of this later.

\subsection{How to read an ANOVA
table}\label{how-to-read-an-anova-table}

An ANOVA table has a row for each term in the underlying linear model --
each of these adds a component of variance to the total, and a row for
the residual variance (this residual variance row is frequently excluded
from the published table). The urchin model has three terms (one level
of \(Temp\), one level of \(CO2\), and one interaction). The statistics
for each term are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Degrees of Freedom} (df) -- If the term is a factor, the df
  will equal the number of levels (\(k\)) for the factor minus 1. Think
  of it this way: the contribution of the variance due to a factor is a
  function of the variability of the \(k\) level means around the grand
  mean. How many degrees of independent variation do these level means
  have, given that we know the grand mean? The answer is \(k-1\) -- once
  the values of \(k-1\) level means are written down, the \(k\)th level
  mean has no freedom to vary; its value has to be
  \(k\bar{\bar{Y}} - \sum_i^{k-1}{Y_i}\). For an interaction term, the
  df is the product of the df of each of the factors in the interaction.
\item
  \textbf{Sum of Squares} -- the sum of squared differences between the
  modeled value and the grand mean. In addition to a sum of squares for
  each term, a \textbf{residual mean square} is computed as the sum of
  squared differences between the measured and modeled values.
\item
  \textbf{Mean Square} -- The sum of squares divided by the df (this is
  a ``mean'' with df acting as the number of things that were summed).
\item
  \textbf{F-ratio} -- the Mean Square of the term dived by the residual
  mean square.
\item
  \textbf{p-value} -- the p-value for the F-ratio. F is compared to an
  F-distribution, which is a distribution of F-ratios under the null.
\end{enumerate}

\subsubsection{Each row in the table tests a null
hypothesis}\label{each-row-in-the-table-tests-a-null-hypothesis}

The row for each term in an ANOVA table tests a null hypothesis. In
order to understand the null hypotheses, I need to define a few more
terms

For the ANOVA table above, which uses Type 3 sum of squares, the
probabilities are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Temp -- \(p = \mathrm{prob}(F \ge F_o|CO2, Temp:CO2)\). The null is no
  difference in means conditional on the level of CO2 and Temp:CO2. This
  is equivalent to no difference between the grand mean and the marginal
  mean of Temp+, or
\end{enumerate}

\begin{equation}
b_1 = \overline{\overline{Resp}} - \mathrm{E}(Resp|Temp^+)
\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  CO2-- \(p = \mathrm{prob}(F \ge F_o|Temp, Temp:CO2)\). The null is no
  difference in means conditional on the level of Temp and Temp:CO2.
  This is equivalent to no difference between the grand mean and the
  marginal mean of CO2+, or
\end{enumerate}

\begin{equation}
b_2 = \overline{\overline{Resp}} - \mathrm{E}(Resp|CO2^+)
\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Temp:CO2 -- \(p = \mathrm{prob}(F \ge F_o|Temp, CO2)\). The null is no
  difference in means conditional on the level of Temp and CO2. This is
  equivalent to the difference between the conditional mean of
  Temp+/CO2+ and the expected conditional mean of Temp+/CO2+ if there
  were no interaction.
\end{enumerate}

\begin{equation}
b_3 = \mathrm{E}(Resp|Temp^+, CO2^+) - (\overline{\overline{Resp}} - b_1 - b_2)
\end{equation}

As noted in the equations, these three differences are the coefficients
of the linear model behind the ANOVA. Here is the coefficient table

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

9.66

0.36

26.6

0.00000

Temp1

-1.58

0.36

-4.4

0.00030

CO21

0.83

0.36

2.3

0.03325

Temp1:CO21

-0.67

0.36

-1.9

0.07910

In ANOVA with type 3 sum of squares, the dummy variables are coded using
effect coding, which differs from the dummy coding introduced in chapter
xxx. The consequence is that the \textbf{grand mean} (the mean of
\(Resp\) across all values) is now the ``reference'' value. The
intercept in this table, then, is the grand mean. The coefficients are
\emph{differences from the grand mean}, as described above.

Use the table of conditional and marginal effects above to check that
the coefficients equal the differences in the equations above. Also not
that the \(p\)-values for the effects in the coefficient table equals
the \(p\)-values in the ANOVA table.

It is important to note that this table differs from the coefficient
table with dummy coding because that reference is the mean of Temp-/CO2-
and not the grand mean.

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

8.23

0.73

11.3

0.00000

TempTemp+

4.51

1.03

4.4

0.00028

CO2CO2+

-0.32

1.03

-0.3

0.76081

TempTemp+:CO2CO2+

-2.68

1.45

-1.9

0.07910

Importantly, note that \(p\)-values for \(b_1\) (the Temp effect) and
\(b_2\) differ between the two tables. This is because the \(t\)-value
tests different hypotheses! In the coefficient table with effect coding
(that behind the ANOVA with type 3 sums of squares), the \(p\)-value
tests marginal effects and so is a function of both marginal means
within a factor. By contrast, in the coefficient table with dummy
coding, the \(p\)-value tests conditional effects, and so is only a
function of the conditional means when the other factor is at its
reference level (right? The coefficient \(b_1\) in the dummy coded
coefficient table is the effect of only increasing \(Temp\) -- \(CO2\)
is left at its reference level). For the interaction effect, the
coefficient differs between the effects coded model and the dummy coded
model (because different reference means) but the \(p\)-value ultimately
tests the same hypothesis (non-additive effects of the factors) and so
the \(t\) and \(p\) values are the same.

\subsubsection{What to do after ANOVA?}\label{what-to-do-after-anova}

Researchers frequently report ANOVA statistics (\(F\) and \(p\) values)
for factorial models in a way that suggests that they misunderstand the
hypotheses tested. It probably doesn't help that there is a
long-standing debate among statisticians about the most sensible
strategy for interpreting factorial ANOVA results. And it doesn't help
that the default ANOVA table in R can suggest a very different
interpretation than the default ANOVA table in some other software
packages.

Here are three strategies for interpreting a factorial ANOVA table that
uses Type III sum of squares. All strategies use \(p\)-values to make a
series of decision rules. In the first strategy, which is a type of
model simplification or model selection, a researcher starts with the
interactions at the bottom of the ANOVA table and works up, eliminating
terms with \(p > 0.05\) and re-fitting the reduced model before
interpreting main effects. In the second strategy, the researcher uses
the original ANOVA table that includes all terms to interpret main
effects.

\textbf{Strategy 1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  is interaction \emph{p} \textless{} 0.05?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    if yes, then do NOT test main effects. Show a graph to show pattern
    of conditional effects. Test conditional effects if this is of
    interest.
  \item
    if no, then refit model without the interaction and test main
    effects -- This now is equivalent to ANOVA using Type II sum of
    squares.

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{1}
    \tightlist
    \item
      is main effect \emph{p} \textless{} 0.05\$?

      \begin{enumerate}
      \def\labelenumiv{\alph{enumiv}.}
      \tightlist
      \item
        if yes, then keep in model
      \item
        if no, then refit model without that main effect
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\textbf{Strategy 2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  is interaction \emph{p} \textless{} 0.05?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    if yes, then do NOT test main effects. Show a graph to show pattern
    of conditional effects. Test conditional effects if this is of
    interest.
  \item
    if no, then use the same table as the test of the main effects. This
    is interpreting the main effects with the interaction term in the
    model. This is the logic of ANOVA using type III sum of squares.
  \end{enumerate}
\end{enumerate}

\textbf{Strategy 3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  is interaction \emph{p} \textless{} 0.05?

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    if yes, then look at interaction plot to determine if it makes sense
    test main effects. For example, if CO2+ had obviously lower \(Resp\)
    at both levels of \(Temp\), even if one was much lower (ie.
    interactaction), then some people would say that the test of the
    main effect is meaningful. Test conditional effects if this is of
    interest.
  \item
    if no, then use the same table as the test of the main effects
  \end{enumerate}
\end{enumerate}

In general, statisticians advise against strategy 3 (interpreting main
effects in the presence of interaction) -- its not wrong, its just that
a main effect has an awkward interpretation if there is an interaction.
Of course this is true if there is \emph{any} interaction term in the
model, not just a statistically significant term. The controversy is
more, if the interaction \(p\) is not significant, then do we implement
stategy 1 (refit model excluding interaction to test main effects) or
strategy 2 (use full factorial anova table to test main effects).

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

45.2

45.2

14.5

0.0011

CO2

1

4.1

4.1

1.3

0.2630

Temp:CO2

1

14.8

14.8

4.8

0.0413

then one shouldn't report the ANOVA results using something like
``Temperature had a significant effect on metabolism
(\(F_{1,20} = 14.5\), \(p=0.001\)). There was no effect of CO2 on
metabolism (\(F_{1,20} = 4.1\), \(p=0.26\))''. There was a significant
interaction effect between Temperature and CO2 on metabolism
(\(F_{1,20} = 14.8\), \(p=0.04\))``. If one accepts that the small
interaction \(p\)-value is evidence of an interaction effect then this
interpretation of the main factors makes no sense, as the first two
results imply that the interaction effect is zero (or, that there is a
constant effect of \(Temp\) or \(CO2\) across both levels of the other
factor), which is then contradicted by the third result.

More specifically, if one is using a \(p\)-value to guide decision
making, then a significant interaction \(p\) indicates that there is no
single ``main'' effect of a factor. Instead, the effect of \(Temp\) is
conditional on the level of \(CO2\), and the effect of \(CO2\) is
conditional on the level of \(Temp\). This is easily seen in the
interaction plot, where the effect of \(Temp\) is large when \(CO2\) is
high but much smaller when \(CO2\) is low. Indeed, the effect of
\(Temp\) at the low CO2 is 0.16.

Instead of interpreting the factors as constant effects, A better
strategy is to compare the \textbf{conditional effects}, that is, the
effects of \(Temp\) within each level of \(CO2\) and the effects of
\(CO2\) within each level of \(Temp\) (conditional effects are sometimes
called the ``simple effects'').

The controversy arises in what to do after an ANOVA if the interaction
effect has a non-significant \(p\)-value. At this point, I am punting
instead of explaining the basis for the controversy, because ultimately
I think the major problem with both strategies is the use of null
hypothesis significance testing to make analysis decisions.

In fact, the entire reason that I use the urchin data as the example for
factorial ANOVA is because it beautifully illustrates the absurdity of
the interaction \(p\)-value decision rule. Why should we interpret the
results of the ANOVA when the interaction \(p\) is 0.079 differently
than when the interaction \(p\) is 0.04? Remember, the \(p\)-value is a
``sample'' statistic (in the sense that it is entirely a function of the
sampled data) and in conditions of low power (which is likely, but not
necessarily, true for the urchin data given n=6), a \(p\)-value is
highly variable.

There are several problems with this approach. 1) a \(p\)-value is not
evidence of ``no effect'', 2) the power to test interaction effects is
small relative to that for the main effects (this is a general rule, not
something specific to these data), 3) the interaction SS accounts for
about 7.2\(\%\) of the total SS, which doesn't seem inconsequential, and
4) the interaction \(p\)-value is small enough to raise a red flag, and,
most importantly, 5) the confidence interval of the interaction effect
indicates that the large, negative values of the interaction are
\emph{as consistent with the data} as trivially small values (or a value
of zero). But the CI is not in an ANOVA table and many researchers fail
to report it. These five points suggest that this experiment be
replicated, with a larger sample size, to get a better estimate of the
interaction effect. The problem of course is that experiments are rarely
replicated, except in biomedical research.

The absurdity of the \(p\)-value decision rule strategy for
interpretation of effects after an ANOVA is highlighted by comparing the
forest plot of model coefficients of the real and fake urchin data. It
would be absurd to use an ANOVA table to interpret these patterns as
radically different (one without an interaction and constant main
effects, the other with an interactioni and conditional effects).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/urchin-anova-ggplot-1.pdf}
\caption{\label{fig:urchin-anova-ggplot}Forest plots (the upper part of a
Harrell plot) of the actual and fake urchin data. A) Real urchin data.
The interaction effect is not significant (\(p=0.079\)). B) Fake urchin
data. The interaction effect is significant (\(p=0.04\)).}
\end{figure}

\subsection{How to read ANOVA results reported in the
text}\label{how-to-read-anova-results-reported-in-the-text}

ANOVA results are often reported in the text of a results section, using
something like ``Temperature had a significant effect on metabolism
(\(F_{1,20} = 14.5\), \(p=0.001\)). There was no effect of CO2 on
metabolism (\(F_{1,20} = 4.1\), \(p=0.26\))''. The subscripts of the
\(F\) statistic are the numerator and denominator degrees of freedom
(df) of the \(F\)-value (These df are a column in the ANOVA table. The
denomintor df may not appear in the table if it is the residual df and
the row for the residual term was not reported). Sometimes I find the
reported df are not consistent with the description of the design and
analysis, which means the data were not analyzed as stated.

\subsection{Better practice -- estimates and their
uncertainty}\label{better-practice-estimates-and-their-uncertainty}

As emphasized in the previous chapter, the decision to include or
exclude an interaction effect in the model should not be based on a
\(p\)-value but on the goals of the model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If the goal is the interaction (because a scientific model predicts
  one, or because this is biology and everything is conditional), then
  estimate the interaction effect (as a coefficient of the model!) and
  its uncertainty, including a CI and \(p\)-value. There is no
  controversy on how to estimate this effect and its uncertainty. The
  coefficient will be different between dummy and effect coded models
  but this is okay because they have different specific interpretations
  but the same general interpretation. Use a Harrel plot with the
  coefficients (including the interaction coefficient) to show this
  estimate and uncertainty.
\item
  If the goal is to estimate constant main effects, then exclude the
  interaction effect from the model and report the main effects (again,
  as coefficients from the model or contrasts if other pairwise effects
  are desired) with their uncertainty. Use an interaction plot (or
  bottom part of the harrell plot) to justify forcing the interaction to
  zero (for example the interaction effect adds little to the total sum
  of squares or the interpretation of a single main effect or two (or
  more) conditional effects would be the same. Use a Harrel plot that
  excludes the interaction term to show these main effects and
  uncertainty.
\item
  And if a researcher is interested in the effects of the factors but
  there is strong evidence for a non-trivial interaction, then report
  the conditional effects (as contrasts) with their uncertainty. Use a
  Harrel plot that includes the interaction term to show these
  conditional effects and uncertainty. If there is an obvious
  interaction, it probably doesn't make sense to interpret the main
  effects, contrary to what some people argue. If there is a positive
  effect of factor A across all levels of factor B, we don't really need
  a \(p\)-value to test that the average of these positive effects is
  significant. This doesn't add value to the plot and any conditional
  effects that are reported.
\end{enumerate}

Notice that an ANOVA table has no role in this recommendation.

\section{Unbalanced designs}\label{unbalanced-designs}

My recommendation above is to not bother with ANOVA, but to simply
compute the contrasts of interest using the linear model. But if you
really want to use ANOVA, you should be aware that \textbf{if the design
is unbalanced, factor order matters in the default R anova function} and
that I routinely find published ANOVA tables that report statistics
(\(F\) and \(p\) values) that are not what the authors think they are.

An \textbf{unbalanced} design is one in which the number of replicates
differs among the cell. The urchin data is balanced because there are
six replicates in each cell. If the respirometer broke before taking the
respiratory measures of the final tank, the design would be unbalanced,
one of the cells would have only five replicates.

Let's look at the effect of row order on the statistics of the urchin
data using R's default anova function.

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

60.20

60.20

19.06

0.00030

CO2

1

16.52

16.52

5.23

0.03325

Temp:CO2

1

10.81

10.81

3.42

0.07910

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

CO2

1

16.52

16.52

5.23

0.03325

Temp

1

60.20

60.20

19.06

0.00030

CO2:Temp

1

10.81

10.81

3.42

0.07910

Now let's unbalance the data, by removing three random replicates (these
may be both in one cell or spread across cells. First, here is the
number of replicates in each cell:

\begin{verbatim}
##        
##         CO2- CO2+
##   Temp-    6    4
##   Temp+    6    5
\end{verbatim}

And here are the two tables with the order of Temp and CO2 reversed in
the model

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

62.25

62.25

18.44

0.00049

CO2

1

21.49

21.49

6.36

0.02190

Temp:CO2

1

6.38

6.38

1.89

0.18720

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

CO2

1

17.59

17.59

5.21

0.03561

Temp

1

66.14

66.14

19.59

0.00037

CO2:Temp

1

6.38

6.38

1.89

0.18720

Several observations are important.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the statistics for the last row, which is the interaction, does not
  change.
\item
  if these data were analyzed in the software package JMP, or SAS, or
  SSPS then \textbf{order wouldn't matter}. Here is what the tables
  would look like
\end{enumerate}

Sum Sq

Df

F value

Pr(\textgreater{}F)

Temp

58.77

1

17.41

0.00064

CO2

19.93

1

5.90

0.02648

Temp:CO2

6.38

1

1.89

0.18720

Sum Sq

Df

F value

Pr(\textgreater{}F)

CO2

19.93

1

5.90

0.02648

Temp

58.77

1

17.41

0.00064

CO2:Temp

6.38

1

1.89

0.18720

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Order does not change the statistics in the coefficient table, even
  for unbalanced data:
\end{enumerate}

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

9.50

0.407

23.367

0.0000

Temp1

-1.70

0.407

-4.172

0.0006

CO21

0.99

0.407

2.430

0.0265

Temp1:CO21

-0.56

0.407

-1.374

0.1872

Estimate

Std. Error

t value

Pr(\textgreater{}\textbar{}t\textbar{})

(Intercept)

9.50

0.407

23.367

0.0000

CO21

0.99

0.407

2.430

0.0265

Temp1

-1.70

0.407

-4.172

0.0006

CO21:Temp1

-0.56

0.407

-1.374

0.1872

\subsection{What is going on in unbalanced ANOVA? -- Type I, II, III sum
of
squares}\label{what-is-going-on-in-unbalanced-anova-type-i-ii-iii-sum-of-squares}

\textbf{Type I sum of squares}. Here is the (default) ANOVA table using
Type I sum of squares for the urchin data with the three missing rows.

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

62.248

62.248

18.4

0.0005

CO2

1

21.488

21.488

6.4

0.0219

Temp:CO2

1

6.377

6.377

1.9

0.1872

Residuals

17

57.399

3.376

The default coding of dummy variables in R's \texttt{lm} function is
dummy coding, which is the coding used for Type I or \textbf{Sequential
Sum of Squares}. The hypothesis tested by each row in the ANOVA table
using Type I sum of squares is the effect of that row's term conditional
on all terms before it in the model (or above it in the table) and
ignoring all terms after it in the model (or below it in the table).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The hypothesis tested by the \(p\)-value for \(Temp\) is the same as
  if \(Temp\) were the only term in the model (other than the
  intercept). That is, the means are estimated for each level of
  \(Temp\) ignoring the fact that half the replicates within each level
  of \(Temp\) experienced low \(CO2\) and half experienced high \(CO2\)
\item
  The hypothesis tested by the \(p\)-value for \(CO2\) is conditional on
  \(Temp\). That is, the difference in metabolism between \(CO2+\) and
  \(CO2-\) when \(Temp\) is ``held constant'' (or for all cases where
  \(Temp\) takes the same value). This is equivalent to the hypothesis
  that the difference in the marginal means of CO2 is zero.
\item
  The hypothesis tested by the \(p\)-value for the interaction is
  conditional on all other terms and nothing is ignored.
\end{enumerate}

\textbf{Type II sum of squares}. Here is the ANOVA table using Type II
sum of squares for the urchin data with missing values. The interaction
term is excluded from the linear model, because type II sum of squares
are used to estimate main effects ignoring the interaction (so this
would make sense only if a plot of the effects suggested a small
interaction relative to the main effects). The sum of squares for the
main effects would be the same if the interaction were included but the
residual df, and thus the F and P-values would differ.

Df

Sum Sq

Mean Sq

F value

Pr(\textgreater{}F)

Temp

1

66.145

66.145

18.7

0.0004

CO2

1

21.488

21.488

6.1

0.0241

Residuals

18

63.776

3.543

The hypothesis tested by each row in the ANOVA table using Type II sum
of squares is the effect of that row's term conditional on all terms
\emph{at the same level or below} but ignoring all terms at a higher
level in the model (or below it in the table). For example, the
hypothesis test for a factor is conditioned on other factors but ignores
interaction terms among the factors. Consequently, these hypotheses
tested are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The hypothesis tested by the \(p\)-value for \(Temp\) is conditional
  on \(CO2\). This is the same hypothesis that would occur using Type I
  sum of squares but placing \(Temp\) second in the model, after \(CO2\)
  (and it is in fact how I computed it for the table).
\item
  The hypothesis tested by the \(p\)-value for \(CO2\) is conditional on
  \(Temp\). This is exactly the hypothesis for \(CO2\) using the Type I
  sum of squares above.
\end{enumerate}

\textbf{Type III sum of squares}. Here is the ANOVA table using Type III
sum of squares for the urchin data for missing data. The interaction
term is excluded from the linear model, and advocates of using Type III
sum of squares explicitly want this in the model.

Sum Sq

Df

F value

Pr(\textgreater{}F)

Temp

58.770

1

17.406

0.0006

CO2

19.935

1

5.904

0.0265

Temp:CO2

6.377

1

1.889

0.1872

Residuals

57.399

17

The hypothesis tested by each row in the ANOVA table using Type III sum
of squares is the effect of that row's term conditional on all terms in
the model.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The hypothesis tested by the \(p\)-value for \(Temp\) is conditional
  on \(CO2\) and \(Temp:CO2\).
\item
  The hypothesis tested by the \(p\)-value for \(CO2\) is conditional on
  \(Temp\) and \(Temp:CO2\).
\item
  The hypothesis tested by the \(p\)-value for \(Temp:CO2\) is
  conditional on \(Temp\) and \(CO2\). This is the same for Type I sum
  of squares (and Type II, if the interaction term were included)
\end{enumerate}

\subsection{Back to interpretation of main
effects}\label{back-to-interpretation-of-main-effects}

\subsection{The anova tables for Type I, II, and III sum of squares are
the same if the design is
balanced.}\label{the-anova-tables-for-type-i-ii-and-iii-sum-of-squares-are-the-same-if-the-design-is-balanced.}

\section{Working in R}\label{working-in-r-5}

\subsection{Type I sum of squares in
R}\label{type-i-sum-of-squares-in-r}

The base R function \texttt{anova()} computes the ANOVA table using Type
I sum of squares for any fit model object, such as that returned by
\texttt{lm}. Here is a script for the urchin data. I first create
unbalanced data by deleting the first row that is the control row.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cn_rows <-}\StringTok{ }\KeywordTok{which}\NormalTok{(urchin[, Temp]}\OperatorTok{==}\StringTok{"Temp-"} \OperatorTok{&}\StringTok{ }\NormalTok{urchin[, CO2]}\OperatorTok{==}\StringTok{"CO2-"}\NormalTok{) }\CommentTok{# gives the rows of the controls}
\NormalTok{urchin_unbalanced <-}\StringTok{ }\NormalTok{urchin[}\OperatorTok{-}\NormalTok{cn_rows[}\DecValTok{1}\NormalTok{],] }\CommentTok{# deletes the row that is in first element of cn_rows}
\NormalTok{urchin.t1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp}\OperatorTok{*}\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin_unbalanced)}
\KeywordTok{anova}\NormalTok{(urchin.t1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: Resp
##           Df Sum Sq Mean Sq F value    Pr(>F)    
## Temp       1 55.696  55.696 16.9244 0.0005907 ***
## CO2        1 18.411  18.411  5.5946 0.0288072 *  
## Temp:CO2   1  9.204   9.204  2.7970 0.1108298    
## Residuals 19 62.527   3.291                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsection{Type II and III Sum of
Squares}\label{type-ii-and-iii-sum-of-squares}

Type II sum of squares can be computed manually simply by fitting the
model twice, once with the factors ordered one way and then with the
factors ordered the opposite way. The car package has the function
\texttt{Anova} that specifically outputs Type II and Type III ANOVA
tables.

Type II sum of squares can be fit with the interaction in the model, and
this generates the Type II sum of squares for the main terms but the
residual is wrong for the \(F\)-ratio because it is the residual from
the full model and Type II assumes the interaction effect is zero. So,
if one wants an ANOVA table with a \(F\) and \(p\) that reflect this,
then the interaction should be dropped from the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{urchin.t2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp}\OperatorTok{*}\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin_unbalanced)}
\KeywordTok{Anova}\NormalTok{(urchin.t2, }\DataTypeTok{type=}\StringTok{"2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Anova Table (Type II tests)
## 
## Response: Resp
##           Sum Sq Df F value    Pr(>F)    
## Temp      52.711  1 16.0173 0.0007624 ***
## CO2       18.411  1  5.5946 0.0288072 *  
## Temp:CO2   9.204  1  2.7970 0.1108298    
## Residuals 62.527 19                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{urchin.t2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp }\OperatorTok{+}\StringTok{ }\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin_unbalanced)}
\KeywordTok{Anova}\NormalTok{(urchin.t2, }\DataTypeTok{type=}\StringTok{"2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Anova Table (Type II tests)
## 
## Response: Resp
##           Sum Sq Df F value   Pr(>F)   
## Temp      52.711  1 14.6968 0.001038 **
## CO2       18.411  1  5.1333 0.034725 * 
## Residuals 71.731 20                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

To get type III sum of squares, we need to specify effects coding for
the model matrix. The safest way to do this is something like this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{con3 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{Temp=}\NormalTok{contr.sum, }\DataTypeTok{CO2=}\NormalTok{contr.sum) }\CommentTok{# change the contrasts coding for the model matrix}
\NormalTok{urchin.t3 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Resp }\OperatorTok{~}\StringTok{ }\NormalTok{Temp}\OperatorTok{*}\NormalTok{CO2, }\DataTypeTok{data=}\NormalTok{urchin_unbalanced, }\DataTypeTok{contrasts=}\NormalTok{con3)}
\KeywordTok{Anova}\NormalTok{(urchin.t3, }\DataTypeTok{type=}\StringTok{"3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Anova Table (Type III tests)
## 
## Response: Resp
##              Sum Sq Df  F value    Pr(>F)    
## (Intercept) 2148.60  1 652.8939 3.559e-16 ***
## Temp          54.71  1  16.6241 0.0006422 ***
## CO2           17.15  1   5.2119 0.0341221 *  
## Temp:CO2       9.20  1   2.7970 0.1108298    
## Residuals     62.53 19                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\chapter{Predictive Models}\label{predictive-models}

This chapter focusses on modeling \textbf{observational data} with
multiple \(X\) variables, both continous and categorical. The classical
analysis of multiple \(X\) variables is \textbf{multiple regression},
sometimes called \textbf{multivariable regression} and occassionally,
but incorrectly, called \textbf{multivariate regression} --
``multivariate'' refers to multiple \(Y\) variables.

The models in this chapter have the structure

\begin{equation}
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + ... \beta_p X_p + \varepsilon 
\end{equation}

\% where \(p\) is the number of \(X\) variables or \textbf{predictors}
in the model. This equation is easily generalized to both generalized
linear models, linear mixed models, and generalized linear mixed models.

\section{Overfitting}\label{overfitting}

When a model is fit to data, the model coefficients are estimates of the
parameters that ``generated the data''. The value of an estimate is
partly a function of the signal (the parameter) and partly a function of
the noise, which is unique to the sample. At a low signal to noise ratio
a model is mostly fitting the noise. A measure of how well the model
``fits'' the data is \(R^2\), which is

\begin{equation}
R^2 <- 1 - \frac{SS_{residual}}{SS_{total}}
\end{equation}

As \(X\) variables are added to a model, the \(R^2\) necessarily
increases. Part of this increase is due to added signal, but part is due
to added noise. If the added noise is more than the added signal, then
the model fit -- that is the parameter estimates -- increasingly
reflects the noise unique to the sample rather the signal common to
every sample. This is the basis of \textbf{overfitting}.

To demonstrate overfitting, I fit completely random \(X\) variables to
the lifespans for the control voles.

Think about it this way: if I create fake data in there are ten \(X\)
variables that are correlewhich \(Y\) is a simple column of random,
normal variables that are not a function of

\section{Model building vs.~Variable selection vs.~Model
selection}\label{model-building-vs.variable-selection-vs.model-selection}

\subsection{Stepwise regression}\label{stepwise-regression}

\subsection{Cross-validation}\label{cross-validation}

\subsection{Penalization}\label{penalization}

\subsubsection{AIC}\label{aic}

\subsubsection{LASSO}\label{lasso}

\section{Shrinkage}\label{shrinkage}

\chapter*{Part V: Expanding the Linear Model -- Generalized Linear
Models and Multilevel (Linear Mixed)
Models}\label{part-v-expanding-the-linear-model-generalized-linear-models-and-multilevel-linear-mixed-models}
\addcontentsline{toc}{chapter}{Part V: Expanding the Linear Model --
Generalized Linear Models and Multilevel (Linear Mixed) Models}

\chapter{Linear mixed models}\label{linear-mixed-models}

\section{Random effects}\label{random-effects}

Researchers often collect data in batches, for example

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An ecologist interested in the effects of insectivorous birds on tree
  seedling performance in a forest stake out ten 1 m\(^2\) plots and use
  a wire-mesh cage to cover half of each plot \footnote{Giffard, B.,
    Corcket, E., Barbaro, L., \& Jactel, H. (2012). Bird predation
    enhances tree seedling resistance to insect herbivores in
    contrasting forest habitats. Oecologia, 168(2), 415-424}. The cage
  allows insect herbivores into the seedlings inside but excludes
  insectivorous birds that eat the insects from the seedlings. In every
  plot, five seedlings are planted within the exclosure and five outside
  of the exclosure. At the end of the experiment, the total leaf mass is
  measured on each seedling. Small, uncontrolled, environmental factors
  (including soil factors and density of insectivorous birds) will
  differ between plots but will be common to all seedlings within a plot
  and we would expect a common response to this uncontrolled variation
  on top of the differential response to each treatment. As a
  consequence, the ten measures of leaf mass within a plot are not
  independent.
\item
  A nutrition researcher wants to compare the effect of glucose
  vs.~fructose on glucose metabolism in humans. Ten individuals are
  recruited. Each individual has blood insulin measured 60 minutes after
  a noon meal over six successive days. The meal alternates between high
  glucose and high fructose on each day. Each individual has three
  measures under high glucose treatment and three measures under high
  fructose treatment. Small, uncontrolled, environmental factors
  (including metabolic variation, other meals, activity levels) will
  differ between the individuals but be common within an individual and
  we would expect a common response to this uncontrolled variation on
  top of the differential response to each treatment. As a consequence,
  the six measures of insulin within an individual are not independent.
\item
  An ecologist wants to measure the effect of an invasive plant on the
  reproduction of a native plant. They stake-out ten, 2 m\(^2\) plots in
  a forest and divide each plot into four quadrants, with each quadrant
  assigned a different treatment: control, activated carbon (a
  procedural control), extract from the invasive plant's leaves, and
  both activated carbon and extract from the invasive plant's leaves.
  The response is seedling count. Small, uncontrolled, environmental
  factors (including soil, drainage, and light) will differ between
  plots but will be common to all four quadrants within a plot and we
  would expect a common response to this uncontrolled variation on top
  of the differential response to each treatment. As a consequence, the
  four sets of counts within a plot are not independent.
\item
  A physiologist has skeletal muscle cells growing in 5 control
  cultures, and 5 treated cultures. The \(Y\) variable is cell diameter,
  which is measured in 10 cells per culture. Small, uncontrolled,
  environmental factors (including chemical) will differ between
  cultures but will be common to all cells within a culture and we would
  expect a common response to this uncontrolled variation on top of the
  differential response to each treatment. As a consequence, the ten
  measures of diameter within a culture are not independent.
\item
  A behavioral biologist wants to measure the effect of a predator fish
  on the preferred feeding location (open water or vegetation) of a prey
  fish. Ten tanks are set up with equal amounts of vegetated and
  unvegetated area. One-third of each tank is screened off to house a
  predator fish, which are added to five of the tanks. Ten prey fish are
  added to each tank. The response is minutes spent foraging in the open
  water as a fraction of total time foraging, which is measured in each
  fish in each tank. Small, uncontrolled, environmental factors
  (including temperature, water chemistry, light, and fish behavior)
  will differ between the tanks but be common within tanks and we would
  expect a common response to this uncontrolled variation on top of the
  differential response to each treatment. As a consequence, the ten
  measures of foraging of each fish within a tank are not independent.
\item
  A microbiologist wants to measure the effect of the microbiome on
  autism-spectrum-disorder(ASD)-like behavior in mice\footnote{Sharon,
    G., Cruz, N.J., Kang, D.W., Gandal, M.J., Wang, B., Kim, Y.M., Zink,
    E.M., Casey, C.P., Taylor, B.C., Lane, C.J. and Bramer, L.M., 2019.
    Human Gut Microbiota from Autism Spectrum Disorder Promote
    Behavioral Symptoms in Mice. Cell, 177(6), pp.1600-1618.}.
  Intestinal microbobial communities from five humans with ASD and five
  humans without ASD are transferred to germ-free mice via fecal
  transplant. Each human donor is used to colonize three mice. The
  response is time socializing in a direct social interaction.
  Uncontrolled features of each microbial community (species composition
  and proportions) will differ among human donors but will be the same
  within human donors and we would expect a common response to this
  uncontrolled variation in addition to any differential response to
  ASD-associated microbiota. As a consequence, the measures of behavior
  in the three mice within a donor group are not independent.
\end{enumerate}

The batches -- plots in experiment 1, individuals in experiment 2, plots
in experiment 3, cultures in experiment 4, tanks in experiment 5, and
mice in experiment 6 -- are the experimental units, meaning that it is
at this level that the experimenter is controlling the conditions. In
each of these studies, there is systematic variation at two levels:
among treatments due to treatment effects and among batches due to
\textbf{batch effects}. This among-batch variation is the \textbf{random
effect}. An assumption of modeling random effects is that the batches
(plots/individuals/cultures/tanks/donor) are a random sample of the
batches that could have been sampled. This is often not strictly true as
batches are often \textbf{convenience samples}.

The multiple measures within a batch are \textbf{subsamples} but are
often called \textbf{repeated measures} if the batch is an individual
(experiment 2 is an example). If multiple measures within a treatment
level within a batch (that is, within a \(batch \times treatment\)
combination) are taken over time, the data are \textbf{longitudinal}.
Not infrequently, subsamples within a treatment within a batch are
called ``replicates'', but this can be confusing because the treatments
are replicated at the level of the batch and not at the level of the
subsamples within a treatment by batch combination. The batches are the
independent experimental units. The subsamples within a batch are not
replicates.

The variation among batches/lack of independence within batches has
different consequences on the uncertainty of the estimate of a treatment
effect. The batches in experiments 1-3 are similar in that each contains
all treatment levels. In these, the researcher is interested in the
treatment effect but not the variation due to differences among the
batches. The batches are nuissance factors that add additional variance
to the response, with the consequence that estimates of treatment
effects are less precise, unless the variance due to the batches is
explicitly modeled. In experiments like 1-3, the batches are known as
\textbf{blocks}. Including block structure in the design is known as
\textbf{blocking}. Adding a blocking factor to a statistical mode is
used to increase the precision of an estimated treatment effect.

Experiments 1 and 2 are examples of a \textbf{randomized complete block
with subsampling} design. ``Complete'' means that each block has all
treatment levels or combinations of levels if there is more than one
factor. \emph{The subsampling is not replication}. The replicates are
the blocks, because \emph{it was at this level that treatment assignment
was randomized}. Experiment 3 is an example of a \textbf{randomized
complete block} design. The blocks are complete but there is only one
measure of the response per treatment.

The batches in experiments 4-6 are similar in that treatment is
randomized \emph{to} batch, so each batch contains only a single
treatment level. In these \textbf{segregated} experimental designs, the
variation among batches that arises from non-treatment related
differences among batches \textbf{confounds} the variation among batches
due to a true treatment effect. An extreme example of this would be
experiment 4 (muscle cell cultures) with only a single culture with
control conditions and a single culture with treatment conditions.
Imagine 1) the true effect of the treatment is zero and 2) an unmeasured
growth factor that happens to be more concentrated in the treatment
culture at the beginning of the experiment. At the end of the experiment
the cells in the treatment culture have an average diameter twice that
of that in the control culture. The researcher is fooled into thinking
that the treatment caused the increased growth. Again, the replicates
are at the level of the cultures, because it was at this level that
treatment assignment was randomized. This means the researcher has a
single replicate (or, \(n=1\)) in each treatment level, regardless of
the number of cells that are measured within each culture. A statistical
analysis that uses the subsampling within a replicate as the sample size
is an example of \textbf{pseudoreplication} (Hurlbert 1984 xxx).

\section{Random effects in statistical
models}\label{random-effects-in-statistical-models}

In all of the above examples, the researcher is interested in the
treatment effect but not the variation due to differences among the
blocks. The blocks are nuissance factors that add additional variance to
the response, with the consequence that estimates of treatment effects
are less precise, unless the variance due to the blocks is explicitly
modeled. Including block structure in the design and in the statistical
model is known as \textbf{blocking}. A natural way to think about the
block factor is as a \textbf{random effect}, meaning that plots in
experiment 1 or the mice in experiment 3 are simply random samples from
a population of plots or mice. Modeling this using the residual-error
specification looks like

\begin{equation}
y_{ij} = (\beta_{0} + \beta_{0j}) + (\beta_{1} + \beta_{1j}) x_i + \varepsilon_i 
\label{eq:lmm-spec1}
\end{equation}

where \(i\) indexes the observation and \(j\) indexes the block
(culture, plot, mouse, etc). The intercept parameter \(\beta_{0j}\) is a
\textbf{random intercept} and the slope parameter \(\beta_{1j}\) is a
\textbf{random slope}. The intercept for observation \emph{i} (that is,
its expected value when \(X=0\)) has a \textbf{fixed} component
(\(\beta_0\)) that is common to all observations and a random component
(\(\beta_{0j}\)) that is common within a block but differs among blocks
(see table below). In the above equation, I've used parentheses to show
how these combine into the random intercept that is unique for each
block. Similarly, the random slope (treatment effect) has a fixed part
(\(\beta_1\)) that is common to all observations and a random component
(\(\beta_{1j}\)) that is common within a block but differs among blocks
(see table below). Again, these are collected within a pair of
parentheses in the equation above.

\label{tab:lmm-table}The linear mixed model specified above estimates a
fixed intercept and fixed slope (treatment effect) that are common to
all observations and a random intercept and random slope for each block,
each of which is common among observations within a block but differ
among observations in different blocks.

block

\(b_0\)

\(b_{0j}\)

\(b_1\)

\(b_{1j}\)

1

\(b_0\)

\(b_{0,j=1}\)

\(b_1\)

\(b_{1,j=1}\)

2

\(b_0\)

\(b_{0,j=2}\)

\(b_1\)

\(b_{1,j=2}\)

3

\(b_0\)

\(b_{0,j=3}\)

\(b_1\)

\(b_{1,j=3}\)

4

\(b_0\)

\(b_{0,j=4}\)

\(b_1\)

\(b_{1,j=4}\)

5

\(b_0\)

\(b_{0,j=5}\)

\(b_1\)

\(b_{1,j=5}\)

6

\(b_0\)

\(b_{0,j=6}\)

\(b_1\)

\(b_{1,j=6}\)

Linear mixed models are called ``mixed models'' because they are a mix
of fixed and random factors. Another useful way to specify this model is
to think about it hierarchically, using

\begin{align}
y_{ij} &= \beta_{0j} + \beta_{1j}x_i + \varepsilon_i \\
\varepsilon_i &\sim N(0, \sigma) \\
\beta_{0j} &= \beta_{0} + N(0, \sigma_{0}) \\
\beta_{1j} &= \beta_{1} + N(0, \sigma_{1})
\label{eq:lmm-spec2}
\end{align}

The first line states that the response is a function of a
block-specific intercept and a block specific slope plus some error that
is unique to each observation. The third and fourth lines state that
these block-specific effects are themselves a function of a common
effect and a random term that is unique to each block. That is, we have
a hierarchical or multi-level structure to the model. Line 1 is the top
level and the effects that are specified in line 1 are a function of
effects at a second, lower level, which are specified in lines 3 and 4.
Because of this structure, linear mixed models are sometimes called
hierarchical or multi-level models.

Finally, it's useful to think how to specify a linear mixed model using
the random-draw specification, as this leads naturally to generalized
linear mixed models, or GLMMs.

\begin{align}
y_{ij} &\sim N(\mu_{ij}, \sigma) \\
\mu_{ij} &=\beta_{0j} + \beta_{1j}x_i \\
\beta_{0j} &\sim N(\beta_0, \sigma_0) \\
\beta_{1j} &\sim N(\beta_1, \sigma_1)
\label{eq:lmm-spec3}
\end{align}

\section{Linear mixed models are
flexible}\label{linear-mixed-models-are-flexible}

The linear mixed model in Equation \eqref{eq:lmm-spec1} specifies both a
random intercept and a random slope but a researcher might limit the
random effect to the intercept only, or less commonly, the slope only.
Excluding the random slope from Equation \eqref{eq:lmm-spec1} results in
the model

\begin{equation}
y_{ij} = (\beta_{0} + \beta_{0j}) + \beta_{1}x_i + \varepsilon_i 
\label{eq:lmm-spec1b}
\end{equation}

We might use a random-intercept-only model if we think that features of
the block would effect the mean response among blocks but not effect the
difference in treatment level (or treatment effect) among blocks. For
example, differences in the immune systems among the individual mice in
experiment 3 might effect growth in both the wild-type and engineered
strains of staph but won't effect the difference in growth between
wild-type and engineered strains from one mouse to another.

\textbf{Not more than you should know} -- For more complex mixed models,
matrix algebra makes the specification of the model much more manageable
than the scalar algebra in \ref{lmm-spec1b}.

\begin{equation}
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Zu} + \boldsymbol{\varepsilon}
\end{equation}

where \(\mathbf{y}\) is the vector of the response,
\(\mathbf{X}\boldsymbol{\beta}\) is the linear predictor of fixed
effects and \(\mathbf{Zu}\) is the linear predictor of random effects.
\(\mathbf{X}\) is the model matrix for the fixed effects and
\(\boldsymbol{\beta}\) is the vector of fixed-effect terms (the fixed
part of the intercept (\(\beta_0\)), including the fixed-effect
coefficients for each of the

\section{Blocking}\label{blocking}

\subsection{Visualing variation due to
blocks}\label{visualing-variation-due-to-blocks}

To visualize random effects due to block, Let's create fake data that
look something like experiment 1, with a single factor with two
treatment levels, \(k=10\) blocks, and \(n=3\) measures for each
treatment level within each block. This is a randomized complete block
design with subsampling and has a total of \(N=2 \times k \times n\)
measures of \(Y\) (and rows of the data.table).

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/lmm1-blocking-fig-1.pdf}
\caption{\label{fig:lmm1-blocking-fig}Visualizing random effects. A) The
response in the two treatment levels. B) The same data but separated by
block. The blue line is at the control mean and the yellow line is at
the treated mean. The black dots are the mean response within a block.}
\end{figure}

Figure \ref{fig:lmm1-blocking-fig}A shows the response as a function of
treatment. The responses are nicely symmetric around the treatment means
(the blue and yellow lines). A linear model (and generalized linear
models, more generally) assumes that a response, conditional on the
\(X\), are independent. Figure \ref{fig:lmm1-blocking-fig}B shows how
this assumption is violated for the simulated data. That pattern of
residuals within a block around the treatment means does not look at all
random. Instead, there is a distinct pattern within a block for the
points to cluster either below the treatment means or above the
treatment means. In blocks a, b, g, and i, all or most of the responses
are below their treatment mean (for example in block b, all the yellow
points are below the yellow line and two of three blue points are below
the blue line). In blocks d, e, f, and j, all or most of the responses
are above their treatment mean (for example, in block e, all three
yellow points are above the yellow line and all three blue points are
above the blue line). In other words, the responses within a block
covary together. For a linear model, this is known as \textbf{correlated
error}.

\subsection{Blocking increases precision of point
estimates}\label{blocking-increases-precision-of-point-estimates}

Block effects are differences in expected mean response among blocks due
to unmeasured factors that are shared within blocks but not among
blocks. A classical linear model fails to model this component of the
total variance in the response, and as a consequence, this
block-specific variance is part of the error variance. One way to think
about this is by moving the random intercept and random slope components
of equation \eqref{eq:lmm-spec1} to the right and combining it with the
observation-specific (or ``conditional'') error (\(\varepsilon_i\))

\begin{equation}
y_{ij} = \beta_{0} + \beta_{1} x_i + (\beta_{0j} + \beta_{1j} + \varepsilon_i)
\label{eq:lmm-spec1b}
\end{equation}

A linear mixed model estimates the random effects parameters, so the
residual from observation \(i\) is \(\varepsilon_i\). A linear model
does not estimate the random effects parameters, so the residual of
observation \(i\) from a linear model is
\(\beta_{0j} + \beta_{1j} + \varepsilon_i\). Consequently, the error
variance from a linear model is larger than the error variance from a
linear mixed model fit to the same data. The consequence of this on
inference depends on the variance of the random effects relative to the
variance of the observation-specific error and on the subsample size. If
the variance due to random effects is relatively big and subsample size
is relatively small, then a linear mixed model estimates treatment
effects with much more precision (and \emph{p}-values will be smaller).

This increased precision is seen in the coefficient table of three
models fit to the fake data in Figure \ref{fig:lmm1-blocking-fig}.

A linear model fit to all data

term

estimate

std.error

statistic

p.value

conf.low

conf.high

TreatmentTr

0.62441

0.3566

1.75

0.085

-0.089

1.338

A linear model fit to the means of each treatment level with each block

term

estimate

std.error

statistic

p.value

conf.low

conf.high

TreatmentTr

0.62441

0.54713

1.14

0.269

-0.525

1.774

A linear mixed model with both a random intercept and random slope fit
to all data

term

estimate

std.error

statistic

df

p.value

conf.low

conf.high

TreatmentTr

0.62441

0.26995

2.31

9

0.046

0.095

1.154

Note that the estimates of treatment effect do not differ among models.
What does differ is the estimated standard error of the treatment
effect, which is 24\% smaller in the linear mixed model relative to that
in the linear model, and 51\% smaller in the linear mixed model relative
to that in the linear model fit to the block means. This difference in
SE propogates to the confidence intervals and \emph{p}-values.

Let's explore this a wee bit more systematically using a simple, Monte
Carlo simulation experiment. 5000 fake data sets were generated. Each
data set simulated an experiment with a single treatment factor with two
levels (``control'' and ``treatment''). The design is a randomized
complete block with subsampling. There are 8 blocks and 3 subsamples in
each treatment level per block. The treatment effect (\(\beta_1\)) is 1.
The observation-specific (or ``within-block'') variance
(\(\sigma^2_{varepsilon}\)) is 1. The (``among-block'') variance of the
random intercept (\(\sigma^2_{\beta_0j}\)) is 1 -- the same as the the
within-block variance. The variance variance of the random slope
(\(\sigma^2_{\beta_1j}\)), which is due to a variable response of the
treatment among blocks, is \(0.1^2\)).

The following three models were fit to all 5000 data sets

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a linear model fit to all data, ignoring blocks (violating the
  independence assumption)
\item
  a linear model fit to the treatment means of the blocks (valid, but
  throwing out data)
\item
  a linear mixed model that models the random intercept\footnote{a
    random slope is not modeled because many of the models specifying a
    random slope fail to converge, which is expected given the
    relatively small variance of the random slope}.
\end{enumerate}

From each fit, I saved the 95\% confidence interval of the treatment
effect and the \emph{p}-value. The median width of the confidence
interval and the power, which is the relative frequency of
\emph{p}-values less than 0.05. The simulation was re-run using the same
parameters except setting the treatment effect (\(\beta_1\)) to 0. In
this new simulation, the relative frequency of \emph{p}-values less than
0.05 is the Type I error.

Model

CI width

Power

Type I

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  lm

  1.58

  0.75

  0.012

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    lm of means

    2.39

    0.31

    0.002

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{2}
    \tightlist
    \item
      lmm b0j

      1.16

      0.92

      0.051
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

For data similar to that simulated, the linear mixed model using a
blocking factor has much more power than the linear model ignoring block
(and ignoring the lack of independence) or the linear model comparing
the treatment level means of the blocks. This lost power in the two
linear models is due to the conservative Type I error rate (extreme in
the case of the linear model of the group means). One take-home lesson
here is, don't throw away perfectly good data if the design of the
experiment includes a blocking factor!

\section{Pseudoreplication}\label{pseudoreplication}

\subsection{Visualizing
pseduoreplication}\label{visualizing-pseduoreplication}

Subsamples from batches are not replicates. Inference from a model fit
to subsampled observations without modeling the batches is called
pseudoreplication, a term famously coined by Hurlbert (1984). For data
from a randomized complete block design, ignoring the batches in the
model will typically result in larger standard errors, wider confidence
intervals, and too conservative \emph{p}-values. In a segregated
experiment, with only a single treatment level per batch (like that in
experiments 4-6 above), ignoring the lack of independence in the model
has the opposite effect. Standard errors are too small. Confidence
intervals are too narrow. \emph{P}-values are too liberal. Type I error
is inflated. Let's visualize a pseudoreplicated data set like this.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/lmm1-pseudoreplication-fig-1.pdf}
\caption{\label{fig:lmm1-pseudoreplication-fig}A data set in which treatment
and batch are confounded because there is only one treatment level per
batch.}
\end{figure}

This consequence of pseudoreplication when batch and treatment are
confounded is seen in the coefficient table of four models fit to the
fake data in Figure \ref{fig:lmm1-pseudoreplication-fig}.

\textbf{A linear model fit to all data (pseudoreplication).}

term

estimate

std.error

statistic

p.value

conf.low

conf.high

TreatmentTr

0.7175

0.31278

2.29

0.025

0.091

1.344

\textbf{A linear model fit the batch means.}

term

estimate

std.error

statistic

p.value

conf.low

conf.high

TreatmentTr

0.7175

0.51803

1.39

0.238

-0.721

2.156

\textbf{A linear mixed model with both random intercept and random
slope.}

term

estimate

std.error

statistic

df

p.value

conf.low

conf.high

TreatmentTr

0.7175

0.51797

1.39

3.32

0.252

-0.298

1.733

\textbf{A linear mixed model with random intercept.}

term

estimate

std.error

statistic

df

p.value

conf.low

conf.high

TreatmentTr

0.7175

0.51803

1.39

4

0.238

-0.298

1.733

As with the blocked design, all models compute the same estimate of the
treatment effect. But in contrast to the blocked design, in this
confounded design, the standard error of the treatment effect is smaller
in the linear model of all data than that of the linear mixed models.
This increased precision is an illusion because the model fails to
account for the lack of independence within the batches.

One interesting result to note is the equivalence of the standard error,
test statistic, \emph{p}-value, and confidence intervals of the linear
model on the batch means and the linear mixed model with a random
intercept (but no random slope) only. The two are equivalent. Murtaugh
(xxx) has argued that with this kind of design, it is much simpler to
the analyst, and to your audience, to simply use the linear model on the
batch means. This raises the question, why bother with subsampling
within batch? One answer is, subsampling increases the precision of the
batch mean, and therefore, the precision of the treatment effect. That
said, \emph{the precision of a treatment effect is increased more
efficiently by adding more replicates (more batches), not more
subsamples}.

Let's explore the consequence of pseudoreplication with a confounded
with a Monte Carlo simulation experiment. 5000 fake data sets were
generated. Each data set simulated an experiment with a single treatment
factor with two levels (``control'' and ``treatment''). The design is
treatment level randomized to batch (only one treatment level per
batch). There are 8 batches and 3 subsamples in each batch. The
treatment effect (\(\beta_1\)) is zero. The observation-specific (or
``within-block'') variance (\(\sigma^2_{varepsilon}\)) is 1. The
(``among-block'') variance of the random intercept
(\(\sigma^2_{\beta_0j}\)) is 1 -- the same as the the within-block
variance. The variance variance of the random slope
(\(\sigma^2_{\beta_1j}\)), which is due to a variable response of the
treatment among blocks, is \(0.1^2\)).

Because the effect is zero, the frequency of \emph{p}-values less than
0.05 is the type I error. The same three models fit to the simulated
blocked data are fit to these data. I don't simulate an effect in order
to compute power (at that effect size) because the increased power in
the linear model of all data is, again, an illusion. It only comes at
the cost of strongly elevated Type I error.

Model

CI width

Type I

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  lm

  2.22

  0.18

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    lm of means

    3.80

    0.05

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{2}
    \tightlist
    \item
      lmm b0j

      3.80

      0.05
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

When treatment level is randomized \emph{to} batch, the type I error
rate of a linear model fit to all data (and ignoring the lack of
independence) is highly inflated. Don't do this.

\section{Mapping NHST to estimation: A paired t-test is a special case
of a linear mixed
model}\label{mapping-nhst-to-estimation-a-paired-t-test-is-a-special-case-of-a-linear-mixed-model}

Specifically, a \textbf{paired t-test} is equivalent to a linear mixed
model with a single factor with two treatment levels, \(k\) blocks, and
a single measure of each treatment level within each block. A good
example is the wild type vs.~engineered staph count in mice in
experiment 3 above. A linear mixed model is much more flexible than a
paired \emph{t}-test because it allows a researcher to add treatment
levels, additional factors, and covariates to the model. In addition, a
linear mixed model can handle missing data.

Here is fake data similar in design to experiment 3 with a single factor
with two treatment levels and both levels applied to the same
experimental unit.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{n <-}\StringTok{ }\DecValTok{10} \CommentTok{# number of mice (blocks)}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"WT"}\NormalTok{,}\StringTok{"Tr"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n) }\CommentTok{# treatments}
\NormalTok{id <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(letters[}\DecValTok{1}\OperatorTok{:}\NormalTok{n], }\DecValTok{2}\NormalTok{) }\CommentTok{# block id}
\NormalTok{y <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=}\DecValTok{10}\NormalTok{), }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{mean=}\DecValTok{11}\NormalTok{))}
\NormalTok{fake_data <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{Y=}\NormalTok{y, }\DataTypeTok{X=}\NormalTok{x, }\DataTypeTok{ID=}\NormalTok{id)}
\end{Highlighting}
\end{Shaded}

The \emph{t}-test \emph{p}-value is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{t.test}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X, }\DataTypeTok{data=}\NormalTok{fake_data, }\DataTypeTok{paired=}\OtherTok{TRUE}\NormalTok{)}\OperatorTok{$}\NormalTok{p.value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05336815
\end{verbatim}

and the coefficient table of the fixed effect in the linear mixed model
is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lme}\NormalTok{(Y}\OperatorTok{~}\NormalTok{X, }\DataTypeTok{random =} \OperatorTok{~}\DecValTok{1}\OperatorTok{|}\NormalTok{ID, }\DataTypeTok{correlation=}\KeywordTok{corCompSymm}\NormalTok{(}\DataTypeTok{form=}\OperatorTok{~}\DecValTok{1}\OperatorTok{|}\NormalTok{ID), }\DataTypeTok{data=}\NormalTok{fake_data)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  Value Std.Error DF   t-value      p-value
## (Intercept) 11.1797704 0.3438775  9 32.510914 1.212113e-10
## XWT         -0.9686188 0.4358740  9 -2.222245 5.336815e-02
\end{verbatim}

\section{Advanced topic -- Linear mixed models shrink coefficients by
partial
pooling}\label{advanced-topic-linear-mixed-models-shrink-coefficients-by-partial-pooling}

In experiment 1 above, there are 10 sites (maybe different woodlots). In
each plot, five seedlings are planted inside a cage and five outside the
cage. The cage excludes insectivorous birds but not herbivorous insects.
The researchers are investigating how birds affect plant growth
indirectly -- by eating insects that feed on the plants. The response is
total leaf area in each seedling.

Let's say we want to know the treatment effect in each of these sites.
There are several ways of estimating this.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fit \(k\) separate models, one for each site. The intercept (control
  mean) and slope (treatment effect) parameters for each site are
  estimated independently from all other sites. Consequently, the model
  parameters are computed using \textbf{no pooling}. For the estimation
  of the \(\beta\) terms, this is equivalent to a single, factorial
  linear model with \(Site\) modeled as a \textbf{fixed effect} (this is
  not true for the estimate of the standard errors of these terms since
  these are computed from the residual sum of squares of the model. For
  balanced data, all of the ``intercept'' or ``slope'' terms will have
  the same SE in the factorial analysis but differ among the \(k\)
  independent analyses).
\item
  Fit a linear model to all the data combined as if these were from a
  single site, and assign the intercept and treatment effect paramters
  to all sites. The model parameters are computed using \textbf{complete
  pooling}.
\item
  Fit a linear mixed model to all the data, using site as a random
  factor to estimate both random intercepts and slopes. Similar to the
  no-pooling analysis, there will be different intercept (control mean)
  and slope (treatment effect) estimates for each site, but unlike the
  no-pooling analysis, these estimates are computed by combining
  information from the other sites. The information used to estimate
  parameters in a linear mixed model is somewhere in between no pooling
  and complete pooling and is sometimes called \textbf{partial pooling}.
\end{enumerate}

The consequence of partial pooling in a linear mixed model is that site
intercepts (control means) are pulled toward the single intercept in the
complete-pooling analysis and the site slopes (treatment effects) are
pulled toward the single slope in the complete-pooling analysis. This
has the consequence that the \textbf{differences} in parameter estimates
among sites are shrunk toward zero. A consequence of this shrinkage is
that the variance of the intercept estimates or of the slope estimates
is smaller than that in the no-pooling analysis. Figure
\ref{fig:lmm-partialpooling} shows this shrinkage effect using fake data
simulating the seedling experiment.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/lmm-partialpooling-1.pdf}
\caption{\label{fig:lmm-partialpooling}Shrinkage estimates of the treatment
effect in a linear mixed model. The grey line is the estimate using
complete pooling (so there is only one estimate which is assigned to
each site). In general, the partial-pooling (linear mixed model)
estimates (yellow) are generally closer to the complete pooling estimate
than the no-pooling (separate linear models) estimates (blue). More
specifically, if the no-pooling estimate is far from the complete
pooling estimate, the partial pooling estimate is \emph{much} closer to
the complete pooling estimate. The consequence of partial pooling is
that the differences among the estimates are shrunk toward zero.}
\end{figure}

The linear mixed model estimates of the treatment effects for each site
are a type of \textbf{shrinkage estimate} and a linear mixed model is
one kind of \textbf{shrinkage estimator}. Shrinkage estimates have
fascinating properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the variance of shrinkage estimates is less than that of ordinary
  least squares estimates (no-pooling, or using the block as a fixed
  factor)
\item
  shrinkage estimates are \textbf{biased} but the OLS estimates are not.
  This means that the expected value of a coefficient from the linear
  mixed model \emph{does not equal} the true (parameter) value! Or, more
  formally, \(\mathrm{E}(b_j) \ne \beta_j\).
\item
  the \textbf{mean square error} of shrinkage estimates will be smaller
  than that for OLS estimates.
\end{enumerate}

The first property was discussed above and shown in Figure
\ref{fig:lmm-partialpooling}. The second property raises the question,
if we want to estimate the treatment effects within each site, why would
we ever want to use \(Site\) as a random instead of fixed effect? The
answer is the third property, which can be summarized as, ``if we were
to replicate the experiment many times, the shrinkage estimates will be,
on average, less wrong (or closer to the true value) than the OLS
estimates, where''wrong" is the absolute deviation from the true value."

When shrinkage estimators were first discovered, the third property
surprised stasticians. The third property has profound consequences.
Consider a scenario where researchers want to compare the performance of
a new expression vector to that of an existing expression vector on
protein production using \emph{E. coli}. The researchers have ten
different \emph{E. coli} strains and are interested in strain-specific
effects because they will choose the three strains with the largest
effects for further testing. The researchers measure the response of
each strain five times.

\label{tab:lmm-fish-passage}Effect of new expression vector on protein
production in ten strains of \emph{E. coli} using a fixed effect
factorial model and linear mixed model.

Strain

\(\beta_{1j}\)

fixed \(b_{1j}\)

random \(b_{1j}\)

a

0.91

1.07

0.98

b

0.87

0.94

0.85

c

0.90

-1.03

0.30

d

0.81

0.64

0.63

e

1.09

1.00

1.07

f

0.62

0.91

1.14

g

1.33

2.26

1.36

h

1.27

1.48

0.96

i

1.61

0.57

1.13

j

0.89

1.50

0.93

The table above shows the true strain-specific effect and both the fixed
(OLS) and random (LMM) effect estimates. The largest OLS estimate is
70\% larger than the true effect and the strain with the largest true
effect is not among the top three biggest OLS estimates (its ranking is
9/10). By contrast, the LMM estimates are closer to the true effects and
the top strain is among the three largest LMM estimates.

These results are specific to these fake data but more generally, 1) the
largest OLS estimates are inflated (larger error from the true effect),
relative to the largest LMM estimates 2) overall, the LMM estimates will
be closer than the OLS estimates to the true effects

To understand this, rank order the treatment effects for each strain. An
individual strain's position in this rank is the sum of the true effect
for that strain and some random error. Because OLS, relative to
shrinkage estimates, have greater variance in the estimate (that is, the
random error component is bigger), the biggest effects estimated by OLS
are more likely to be big because of the error component, compared to
shrinkage estimates.

\textbf{Not more than you want to know} -- Shrinkage estimators are not
only useful when we are interested in block-specific effects but are
also useful for estimating effects when there are \textbf{multiple
responses}. For example, consider a researcher interested in measuring
the effects of some exercise treatment on gene expression in adipose
cells. The researcher measures expression levels in 10,000 genes. Given
the typical content in undergraduate biostatics courses, a researcher
would probably model these responses using 10,000 \emph{t}-tests, or
equivalently, 10,000 separate linear models. If the tests were ranked by
\(p\)-value or absolute effect size, many of the genes with largest
absolute effect would be there because of a large error component and
many of the largest effects would be massively overinflated.
Re-imagining the design as a single, linear mixed model with each gene
modeled as a block would lead to a rank order in which the biggest
measured effects more closely approximate the true effects.

\section{Working in R}\label{working-in-r-6}

The major function for working with linear mixed models is
\texttt{lmer()} from the lme4 package. An older, and still sometimes
used and useful function is \texttt{lme()} from the nlme package. The
authors of the lme4 package argue that the df in a linear mixed model
are too approximate for a useful \(p\)-value and, consequently, the
\texttt{lme} function does not return a \(p\)-value. Many biological
researchers want a \(p\)-value and typically use the \texttt{lmerTest}
package to get this.

\textbf{Specifying a linear mixed model using \texttt{lme}}. The random
factor is in the column ``block''. To conform to some packages that use
lme4 objects, any variable used to model a random effect should be
converted to type \texttt{factor}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  add a random intercept using
  \texttt{y\ \textasciitilde{}\ treatment\ +\ (1\textbar{}block)}. This
  adds a random intercept for each level of treatment.
\item
  add a random slope and intercept using
  \texttt{y\ \textasciitilde{}\ treatment\ +\ (treatment\textbar{}block)}.
  This adds a random intercept for each level of treatment and a random
  slope for each level of treatment.
\end{enumerate}

A message that might appear is ``boundary (singular) fit: see
?isSingular''. This does not mean there is a problem with the fit model.

A warning that ``Model failed to converge with 1 negative eigenvalue''
does mean there is a problem. A solution is to simplify the model by,
for example, removing a random slope.

\subsection{coral data}\label{coral-data}

\textbf{Source} Zill, J. A., Gil, M. A., \& Osenberg, C. W. (2017). When
environmental factors become stressors: interactive effects of vermetid
gastropods and sedimentation on corals. Biology letters, 13(3),
20160957.

\textbf{Dryad source}
\url{https://datadryad.org/resource/doi:10.5061/dryad.p59n8}

\textbf{file name} ``VermetidSedimentData\_ZillGilOsenberg\_DRYAD.xlsx''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{folder <-}\StringTok{ "Data from When environmental factors become stressors- interactive effects of vermetid gastropods and sedimentation on corals"}
\NormalTok{fn <-}\StringTok{ "VermetidSedimentData_ZillGilOsenberg_DRYAD.xlsx"}
\NormalTok{sheet_i <-}\StringTok{ "Coral Growth Rate Data"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{here}\NormalTok{(data_path, folder, fn)}
\NormalTok{coral <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\NormalTok{sheet_i) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{clean_names}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{data.table}\NormalTok{()}
\NormalTok{coral[, vermetids}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(vermetids)]}
\NormalTok{coral[, sediment}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(sediment)]}

\CommentTok{# recode levels of factors since 0 and 1}
\NormalTok{coral[, vermetids }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{fct_recode}\NormalTok{(vermetids,}
                                \DataTypeTok{absent =} \StringTok{"0"}\NormalTok{,}
                                \DataTypeTok{present =} \StringTok{"1"}\NormalTok{)]}
\NormalTok{coral[, sediment }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{fct_recode}\NormalTok{(sediment,}
                                \DataTypeTok{control =} \StringTok{"0"}\NormalTok{,}
                                \DataTypeTok{addition =} \StringTok{"1"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Fitting models}\label{fitting-models}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# to reproduce the results}
\CommentTok{# observation 2 should be excluded from the analysis}
\NormalTok{inc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(coral))}

\CommentTok{# random intercept only}
\NormalTok{m1 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(growth_rate }\OperatorTok{~}\StringTok{ }\NormalTok{vermetids}\OperatorTok{*}\NormalTok{sediment }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{block), }\DataTypeTok{data=}\NormalTok{coral[inc])}
\CommentTok{# random intercept and slope}
\NormalTok{m2 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(growth_rate }\OperatorTok{~}\StringTok{ }\NormalTok{vermetids}\OperatorTok{*}\NormalTok{sediment }\OperatorTok{+}\StringTok{ }\NormalTok{(vermetids}\OperatorTok{|}\NormalTok{block) }\OperatorTok{+}\StringTok{ }\NormalTok{(sediment}\OperatorTok{|}\NormalTok{block), }\DataTypeTok{data=}\NormalTok{coral[inc])}
\CommentTok{# to include the interaction as a random effect we'd need subsampling within each factorial treatment combination}
\end{Highlighting}
\end{Shaded}

The conditional effects of m1 are

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# results using lmer fit}
\NormalTok{fit.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }\DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"vermetids"}\NormalTok{, }\StringTok{"sediment"}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(fit.emm, }
                 \DataTypeTok{method =} \StringTok{"revpairwise"}\NormalTok{, }
                 \DataTypeTok{simple =} \StringTok{"each"}\NormalTok{,}
                 \DataTypeTok{combine =} \OtherTok{TRUE}\NormalTok{,}
                 \DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  sediment vermetids contrast           estimate    SE   df lower.CL
##  control  .         present - absent    0.00466 0.209 23.0   -0.428
##  addition .         present - absent   -0.76889 0.217 23.6   -1.217
##  .        absent    addition - control  0.28520 0.217 23.6   -0.163
##  .        present   addition - control -0.48834 0.209 23.0   -0.921
##  upper.CL t.ratio p.value
##    0.4373  0.022  0.9824 
##   -0.3211 -3.547  0.0017 
##    0.7330  1.316  0.2009 
##   -0.0557 -2.335  0.0286 
## 
## Degrees-of-freedom method: kenward-roger 
## Confidence level used: 0.95
\end{verbatim}

There is no ``correct'' way to compute the degrees of freedom for
inferential statistics (SE, CIs, \emph{p}-values). Two common choices
are ``Kenward-Roger'' and ``Satterthwaite''. There is little empirical
reason to vastly prefer one over the other (they each seem to perform a
wee bit bitter under different conditions).

In \texttt{emmeans}, the Kenward-Roger degrees of freedom are the
default. For Satterthwaite, use the \texttt{lmer.df} argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(m1, }
                   \DataTypeTok{specs=}\KeywordTok{c}\NormalTok{(}\StringTok{"vermetids"}\NormalTok{, }\StringTok{"sediment"}\NormalTok{),}
                   \DataTypeTok{lmer.df =} \StringTok{"satterthwaite"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(fit.emm, }
                 \DataTypeTok{method =} \StringTok{"revpairwise"}\NormalTok{, }
                 \DataTypeTok{simple =} \StringTok{"each"}\NormalTok{,}
                 \DataTypeTok{combine =} \OtherTok{TRUE}\NormalTok{,}
                 \DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{),}
        \DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  sediment vermetids contrast           estimate    SE   df lower.CL
##  control  .         present - absent    0.00466 0.209 22.9   -0.428
##  addition .         present - absent   -0.76889 0.216 23.5   -1.215
##  .        absent    addition - control  0.28520 0.216 23.5   -0.161
##  .        present   addition - control -0.48834 0.209 22.9   -0.921
##  upper.CL t.ratio p.value
##    0.4374  0.022  0.9824 
##   -0.3226 -3.559  0.0016 
##    0.7315  1.320  0.1994 
##   -0.0556 -2.335  0.0287 
## 
## Degrees-of-freedom method: satterthwaite 
## Confidence level used: 0.95
\end{verbatim}

If you want to compute the coefficient table or an ANOVA, lmer does not
output test statistics. To get test statistics, you have to load the
library ``lmerTest'' (which automatically loads ``lme4''). With
\texttt{lmerTest}, the Satterthwaite degrees of freedom are the default.
For Kenward-Roger, use the \texttt{ddf} argument in either
\texttt{summary()} (for the coefficients) or \texttt{anova()} (for
ANOVA).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1)) }\CommentTok{# default is Satterthwaite}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                       Estimate Std. Error       df
## (Intercept)                        1.268411111  0.1541680 30.42768
## vermetidspresent                   0.004655556  0.2091398 22.94243
## sedimentaddition                   0.285202305  0.2160126 23.53130
## vermetidspresent:sedimentaddition -0.773546750  0.3006674 23.24638
##                                       t value     Pr(>|t|)
## (Intercept)                        8.22745788 3.129900e-09
## vermetidspresent                   0.02226049 9.824326e-01
## sedimentaddition                   1.32030428 1.994327e-01
## vermetidspresent:sedimentaddition -2.57276556 1.693404e-02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1, }\DataTypeTok{ddf=}\StringTok{"Kenward-Roger"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                       Estimate Std. Error       df
## (Intercept)                        1.268411111  0.1541680 30.43671
## vermetidspresent                   0.004655556  0.2091398 23.03604
## sedimentaddition                   0.285202305  0.2167687 23.62024
## vermetidspresent:sedimentaddition -0.773546750  0.3012111 23.33762
##                                       t value     Pr(>|t|)
## (Intercept)                        8.22745788 3.122797e-09
## vermetidspresent                   0.02226049 9.824319e-01
## sedimentaddition                   1.31569876 2.009032e-01
## vermetidspresent:sedimentaddition -2.56812158 1.708128e-02
\end{verbatim}

Compare the output from emmeans and \texttt{coef(summary())} using the
different methods for computing the df.

\chapter{Generalized linear models I: Count
data}\label{generalized-linear-models-i-count-data}

Biologists frequently count stuff, and design experiments to estimate
the effects of different factors on these counts. For example, the
effects of environmental mercury on clutch size in a bird, the effects
of warming on parasite load in a fish, or the effect of exercise on RNA
expression.

Count data differ from data with normal error in many ways, including 1)
counts are discrete, and can be zero or positive integers only, 2)
counts tend to bunch up on the small side of the range, creating a
distribution with a positive skew, 3) a sample of counts can have an
abundance of zeros, and 4) the variance of counts increases with the
mean (see Figure \ref{fig:glm1-plot1} for some of these properties).
Some count data can be approximated by a normal distribution and
reasonably modeled with a linear model but more often, count data are
modeled with \textbf{Poisson distribution} or \textbf{negative binomial
distribution} using a \textbf{generalized linear model} (GLM). Poisson
and negative binomial distributions are \textbf{discrete probability
distributions} with two important properties: 1) the distribution
contains only zero and positive integers and 2) the variance is a
function of the mean. Back before modern computing and fast processors,
count data were often analyzed by either \textbf{transforming} the
response or by \textbf{non-parametric hypothesis tests}. One reason to
prefer a statistical modeling approach with a GLM is that we can get
interpretable parameter estimates. By contrast, both the analysis of
transformed data and non-parametric hypothesis tests are really tools
for computing ``correct'' \(p\)-values.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-plot1-1.pdf}
\caption{\label{fig:glm1-plot1}Histogram of the count of a trematode
parasite larvae in Control vs.~Infected fish. Fish in the Infected
treatment are infected with a tapeworm.}
\end{figure}

\section{The generalized linear
model}\label{the-generalized-linear-model}

As outlined in section {[}Assumptions for inference with statistical
models{]} in Chapter 1, a common way that biological researchers think
about a response variable is

\begin{align}
y_i &= \beta_0 + \beta_1 x_i + \varepsilon_i \\
\varepsilon &\sim N(0, \sigma)
\label{eq:spec1}
\end{align}

That is, we can think of a response as the sum of some systematic part
and ``random errror'', which is a random draw from a normal distribution
with mean zero and variance \(\sigma^2\). This way of thinking about the
generation of the response is useful for linear models, and model
checking linear models, but is not useful for generalized linear models
or model checking generalized liner models. For example, if we want to
model the number of parasites that have infected a fish using a Poisson
distribution, the following is the \textbf{wrong} way to think about the
statistical model

\begin{align}
y_i &= \beta_0 + \beta_1 x_i + \varepsilon_i\\
\varepsilon &\sim Poisson(\lambda)
\label{eq:spec1-poisson}
\end{align}

That is, we should not think of a count as the sum of a systematic part
and a random draw from a Poisson distribution. Why? Because it is the
counts (or the counts conditional on \(X\)) that are poisson
distributed, not the residuals from the fit model.

Thinking about the distribution of count data using model
\eqref{eq:spec1-poisson} leads to absurd consequences. For example, if we
set the mean of the Poisson ``error'' to zero (like with a normal
distribution), then the error term for every observation would
\emph{have to} be zero (because the only way to get a mean of zero with
non-negative integers is if every value is zero). Or, if the study is
modeling the effect of a treatment on the counts (that is, the \(X\) are
dummy variables) then \(\beta_0\) is the expected mean count of the
control (or reference) group. But if we add non-zero Poisson error to
this, then the mean of the control group would be larger than
\(\beta_0\). This doesn't make sense. And finally, equation
\eqref{eq:spec1-poisson} generates a continuous response, instead of an
integer, because \(\beta_0\) and \(\beta_1\) are continuous.

A better way to think about the data generation for a linear model,
because this naturally leads to the \emph{correct} way to think about
data generation for a generalized linear model, is

\begin{align}
y_i &\sim N(\mu_i, \sigma)\\
\mathrm{E}(Y|X) &= \mu\\
\mu_i &= \beta_0 + \beta_1 x_i
\label{eq:lm-spec2}
\end{align}

That is, a response is a random draw from a normal distribution with
mean \(mu\) (not zero!) and variance \(\sigma^2\). Line 1 is the
stochastic part of this specification. Line 3 is the systematic part.

The specification of a generalized linear model has both stochastic and
systematic parts but adds a third part, which is a \textbf{link
function} connecting the stochastic and systematic parts.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The stochastic part}, which is a probability distribution from
  the exponential family (this is sometimes called the ``random part'')

  \begin{equation}
  y_i \sim \mathrm{Prob}(\mu_i)
  \end{equation}
\item
  \textbf{the systematic part}, which is a linear predictor (I like to
  think about this as the deterministic part)

  \begin{equation}
  \eta=\mathbf{X}\boldsymbol{\beta}
  \end{equation}
\item
  a \textbf{link function} connecting the two parts

  \begin{equation}
  \eta_i = g(\mu_i)
  \end{equation}
\end{enumerate}

\(\mu\) (the Greek symbol mu) is the conditional mean (or expectation
\(\mathrm{E}(Y|X)\)) of the response on the \textbf{response scale} and
\(\eta\) (the Greek symbol eta) is the conditional mean of the response
on the \textbf{link scale}. A GLM models the response with a
distribution specified in the stochastic part. The probability
distributions introduced in this chapter are the Poisson and Negative
Binomial. The natural link function for the Poisson and Negative
Binomial is the ``log link'', \(\eta = log(\mu)\). More generally, while
each distribution has a natural (or, ``canonical'') link function, one
can use alternatives. Given this definition of a generalized linear
model, a linear model is a GLM with a normal distribution and an
Identity link (\(\eta = \mu\)).

When modeling counts using the Poisson or negative binomial
distributions with a log link, the link scale is linear, and so the
effects are additive on the link scale, while the response scale is
nonlinear (it is the exponent of the link scale), and so the effects are
multiplicative on the response scale. If this doesn't make sense now, an
example is worked out below. The inverse of the link function
backtransforms the parameters from the link scale back to the response
scale. So, for example, a prediction on the response sale is
\(\mathrm{exp}(\hat{\eta})\) and a coefficient on the response scale is
\(\mathrm{exp}(b_j)\).

\section{Count data example -- number of trematode worm larvae in eyes
of threespine stickleback
fish}\label{count-data-example-number-of-trematode-worm-larvae-in-eyes-of-threespine-stickleback-fish}

The example is an experiment measuring the effect of the parasitic
tapeworm \emph{Schistocephalus solidus} infection on the susceptibility
of infection from a second parasite, the trematode \emph{Diplostomum
pseudospathaceum}, in the threespine stickleback fish \emph{Gasterosteus
aculeatus}\footnote{Benesh, D. P., \& Kalbe, M. (2016). Experimental
  parasite community ecology: intraspecific variation in a large
  tapeworm affects community assembly. Journal of Animal Ecology, 85(4),
  1004-1013}. The treatment levels are ``Control'' (unexposed to the
tapeworm), ``Uninfected'' " (exposed to the tapeworm but uninfected),
``Infected LG'' (exposed and infected with the low growth population of
the tapeworm), and ``Infected HG'' (exposed and infected with the high
growth population of tapeworm). The response is the number of trematode
larvae counted in the eyes (right and left combined) of the fish. A
histogram of the counts is shown in Figure \ref{fig:glm1-plot1} for the
control and Infected HG treatment levels.

\subsection{Modeling strategy}\label{modeling-strategy-1}

\textbf{NHST blues} -- Students are often encouraged by textbooks,
colleagues, or the literature to start the analysis by first ``testing''
assumptions with hypothesis tests -- for example using a Shaprio-Wilks
test of normality as a decision rule to decide if to use a parametric
test such as a \(t\)-test or ANOVA if the null hypothesis of normality
is not rejected, or a non-parametric test such as a Mann-Whitney U test
if the null hypothesis of normality is rejected. I advise against this,
because 1) this pre-test filtering automatically invalidates the
\(p\)-value of the hypothesis test as it does not adjust for the
filtering procedure, 2) real data are only approximately normal and as
\(n\) increses, a normality test will reject any real dataset, and 3)
hypothesis tests are pretty robust to non-normality anyway.

Instead of testing assumptions of a model using formal hypothesis tests
\emph{before} fitting the model, a better strategy is to 1) fit a model,
and then do 2) \textbf{model checking} using \textbf{diagnostic plots},
diagnostic statistics, and simulation.

With these data, a researcher would typically fit a GLM with a Poisson
or negative binomial distribution and log link. Here, I start with a
linear model to illustrate the interpretation of diagnostic plots with
non-normal data. I use the ``linear model'' specification (equation
\eqref{eq:spec1}) because the diganostic plots for model checking a linear
model use the residuals of the fit model.

\begin{align}
Diplo\_intensity_i &= \beta_0 + \beta_1 Uninfected_i + \beta_2 Infected\_LG_i + \beta_3 Infected\_HG_i + \varepsilon_i \\
\varepsilon &\sim N(0, \sigma)
\label{eq:worm-lm}
\end{align}

\subsection{Checking the model I -- a Normal Q-Q
plot}\label{checking-the-model-i-a-normal-q-q-plot}

Figure \ref{fig:glm1-plot2}A shows a histogram of the residuals from the
fit linear model. The plot shows that the residuals are clumped at the
negative end of the range, which suggests that a model with a normally
distributed conditional outcome (or normal error) is not well
approximated.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-plot2-1.pdf}
\caption{\label{fig:glm1-plot2}Diagnostic plots of stickleback parasite
data. A) Distribution of the residuals of the fit linear model. B)
Normal Q-Q plot of the residuals of the fit linear model.}
\end{figure}

A better way to investigate this is with the \textbf{Normal Q-Q} plot in
Figure \ref{fig:glm1-plot2}B, which plots the sample quantiles for a
variable against their theoretical quantiles. If the conditional outcome
approximates a normal distribution, the points should roughly follow the
line. Instead, for the worm data, the points are above the line at both
ends. At the left (negative) end, this means that we aren't seeing the
most negative values that would be expected (the observed values are
more positive than the theoretical values). Remembering that this plot
is of residuals, if we think about this as counts, this means that our
smallest counts are not as small as we would expect given the mean and a
normal distribution. This shouldn't be surprising -- the counts range
down to zero and counts cannot be below zero. At the positive end, the
sample values are again more positive than the theoretical values.
Thinking about this as counts, this means that are largest counts are
larger than expected given the mean and a normal distribution. This
pattern is exactly what we'd expect of count data, or at least count
data that borders zero.

\textbf{Intuition Pump} -- Let's construct a Normal Q-Q plot. A
\textbf{quantile} (or percentile) of a vector of numbers is the value of
the point at a specified percentage rank. The median is the 50\%
quantile. The 95\% confidence intervals are at the 2.5\% and 97.5\%
quantiles. In a Normal Q-Q plot, we want to plot the quantiles of the
residuals against a set of theoretical quantiles.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To get the observed quantiles, rank the residuals of the fit linear
  model from most negative to most positive -- these are your quantiles!
  For example, if you have \(n=145\) residuals, then the 73rd point is
  the 50\% quantile.
\item
  A theoretical quantile from the normal distribution can be constructed
  using the \texttt{qnorm} function which returns the normal quantiles
  for a specified vector of percents. Alternatively, one could randomly
  sample \(n\) points using \texttt{rnorm}. These of course will be
  sampled quantiles so will only approximate the expected theoretical
  quantiles, but I add this here because we use this method below.
\end{enumerate}

Now simply plot the observed against theoretical quantiles. Often, the
\textbf{standardized} quantiles are plotted. A standardized variable has
a mean of zero and a standard deviation of one and is computed by 1)
centering the vector at zero by subtracting the mean from every value,
and 2) dividing each value by the standard deviation of the vector.
Recognize that because a standard deviation is a function of deviations
from the mean, it doesn't matter which of these operations is done
first. A standardized theoretical quantile is specified by
\texttt{qnorm(p,\ mean\ =\ 0,\ sd\ =\ 1)}, which is the default.

Below, I've plotted the standardized observed and theoretical quantiles
against the vector of percents (from 0 to 100\%). This plot also nicely
shows how the residuals of the worm data deviate from that expected if
these had a normal distribution. The plot nicely shows that the most
negative observed quintiles are not as negative as expected given a
normal distribution, which again makes sense because this would imply
negative counts since the mean is close to zero. And it nicely shows
that the most positive observed quantiles are more positive than
expected given a normal distribution, again this makes sense in right
skewed count data. Finally, the plot nicely shows that the median is
less positive than that expected given a normal distribution, which is
at the mean (a right skew tends to pull the mean to the right of the
median).

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-qqplot-sim-1.pdf}

\subsection{Checking the model II -- scale-location plot for checking
homoskedasticity}\label{checking-the-model-ii-scale-location-plot-for-checking-homoskedasticity}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-worm-scale-location-1.pdf}

A linear model also assumes the error has constant variance (that is,
the error variance is not a function of the value of \(X\)), or
homoskedasticity. The fit model can be checked for homoskedasticity
using a scale-location plot, which is a scatterplot of the positive
square-root of the standardized residuals against the fitted
values\footnote{fitted values are the predicted values, \(\hat{Y}\)}. If
the residuals approximate a normal distribution, then a regression line
through the scatter should be close to horizontal. The regression line
in the scale-location plot of the fit of the linear model to the worm
data shows a distinct increase in the ``scale'' (the square root of the
standardized residuals) with increased fitted value, which is expected
of data that are lognormally, Poisson, or negative binomially
distributed.

\subsection{Two distributions for count data -- Poisson and Negative
Binomial}\label{two-distributions-for-count-data-poisson-and-negative-binomial}

The pattern in the normal Q-Q plot in Figure \ref{fig:glm1-plot2}B
should discourage one from modeling the data with a normal distribution
and instead model the data with an alternative distribution using a
Generalized Linear Model. There is no unique mapping between how data
are generated and a specific distribution, so this decision is not as
easy as thinking about the data generation mechanism and then simply
choosing the ``correct'' distribution. Section 4.5 in Bolker (xxx) is an
excellent summary of how to think about the generating processes for
different distributions in the context of ecological data. Since the
response in the worm data are counts, we need to choose a distribution
that generates integer values, such as the Poisson or the negative
binomial.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Poisson -- A Poisson distribution is the probability distribution of
  the number of occurrences of some thing (an egg, a parasite, or a
  specific mRNA transcript) generated by a process that generates the
  thing at a constant rate per unit effort (duration or space). This
  constant rate is \(\lambda\), which is the expectation, so
  \(\mathrm{E}(Y) = \mu = \lambda\). Because the rate per effort is
  constant, \emph{the variance of a Poisson variable equals the mean},
  \(\sigma^2 = \mu = \lambda\). Figure \ref{fig:glm1-poisson} shows
  three samples from a Poisson distribution with \(\lambda\) set to 1,
  5, and 10. The plots show that, as the mean count (\(\lambda\)) moves
  away from zero, a Poisson distribution 1) becomes less skewed and more
  closely approximates a normal distribution and 2) has an increasingly
  low probability of including zero (less than 1\% zeros when the mean
  is 5).
\end{enumerate}

A Poisson distribution, then, is useful for count data in which the
conditional variance is close to the conditional mean. Very often,
biological count data are not well approximated by a Poisson
distribution because the variance is either less than the mean, an
example of \textbf{underdispersion}\footnote{the variance is less than
  that expected by the probability model}, or greater than the mean, an
example of \textbf{overdispersion}\footnote{the variance is greater than
  that expected by the probability model}. A useful distribution for
count data with overdispersion is the negative binomial.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-poisson-1.pdf}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Negative Binomial -- The negative binomial distribution is a discrete
  probability distribution of the number of successes that occur before
  a specified number of failures \(k\) given a probability \(p\) of
  success. This isn't a very useful way of thinking about modeling count
  data in biology. What is useful is that the Negative Binomial
  distribution can be used simply as way of modeling an
  ``overdispersed'' Poisson process. The mean of a negative binomial
  variable is \(\mu = k\frac{p}{1-p}\) and the variance is
  \(\sigma^2 = \mu + \mu^2/k\). As a method for modeling an
  overdispersed Poisson variable, \(k\) functions as a **dispersion
  parameter* controlling the amount of overdispersion and can be any
  real, positive value (not simply a positive integer), including values
  less than 1.
\end{enumerate}

\subsection{Fitting a GLM with a Poisson distribution to the worm
data}\label{fitting-a-glm-with-a-poisson-distribution-to-the-worm-data}

Let's fit a GLM with a Poisson distribution to the worm data. The model
is

\begin{align}
Diplo\_intensity_i &\sim Poisson(\mu_i)\\
\mathrm{E}({Diplo\_intensity|Treatment}) &= \mu\\
\mu_i &= \mathrm{exp}(\eta_i)\\
\eta_i &= \beta_0 + \beta_1 Uninfected_i + \beta_2 Infected\_LG_i + \beta_3 Infected\_HG_i
\end{align}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first line of the model is the stochastic part stating the
  response is modeled as a random Poisson variable with mean and
  variance \(\mu\) (the rate parameter \(\lambda\) of the Poisson
  distribution).
\item
  The second line states the \(\mu\) is the conditional mean or
  conditional expectation
\item
  The third line connects the conditional mean on the link scale
  (\(\eta\)) with the conditional mean on the response scale (\(\mu\))
\item
  The fourth line is the linear predictor, and includes three dummy
  variables.
\end{enumerate}

Remember that the conditional mean is the
expected/predicted/fitted/modeled value when \(X=x_i\).

\subsection{Model checking fits to count
data}\label{model-checking-fits-to-count-data}

we use the fit model to check 1. the overall similarity of observed and
theoretical distributions 2. if the observed distribution is over or
under dispersed 3. if there more zeros than expected by the theoretical
distribution. If so, the observed distribution is \textbf{zero-inflated}

\subsubsection{Model checking a GLM I -- the quantile residual Q-Q
plot}\label{model-checking-a-glm-i-the-quantile-residual-q-q-plot}

A quantile-quantile (Q-Q) plot is used to check overall similarity of
the observed distribution with the distribution that would be expected
under the model. An alternative to a Normal Q-Q plot for a GLM fit is a
quantile residual Q-Q plot of observed vs.~expected \textbf{quantile
residuals}. The basic algorithm for this is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the model parameters to simulate \(p\) fake values of the response
  for each row of the data. This will be a \(n \times p\) matrix of fake
  data where each column is a new, random sample of a population with
  parameters equal to that estimated for the observed data. For the
  Poisson, the parameter for each observation will be \(\hat{\mu}_i\),
  the modeled value of observation \(i\). For the negative binomial, the
  parameters will be \(\hat{\mu}_i\) and the dispersion parameter \(k\),
  which is the same for all observations.
\item
  For each observation (each row of the matrix of fake data), compute
  the fraction of simulated values smaller than the observed value of
  the response variable for that row. This fraction is the observed
  \textbf{quantile residual}, which ranges in value from 0 to 1. If the
  true data are distribitued as that specified by the model, then
  quantile residuals will have a uniform distribution.
\item
  Sort the observed quantile residuals from smallest to largest and plot
  against theoretical quantile residuals from a uniform distribution.
  One could transform the quantile residuals to standard, normal
  residuals and then plot using a traditional Normal Q-Q plot but this
  step isn't necessary (if reported, a Normal Q-Q plot of transformed
  quantile residuals might confuse readers who failed to read the fine
  print).
\end{enumerate}

\textbf{Misconceivable} -- A common misconception is that if the
distribution of the response approximates a Poisson distribution, then
the residuals of a GLM fit with a Poisson distribution should be
normally distributed, which could then be checked with a Normal Q-Q
plot, and homoskedastic, which could be checked with a scale-location
plot. Neither of these is true because a GLM does not transform the data
and, in fact, the model definition does not specify anything about the
distribution of an ``error'' term -- there is no \(\varepsilon\) in the
model defintion above! This is why thinking about the definition of a
linear model by specifying an error term with a normal distribution can
be confusing and lead to misconceptions when learning GLMs.

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-worm-poisson-qr-1.pdf}

The Q-Q plot using quantile residuals with a Poisson distribution
indicates that the counts of \emph{Diplostomum} larvae in the eyes of
the threespine stickleback are not well approximated by a Poisson
distribution -- there are too many observed values near the ends of the
expected tails, indicating the expected values are not spread out
enough. This pattern emerges because the observed counts are
overdispersed compared to a Poisson distribution.

\subsubsection{Model checking a GLM II -- a dispersion
plot}\label{model-checking-a-glm-ii-a-dispersion-plot}

If observed counts are Poisson distributed, then the \textbf{Pearson
residuals} (\(r_i\)) and the residual degrees of freedom of the fit
model (\(df\)) can be used to compute a dispersion statistic

\begin{equation}
\frac{\sum{r_i}}{df}
\end{equation}

that has an expected value of 1. Instead of a formal hypothesis test of
this statistic, I use a simulation approach and ask, ``if the observed
counts are Poisson distributed, what is the expected frequency
distribution of this dispersion statistic?'' and then use simulation to
generate this expected distribution. The algorithm for this is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For each observation \(i\), generate a random Poisson count using
  \(\hat{\mu}\) as the parameter.
\item
  Fit the model and compute the dispersion statistic.
\item
  Repeat 1 and 2 \(N_{iter}\) times.
\end{enumerate}

The plot below shows a histogram of the dispersion statistic computed
for 1000 simulations of the worm data. The observed dispersion statistic
is 3.4. The expected value is 1.0. The mean of the simulated values is
1.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-worm-poisson-dispersion-plot-1.pdf}
\caption{\label{fig:glm1-worm-poisson-dispersion-plot}Observed vs.~expected
dispersion statistic. The observed statistic marked by the red line. The
histogram of expected statistics are from 1000 simulations of the
observed data.}
\end{figure}

\subsection{Fitting a GLM with a Negative Binomial distribution to the
worm
data}\label{fitting-a-glm-with-a-negative-binomial-distribution-to-the-worm-data}

The model is

\begin{align}
Diplo\_intensity &\sim NB(\mu, k)\\
\mathrm{E}({Diplo\_intensity|Treatment}) &= \mu\\
\mu &= \mathrm{exp}(\eta)\\
\eta &= \beta_0 + \beta_1 Uninfected + \beta_2 Infected\_LG + \beta_3 Infected\_HG
\end{align}

This model specifies a negative binomial distribution but otherwise is
just like that above specifying a Poisson distribution.

\subsubsection{Model checking}\label{model-checking-2}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-worm-nb-1.pdf}

A quantile residual Q-Q plot of the GLM model fit with negative binomial
distribution is illustrated above. This looks pretty good.

\subsubsection{Model means and
coefficients}\label{model-means-and-coefficients}

In a Generalized Linear Model of counts using either a Poisson or
negative binomial distribution, modeled means, coefficients, and
contrasts can be reported either on the link or response scale.
Remember, the response scale is a count, while the link scale is a
log(count).

The modeled means on the link scale are

\begin{verbatim}
##  Treatment   emmean     SE  df asymp.LCL asymp.UCL
##  Control       1.82 0.0804 Inf      1.66      1.98
##  Uninfected    1.50 0.1093 Inf      1.29      1.72
##  Infected LG   1.62 0.1362 Inf      1.36      1.89
##  Infected HG   2.36 0.0714 Inf      2.22      2.50
## 
## Results are given on the log (not the response) scale. 
## Confidence level used: 0.95
\end{verbatim}

While the means on response scale are

\begin{verbatim}
##  Treatment   response    SE  df asymp.LCL asymp.UCL
##  Control         6.18 0.497 Inf      5.28      7.24
##  Uninfected      4.50 0.492 Inf      3.63      5.58
##  Infected LG     5.07 0.691 Inf      3.89      6.63
##  Infected HG    10.60 0.757 Inf      9.22     12.20
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A mean on the response scale is simply the exponent of the mean on the
  link scale. For example, the mean of the Control treatment level on
  the response scale is \(\mathrm{exp}(1.821408) = 6.180555\).
\item
  The CIs on the link scale are symmetric around the mean but those on
  the response scale are not. This is a feature, not a bug. Remember
  that counts are right skewed which means a CI will have a wider right
  than left interval. Check this!
\item
  If a plot includes a 1 SE error bar on the response scale, this is
  technically correct but it encourages the practice of computing CIs
  using the 2*SE rule of thumb. This rule breaks down for count data
  with right skewed distributions.
\item
  Plotting the response scale CIs is both technically correct and makes
  the 2*SE rule of thumb unnecessary.
\end{enumerate}

The model coefficients on the link scale are

\begin{verbatim}
##  contrast              estimate    SE  df asymp.LCL asymp.UCL z.ratio
##  Uninfected - Control    -0.317 0.136 Inf    -0.583   -0.0514 -2.339 
##  Infected LG - Control   -0.197 0.158 Inf    -0.507    0.1126 -1.248 
##  Infected HG - Control    0.540 0.108 Inf     0.329    0.7504  5.019 
##  p.value
##  0.0193 
##  0.2122 
##  <.0001 
## 
## Results are given on the log (not the response) scale. 
## Confidence level used: 0.95
\end{verbatim}

Backtransforming the coefficients (but not the intercept) to the
response scale (using \(\mathrm{exp}{b_j}\)) results in a
\textbf{response ratio}.

\begin{verbatim}
##  contrast              ratio     SE  df asymp.LCL asymp.UCL z.ratio
##  Uninfected / Control  0.728 0.0988 Inf     0.558      0.95 -2.339 
##  Infected LG / Control 0.821 0.1298 Inf     0.602      1.12 -1.248 
##  Infected HG / Control 1.715 0.1845 Inf     1.389      2.12  5.019 
##  p.value
##  0.0193 
##  0.2122 
##  <.0001 
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale 
## Tests are performed on the log scale
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Note how the emmeans package reports the name of the term as the ratio
  of the coefficient term to the intercept term (the reference treatment
  level). Why are the coefficients tranformed to ratios on the response
  scale? Remember that a coefficient is a difference in conditional
  means and that
  \(\mathrm{exp}(B-A) = \frac{\mathrm{exp}(B)}{\mathrm{exp}(A)}\). For a
  dummy variable as here (say ``Infected HG''), the response ratio is
\end{enumerate}

\begin{equation}
RR_{Infected\_HG} = \frac{\overline{Infected\_HG}}{\overline{Control}}
\end{equation}

which give us the \textbf{relative effect} of Infected\_HG compared to
the Control. Relative effects could be reported as a response ratio in a
table, or in the text it could be reported as a percent ``Infected HG
fish had 71.5\% (95\%CI: 38.9\% - 111.8\%) more \emph{Diplostomum}
larvae than Control fish.'' Where do these percents come from? The
percent effect is \(100(RR_j - 1)\) larger than the reference mean if
the \(RR_j > 1\) or \(100(1 - RR_j)\) smaller than the reference mean if
the \(RR_j < 1\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Backtransforming the intercept does not generate a ratio since the
  intercept on the link scale is not a difference. For the worm
  analysis, the intercept on the link scale is the mean count of the
  control group on the link scale and the backtransformed intercept is
  the mean count of the control group on the response scale.
\item
  Effects on the response scale are not additive but multiplicative! So,
  for example, the mean of the Infected HG treatment level on the
  response scale is \(\overline{Control}*RR_{Infected\_HG}\) (remember
  that with a linear model the mean would be
  \(b_{Control} + b_{Infected\_HG}\)). Check and see if this works.
\end{enumerate}

\section{Working in R}\label{working-in-r-7}

\textbf{Fitting a GLM to count data}. The poisson family is specified
with the base R \texttt{glm()} function. For negative binomial, use
\texttt{glm.nb} from the MASS package

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# poisson - less likely to fit to real biological data well because of overdispersion}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{family =} \StringTok{"poisson"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}

\CommentTok{# two alternatives to overdispersed poisson fit}
\CommentTok{# quasipoisson}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{family =} \StringTok{"quasipoisson"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{dt)}
\CommentTok{# negative binomial - more likely to fit to real biological data well}
\CommentTok{# note that "family" is not an argument since this function is used only to fit a negative binomial distribution!}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm.nb}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data =}\NormalTok{ dt)}
\end{Highlighting}
\end{Shaded}

\textbf{Fitting a GLM to a continuous conditional response with right
skew}. The Gamma family is specified with the base R \texttt{glm()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ dt)}
\end{Highlighting}
\end{Shaded}

\textbf{Fitting a GLM to a binary (success or failure, presence or
absence, survived or died) response}

The binomial family is specified with base R \texttt{glm()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# if the data includes a 0 or 1 for every observation of y}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}

\CommentTok{# if the data includes the frequency of success AND there is a measure of the total n}
\NormalTok{dt[ , failure }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\NormalTok{n }\OperatorTok{-}\StringTok{ }\NormalTok{success]}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(success, failure) }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}
\end{Highlighting}
\end{Shaded}

\textbf{Fitting Generalized Linear Mixed Models} Generalized linear
mixed models are fit with \texttt{glmer} from the lmer package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# random intercept of factor "id"}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{id), }\DataTypeTok{family =} \StringTok{"poisson"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}

\CommentTok{# random intercept and slope of factor "id"}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(treatment}\OperatorTok{|}\NormalTok{id), }\DataTypeTok{family =} \KeywordTok{Gamma}\NormalTok{(}\DataTypeTok{link =} \StringTok{"log"}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ dt)}

\CommentTok{# Again, negative binomial uses a special function}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmer.nb}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(treatment}\OperatorTok{|}\NormalTok{id), }\DataTypeTok{data =}\NormalTok{ dt)}
\end{Highlighting}
\end{Shaded}

Another good package for GLMMs is \texttt{glmmTMB} from the glmmTMB
package

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# negative binomial}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmmTMB}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{id), }\DataTypeTok{family=}\StringTok{"nbinom2"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}

\CommentTok{# nbinom1, the mean variance relationship is that of quasipoisson}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmmTMB}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{id), }\DataTypeTok{family=}\StringTok{"nbinom1"}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ dt)}
\end{Highlighting}
\end{Shaded}

\subsection{Fitting a GLM to count
data}\label{fitting-a-glm-to-count-data}

Source publication: Benesh, D. P., \& Kalbe, M. (2016). Experimental
parasite community ecology: intraspecific variation in a large tapeworm
affects community assembly. Journal of Animal Ecology, 85(4), 1004-1013.

Source data URL:
\url{https://datadryad.org/resource/doi:10.5061/dryad.bq8j8}

Source file: ``Lab\_exp.csv''

Poisson fit. A quantile residual Q-Q plot can be generated using the
package DHARMa

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.pois <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(Diplo_intensity }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{family=}\StringTok{"poisson"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{worm)}

\CommentTok{# from the DHARMa package}
\NormalTok{  n_sim <-}\StringTok{ }\DecValTok{250}
\NormalTok{  simulationOutput <-}\StringTok{ }\KeywordTok{simulateResiduals}\NormalTok{(}\DataTypeTok{fittedModel =}\NormalTok{ fit.pois, }\DataTypeTok{n =}\NormalTok{ n_sim)}
  \KeywordTok{plot}\NormalTok{(simulationOutput, }\DataTypeTok{asFactor =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm1-working-in-r-1.pdf}

A plot of the dispersion statistic can be generated using the object
returned by the \texttt{SimulateOutput} function but with
\texttt{refit\ =\ TRUE}, which refits a model each iteration. This
refitting isn't necessary if only the quantiile residuals are needed.
The Dharma package does not divide the sum of squared Pearson residuals
by the residual degrees of freedom and so the expected value of the
statistic is \(df\).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# from the DHARMa package}
\NormalTok{  n_sim <-}\StringTok{ }\DecValTok{250}
\NormalTok{  simulationOutput <-}\StringTok{ }\KeywordTok{simulateResiduals}\NormalTok{(}\DataTypeTok{fittedModel=}\NormalTok{fit.pois, }\DataTypeTok{n=}\NormalTok{n_sim, }\DataTypeTok{refit=}\OtherTok{TRUE}\NormalTok{)}
  \KeywordTok{testDispersion}\NormalTok{(simulationOutput)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-112-1.pdf}

\begin{verbatim}
## 
##  DHARMa nonparametric dispersion test via mean deviance residual
##  fitted vs. simulated-refitted
## 
## data:  simulationOutput
## dispersion = 3.3788, p-value < 2.2e-16
## alternative hypothesis: two.sided
\end{verbatim}

Negative binomial fit.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.nb <-}\StringTok{ }\KeywordTok{glm.nb}\NormalTok{(Diplo_intensity }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data=}\NormalTok{worm)}
\CommentTok{# from the DHARMa package}
\NormalTok{  simulationOutput <-}\StringTok{ }\KeywordTok{simulateResiduals}\NormalTok{(}\DataTypeTok{fittedModel =}\NormalTok{ fit.nb, }\DataTypeTok{n =}\NormalTok{ n_sim)}
  \KeywordTok{plot}\NormalTok{(simulationOutput, }\DataTypeTok{asFactor =}\NormalTok{ F)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/glm-working-in-r-2-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# link scale}
\NormalTok{emm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit.nb, }\DataTypeTok{specs=}\StringTok{"Treatment"}\NormalTok{)}
\NormalTok{emm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Treatment   emmean     SE  df asymp.LCL asymp.UCL
##  Control       1.82 0.0804 Inf      1.66      1.98
##  Uninfected    1.50 0.1093 Inf      1.29      1.72
##  Infected LG   1.62 0.1362 Inf      1.36      1.89
##  Infected HG   2.36 0.0714 Inf      2.22      2.50
## 
## Results are given on the log (not the response) scale. 
## Confidence level used: 0.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(emm, }\DataTypeTok{method=}\StringTok{"trt.vs.ctrl"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast              estimate    SE  df asymp.LCL asymp.UCL z.ratio
##  Uninfected - Control    -0.317 0.136 Inf    -0.583   -0.0514 -2.339 
##  Infected LG - Control   -0.197 0.158 Inf    -0.507    0.1126 -1.248 
##  Infected HG - Control    0.540 0.108 Inf     0.329    0.7504  5.019 
##  p.value
##  0.0193 
##  0.2122 
##  <.0001 
## 
## Results are given on the log (not the response) scale. 
## Confidence level used: 0.95
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# response scale}
\NormalTok{emm.response <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit.nb, }\DataTypeTok{specs=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(emm, }\DataTypeTok{method=}\StringTok{"trt.vs.ctrl"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{, }\DataTypeTok{type=}\StringTok{"response"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast              ratio     SE  df asymp.LCL asymp.UCL z.ratio
##  Uninfected / Control  0.728 0.0988 Inf     0.558      0.95 -2.339 
##  Infected LG / Control 0.821 0.1298 Inf     0.602      1.12 -1.248 
##  Infected HG / Control 1.715 0.1845 Inf     1.389      2.12  5.019 
##  p.value
##  0.0193 
##  0.2122 
##  <.0001 
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the log scale 
## Tests are performed on the log scale
\end{verbatim}

\subsection{Fitting a generalized linear mixed model (GLMM) to count
data}\label{fitting-a-generalized-linear-mixed-model-glmm-to-count-data}

\subsection{Fitting a generalized linear model to continouus
data}\label{fitting-a-generalized-linear-model-to-continouus-data}

\section{Problems}\label{problems-3}

Analyze the data that went into Fig 6B of Tena, A., Pekas, A., Cano, D.,
Wäckers, F. L., \& Urbaneja, A. (2015). Sugar provisioning maximizes the
biocontrol service of parasitoids. Journal of Applied Ecology, 52(3),
795-804.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute contrasts and CIs among all pairs of all three treatment
  levels
\item
  Make a better plot like 6b including 1) use the modeled mean instead
  of the simple group mean and 2) use the modeled CI of the mean instead
  of the SE computed within each group independently.
\end{enumerate}

source URL: \url{https://datadryad.org/resource/doi:10.5061/dryad.bj001}

source file: ``4\_Parastism\_Fig\_6.csv''

\chapter{Linear models with heterogenous
variance}\label{linear-models-with-heterogenous-variance}

\section{gls}\label{gls}

\chapter*{Appendix 1: Getting Started with
R}\label{appendix-1-getting-started-with-r}
\addcontentsline{toc}{chapter}{Appendix 1: Getting Started with R}

\section{Get your computer ready}\label{get-your-computer-ready}

\subsection{Install R}\label{install-r}

R is the core software

\href{https://cran.r-project.org}{Download R for your OS}

\subsection{Install R Studio}\label{install-r-studio}

R Studio is a slick (very slick) GUI interface for developing R projects

\href{https://www.rstudio.com/products/rstudio/download/}{Download R
Studio Desktop}

\subsection{Resources for installing R and R
Studio}\label{resources-for-installing-r-and-r-studio}

\href{https://medium.com/@GalarnykMichael/install-r-and-rstudio-on-windows-5f503f708027}{On
Windows}

\href{https://medium.com/@GalarnykMichael/install-r-and-rstudio-on-mac-e911606ce4f4}{On
a Mac}

\subsection{Install LaTeX}\label{install-latex}

LaTeX (``la-tek'') is necessary to use the pdf output of R Markdown.

\href{https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-windows-10-952b0c48bfa9}{On
Windows}

\href{https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-macos-high-sierra-e7b5705c9fd}{On
a Mac}

\section{Start learning}\label{start-learning}

\subsection{Start with Data Camp Introduction to
R}\label{start-with-data-camp-introduction-to-r}

\href{https://www.datacamp.com/courses/free-introduction-to-r}{Data
Camp: Introduction to R (free online course)}

\subsection{Then Move to Introduction to R
Studio}\label{then-move-to-introduction-to-r-studio}

\href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-part-1/}{R
Studio Essentials, Programming Part 1 (Writing code in RStudio)}

\subsection{Develop your project with an R Studio
Notebook}\label{develop-your-project-with-an-r-studio-notebook}

\href{https://www.rstudio.com/resources/webinars/getting-started-with-r-markdown/}{Getting
Started with R Markdown}

\href{https://www.rstudio.com/resources/webinars/introducing-notebooks-with-r-markdown/}{Introducing
Notebooks with R Markdown}

\section{Getting Data into R}\label{getting-data-into-r}

\href{https://www.rstudio.com/resources/webinars/getting-your-data-into-r/}{Getting
your data into R}

\section{Additional R learning
resources}\label{additional-r-learning-resources}

\href{https://bookdown.org/chesterismay/rbasics/}{Getting used to R,
RStudio, and R Markdown}

\href{https://www.rstudio.com/resources/webinars/}{Link to list of R
Studio webinars}

\href{https://www.rstudio.com/resources/cheatsheets/}{Link to set of R
package cheat sheets (amazing!)}

\href{https://bookdown.org}{Bookdown online books}

\section{Packages used extensively in this
text}\label{packages-used-extensively-in-this-text}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ggplot2
\item
  data.table
\item
  mvtnorm
\item
  lme4
\item
  nlme
\item
  emmeans
\item
  readxl
\item
  reshape2
\end{enumerate}

\href{http://r4ds.had.co.nz/data-visualisation.html}{Data Visualisation
chapter from \emph{R for Data Science}}

\href{http://r4ds.had.co.nz/graphics-for-communication.html}{Graphics
for communication chapter from \emph{R for Data Science}}

Youtube: \href{https://www.youtube.com/watch?v=pc1ARG6kbAM}{An
Introduction to The data.table Package}

Coursera:
\href{https://www.coursera.org/learn/data-cleaning/lecture/trMZ7/the-data-table-package}{The
data.table Package}

\chapter*{Appendix 2: Online Resources for Getting Started with
Statistical Modeling in
R}\label{appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r}
\addcontentsline{toc}{chapter}{Appendix 2: Online Resources for Getting
Started with Statistical Modeling in R}

Roughly, in order from most elementary to most advanced

\href{https://https://learningstatisticswithr-bookdown.netlify.com}{Learning
Statistics with R} by Danielle Navarro and adapted to Bookdown (for web
viewing) by Emily Kothe.

\href{http://statsthinking21.org}{Statististical Thinking for the 21st
Century} by Russell A. Poldrack

\href{https://leanpub.com/regmods}{Regression Models for Data Science in
R} by Brian Caffo

\href{https://bookdown.org/roback/bookdown-bysh/}{Broadening Your
Statistical Horizons: Generalized Linear Models and Multilevel Models by
J. Legler and P. Roback}

\href{https://www.huber.embl.de/msmb/index.html}{Modern Statistics for
Modern Biology}

\href{https://bookdown.org/rdpeng/artofdatascience/}{The Art of Data
Science by Roger D. Peng and Elizabeth Matsui}

\chapter*{Appendix 3: Fake Data
Simulations}\label{appendix-3-fake-data-simulations}
\addcontentsline{toc}{chapter}{Appendix 3: Fake Data Simulations}

\section{Performance of Blocking relative to a linear
model}\label{performance-of-blocking-relative-to-a-linear-model}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use these to debug code}
\NormalTok{sigma=}\DecValTok{1}
\NormalTok{sigma_b0=}\DecValTok{1}
\NormalTok{sigma_b1=}\DecValTok{1}
\NormalTok{beta_}\DecValTok{0}\NormalTok{=}\DecValTok{10}
\NormalTok{beta_}\DecValTok{1}\NormalTok{=}\DecValTok{1}
\NormalTok{n_batch=}\DecValTok{4}
\NormalTok{n_subsamp=}\DecValTok{2}
\NormalTok{y_label=}\StringTok{"Y"}
\NormalTok{trt_levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{,}\StringTok{"tr"}\NormalTok{)}
\NormalTok{block_label=}\StringTok{"block"}

\NormalTok{fake_lmm_data <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{iterations=}\DecValTok{1000}\NormalTok{, }\DataTypeTok{sigma=}\DecValTok{1}\NormalTok{, }\DataTypeTok{sigma_b0=}\DecValTok{1}\NormalTok{, }\DataTypeTok{sigma_b1=}\DecValTok{1}\NormalTok{, }\DataTypeTok{beta_0=}\DecValTok{10}\NormalTok{, }\DataTypeTok{beta_1=}\DecValTok{1}\NormalTok{, }\DataTypeTok{n_batch=}\DecValTok{6}\NormalTok{, }\DataTypeTok{n_subsamp=}\DecValTok{10}\NormalTok{, }\DataTypeTok{y_label=}\StringTok{"y"}\NormalTok{, }\DataTypeTok{trt_levels=}\KeywordTok{c}\NormalTok{(}\StringTok{"cn"}\NormalTok{,}\StringTok{"tr"}\NormalTok{), }\DataTypeTok{batch_label=}\StringTok{"block"}\NormalTok{, }\DataTypeTok{confound=}\OtherTok{FALSE}\NormalTok{)\{}
  \CommentTok{# this function is constrained to simulate a single treatment with two levels}
  \CommentTok{#}
  \CommentTok{#                   arguments }
  \CommentTok{# iterations - number of datasets to generate}
  \CommentTok{# sigma: conditional error sd}
  \CommentTok{# sigma_b0: sd of random intercepts}
  \CommentTok{# sigma_b1: sd of random slope}
  \CommentTok{# beta_0: fixed intercept (mean of reference)}
  \CommentTok{# beta_1: fixed slope (difference tr - cn)}
  \CommentTok{# n_batch: number of batches}
  \CommentTok{# n_subsamp: number of observations per batch per treatment level}
  \CommentTok{# confound: FALSE is randomized complete block, TRUE is confounded case where}
  \CommentTok{#       there is only one treatment level per batch}
  \CommentTok{#}
  \CommentTok{#                    output}
  \CommentTok{# A single matrix with each dataset stacked. The datasets are identified by}
  \CommentTok{# the first column ("data_id")}
  
  \ControlFlowTok{if}\NormalTok{(sigma_b0}\OperatorTok{==}\DecValTok{0}\NormalTok{)\{sigma_b0 <-}\StringTok{ }\FloatTok{1e-10}\NormalTok{\}}
  \ControlFlowTok{if}\NormalTok{(sigma_b1}\OperatorTok{==}\DecValTok{0}\NormalTok{)\{sigma_b1 <-}\StringTok{ }\FloatTok{1e-10}\NormalTok{\}}
\NormalTok{  n_iter <-}\StringTok{ }\NormalTok{iterations}
  
\NormalTok{  levels_per_batch <-}\StringTok{ }\KeywordTok{ifelse}\NormalTok{(confound}\OperatorTok{==}\OtherTok{FALSE}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  fake_data <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}
    \DataTypeTok{data_id =} \KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{n_iter, }\DataTypeTok{each=}\NormalTok{n_batch}\OperatorTok{*}\NormalTok{n_subsamp}\OperatorTok{*}\NormalTok{levels_per_batch),}
    \DataTypeTok{sigma =}\NormalTok{ sigma,}
    \DataTypeTok{sigma_b0 =}\NormalTok{ sigma_b0,}
    \DataTypeTok{sigma_b1 =}\NormalTok{ sigma_b1,}
    \DataTypeTok{beta_0 =}\NormalTok{ beta_}\DecValTok{0}\NormalTok{,}
    \DataTypeTok{beta_1 =}\NormalTok{ beta_}\DecValTok{1}\NormalTok{,}
    \DataTypeTok{treatment =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rep}\NormalTok{(trt_levels, }\DataTypeTok{each=}\NormalTok{n_subsamp), n_batch}\OperatorTok{*}\NormalTok{levels_per_batch}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\NormalTok{n_iter),}
    \DataTypeTok{batch =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"batch_"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\NormalTok{(n_batch)), }\DataTypeTok{each=}\NormalTok{n_subsamp}\OperatorTok{*}\NormalTok{levels_per_batch), n_iter),}
    \DataTypeTok{beta_0_j =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n_batch}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\NormalTok{sigma_b0), }\DataTypeTok{each=}\NormalTok{n_subsamp}\OperatorTok{*}\NormalTok{levels_per_batch),}
    \DataTypeTok{beta_1_j =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n_batch}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\NormalTok{sigma_b1), }\DataTypeTok{each=}\NormalTok{n_subsamp}\OperatorTok{*}\NormalTok{levels_per_batch),}
    \DataTypeTok{x =} \KeywordTok{rep}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n_subsamp), n_batch}\OperatorTok{*}\NormalTok{levels_per_batch}\OperatorTok{/}\DecValTok{2}\OperatorTok{*}\NormalTok{n_iter),}
    \DataTypeTok{e =} \KeywordTok{rnorm}\NormalTok{(n_subsamp}\OperatorTok{*}\NormalTok{n_batch}\OperatorTok{*}\NormalTok{levels_per_batch}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\NormalTok{sigma)}
\NormalTok{  )}
\NormalTok{  fake_data[, y}\OperatorTok{:}\ErrorTok{=}\StringTok{ }\NormalTok{(beta_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{beta_0_j) }\OperatorTok{+}\StringTok{ }\NormalTok{(beta_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{beta_1_j)}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\NormalTok{e]}
  \KeywordTok{setnames}\NormalTok{(fake_data, }\DataTypeTok{old=}\KeywordTok{c}\NormalTok{(}\StringTok{"y"}\NormalTok{, }\StringTok{"batch"}\NormalTok{), }\DataTypeTok{new=}\KeywordTok{c}\NormalTok{(y_label, batch_label))}
\NormalTok{  fake_data[, treatment }\OperatorTok{:}\ErrorTok{=}\StringTok{ }\KeywordTok{factor}\NormalTok{(treatment)]}
  \KeywordTok{return}\NormalTok{(fake_data)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# depending on parameterization, can get many "failed to converge"}
\CommentTok{# and "isSingular" warnings}
\NormalTok{write_it <-}\StringTok{ }\OtherTok{FALSE}

\NormalTok{n_iter <-}\StringTok{ }\DecValTok{5000}
\NormalTok{beta_1_i <-}\StringTok{ }\DecValTok{0}  \CommentTok{# 0 = Type I, !0 = Power.}
\NormalTok{confound_i <-}\StringTok{ }\OtherTok{FALSE} \CommentTok{# FALSE is randomized complete block, TRUE is confounded}
 \CommentTok{#case where there is only one treatment level per batch}
\NormalTok{n <-}\StringTok{ }\DecValTok{3} \CommentTok{# subsamples}
\NormalTok{k <-}\StringTok{ }\DecValTok{8} \CommentTok{# batches}

\CommentTok{# model_list <- c("lm_complete", "lm_mean", "lmm_slope", "lmm_inter")}
\NormalTok{model_list <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"lm_complete"}\NormalTok{, }\StringTok{"lm_mean"}\NormalTok{, }\StringTok{"lmm_inter"}\NormalTok{)}

\NormalTok{se <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_iter, }\DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(model_list))}
\KeywordTok{colnames}\NormalTok{(se) <-}\StringTok{ }\NormalTok{model_list}
\NormalTok{prob <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_iter, }\DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(model_list))}
\KeywordTok{colnames}\NormalTok{(prob) <-}\StringTok{ }\NormalTok{model_list}
\NormalTok{ci <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow=}\NormalTok{n_iter, }\DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(model_list))}
\KeywordTok{colnames}\NormalTok{(ci) <-}\StringTok{ }\NormalTok{model_list}

\NormalTok{fd_set <-}\StringTok{ }\KeywordTok{fake_lmm_data}\NormalTok{(n_iter, }
                        \DataTypeTok{sigma =} \DecValTok{1}\NormalTok{, }
                        \DataTypeTok{sigma_b0 =} \DecValTok{1}\NormalTok{, }\CommentTok{# 1 for big, 0.1 for small}
                        \DataTypeTok{sigma_b1 =} \FloatTok{0.1}\NormalTok{, }
                        \DataTypeTok{beta_0 =} \DecValTok{10}\NormalTok{,}
                        \DataTypeTok{beta_1 =}\NormalTok{ beta_1_i, }
                        \DataTypeTok{n_batch =}\NormalTok{ k, }
                        \DataTypeTok{n_subsamp =}\NormalTok{ n, }
                        \DataTypeTok{confound =}\NormalTok{ confound_i)}

\ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{}
\NormalTok{  fd <-}\StringTok{ }\NormalTok{fd_set[data_id}\OperatorTok{==}\NormalTok{iter,]}
  
\NormalTok{  m1 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fd)}
\NormalTok{  m2 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{fd[, .(}\DataTypeTok{y=}\KeywordTok{mean}\NormalTok{(y)), }\DataTypeTok{by=}\NormalTok{.(treatment, block)])}
  
  \ControlFlowTok{if}\NormalTok{(}\StringTok{"lmm_slope"} \OperatorTok{%in%}\StringTok{ }\KeywordTok{length}\NormalTok{(model_list))\{}
\NormalTok{    m3 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(treatment}\OperatorTok{|}\NormalTok{block), }\DataTypeTok{data=}\NormalTok{fd)}
\NormalTok{    m3.pairs <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(}\KeywordTok{emmeans}\NormalTok{(m3, }\DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{), }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\NormalTok{  \}}
\NormalTok{  m4 <-}\StringTok{ }\KeywordTok{lmer}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{treatment }\OperatorTok{+}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{|}\NormalTok{block), }\DataTypeTok{data=}\NormalTok{fd)}
\NormalTok{  m4.pairs <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(}\KeywordTok{emmeans}\NormalTok{(m4, }\DataTypeTok{specs=}\StringTok{"treatment"}\NormalTok{), }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}

  
\NormalTok{  se[iter, }\StringTok{"lm_complete"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{]}
\NormalTok{  se[iter, }\StringTok{"lm_mean"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m2))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{]}
  \ControlFlowTok{if}\NormalTok{(}\StringTok{"lmm_slope"} \OperatorTok{%in%}\StringTok{ }\KeywordTok{length}\NormalTok{(model_list))\{}
\NormalTok{    se[iter, }\StringTok{"lmm_slope"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m3))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{]}
\NormalTok{  \}}
\NormalTok{  se[iter, }\StringTok{"lmm_inter"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m4))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Std. Error"}\NormalTok{]}
  
\NormalTok{  prob[iter, }\StringTok{"lm_complete"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m1))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  prob[iter, }\StringTok{"lm_mean"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m2))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
  \ControlFlowTok{if}\NormalTok{(}\StringTok{"lmm_slope"} \OperatorTok{%in%}\StringTok{ }\KeywordTok{length}\NormalTok{(model_list))\{}
\NormalTok{    prob[iter, }\StringTok{"lmm_slope"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m3))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
\NormalTok{  \}}
\NormalTok{  prob[iter, }\StringTok{"lmm_inter"}\NormalTok{] <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(m4))[}\StringTok{"treatmenttr"}\NormalTok{, }\StringTok{"Pr(>|t|)"}\NormalTok{]}
  
\NormalTok{  ci[iter, }\StringTok{"lm_complete"}\NormalTok{] <-}\StringTok{ }\KeywordTok{confint}\NormalTok{(m1)[}\StringTok{"treatmenttr"}\NormalTok{, }\DecValTok{2}\NormalTok{] }\OperatorTok{-}
\StringTok{    }\KeywordTok{confint}\NormalTok{(m1)[}\StringTok{"treatmenttr"}\NormalTok{, }\DecValTok{1}\NormalTok{]}
\NormalTok{  ci[iter, }\StringTok{"lm_mean"}\NormalTok{] <-}\StringTok{ }\KeywordTok{confint}\NormalTok{(m2)[}\StringTok{"treatmenttr"}\NormalTok{, }\DecValTok{2}\NormalTok{] }\OperatorTok{-}
\StringTok{    }\KeywordTok{confint}\NormalTok{(m2)[}\StringTok{"treatmenttr"}\NormalTok{, }\DecValTok{1}\NormalTok{]}
  \ControlFlowTok{if}\NormalTok{(}\StringTok{"lmm_slope"} \OperatorTok{%in%}\StringTok{ }\KeywordTok{length}\NormalTok{(model_list))\{}
\NormalTok{    ci[iter, }\StringTok{"lmm_slope"}\NormalTok{] <-}\StringTok{ }\NormalTok{m3.pairs[,}\StringTok{"upper.CL"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{m3.pairs[,}\StringTok{"lower.CL"}\NormalTok{]}
\NormalTok{  \}}
\NormalTok{  ci[iter, }\StringTok{"lmm_inter"}\NormalTok{] <-}\StringTok{ }\NormalTok{m4.pairs[,}\StringTok{"upper.CL"}\NormalTok{] }\OperatorTok{-}\StringTok{ }\NormalTok{m4.pairs[,}\StringTok{"lower.CL"}\NormalTok{]}
  \CommentTok{# m4.pairs.LT <- difflsmeans(m4, which="treatment", ddf="Kenward-Roger")}
  \CommentTok{# m4.pairs.LT[, "upper"] - m4.pairs.LT[, "lower"]}
\NormalTok{\}}

\ControlFlowTok{if}\NormalTok{(write_it }\OperatorTok{==}\OtherTok{TRUE}\NormalTok{)\{}
\NormalTok{  id <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(letters, LETTERS), }\DecValTok{4}\NormalTok{), }\DataTypeTok{collapse=}\StringTok{""}\NormalTok{)}
\NormalTok{  fn <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(}\StringTok{"lmm_fd_beta1="}\NormalTok{, beta_1_i,}
               \StringTok{"_confound="}\NormalTok{,confound_i,}
               \StringTok{"_id="}\NormalTok{, id,}
               \StringTok{".txt"}\NormalTok{)}
\NormalTok{  fp <-}\StringTok{ }\KeywordTok{here}\NormalTok{(}\StringTok{"output"}\NormalTok{, }\StringTok{"chapter_lmm"}\NormalTok{, fn)}
  \KeywordTok{write.table}\NormalTok{(fd_set, fp, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{  fp <-}\StringTok{ }\KeywordTok{here}\NormalTok{(}\StringTok{"output"}\NormalTok{, }\StringTok{"chapter_lmm"}\NormalTok{, }\KeywordTok{paste0}\NormalTok{(}\StringTok{"lmm_se-"}\NormalTok{,id,}\StringTok{".txt"}\NormalTok{))}
  \KeywordTok{write.table}\NormalTok{(se, fp, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{  fp <-}\StringTok{ }\KeywordTok{here}\NormalTok{(}\StringTok{"output"}\NormalTok{, }\StringTok{"chapter_lmm"}\NormalTok{, }\KeywordTok{paste0}\NormalTok{(}\StringTok{"lmm_prob-"}\NormalTok{,id,}\StringTok{".txt"}\NormalTok{))}
  \KeywordTok{write.table}\NormalTok{(prob, fp, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{  fp <-}\StringTok{ }\KeywordTok{here}\NormalTok{(}\StringTok{"output"}\NormalTok{, }\StringTok{"chapter_lmm"}\NormalTok{, }\KeywordTok{paste0}\NormalTok{(}\StringTok{"lmm_ci-"}\NormalTok{,id,}\StringTok{".txt"}\NormalTok{))}
  \KeywordTok{write.table}\NormalTok{(ci, fp, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{row.names=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{\}}


\KeywordTok{apply}\NormalTok{(se, }\DecValTok{2}\NormalTok{, quantile, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{))}
\KeywordTok{apply}\NormalTok{(prob, }\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x}\OperatorTok{<}\FloatTok{0.05}\NormalTok{)}\OperatorTok{/}\NormalTok{n_iter)}
\KeywordTok{apply}\NormalTok{(ci, }\DecValTok{2}\NormalTok{, quantile, }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\bibliography{book.bib,packages.bib}


\end{document}
