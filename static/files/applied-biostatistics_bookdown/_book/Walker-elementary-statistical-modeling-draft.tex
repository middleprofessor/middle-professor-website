\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Elementary Statistical Modeling for Applied Biostatistics},
            pdfauthor={Jeffrey A. Walker},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Elementary Statistical Modeling for Applied Biostatistics}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Jeffrey A. Walker}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{2018-09-26}

\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Statistical Modeling}\label{statistical-modeling}

\emph{More cynically, one could also well ask ``Why has medicine not
adopted frequentist inference, even though everyone presents P-values
and hypothesis tests?'' My answer is: Because frequentist inference,
like Bayesian inference, is not taught. Instead everyone gets taught a
misleading pseudo-frequentism: a set of rituals and misinterpretations
caricaturing frequentist inference, leading to all kinds of
misunderstandings.} -- Sander Greenland

We use statistics to learn from data with uncertainty. Traditional
introductory textbooks in biostatistics implicitly or explicitly train
students and researchers to ``discover by p-value'' using hypothesis
tests (appendix xxx). Over the course of many chapters, the student is
trained to use something like a dichotomous key to choose the correct
``test'' for the data at hand, compute a test statistic for their data,
compute a \(p\)-value based on the test statistic, and compares the
\emph{p}-value to 0.05. Textbooks typically give very little guidance
about what can be concluded if \(p < 0.05\) or if \(p > 0.05\), but many
researchers conclude (incorrectly) they have ``discovered'' something if
\(p < 0.05\) but found ``no effect'' if \(p > 0.05\).

Researchers learn almost nothing useful from a hypothesis test. If we
are investigating the effects of an increasingly acidified ocean on
coral growth, \(p=0.002\) may be evidence that pH affects growth, but,
from everything we know about pH and cell biology, it would be absurd to
conclude from any data that ocean acidification does not affect growth.
Instead, we want to know the magnitude of the effect and our uncertainty
in estimating this magnitude. We can use this magnitude and uncertainty
to make predictions about the future of coral reefs, under different
scenarios of ocean acidification. We can use the estimated effects and
uncertainty to model the consquences of the effects of acidification on
coral growth on fish production or carbon cycling.

The ``discovery by p-value'' strategy, or Null-Hypothesis Significance
Testing (NHST), has been criticized by statisticians for many, many
decades. Nevertheless, introductory biostatistics textbooks written by
both biologists and statisticians continue to organize textbooks around
a collection of hypothesis tests, with little emphasis on estimation and
uncertainty.

\section{Statistical modeling with linear
models}\label{statistical-modeling-with-linear-models}

This book is an introduction to the analysis of biological data using a
statistical modeling approach. As an introduction, the focus will be
linear models and extensions of the linear models including linear mixed
models and generalized linear models. Here, I refer to all of these as
``linear models'' because all are a function of a linear predictor.
Linear models are the engine behind many hypothesis tests but the
emphasis in statistical modeling is estimation and uncertainty instead
of test statistics and \(p\)-values. A modeling view of statistics is
also more coherent than a dichotomous key strategy.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/line-1.pdf}
\caption{\label{fig:line}A line vs.~a linear model. (A) the line \$y=-3.48X
+ 105.7 is drawn. (B) A linear model fit to the data. The model
coefficients are numerically equal to the slope and intercept of the
line in A.}
\end{figure}

All students are familiar with the idea of a linear model from learning
the equation of a line, which is

\begin{equation}
Y = mX + b
\label{eq:line}
\end{equation}

where \(m\) is the slope of the line and \(b\) is the \(Y\)-intercept.
It is useful to think of equation \eqref{eq:line} as a function that maps
values of \(X\) to values of \(Y\). Using this function, if we input
some value of \(X\), we always get the same value of Y as the output.

A linear model is a function, like that in equation \eqref{eq:line}, that
is fit to a set of data, often to model a process that generated the
data or something like the data. The line in Figure \ref{fig:line}A is
just that, a line, but the line in Figure \ref{fig:line}B is a model of
the data in Figure \ref{fig:line}B. The basic structure of a linear
model is

\begin{equation}
Y = \beta_0 + \beta_1 X + \varepsilon
\label{eq:lm}
\end{equation}

A linear model has two parts: the ``linear predictor''
(\(Y = \beta_0 + \beta_1 X\)) and the ``error'' (\(\varepsilon\)). The
linear predictor part looks like the equation for a line except that
I've used \(\beta_0\) for the intercept and \(\beta_1\) for the slope
and I've put the intercept term first. This re-labeling and
re-arrangement make the notation for a linear model more flexible for
more complicated linear models. For example
\(Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \varepsilon\) is a model
where \(Y\) is a function of two \(X\) variables.

As with the equation for a line, the linear predictor part of a linear
model is a function that maps a value of \(X\) to a specific value of
\(Y\). This mapped value is the \textbf{expected value} given a specific
input value of \(X\). This is often written as \(\mathrm{E}[Y|X]\). The
error part of a linear model is a random variable that adds some random
value to this expected value. Nothing about the model part of a linear
model can predict its value.

The inputs to a linear model (the \(X\) variables) have many names
including ``independent variables,'' ``predictor variables,'',
``explanatory variables,'' ``treatment variables,'' and ``covariates''.
The output of a linear model (the \(Y\) variable or variables if the
model is multivariate) is the ``dependent variable,'' ``response,'' or
``outcome.'' The \(\beta\) in the linear model are model
\textbf{parameters} There can be additional parameters in more
sophisticated models. The coefficients of the \(X\) in a linear model
(\(\beta_1\) in model \eqref{eq:lm}) are often called ``the effects'' (so
\(\beta_1\) is the effect of \(X_1\)).

Although a linear model is a model of a data-generating process, linear
models are not typically used to actually generate any data. Instead,
when we use a linear model to understand something about a real dataset,
we think of our data as one realization of a process that generates data
like ours. A linear model is a model of that process. That said, it is
incredibly useful to use linear models to create fake datasets for at
least two reasons: to probe our understanding of statistical modeling
generally and, more specifically, to check that a model actually creates
data like that in the real dataset that we are analyzing.

\subsection{Linear models are used for prediction, explanation, and
description}\label{linear-models-are-used-for-prediction-explanation-and-description}

Researchers typically use linear models to understand relationships
between one or more \(Y\) variables and one or more \(X\) variables.
These relationships include

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Descriptive modeling. Sometimes a researcher merely wants to describe
  the relationship between \(Y\) and a set of \(X\) variables, perhaps
  to discover patterns. For example, the arrival of a spring migrant
  bird (\(Y\)) as a function of sex (\(X_1\)) and age (\(X_2\)) might
  show that males and younger individuals arrive earlier. Importantly,
  if another \(X\) variable is added to the model (or one dropped), the
  coefficients, and therefore, the precise description, will change.
  That is, the interpretation of a coefficient as a descriptor is
  \emph{conditional} on the other covariates (\(X\) variables) in the
  model. In a descriptive model, there is no implication of causal
  effects and the goal is not prediction. Nevertheless, it is very hard
  for humans to discuss a descriptive model without using causal
  language, which probably means that it is hard for us to think of
  these models as \emph{mere description}. Like natural history,
  descriptive models are useful as patterns in want of an explanation,
  using more explicit causal models including experiments.
\item
  Predictive modeling. Predictive modeling is very common in applied
  research. For example, fisheries researchers might model the
  relationship between population density and habitat variables to
  predict which subset of ponds in a region are most suitable for brook
  trout (\emph{Salvelinus fontinalis}) reintroduction. The goal is to
  build a model with minimal prediction error, which is the error
  between predicted and actual values for a future sample. In predictive
  modeling, the \(X\) (``predictor'') variables are largely instrumental
  -- how these are related to \(Y\) is not a goal of the modeling,
  although sometimes an investigator may be interested in the relative
  importance among the \(X\) for predicting \(Y\) (for example,
  collecting the data may be time consuming, or expensive, or
  enviromentally destructive, so know which subset of \(X\) are most
  important for predicting \(Y\) is a useful strategy).
\item
  Explanatory (causal) modeling. Very often, researchers are explicitly
  interested in \emph{how} the \(X\) variables are causally related to
  \(Y\). The fisheries researchers that want to reintroduce trout may
  want to develop and manage a set of ponds to maintain healthy trout
  populations. This active management requires intervention to change
  habitat traits in a direction, and with a magnitude, to cause the
  desired response. This model is predictive -- a specific change in
  \(X\) predicts a specific response in \(Y\) -- because the
  coefficients of the model provide knowledge on how the system
  functions -- how changes in the inputs \emph{cause} change in the
  output. Causal interpretation of model coefficients requires a set of
  strong assumptions about the \(X\) variables in the model. These
  assumptions are typically met in \textbf{experimental designs} but not
  \textbf{observational designs}.
\end{enumerate}

With observational designs, biologists are often not very explicit about
which of these is the goal of the modeling and use a combination of
descriptive, predictive, and causal language to describe and discuss
results. Many papers read as if the researchers intend explanatory
inference but because of norms within the biology community, mask this
intention with ``predictive'' language. Here, I advocate embracing
explicit, explanatory modeling by being very transparent about the
model's goal and assumptions.

\section{Model fitting}\label{model-fitting}

In order to use a linear model to describe, predict, or explain, we need
to fit a model to data in order to estimate the parameters. If we fit
model \eqref{eq:conditional-expectation} to some data, the estimated
parameters are the coefficients (\(b_0\) and \(b_1\)) of the fit model

\begin{equation}
\mathrm{E}[Y|X] = b_0 + b_1 X
\label{eq:conditional-expectation}
\end{equation}

The left-hand side of equation \eqref{eq:conditional-expectation} is the
\textbf{conditional expectation} and is read as ``the expectation of Y
given X'' or ``the expected value of Y given X''. Throughout this book,
I use the greek \(\beta\) to refer to a theoretical, data-generating
parameter and the roman ``b'' to refer its estimate.

The goal of descriptive and explanatory modeling is the estimate of the
coefficients of the \(X\) variables and their uncertainty. The goal of
predictive modeling is the estimate of predicted values, and their
uncertainty, given specific values of \(X\). These predicted values are
the conditional expectations.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/coldVoles-1.pdf}
\caption{\label{fig:coldVoles}HarrellPlot of vole data.}
\end{figure}

For the model fit to the data in Figure \ref{fig:line}B, the coefficient
of \(X\) is the slope of the line. Perhaps surprisingly, we can fit a
model like equation \eqref{eq:lm} to data in which the \(X\) variable is
categorical. A simple example is the experiment of antioxidants
(vitamins C and E) on lifespan in Voles (Fig. \ref{fig:coldVoles}). In
this experiment, the \(X\) variable is categorical, with three
\textbf{levels}: ``Control'', ``Vitamin\_E'' and ``Vitamin\_C''.
Categorical \(X\) variables are often called \textbf{factors}. The trick
to using a linear model with categorical \(X\) is to recode the factor
levels into numbers -- how this is done is explained in Chapter xxx.
When the \(X\) variable is categorical, the coefficients of the \(X\)
are \emph{differences in group means}. The linear model fit to the vole
data has two coefficients, one for Vitamin E and one for vitamin C. The
estimate and uncertainty of the these two coefficients are shown in the
top part of Figure \ref{fig:coldVoles}. The bottom part shows the raw
data, as well as the group (factor level) means and the uncertainty in
the estimate of these means.

The simplest possible model that can be fit to the data is

\begin{equation}
\mathrm{E}[Y] = b_0
\label{eq:unconditional}
\end{equation}

which is simply the mean of \(Y\), or, more specifically, the
\textbf{unconditional mean} of \(Y\), since its value is not conditional
on any value of \(X\).

\subsection{\texorpdfstring{``Statistical model'' not ``regression
model''}{Statistical model not regression model}}\label{statistical-model-not-regression-model}

Statistical modeling terminology can be confusing. The \(X\) variables
in a statistical model may be quantitative (continuous or integers) or
categorical (names or qualitative amounts) or some mix of the two.
Linear models with all quantitative independent variables are often
called ``regression models.'' Linear models with all categorical
independent variables are often called ``ANOVA models.'' Linear models
with a mix of quantitative and categorical variables are often called
``ANCOVA models'' if the focus is on one of the categorical \(X\) or
``regression models'' if there tend to be many independent variables.
Other patterns occur. For example ``ANCOVA models'' often include
interaction effects but ``regression models'' rarely do. To avoid
thinking of statistical analysis as ``regression vs.~ANOVA'', I will
most often use the term ``statistical model'' for general usage, and use
a more specific term only to emphasize something about the model in that
particluar context.

\section{Multilevel models}\label{multilevel-models}

\section{Linear models versus non-linear
models}\label{linear-models-versus-non-linear-models}

In this text, I use ``linear model'' for any model that is linear in the
parameters, which means that the different components of the model are
added together. Or, using the language of matrix algebra, the predictor
is a simple dot product of the model matrix and the coefficients. For
example, a cubic polynomial model

\begin{equation}
Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \varepsilon
\end{equation}

is a linear model, even though the function is non-linear, because the
different components are added (or, using matrix algebra, the predictor
is \(\mathbf{X}\boldsymbol{\beta}\)).

A generalized linear model (GLM) has the form \(g(\mu_i) = \eta_i\)
where \(\eta\) (the greek letter eta) is the linear predictor, which is
linear in the parameters.

\begin{equation}
\eta = \mathbf{X}\boldsymbol{\beta} 
\end{equation}

Many sources do not consider a GLM to be a ``linear model'' but an
``extension'' of a linear model. Regardless, a GLM is linear in the
parameters and here, I include GLMs under the ``linear model'' umbrella.

Non-linear models, in conrast to a GLM or classical linear model, are
not linear in the parameters (the predictor is not a simple dot product
of the model matrix and a vector of parameters). For example, the
Michaelis-Menten model is a nonlinear model

\begin{equation}
Y = \frac{\beta_1 X}{\beta_2 + X} + \varepsilon
\end{equation}

\chapter{Organization -- R Projects and R
Notebooks}\label{organization-r-projects-and-r-notebooks}

\section{Importing Packages}\label{importing-packages}

The R scripts you write will include functions in packages that are not
included in Base R. These packages need to be downloaded from an
internet server to your computer. You only need to do this once. But,
each time you start a new R session, you will need to load a package
using the \texttt{library()} function. Now is a good time to import
packages that we will use

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open R Studio and choose the menu item ``Tools'' \textgreater{}
  ``Install Packages''
\item
  In the ``packages'' input box, insert ``ggplot2, data.table, emmeans,
  lme4, reshape2''. Make sure that ``install dependencies'' is clicked
  before you click ``Install''
\end{enumerate}

Again, once these are installed, you don't need to do this again. You
simply need to use the \texttt{library()} function at the start of a
script.

\section{Create an R Studio Project for this
Class}\label{create-an-r-studio-project-for-this-class}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a folder named ``BIO\_413''
\item
  Within this folder, create new folders named

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    ``notebooks'' -- this is where your R notebooks are stored
  \item
    ``R'' -- this is where R scripts are stored
  \item
    ``data'' -- this is where data that we download from public archives
    are stored
  \item
    ``output'' -- this is where you will store fake data generated in
    this class
  \item
    ``images'' -- this is where image files are stored
  \end{enumerate}
\item
  Open R Studio and click the menu item File \textgreater{} New
  Project\ldots{}
\item
  Choose ``Existing Directory'' and navigate to your BIO\_413 folder
\item
  Choose ``Create Project''
\item
  Check that a file named ``BIO\_413.Rproj'' is in your BIO\_413 folder
\end{enumerate}

\section{R Notebooks}\label{r-notebooks}

A typical statistical modeling project will consist of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  reading data from Excel or text (.csv or .txt) files
\item
  cleaning data
\item
  analysis
\item
  generating plots
\item
  generating tables
\item
  writing text to describe the project, the methods, the analysis, and
  the interpretation of the results (plots and tables)
\end{enumerate}

The best practice for reproducible research is to have all six of these
steps in your R Notebook. Too many research projects are not
reproducible because the data were cleaned in Excel, and then different
parts of the data were separately imported into a GUI statistics
software for analysis, and then output from the statistics software was
transcribed to Excel to make a table. And other parts of the analysis
are used to create a plot in some plotting software. And then the tables
and plots are pasted into Microsoft Word to create a report. Any change
at any step in this process will require the researcher to remember all
the downstream parts that are dependent on the change and to re-do an
analysis, or a table, or a plot, etc. etc.

The goal with an R Studio Notebook is to explicitly link all this so
that changes in earlier steps automatically flow into the later steps.
So, at the end of a project, a researcher can choose ``run all'' from
the menu and the data are read, cleaned, analyzed, ploted, tabled, and
put into a report with the text.

This means that you have to think of the organization of the R code that
your write in a Notebook. Your cannot simply append new code to the end
of a script if something earlier (or above) is dependent on it. You need
to go back up and insert the new code at some earlier (and meaningful)
point.

For example, an R chunk generates 100 random normal values and then
plots these with a histogram. This was the chunk that I wrote

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

When I ran the chunk, I got the error ``Error in rnorm(n) : object n not
found''. I was using the function \texttt{rnorm()} to generate values
but I hadn't assigned any value to \texttt{n} yet, so I got the error.
To get this to work properly, I could have just typed
\texttt{n\ \textless{}-\ 100} in the console and then re-run the script
but I want it to work properly on a fresh run of the chunk (after
quitting and re-opening R Studio) so I instead inserted
\texttt{n\ \textless{}-\ 100} at the start of the chunk, like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\subsection{Create an R Notebook for this
Chapter}\label{create-an-r-notebook-for-this-chapter}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The top-left icon in R Studio is a little plus sign within a green
  circle. Click this and choose ``R Notebook'' from the pull-down menu.
\item
  Change the title of the notebook to ``Notebook\_01-organization''
\item
  Delete the default R Markdown text starting with ``This is an {[}R
  Markdown{]}\ldots{}''
\end{enumerate}

Now write some text documenting which packages you installed.

\subsection{\texorpdfstring{Create a ``setup''
chunk}{Create a setup chunk}}\label{create-a-setup-chunk}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Click on the ``Insert'' menu on the right hand side of the script (R
  Markdown) pane and choose ``R''. This will insert an R code chunk into
  your R markdown document.
\item
  The first R chunk of a notebook should be a setup chunk. Name the
  chunk ``setup''
\item
  load the libraries ggplot2 and data.table and click the chunk's run
  button (the green triangle to the right of the chunk)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

I added the chunk option ``message=FALSE''. Run your chunk with and
without this as an option.

\subsection{\texorpdfstring{Create a ``simple plot''
chunk}{Create a simple plot chunk}}\label{create-a-simple-plot-chunk}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Create a new chunk and label it ``simple plot''
\item
  insert the following R script and then click the chunk's run button.
  Do you get a plot?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{100}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\KeywordTok{qplot}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/simple-plot-1.pdf}

\subsection{Create more R chunks and explore options and play with R
code}\label{create-more-r-chunks-and-explore-options-and-play-with-r-code}

\chapter{Data -- Reading, Writing, and
Fake}\label{data-reading-writing-and-fake}

\section{Create new notebook for this
chapter}\label{create-new-notebook-for-this-chapter}

Be sure to save the notebook in the ``notebooks'' folder of your
BIO\_413 project. Annotate your notebook with notes! Update it as you
learn more! We will use data.table for importing text files in
tab-delimited or comma-separated formats and the readxl package for
importing excel files.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(ggpubr)}
\KeywordTok{library}\NormalTok{(data.table)}
\KeywordTok{library}\NormalTok{(readxl)}
\KeywordTok{library}\NormalTok{(emmeans)}
\KeywordTok{library}\NormalTok{(mvtnorm)}

\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{fig.width=}\DecValTok{6}\NormalTok{, }\DataTypeTok{fig.height=}\DecValTok{4}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\section{Importing Data}\label{importing-data}

Throughout this book, we will download data from the
\href{https://datadryad.org}{Dryad Digital Repository}, which is a major
resource for increasing reproducibility in science. My own view is that
\emph{all data} should be archived on some public server (exceptions
include data that are proprietary or contain sensitive information --
such as human health measures).

The downloaded data will be inserted into the ``data'' folder. To access
these data in an R script, the script needs to know ``where to look'' or
the ``address.'' This address is the \textbf{directory path}. The
default path for an R notebook is the directory containing the notebook
.Rmd file. This file should be in the ``notebooks'' folder within
``BIO\_413''. The ``BIO\_413'' Folder is the parent of the ``notebooks''
folder. It is also the parent of the ``data'' folder. To see any content
within the ``data'' folder, the R script needs to tell R to move back
(or up) the directory structure out of the ``notebooks'' folder into the
parent ``BIO\_413'' folder and then forward (or down) into the ``data''
folder. This is done with

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_path <-}\StringTok{ "../data"}
\end{Highlighting}
\end{Shaded}

The \texttt{..} moves the address (of where to read input or write
output) back one step and \texttt{/data} moves the address forward into
the ``data'' folder. This folder will eventually contains lots of data
from Dryad Digital Repository.

\subsection{Excel File}\label{excel-file}

The Excel dataset is from an experiment on the growth response of zebra
finch chicks to an incubation call that presumably signals ``hot
environment'' to the embryos
(\href{http://science.sciencemag.org/content/353/6301/812}{Mariette,
M.M. and Buchanan, K.L., 2016. Prenatal acoustic communication programs
offspring for high posthatching temperatures in a songbird. Science,
353(6301), pp.812-814}). The source file is from the Dryad Repository
here:

\textbf{file name}: ``allDatasetsMarietteBuchanan2016.xls''

\textbf{source}: \url{https://datadryad.org//handle/10255/dryad.122315}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Copy the title of the Dryad page, which is ``Data from: Prenatal
  acoustic communication programs offspring for high post-hatching
  temperatures in a songbird''
\item
  Create a new folder within ``data'' and paste in the copied title as
  the folder name
\item
  Remove the colon from the name, so the folder name is ``Data from
  Prenatal acoustic communication programs offspring for high
  post-hatching temperatures in a songbird''
\item
  Download the .xls file into this folder
\end{enumerate}

A .xls file is an old (pre 2007) Microsoft Excel file type. It is a
binary file and can only be opened into a readable format with
specialized software. The more modern Excel file type is .xlsx, which
contains within it multiple xml components. An xml file is a text file,
and so contains readable content, but the content is xml code to display
something. In general, I am a big advocate of archiving stuff as text
files (manuscripts, data, scripts, blog posts) because these will
\emph{always} be readable by future software. Microsoft Excel is not
likely to die anytime soon and software that can read .xls and
especially .xlsx files (again, .xlsx files are text files) is even less
likely to disappear but we can feel even more confident if data are
archived as text files. That said, a single microsoft excel file with
multiple sheets is an efficient method for distributing data and the
readxl package provides excellent tools for reading different sheets of
a single .xls or .xlsx file.

The code below uses the function \texttt{read\_excel()} from the package
readxl. More about the amazing power of this package is the
\href{https://readxl.tidyverse.org}{tidyverse page} and
\href{http://r4ds.had.co.nz/data-import.html}{chapter 11} in the \emph{R
for Data Science} book.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_folder <-}\StringTok{ "Data from Prenatal acoustic communication programs offspring for high post-hatching temperatures in a songbird"}
\NormalTok{filename <-}\StringTok{ "allDatasetsMarietteBuchanan2016.xls"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(data_path, data_folder, filename, }\DataTypeTok{sep=}\StringTok{"/"}\NormalTok{)}
\NormalTok{chick <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\KeywordTok{read_excel}\NormalTok{(file_path, }\DataTypeTok{sheet=}\StringTok{"nestlingMass"}\NormalTok{))}
\KeywordTok{head}\NormalTok{(chick) }\CommentTok{# check -- are there headers? are there the correct number of columns?}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       chick ID brood ID brood composition sex rank in nest
## 1:    N1.10LF3  N1.10m3             mixed   F            2
## 2: N1.10noCut3  N1.10m3             mixed   M            4
## 3:    N1.10RB3  N1.10m3             mixed   F            2
## 4:    N1.10RF3  N1.10m3             mixed   F            5
## 5:    N1.12LB3  N1.12m3             mixed   F            3
## 6:    N1.12LF3  N1.12m3             mixed   F            1
##    playback treatment nest temperature above ambient
## 1:              treat                       4.289583
## 2:               cont                       4.289583
## 3:               cont                       4.289583
## 4:               cont                       4.289583
## 5:               cont                       3.972917
## 6:              treat                       3.972917
##    max daily temp hatch day mean max temp hatch to day2
## 1:                     17.4                    18.83333
## 2:                     19.0                    20.53333
## 3:                     17.4                    18.83333
## 4:                     19.0                    20.53333
## 5:                     29.0                    24.63333
## 6:                     25.1                    24.80000
##    mean max temp hatch to day10 mean max temp hatch to day13 hatching mass
## 1:                        22.70                     23.05714           0.7
## 2:                        24.53                     23.41429           0.6
## 3:                        22.70                     23.05714           0.7
## 4:                        24.53                     23.41429           0.6
## 5:                        22.85                     22.91429           0.7
## 6:                        23.35                     23.24286           0.6
##              day1 mass           day2 mass         day10 mass day13 mass
## 1:  1.1000000000000001                 1.2                 NA        9.8
## 2: 0.80000000000000004  1.1000000000000001                 NA        9.1
## 3: 0.90000000000000002  1.3999999999999999                 NA        9.3
## 4:                 0.5 0.90000000000000002                 NA        7.7
## 5:                   1  1.3999999999999999 9.4000000000000004       10.1
## 6: 0.90000000000000002  1.3999999999999999 8.0999999999999996        9.6
##    day13 tarsus
## 1:        14.11
## 2:        12.90
## 3:        13.60
## 4:        13.06
## 5:        14.08
## 6:        13.46
\end{verbatim}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{NOTE}

If you are getting errors when trying to read a file, it is probably a
bug in the construction of the variable \texttt{file\_path}, which is a
string variable and the value has to be exactly match the directly path
to the file you are trying to read. \texttt{file\_path} is constructed
by pasting together the variables \texttt{data\_path},
\texttt{data\_folder}, and \texttt{filename}. Type \texttt{file\_path}
into the console and look at the value. Then check

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Spelling. Humans are very good at understanding misspelled words but
  the R language (or any computer language) is very literal. ``../data''
  does not equal ``./data'' or ``../ data'' or ``../data''
\item
  Capitalization. R is \textbf{case sensitive} (some programming
  languages are not). ``../data'' does not equal ``../Data'' or
  ``../DATA''.
\item
  is the file you are trying to read actually in the folder you are
  trying to read from?
\item
  is the notebook that you are writing in the folder ``notebooks''? (the
  construction of \texttt{file\_path} assumes that notebook is one
  folder deep within the project folder.
\end{enumerate}

If the spelling or capitalization of any of these components is wrong,
then \texttt{file\_path} will be wrong. If there is any difference in
any character in the string, then R will return an error. So spelling
AND capitalization have to be perfect, not simply close. Humans are very
good at understanding misspelled and OdDLy capitalized words but the R
language (or any computer language) is very literal.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

In this book, we will consistently uses the protocol for storing and
retrieving downloaded files. The first three lines in the script above
creates the directory path to the file. This path includes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  data\_path -- the relative path into the folder ``data'' (relative to
  the location of the notebook file)
\item
  data\_folder -- the name of the folder within ``data'' containing the
  file
\item
  filename -- the name of the file to read
\end{enumerate}

These are all put together into a single path using the function
\texttt{paste()}. Read about paste. It will be used repeatedly. The
\texttt{read\_excel(file\_path,\ sheet="nestlingMass")} reads the
nestlingMass sheet only. This function is embedded within the
\texttt{data.table()} function and so is converted into a data.table.
The data.table is assigned to the object ``chick''

The \texttt{head(chick)} script simply displays the first few lines of
the data.table. This is one way to check that the data were imported
correctly. In this case, it is easy to see that the column names have
spaces in them. It can sometimes be hard to work with column names with
spaces and so this next line of code changes all spaces to an underscore

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setnames}\NormalTok{(chick, }\DataTypeTok{old=}\KeywordTok{colnames}\NormalTok{(chick), }\DataTypeTok{new=}\KeywordTok{gsub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{"_"}\NormalTok{, }\KeywordTok{colnames}\NormalTok{(chick)))}
\end{Highlighting}
\end{Shaded}

Resist the temptation to change the column names in the data file, which
reduces reproducibility. Always increase reproducibility!

Just for fun, let's plot the data and reproduce Fig. 2A and B. We are
using the \texttt{qplot} function, which is from the ggplot2 package.
Two plots are made and only a subset of the rows are plotted in each (in
A, the subset in which playback\_treatment==``treat'' and, in B, the
subset in which playback\_treatment==``cont''). This book uses the
ggplot2 package extensively.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nest_temperature_above_ambient, }\DataTypeTok{y=}\NormalTok{day13_mass, }\DataTypeTok{data=}\NormalTok{chick[playback_treatment}\OperatorTok{==}\StringTok{"treat"}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plotchickdata-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(}\DataTypeTok{x=}\NormalTok{nest_temperature_above_ambient, }\DataTypeTok{y=}\NormalTok{day13_mass, }\DataTypeTok{data=}\NormalTok{chick[playback_treatment}\OperatorTok{==}\StringTok{"cont"}\NormalTok{]) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plotchickdata-2.pdf}

\subsection{Text File}\label{text-file}

The example dataset comes from an experiment on the effect of
\href{http://science.sciencemag.org/content/early/2012/03/28/science.1215025}{neonicotinoid
pesticides on bumble bee colony growth}.

\textbf{file name}: ``Whitehorn, O'Connor, Wackers, Goulson (2012) Data
from `Neonicotinoid pesticide reduces bumblebee colony growth and queen
production'.csv.csv''

\textbf{source}:
\url{https://datadryad.org//resource/doi:10.5061/dryad.1805c973}

Steps

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Copy the title of the Dryad page, which is ``Data from: Neonicotinoid
  pesticide reduces bumblebee colony growth and queen production''
\item
  Create a new folder within ``data'' and paste in the copied title as
  the folder name
\item
  Remove the colon from the name, so the folder name is ``Data from
  Neonicotinoid pesticide reduces bumblebee colony growth and queen
  production''
\item
  Download the .csv file into this folder
\end{enumerate}

A .csv file is a text file that is comma-delimted, which means that the
entries of a row are separated by commas. A text file is readable by any
text editor software and most other kinds of software. Datasets that are
stored as text files are typically saved as either .csv (where the
entries of a row are separated by commas) or .txt (where the entries are
separated by tabs). The base R way to read a .csv file is using
\texttt{read.csv}. The \texttt{read.table} function is more versatile,
as the delimiter can be specified. The function \texttt{fread()} from
the data.table package is fast, smart, and flexible. It is smart in the
sense that it guesses what the delimter is. Unfortunately, because of
spaces in the column labels for this file, fread guesses incorrectly
(another reason why spaces in column labels should be avoided). To
overcome this, the statement below specifies that the file contains a
``header'' (a line containing column labels)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_folder <-}\StringTok{ "Data from Neonicotinoid pesticide reduces bumblebee colony growth and queen production"}
\NormalTok{filename <-}\StringTok{ "Whitehorn, O'Connor, Wackers, Goulson (2012) Data from 'Neonicotinoid pesticide reduces bumblebee colony growth and queen production'.csv.csv"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(data_path, data_folder, filename, }\DataTypeTok{sep=}\StringTok{"/"}\NormalTok{)}
\NormalTok{bee <-}\StringTok{ }\KeywordTok{fread}\NormalTok{(file_path, }\DataTypeTok{header=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{bee[, Treatment}\OperatorTok{:}\ErrorTok{=}\KeywordTok{factor}\NormalTok{(Treatment, }\KeywordTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Low"}\NormalTok{, }\StringTok{"High"}\NormalTok{))]}
\KeywordTok{head}\NormalTok{(bee)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Treatment Nest ID No. workers      0      1      2   3   4    5    6
## 1:   Control      C1          13 712.95 748.30 800.57 865 966  997  850
## 2:   Control      C2          14 719.58 750.00 789.25 822 812  846  827
## 3:   Control      C3          17 704.92 736.31 767.99 837 976 1117 1050
## 4:   Control      C4          20 726.42 763.31 795.60 813 801  784   NA
## 5:   Control      C5          28 740.60 785.52 808.42 837 871  906  886
## 6:   Control      C6          15 727.10 751.90 774.80 807 847  859  827
##      7   8 V13 Workers left Males New queens Total unhatched pupae
## 1: 791 775  NA            2     0          1                    NA
## 2: 820 802  NA            6    15          0                    20
## 3: 866 808  NA            1     0          9                    NA
## 4:  NA  NA  NA            0     0          0                    12
## 5: 807 775  NA            3     0          0                    NA
## 6:  NA  NA  NA            0     0          0                   118
##    Queen pupae Empty cells
## 1:          NA          NA
## 2:           0         120
## 3:          NA          NA
## 4:           0          72
## 5:          NA          NA
## 6:          20         132
\end{verbatim}

Here, as with the import of the Excel file, the first three lines create
the directory path to the file. The treatment column is a factor
variable containing three levels (Control, Low, and High). R
automatically orders these alphabetically. For plotting and analysis, we
might want a different order. For example, we want Control to be first
in the order, since this is a natural ``reference'' level (what
everything is compared to). And if we think of ``Control'' as no
treatment, then it makes sense to have ``Low'' second in order and
``Hight'' last in order. The line
\texttt{bee{[},\ Treatment:=factor(Treatment,\ c("Control",\ "Low",\ "High")){]}}
re-orders these levels to this more meaningful order.

Again, there are spaces in the column names. \textbf{Here I'll leave it
to you to change this}

Here is a reproduction of Fig 2.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"New_queens"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/plot-bee-fig-2-1.pdf}

The plot suggests immediately some problems with the plot itself and the
associated analysis. First, the y-axis is counts, which means that
negative values are impossible. But the standard error bars look like
they use standard errors computed from a model that allows infinetly
large negative values, and the illustrated standard error bars imply
that negative values exist. So these error bars are misleading. Second,
it is good practice, especially if sample sizes are modest or small, to
``show the data'', which means, show the individual data points and not
just a summary of the distribution.

Here are three alternative plots for exploratory purposes. The first
simply ``shows the data'' but still uses the misleading standard error
bars. The second uses a box plot. The last plots the means and 95\%
confidence intervals modeled with a GLM (generalized linear model) to
account for the count data (the model used could be improved). Notice
that the bar length above the mean is longer than the bar length below
the mean (that is the interval is asymmetric about the mean). In order
to stay focussed on importing data, I leave explanation of these plots
and analysis to later chapters.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggbarplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"New_queens"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{, }\StringTok{"point"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/alternative-plots-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggboxplot}\NormalTok{(}\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{x=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"New_queens"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/alternative-plots-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.glm <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(New_queens }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data=}\NormalTok{bee, }\DataTypeTok{family=}\KeywordTok{poisson}\NormalTok{())}
\NormalTok{means.glm <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit.glm, }\DataTypeTok{specs=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{gg <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data=}\KeywordTok{data.frame}\NormalTok{(means.glm), }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Treatment, }\DataTypeTok{y=}\NormalTok{rate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"gray"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{Treatment, }\DataTypeTok{ymin=}\NormalTok{asymp.LCL, }\DataTypeTok{ymax=}\NormalTok{asymp.UCL), }\DataTypeTok{width=}\FloatTok{0.3}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"New queens"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\OtherTok{NULL}
\NormalTok{gg}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/alternative-plots-3.pdf}

\section{Creating Fake Data}\label{creating-fake-data}

\subsection{Continuous X (fake observational
data)}\label{continuous-x-fake-observational-data}

A very simple simulation of a regression model

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{25}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\DecValTok{25}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\FloatTok{3.4}
\NormalTok{sigma <-}\StringTok{ }\DecValTok{2}
\NormalTok{x <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n)}
\NormalTok{y <-}\StringTok{ }\NormalTok{beta_}\DecValTok{0} \OperatorTok{+}\StringTok{ }\NormalTok{beta_}\DecValTok{1}\OperatorTok{*}\NormalTok{x }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma)}
\KeywordTok{qplot}\NormalTok{(x, y)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/continuous-X-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{coefficients}\NormalTok{(}\KeywordTok{summary}\NormalTok{(}\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x))), }\DataTypeTok{digits=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 24.46 & 0.39 & 62.43 & 0\\
\hline
x & 3.05 & 0.37 & 8.25 & 0\\
\hline
\end{tabular}

The coefficient of \(x\) is the ``Estimate''. How close is the estimate?
Run the simulation several times to look at the variation in the
estimate -- this will give you a sense of the uncertainty. Increase
\(n\) and explore this uncertainty. Increase all the way up to
\(n=10^5\). Commenting out the qplot line will make this exploration
easier.

\subsection{Categorical X (fake experimental
data)}\label{categorical-x-fake-experimental-data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{5}

\NormalTok{fake_data <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(}\DataTypeTok{Treatment=}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"control"}\NormalTok{, }\StringTok{"treated"}\NormalTok{), }\DataTypeTok{each=}\NormalTok{n))}
\NormalTok{beta_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\FloatTok{10.5} \CommentTok{# mean of untreated}
\NormalTok{beta_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\FloatTok{2.1} \CommentTok{# difference in means (treated - untreated)}
\NormalTok{sigma <-}\StringTok{ }\DecValTok{3} \CommentTok{# the error standard deviation}
\CommentTok{# the Y variable ("Response") is a function of treatment. We use some matrix}
\CommentTok{# algebra to get this done.}
\CommentTok{# Turn the Treatment assignment into a model matrix. Take a peak at X!}
\NormalTok{X <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(}\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, fake_data)}
\CommentTok{# to make the math easier the coefficients are collected into a vector}
\NormalTok{beta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(beta_}\DecValTok{0}\NormalTok{, beta_}\DecValTok{1}\NormalTok{)}
\CommentTok{# you will see the formula Y=Xb many times. Here it is coded in R}
\NormalTok{fake_data[, Response}\OperatorTok{:}\ErrorTok{=}\NormalTok{X}\OperatorTok{%*%}\NormalTok{beta }\OperatorTok{+}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n, }\DataTypeTok{sd=}\NormalTok{sigma)]}
\CommentTok{# plot it with a strip chart (often called a "dot plot")}
\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{fake_data, }\DataTypeTok{x=}\StringTok{"Treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"Response"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean_se"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/categorical-X-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit using base R linear model function}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Response }\OperatorTok{~}\StringTok{ }\NormalTok{Treatment, }\DataTypeTok{data=}\NormalTok{fake_data)}
\CommentTok{# display a pretty table of the coefficients}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{coefficients}\NormalTok{(}\KeywordTok{summary}\NormalTok{(fit)), }\DataTypeTok{digits=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 11.528 & 1.521 & 7.579 & 0.000\\
\hline
Treatmenttreated & 2.100 & 2.151 & 0.976 & 0.358\\
\hline
\end{tabular}

Check that the intercept is close to beta\_0 and the coefficient for
Treatment is close to beta\_1. This coefficient is the different in
means between the treatment levels. It is the simulated effect. Again,
change \(n\). Good values are \(n=20\) and \(n=100\). Again, comment out
the plot line to make exploration more efficient.

\section{Saving Data}\label{saving-data}

Let's save the fake data to the ``Fake\_Data'' folder. In the ``output''
folder create a new folder named ``week 01''. Then set the path to the
output folder:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output_path <-}\StringTok{ "../output"} \CommentTok{# out to parent directory than down into Fake_data}
\end{Highlighting}
\end{Shaded}

This could be done at the beginning of the notebook, especially if many
output files are saved. Regardless, now complete the file\_path with the
specifics of this save.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_folder <-}\StringTok{ "week 01"}
\NormalTok{filename <-}\StringTok{ "my_first_fake_data.txt"}
\NormalTok{file_path <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(output_path, data_folder, filename, }\DataTypeTok{sep=}\StringTok{"/"}\NormalTok{)}
\KeywordTok{write.table}\NormalTok{(fake_data, file_path, }\DataTypeTok{sep=}\StringTok{"}\CharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\DataTypeTok{quote=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We used \texttt{write.table()} to create a tab-delimited text file using
\texttt{sep="\textbackslash{}t"} to specify tabs to separate the row
elements. ``\t'' is the standard character string for a tab. Check in
your Fake\_Data folder and open the file in a text editor.

\section{Problems}\label{problems}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Download the dataset ``data-Lodjak.et.al-2016-FuncEcol.xlsx'' from the
  Dryad repository at
  \url{https://datadryad.org/resource/doi:10.5061/dryad.rd01s}. The
  .xlsx file presents the data cleanly but the trade-off is that the 1)
  multiple header rows, and 2) spaces in the header labels, 3)
  parentheses in the header labels make it more complex to import in a
  usable way. Import the data and plot Body Mass against Age (that is
  make Body Mass the ``Y'' variable and Age the ``X'' variable) using
  the qplot function. You should recode the column labels to remove
  spaces and parentheses using the setnames function.
\item
  Download the dataset ``Results2015.txt'' from the Dryad repository at
  \url{https://datadryad.org//resource/doi:10.5061/dryad.65vk4}. Try to
  reproduce Fig. 1. It's not easy. I've inserted the figure below.
\item
  (grad students only) Download and plot data from a Dryad Repository
  dataset of your choice.
\item
  (grad students only) Create fake experimental data with three
  treatment levels (control, lo\_temp, high\_temp). This will require
  three parameters: an intercept (beta\_0), an effect of lo\_temp
  (beta\_1), and an effect of high\_temp (beta\_2). You should be able
  to plug and play from the script above even if you don't underdstand
  at this point what any of it is! Plot it as a strip chart, as above.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=0.50000\textwidth]{images/Dung_beetles_reduce_livestock-fig_1.png}
\caption{Fig. 1 from ``Dung beetles reduce livestock\ldots{}''}
\end{figure}

\chapter{Variability and Uncertainty (Standard Deviations and Standard
Errors)}\label{variability-and-uncertainty-standard-deviations-and-standard-errors}

\textbf{Uncertainty} is the stuff of science. A major goal of statistics
is measuring uncertainty. What do we mean by uncertainty? Uncertainty is
the error in estimating a parameter, such as the mean of a sample, or
the difference in means between two experimental treatments, or the
predicted response given a certain change in conditions. Uncertainty is
measured with a \textbf{variance} or its square root, which is a
\textbf{standard deviation}. The standard deviation of a statistic is
also (and more commonly) called a \textbf{standard error}.

Uncertainty emerges because of variability. In any introductory
statistics class, students are introduced to two measures of
variability, the ``standard deviation'' and the ``standard error.''
These terms are absolutely fundamental to statistics -- they are the
start of everything else. Yet, many biology professors confuse these
terms and certainly, introductory students do too.

When a research biologist uses the term ``standard deviation,'' they are
probably referring to the sample standard deviation which is a measure
of the variability of a sample. When a research biologist uses the term
``standard error,'' they are probably referring to the standard error of
a mean, but it could be the standard error of another statistics, such
as a regression slope. An important point to remember and understand is
that all standard errors \emph{are} standard deviations. This will make
more sense soon.

\section{The sample standard deviation vs.~the standard error of the
mean}\label{the-sample-standard-deviation-vs.the-standard-error-of-the-mean}

\subsection{Sample standard deviation}\label{sample-standard-deviation}

The sample standard deviation is a measure of the variability of a
sample. For example, were we to look at a histological section of
skeletal muscle we would see that the diameter of the fibers (the muscle
cells) is variable. We could use imaging software to measure the
diameter of a sample of 100 cells and get a \textbf{distribution} like
this

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/histogram-1.pdf}

The mean of this sample is 69.4 and the standard deviation is 2.8. The
standard deviation is the square root of the variance, and so computed
by

\begin{equation}
s_y = \sqrt{\frac{\sum_{i=1}^n{(y_i - \overline{y})^2}}{n-1}}
\label{eq:variance}
\end{equation}

Memorize this equation. To understand the logic of this measure of
variability, note that \(y_i - \overline{y}\) is the \textbf{deviation}
of the \(i\)th value from the sample mean, so the numerator is the sum
of squared deviations. The numerator is a sum over \(n\) items and the
denominator is \(n-1\) so the variance is (almost!) an averaged squared
deviation. More variable samples will have bigger deviations and,
therefore, bigger average squared deviations. Since the standard
deviation is the square root of the variance, a standard deviation is
the square root of an average squared deviation. This makes it similar
in value to the averaged deviation (or average of the absolute values of
the deviations since the average deviation is, by definition of a mean,
zero).

Notes on the variance and standard deviation

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Variances are additive but standard deviations are not. This means
  that the variance of the sum of two independent (uncorrelated) random
  variables is simply the sum of the variances of each of the variables.
  This is important for many statistical analyses.
\item
  The units of variance are the square of the original units, which is
  awkward for interpretation. The units of a standard deviation is the
  same as that of the original variable, and so is much easier to
  interpet.
\item
  For variables that are approximately normally distributed, we can map
  the standard deviation to the quantiles of the distribution. For
  example, 68\% of the values are within one standard deviation of the
  mean, 95\% of the values are within two standard deviations, and 99\%
  of the values are within three standard deviations.
\end{enumerate}

\subsection{Standard error of the
mean}\label{standard-error-of-the-mean}

A standard error of a statistic is a measure of the precision of the
statistic. The standard error of the mean is a measure of the precision
of the estimate of the mean. The smaller the standard error, the more
precise the estimate. The standard error of the mean (SEM) is computed
as

\begin{equation}
SEM = \frac{s_y}{\sqrt{n}}
\label{eq:se}
\end{equation}

The SEM is often denoted \(s_{\bar{y}}\) to indicate that it is a
standard deviation of the mean (\(\bar{y}\)). In what sense is a
standard error a measure of variability? This is kinda weird. If we
sample 100 cells in the slide of muscle tissue and compute the mean
diameter, how can the mean have a standard deviation? There is only one
value! To understand how the SEM is a standard deviation, imagine 1)
resampling 100 cells and 2) recomputing a mean from the re-sampled data
an infinite number of times and each time, you write down the newly
computed mean. The true standard error of the mean is the standard
deviation of this infinitely long column of means. This means that a
standard error of the mean, computed from a single sample using equation
\eqref{eq:se} is itself a sample statistic.

Notes on standard errors

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The SEM is only one kind of standard error. A standard deviation can
  be computed for any statistic -- these are all standard errors. For
  some statistics, such as the mean, the standard error can be computed
  directly using an equation, such as that for the SEM (equation
  \eqref{eq:se}. For other statistics, a computer intensive method such as
  the \textbf{bootstrap} is necessary to compute a standard error. We
  will return to the bootstrap at the end of this chapter.
\item
  The units of a standard error are the units of the measured variable.
\item
  A standard error is proportional to sample variability (the sample
  standard deviation, \(s_y\)) and inversely proportional to sample size
  (\(n\)). Sample variability is a function of both natural variation
  (there really is variation in diameter among fibers in the quadriceps
  muscle) and measurement error (imaging software with higher resolution
  can measure a diameter with less error). Since the SEM is a measure of
  the precision of estimating a mean, this means this precision will
  increase (or the SEM will decrease) if 1) an investigator uses methods
  that reduce measurement error and 2) an investigator computes the mean
  from a larger sample.
\item
  This last point (the SEM decreases with sample size) seems obvious
  when looking at equation \eqref{eq:se}, since \(n\) is in the
  denominator. Of course \(n\) is also in the denominator of equation
  \eqref{eq:variance} for the sample standard deviation but the standard
  deviation does not decrease as sample size increases. First this
  wouldn't make any sense -- variability is variability. A sample of
  10,000 cell diameters should be no more variable than a sample of 100
  cell diameters (think about if you agree with this or not). Second,
  this should also be obvious from equation \eqref{eq:variance}. The
  standard deviation is the square root of an average and averages don't
  increase with the number of things summed since both the the numerator
  (a sum) and denominator increase with \(n\).
\end{enumerate}

\section{Using Google Sheets to generate fake data to explore
uncertainty}\label{using-google-sheets-to-generate-fake-data-to-explore-uncertainty}

In statistics we are interested in estimated parameters of a
\textbf{population} using measures from a \textbf{sample}. The goal in
this section is to use Google Sheets (or Microsoft Excel) to use fake
data to discover the behavior of sampling and to gain some intuition
about uncertainty using standard errors.

\subsection{Steps}\label{steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open Google Sheets
\item
  In cell A1 type ``mu''. mu is the greek letter \(\mu\) and is very
  common notation for the poplation value (the TRUE value!) of the mean
  of some hypothetical measure. In cell B1, insert some number as the
  value of \(\mu\). Any number! It can be negative or positive.
\item
  In cell A2 type ``sigma''. sigma is the greek letter \(\sigma\).
  \(\sigma^2\) is very common (universal!) notation for the population
  (TRUE) variance of some measure or parameter. Notice that the true
  (population) values of the mean and variance are greek letters. This
  is pretty standard in statistics. In cell B2, insert some positive
  number (standard deviations are the positive square roots of the
  variance).
\item
  In cell A8 type the number 1
\item
  In cell A9 insert the equation ``=A8 + 1''. What is this equation
  doing? It is adding the number 1 to to the value in the cell above, so
  the resulting value should be 2.
\item
  In Cell B8, insert the equation ``=normsinv(rand())*\$B\$2 + \$B\$1``.
  The first part of the equation creates a random normal variable with
  mean 0 and standard deviation 1. multiplication and addition transform
  this to a random normal variable with mean \(\mu\) and standard
  deviation \(\sigma\) (the values you set in cells B1 and B2).
\item
  copy cell B8 and paste into cell B9. Now Higlight cells A9:B9 and copy
  the equations down to row 107. You now have 100 random variables
  sampled from a infinite population with mean \(\mu\) and standard
  deviation \(\sigma\).
\item
  In cell A4 write ``mean 10''. In cell B4 insert the equation
  ``=average(B8:B17)''. The resulting value is the \textbf{sample mean}
  of the first 10 random variables you created. Is the mean close to
  \(\mu\)?
\item
  In cell A5 write ``sd 10''. In cell B5 insert the equation
  ``stdev(B8:B17)''. The result is the \textbf{sample standard
  deviation} of the first 10 random variables. Is this close to
  \(\sigma\)?
\item
  In cell A6 write ``mean 100''. In cell B6 insert the equation
  ``=average(B8:B107)''. The resulting value is the \textbf{sample mean}
  of the all 100 random variables you created. Is this mean closer to
  \(\mu\) than mean 10?
\item
  In cell A7 write ``sd 100''. In cell B7 insert the equation
  ``=stdev(B8:B107)''. The resulting value is the \textbf{sample
  standard deviation} of the all 100 random variables you created. Is
  this SD closer to \(\sigma\) than sd 10?
\end{enumerate}

The sample standard deviation is a measure of the variability of the
sample. The more spread out the sample (the further each value is from
the mean), the bigger the sample standard deviation. The sample standard
deviation is most often simply known as ``The'' standard deviation,
which is a bit misleading since there are many kinds of standard
deviations!

Remember that your computed mean and standard deviations are estimates
computed from a sample. They are estimates of the true values \(\mu\)
and \(\sigma\). Explore the behavior of the sample mean and standard
deviation by re-calculating the spreadsheet. In Excel, a spreadsheet is
re-calculated by simultaneously pressing the command and equal key. In
Google, command-R recalculates but is painfully slow. Instead, if using
Google Sheets, just type the number 1 into a blank cell, and the sheet
recalculates quickly. Do it again. And again.

Each time you re-calculate, a new set of random numbers are generated
and the new means and standard deviations are computed. Compare mean 10
and mean 100 each re-calculation. Notice that these estimates are
variable. They change with each re-calculation. How variable is mean 10
compared to mean 100? The variability of the estimate of the mean is a
measure of \textbf{uncertainty} in the estimate. Are we more uncertain
with mean 10 or with mean 100? This variability is measured by a
standard deviation. This \textbf{standard deviation of the mean} is also
called the \textbf{standard error of the mean}. Many researchers are
loose with terms and use ``The'' standard error to mean the standard
error of the mean, even though there are many kinds of standard errors.
In general, ``standard error''" is abbreviated as ``SE.'' Sometimes
``standard error of the mean'' is specifically abbreviated to ``SEM.''

The standard error of the mean is a measure of the precision in
estimating the mean. The smaller the value the more precise the
estimate. The standard error of the mean \emph{is} a standard deviation
of the mean. This is kinda weird. If we sample a population one time and
compute a mean, how can the mean have a standard deviation? There is
only one value! And we compute this value using the sample standard
deviation: \(SEM = \frac{SD}{\sqrt{N}}\). To understand how the SEM is a
standard deviation, Imagine recalculating the spread sheet an infinite
number of times and each time, you write down the newly computed mean.
The standard error of the mean is the standard deviation of this
infinitely long column of means.

\section{Using R to generate fake data to explore
uncertainty}\label{using-r-to-generate-fake-data-to-explore-uncertainty}

note that I use ``standard deviation'' to refer to the sample standard
deviation and ``standard error'' to refer to the standard error of the
mean (again, we can compute standard errors as a standard deviation of
any kind of estimate)

\subsection{part I}\label{part-i}

In the exercise above, you used Google Sheets to generate \(p\) columns
of fake data. Each column had \(n\) elements, so the matrix of fake data
was \(n \times m\) (it is standard in most fields to specify a matrix as
rows by columns). This is \emph{much} easier to do in R and how much
grows exponentially as the size of the matrix grows.

To start, we just generate a \(n \times p\) matrix of normal random
numbers.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# R script to gain some intuition about standard deviation (sd) and standard error (se)}
\CommentTok{# you will probably need to install ggplot2 using library(ggplot2) }
\NormalTok{n <-}\StringTok{ }\DecValTok{6} \CommentTok{# sample size}
\NormalTok{p <-}\StringTok{ }\DecValTok{100} \CommentTok{# number of columns of fake data to generate}
\NormalTok{fake_data <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{p, }\DataTypeTok{mean=}\DecValTok{0}\NormalTok{, }\DataTypeTok{sd=}\DecValTok{1}\NormalTok{), }\DataTypeTok{nrow=}\NormalTok{n, }\DataTypeTok{ncol=}\NormalTok{p) }\CommentTok{# create a matrix}
\end{Highlighting}
\end{Shaded}

the 3rd line is the cool thing about R. In one line I'm creating a
dataset with \(n\) rows and \(p\) columns. Each column is a sample of
the standard normal distribution which by definition has mean zero and
standard deviation of 1. But, and this is important, any sample from
this distribution will not have exactly mean zero and standard deviation
of 1, because it's a sample, the mean and standard deviation will have
some small errror from the truth. The line has two parts to it: first
I'm using the function ``rnorm'' (for random normal) to create a vector
of n*m random, normal deviates (draws from the random normal
distribution) and then I'm organizing these into a matrix (using the
function ``matrix'')

To compute the vector of means, standard deviations, and standard errors
for each column of \texttt{fake\_data}, use the \texttt{apply()}
function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(fake_data,}\DecValTok{2}\NormalTok{,mean) }\CommentTok{# the apply function is super useful}
\NormalTok{sds <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(fake_data,}\DecValTok{2}\NormalTok{,sd)}
\NormalTok{sems <-}\StringTok{ }\NormalTok{sds}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\texttt{apply()} is a workhorse in many R scripts. Learn it. Know it.
Live it.

The SEM is the standard deviation of the mean, so let's see if the
standard deviation of the means is close to the true standard error. We
sampled from a normal distribution with SD=1 so the true standard is

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4082483
\end{verbatim}

and the standard deviation of the \(p\) means is

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sd}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3731974
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  how close is \texttt{sd(means)} to the true SE?
\item
  change p above to 1000. Now how close is sd(means) to the true SE?
\item
  change p above to 10,000. Now how close is sd(means) to the true SE?
\end{enumerate}

\subsection{part II - means}\label{part-ii---means}

This is a visualization of the spread, or variability, of the sampled
means

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-6-1.pdf}

Compute the mean of the means

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.039961
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remember that the true mean is zero. How close, in general, are the
  sampled means to the true mean. How variable are the means? How is
  this quantified?
\item
  change n to 100, then replot. Are the means, in general, closer to the
  true mean? How variable are the means now?
\item
  Is the mean estimated with \(n=100\) closer to the truth, in general,
  then the mean estimated with \(n=6\)?
\item
  Redo with \(n=10000\)
\end{enumerate}

\subsection{part III - how do SD and SE change as sample size (n)
increases?}\label{part-iii---how-do-sd-and-se-change-as-sample-size-n-increases}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.017144
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  what is the mean of the standard deviations when n=6 (set p=1000)
\item
  what is the mean of the standard deviations when n=100 (set p=1000)
\item
  when n = 1000? (set p=1000)
\item
  when n = 10000? (set p=1000)
\item
  how does the mean of the standard deviations change as n increases
  (does it get smaller? or stay about the same size)
\item
  repeat the above with SEM
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4152472
\end{verbatim}

Congratulations, you have just done a Monte Carlo simulation!

\subsection{\texorpdfstring{Part IV -- Generating fake data with ``for
loops''}{Part IV -- Generating fake data with for loops}}\label{part-iv-generating-fake-data-with-for-loops}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n <-}\StringTok{ }\DecValTok{6} \CommentTok{# sample size}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5} \CommentTok{# number of iterations of loop (equivalent to p)}
\NormalTok{means <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\NormalTok{sds <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\NormalTok{sems <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{}
\NormalTok{  y <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(n) }\CommentTok{# mean=0 and sd=1 are default so not necessary to specify}
\NormalTok{  means[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y)}
\NormalTok{  sds[i] <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(y)}
\NormalTok{  sems[i] <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(y)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n)}
\NormalTok{\}}
\KeywordTok{sd}\NormalTok{(means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4090702
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(sems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3883867
\end{verbatim}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do \texttt{sd(means)} and \texttt{mean(sems)} converge to as
  \texttt{n\_iter} is increased from 100 to 1000 to 10,000?
\item
  Do they converge to the same number?
\item
  Should they?
\item
  What is the correct number?
\end{enumerate}

Question number 4 is asking what is E(SEM), the ``expected standard
error of the mean''. There is a very easy formula to compute this. What
is it?

\section{Bootstrapped standard
errors}\label{bootstrapped-standard-errors}

The bootstrap is the best invention since duct tape. Really.

A standard error is the standard deviation of an infinite number of
hypothetically re-sampled means. A bootstrap standard error of a
statistic is the standard deviation of the statistic from a finite
number of resamples of the data! Wait what?

Let's download some data to explore this concept. The data are archived
at Dryad Repository.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  URL: \url{https://datadryad.org//resource/doi:10.5061/dryad.31cc4}
\item
  file: RSBL-2013-0432 vole data.xlsx
\item
  sheet: COLD VOLES LIFESPAN
\end{enumerate}

The data are the measured lifespans of the short-tailed field vole
(\emph{Microtus agrestis}) under three different experimental
treatments: vitamin E supplementation, vitamin C supplementation, and
control (no vitamin supplementation). Vitamins C and E are antioxidants,
which are thought to be protective of basic cell function since they
bind to the cell-damaging reactive oxygen species that result from cell
metabolism.

I've read in the file using read\_excel and converted to a data.table
named \texttt{vole}. I used \texttt{setnames} to rename the columns to
lifespan, control, vitamin\_E, and vitamin\_C. The data are in a
\textbf{wide format} -- that is instead of a single ``treatment''
column, there are three columns (``control'', ``vitamin C'', ``vitamin
E'') with value = 1, if that row (or lifespan) was assigned the
treatment of the column label and zero otherwise. In general, we want
data.tables to be in long format. Wide formats can be useful for some
computations but not really for these data.

Compute the standard error of the mean of the lifespan for the control
group using equation \eqref{eq:se}. One simple way to do this for the
control group is to extract the subset of the data satisfying the
condition control = 1 (the value in the column ``control'' equals 1). In
R, these conditional querries use \texttt{==}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(vole[control}\OperatorTok{==}\DecValTok{1}\NormalTok{, lifespan]) }\CommentTok{# subset of data satisfying condition and omitting missing data, if these exist}
\NormalTok{n <-}\StringTok{ }\KeywordTok{length}\NormalTok{(y) }\CommentTok{# the sample size}
\NormalTok{se <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(y)}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(n}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\CommentTok{# standard error of the mean}
\end{Highlighting}
\end{Shaded}

Okay, the SEM using equation \eqref{eq:se} is 31.9. Let's compare this
with a bootstrap estimate of the SEM.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{2000} \CommentTok{# number of bootstrap iterations}
\NormalTok{means <-}\StringTok{ }\KeywordTok{numeric}\NormalTok{(n_iter) }\CommentTok{# we will save the means each iteration to this}
\NormalTok{inc <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\NormalTok{n }\CommentTok{# the first sample is the actual sample}
\ControlFlowTok{for}\NormalTok{(iter }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{n_iter)\{ }\CommentTok{# the for loop}
\NormalTok{  means[iter] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(y[inc])}
\NormalTok{  inc <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{n, }\DataTypeTok{replace=}\OtherTok{TRUE}\NormalTok{) }\CommentTok{# re-sample for the next iteration}
\NormalTok{\}}
\NormalTok{se_boot <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(means)}

\CommentTok{#compare}
\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 31.89536
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se_boot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 30.93843
\end{verbatim}

dayum!

\chapter{\texorpdfstring{A linear model with a single, continous
\emph{X}}{A linear model with a single, continous X}}\label{a-linear-model-with-a-single-continous-x}

\section{\texorpdfstring{A linear model with a single, continous
\emph{X} is classical
``regression''}{A linear model with a single, continous X is classical regression}}\label{a-linear-model-with-a-single-continous-x-is-classical-regression}

To introduce modeling with a single continous \(X\) variable, I'll use
data from

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.b3h4q}
\item
  File: ``FCM data dryad.csv''
\end{enumerate}

The data are from \citet{Dantzer_xxx}, who showed that North American
red squirrel (\emph{Tamiasciurus hudsonicus}) mothers from Yukon, Alaska
produce faster growing pups in years with increased squirrel density.
Remarkably, they even showed that perceived (but not actual) density
results in faster growing pups. To begin to investigate how pregnant
mothers control the future growth rate of pups, the researchers measured
the relationship between local squirrel density and the amount of fecal
cortisol metabolites from pregnant mothers. Cortisol is a hormone that
is secreted as part of stress response. The researchers were interested
in cortisol because it had previously been shownt that, in mammals,
blood cortisol levels in pregnant mothers have numerous effects on
offspring long past birth. If increased squirrel density causes
increased blood cortisol levels then we would expect to find a positive
relationship between \(Density\) and

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/squirrel-1.pdf}
\caption{\label{fig:squirrel}A scatterplot of Fecal cortisol matabolites and
squirrel density.}
\end{figure}

Figure \ref{fig:squirrel} is a \textbf{scatterplot} of the data with the
amount of cortisol metabolites in the feces on the \(Y\) axis and local
squirrel density on the \(X\) axis. The line through the data is a
graphical representation of a linear model fit to the data and the gray
cloud around the line is a graphical representation of the uncertainty
in the model. The researchers wanted to model the ``effect'' of squirrel
density on the amount of cortisol metabolites in the feces of the
pregnant mothers. Graphically, this effect is the slope of the line in
Figure \ref{fig:squirrel}.

The model fit to the data is

\begin{equation}
FCM_i = \beta_0 + \beta_1 Density_i + \varepsilon_i
\label{eq:fcm-model}
\end{equation}

which contains both the linear predictor and the error. For inference,
for example, computing standard errors of the coefficients, We need to
model the error. Here, we use the simplest model of error which is ``IID
\(N(0, \sigma)\)''. This means, the modeled error is

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Independent -- individual error values are independent of other
  values.
\item
  Identical -- individual error can be thought of as a sample from a
  single \textbf{random distribution} (the same for each individual
  value). For this model, this distribution is
\item
  \(N(0, \sigma)\) -- the modeled distribution is ``Normal'' or
  ``Gaussian'', with a mean of zero and a standard deviation of
  \(\sigma\).
\end{enumerate}

The predictor part of the model is

\begin{equation}
\textrm{E}[FCM|Density] = \beta_0 + \beta_1 Density
\label{eq:regression}
\end{equation}

In words, model \eqref{eq:regression} reads ``the expected value of
\(FCM\) conditional on density is beta-knot plus beta-one times
density''. An \textbf{expected value} is a long run average -- if we
were to sample lots and lots of red squirrel populations with
\(Density=x\) (where \(x\) is a specific value), we'd expect the average
\(FCM\) across these samples to be \(\beta_0 + \beta_1 x\).

\begin{quote}
Let's unpack this. \(\textrm{E}[Y]\) is the \textbf{expectation} or
\textbf{expected value} of \(Y\). An expection is the long-run average
of \(Y\) if we were to run an experiment or re-sample a population many
times. The sample mean of \(Y\) is an estimate of \(\textrm{E}[Y]\).
\(\textrm{E}[Y|X]\) is a conditional expectation of \(Y\) -- it is the
expectation given additional conditions. Using the red squirrel example,
these conditions are a specific value of \(Density\). If \(FCM\) is
linearly related to \(Density\) (the right-hand side of equation
\eqref{eq:regression}) then the expected value of \(FCM\) given a local
density of 2.8 squirrels differs from the expected value of \(FCM\)
given a local density of 1.4 squirrels (the units of \(Density\) are
squirrels per 150 meter radius of the individual female's midden).
\end{quote}

In model \eqref{eq:regression}, there is a single \(X\) variable
(\(FCM\)). While the \(X\) variables are often called the ``dependent''
variables, in this model \(FCM\) does not ``depend'' on the independent
variable \(Density\) in any causal sense -- meaning if I were to
intervene and set \(Density\) to some value \(x\), I would expect
\(FCM\) to equal \(\beta_0 + \beta_1 x\). Rather, \(FCM\) only
``depends'' on \(Density\) in a probablistic sense -- if \(Density = x\)
then the most probable value of \(FCM\) is \(\beta_0 + \beta_1 x\). With
some strong assumptions model \eqref{eq:regression} can be turned into a
model of causal dependency, which is the focus of chapter xxx.

\(\beta_0\) and \(\beta_1\) are the \textbf{parameters} of model
\eqref{eq:regression}. Specifically \(\beta_0\) is the model
\textbf{intercept} and \(\beta_1\) is the modeled \textbf{effect} of
\(Density\). Again, the effect (\(\beta_1\)) has a probabilistic, and
not causal, interpretation. This interpretation is

\begin{equation}
\beta_1 = \textrm{E}[FCM|Density=x+1] - \textrm{E}[FCM|Density=x] 
\label{eq:beta1}
\end{equation}

Or, in words, ``beta-1 is the expected value of FCM when density equals
x + 1 minus the expected value of FCM when the density equals x.''
\(\beta_1\) is simply the difference in expected values given a one unit
difference in \(Density\). A very short way to state this is
``\(\beta_1\) is a difference in conditional means''.

\subsection{Using a linear model to estimate explanatory
effects}\label{using-a-linear-model-to-estimate-explanatory-effects}

The goal of the statistical model here is to estimate \(\beta_1\) -- the
probabalistic effect of \(Density\) on \(FCM\). This estimate, and a
measure of the uncertainty of this estimate, are in the table of
coefficients of the fit model

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 736.0 & 331.9 & 2.2 & 0.0281\\
\hline
Density & 671.1 & 178.9 & 3.8 & 0.0002\\
\hline
\end{tabular}

where the entries in the column ``Estimate'' are estimates of the
parameters \(\beta_0\) and \(\beta_1\) in model \eqref{eq:regression}. The
entries in the column ``Std. Error'' are the standard errors (SE) of the
estimates, which are measures of the uncertainty of the estimates.

The parameter estimates in the table above are the coefficients of the
fitted model

\begin{equation}
FCM_i = b_0 + b_1 Density_i + e_i
\label{eq:fcmi}
\end{equation}

where the subscript \emph{i} refers to the \emph{i}th individual. The
coefficients \(b_0\) and \(b_1\) are the y-intercept and the slope of
the line in Figure \ref{fig:squirrel}. The coefficient for \(Density\)
(\(b_1\)) is 671.1, and (given the definition of the parameter
\(\beta_1\) in equation \eqref{eq:beta1}) we expect squirrel mothers with
a local density of 2 squirrels within a 150 m radius of her midden to
average 671.1 more units of FCM (ng of fecal cortical metabolites per
gram dry food) than mother squirrels with a local density of only 1
squirrel within a 150 m radius of her midden.

\subsubsection{Probabilistic vs.~causal
conditioning}\label{probabilistic-vs.causal-conditioning}

Remember that this coefficient is estimating a probabilistic parameter.
Consequently, the coefficient \(b_1\) is simply a descriptor of a
pattern of relationship between local density and fecal cortisol
metabolites - no causal effect is implied. With the strong assumptions
explained in chapter xxx, however, \(b_1\) can estimate a causal effect.

\subsection{Using a linear model for
prediction}\label{using-a-linear-model-for-prediction}

Model \eqref{eq:fcmi} gives the measured value of \emph{FCM} for each
squirrel. The equation includes the linear predictor
(\(b_0 + b_1 Density_i\)) and the \textbf{residual} from the predictor
(\(e_i\)). The predictor part is called ``predictor'' because it is the
equation for predicting the value of an individual's \(FCM\) given that
individual's value of \(Density\):

\begin{equation}
\widehat{FCM} = b_0 + b_1 Density
\label{eq:fcmhat}
\end{equation}

where \(\widehat{FCM}\) is read as ``FCM hat'' and is the
\textbf{predicted value} or simply ``prediction''. Very often, we use
the predictor part (equation \eqref{eq:fcmhat}) to predict unknown or
future values given different modeled inputs (the \(X\)).

\subsection{Reporting results}\label{reporting-results}

The authors of the squirrel fcm data published a figure and table
similar to fig. xxx and table above but used a slightly more complex
linear model. Here is how the author's reported the results:

\begin{quote}
Across 6 years (2006 to 2011), we found a positive relationship between
local density and concentrations of fecal cortisol metabolites {[}FCM;
t\(_155\) = 3.63, P = 0.0002 (table S4 and Fig. 3A){]}.
\end{quote}

I would advocate reporting the estimate and a confidence interval
instead of \(t\) and \(p\). For example ``Across 6 years (2006 to 2011),
the probabilistic effect of local density on fecal cortisol metabolites
is 671.1 (95\% CI: 317.7, 1024.5). If a \(p\)-value is report \emph{in
addition} to the effect and CI, always report the exact \emph{p}-value,
which emphasizes the continuous nature of evidence against the null, and
not something like''\(p < 0.05\)``, which artificially dichotomizes the
evidence against the null.

\section{Working in R}\label{working-in-r}

\subsection{\texorpdfstring{Exploring the bivariate relationship between
\emph{Y} and
\emph{X}}{Exploring the bivariate relationship between Y and X}}\label{exploring-the-bivariate-relationship-between-y-and-x}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import the ``FCM data dryad.csv'' data from the Dryad repository as
  the data.table \texttt{fcm}
\item
  How are different words in the column labels demarcated? Is this good
  practice?
\end{enumerate}

Here we want to fit a model of \texttt{FCM.ng.g.dry} as a function of
\texttt{Raw.Squirrel.Density}. The authors used prior knowledge to
expect a positive relationship between these two variables. Use qplot to
generate a scatterplot of \(FCM\) against \(Density\)

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Is there a trend? If so, does the trend look linear or non-linear?
\item
  Does the residual variation (the deviation from the trend on the \(Y\)
  axis) look homogenous along the \(X\)-axis?
\item
  Are there any obvious outliers?
\end{enumerate}

\subsection{Fitting the linear model}\label{fitting-the-linear-model}

We will fit a linear model to the data using the \texttt{lm} function,
which is very general and will be our workhorse throughout the class.
The minimal input to the function is a model formula and the name of the
data.frame (remember, a data.table is a data.frame). A formula is of the
form \texttt{Y\ \textasciitilde{}\ X}. All of the output we assign to
the object \texttt{fit}.

Let's fit the linear model to the data using density as the predictor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(FCM.ng.g.dry }\OperatorTok{~}\StringTok{ }\NormalTok{Raw.Squirrel.Density, }\DataTypeTok{data=}\NormalTok{fcm)}
\end{Highlighting}
\end{Shaded}

R will look for the specified \(Y\) and \(X\) variables in the column
names of \texttt{fcm}. If these are not found, R will return an error,
for example

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(FCM_ng_g_dry }\OperatorTok{~}\StringTok{ }\NormalTok{Raw_Squirrel_Density, }\DataTypeTok{data=}\NormalTok{fcm)}
\end{Highlighting}
\end{Shaded}

will return the error ``Error in eval(predvars, data, env) : object
`FCM\_ng\_g\_dry' not found''. This means your spelling and
capitalization have to be exact!

\subsection{\texorpdfstring{Getting to know the linear model: the
\texttt{summary}
function}{Getting to know the linear model: the summary function}}\label{getting-to-know-the-linear-model-the-summary-function}

The \texttt{lm} function returns an \texttt{lm} object, which we've
assigned to the name \texttt{fit}. \texttt{fit} contains lots of
information about our fit of the linear model to the data. Most of the
information that we want for most purposes can be retrieved with the
\texttt{summary} function, which is a general-purpose R command the
works with many R objects.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = FCM.ng.g.dry ~ Raw.Squirrel.Density, data = fcm)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2107.5 -1108.3  -434.9   511.8  8186.8 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(>|t|)    
## (Intercept)             736.0      331.9   2.217 0.028078 *  
## Raw.Squirrel.Density    671.1      178.9   3.752 0.000248 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1732 on 154 degrees of freedom
##   (7 observations deleted due to missingness)
## Multiple R-squared:  0.08374,    Adjusted R-squared:  0.07779 
## F-statistic: 14.07 on 1 and 154 DF,  p-value: 0.0002484
\end{verbatim}

What is here:

\textbf{Call}. This is the model that was fit

\textbf{Residuals}. This is a summary of the distribution of the
residuals. From this one can get a sense of the distribution (for
inference, the model assumes a normal distribution with mean zero). More
useful ways to examine this distribution will be introduced later in
this chapter.

\textbf{Coefficients table}. This contains the linear model coefficients
and their standard error and associated \(t\) and \(p\) values.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The column of values under ``Estimate'' are the coefficients of the
  fitted model (equation \eqref{eq:fcmi}). Here, 735.9604344 is the
  intercept (\(b_0\)) and 671.1379749 is the effect of \(Density\)
  (\(b_1\)).
\item
  The column of values under ``Std. Error'' are the standard errors of
  the coefficients.
\item
  The column of values under ``t value'' are the \emph{t-statistics} for
  each coefficient. A \(t\)-value is a \textbf{signal to noise ratio}.
  The coefficient \(b_1\) is the ``signal'' and the SE is the noise. Get
  used to thinking about this ratio. Any \(t\) less than 2 is indicative
  of too much noise to say much about the signal. A \(t\) between 2 and
  3 means the noise is large enough to suggest an effect. A \(t\)
  greater than 3 is pretty good evidence of an effect.
\item
  The column of values under ``Pr(\textgreater{}\textbar{}t\textbar{})''
  is the \(p\)-value, which is the exact probability associated with a
  particular \(t\). What is the \(p\)-value a test of? The \(p\)-value
  tests the hypothesis ``how probable are the data if the coefficient is
  zero?''. Formally \(P = \mathrm{freq(t' \ge t|H_o)}\), where \(t'\) is
  the hypothetical t-value, t is the observed \(t\)-value, and \(H_o\)
  is the null hypothesis. We will return to \(p\)-values in Chapter xxx.
\end{enumerate}

\textbf{Signif. codes}. I am surprised that base R returns this. These
are useless because the concept of ``levels of significance'' is
muddled, as will be discussed in Chapter xxx.

Beneath the Signif. codes are some model statistics which are useful

\textbf{Residual standard error} This is \(\sqrt{\sum{e_i^2}/(n-2)}\),
where \(e_i\) are the residuals in the fitted model. ``degrees of
freedom'' is the number of \(e_i\) that are ``allowed to vary'' after
fitting the parameters, so is the total sample size (\(n\)) minus the
number of parameters fit. The fit model has two fit parameters (\(b_0\)
and \(b_1\) so the df is \(n-2\). Note that this is the denominator in
the residual standard error equation.

\textbf{Multiple R-squared}. This is an important but imperfect summary
measure of the whole model that effectively measures how much of the
total variance in the response variable ``is explained by'' the model.
Its value lies between zero and 1. \textbf{It's a good measure to report
in a manuscript}.

\textbf{F-statistic and p-value}. These are statistics for the whole
model (not the individual coefficients) and I just don't find these very
useful.

Note that the \(p\)-value for the coefficient for Raw.Squirrel.Density
is very small and we could conclude that the data are not consistant
with a model of no slope. But did we need a formal hypothesis test for
this? We haven't learned much if we have only learned that the slope is
``not likely to be exactly zero''. What we want to know is not \emph{if}
there is a relationship between \(FCM\) and \(Density\), which is
imperfectly answered with a \(p\)-value, but \emph{the sign and
magnitude} of the relationship and the uncertainty in this estimate. For
this, we don't need the \(p\)-value. Instead, we want to interpret the
coefficient to its SE directly (for a quick-and-dirty interpretation) or
the confidence interval of the effect (for a more formal
interpretation). Please read this paragraph again. We will come back to
it over and over.

\subsection{display: An alternative to
summary}\label{display-an-alternative-to-summary}

Much of what we want to know about a model fit is returned by the
\texttt{display} function from the \texttt{arm} package.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{display}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## lm(formula = FCM.ng.g.dry ~ Raw.Squirrel.Density, data = fcm)
##                      coef.est coef.se
## (Intercept)          735.96   331.94 
## Raw.Squirrel.Density 671.14   178.90 
## ---
## n = 156, k = 2
## residual sd = 1732.02, R-Squared = 0.08
\end{verbatim}

The \texttt{display} function does not give a \(t\)-value or a
\(p\)-value of the coefficients because the authors of the arm package
do not think \(p\)-values are very valuable. We don't need a \(t\)
because one can mentally compute the approximate ratio of the
coefficient to its SE and get a sense of the signal to noise, and that's
all the authors of the display function think we need.

\subsection{Confidence intervals}\label{confidence-intervals}

Confidence intervals for the coefficients of the model are obtained by

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                          2.5 %   97.5 %
## (Intercept)           80.21785 1391.703
## Raw.Squirrel.Density 317.73057 1024.545
\end{verbatim}

\texttt{confint} returns by default the 95\% confidence interval (CI) of
all parameters. The most useful way of thinking about the meaning of a
CI is

\textbf{A confidence interval contains the range of parameter values
that are consistent with the data, in the sense that a \(t\)-test would
not reject the null hypothesis of a difference between the estimate and
any value within the interval}

A more textbook way of defining a CI is: A 95\% CI of a parameter has a
95\% probability of including the true value of the parameter. It does
not mean that there is a 95\% probability that the true value lies in
the interval. This is a subtle but important difference. Here is a way
of thinking about the proper meaning of the textbook definition: we
don't know the true value of \(\beta_1\) but we can 1) repeat the
experiment or sampling, 2) re-estimate \(\beta_1\), and 3) re-compute a
95\% CI. If we do 1-3 many times, 95\% of the CIs will include
\(\beta_1\) within the interval.

Confidence intervals are often interpreted like \(p\)-values. That is,
the researcher looks to see if the CI overlaps with zero and if it does,
concludes there is ``no effect''. First, this conclusion is not correct
-- \textbf{the inability to find sufficient evidence for an effect does
not mean there is no effect, it simply means there is insufficient
evidence to conclude there is an effect}!

Second, what we want to use the CI for is to guide us about how big or
small the effect might reasonably be, given the data. Again, A CI is a
measure of parameter values that are ``consistent'' with the data. If
our biological interpretations at the small-end and at the big-end of
the interval's range radically differ, then we don't have enough
\emph{precision} in our analysis to reach an unambiguous conclusion.
Remember this.

\subsection{How good is our model?}\label{how-good-is-our-model}

How well does variation in \(Density\) ``explain'' variation in \(FCM\)?
The answer to this is in the \(R^2\) value, which is given in
\texttt{display(fit)} and in \texttt{summary(fit)} and accessed directly
with

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(fit)}\OperatorTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08373756
\end{verbatim}

\(R^2\) is the fraction of the total variance of \(Y\) explained by the
model, or more specifically, the linear predictor. It will vary from
zero (the model explains nothing) to one (the model explains
everything). If \(R^2=0\) the response is completely unpredictable by
the predictors. We can think of the values of the response as white
noise or all error. This doesn't mean that the values are ``not caused''
or ``random'' or not predicted by some other variable. It only means the
values are random with respect to the \(X\) variable(s) in the model. If
\(R^2=1\) we can \emph{exactly} predict the response from the \(X\)
variables in the model. So the bigger the \(R^2\), the better the model
in the sense that the response is more predicatable. \textbf{Super
importantly}, ``explains'' is in a probabilistic and not causal sense.
We will explore this concept much more in future worksheets.

\subsection{Model checking}

\texttt{plot} is a very useful base R function for ``model checking'' or
``model diagnostics'' to see if our model fit is acceptable.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-1.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-2.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-3.pdf}
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-diagnostics-4.pdf}

Compare the four diagnostic plots using the guidelines from here
\url{http://data.library.virginia.edu/diagnostic-plots/}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Look at the plots you just made. What is a residual? What is a fitted
  value?
\end{enumerate}

\subsection{exploring a lm object}\label{exploring-a-lm-object}

\texttt{fit} contains much information but simply typing \texttt{fit}
into the console gives us only the model and the coefficients.
\texttt{names} is a super important R function. It gives us the names of
all the parts of some R object. \texttt{fit} is an lm object.
\texttt{names(fit)} gives us all the parts contained in an lm object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "coefficients"  "residuals"     "effects"       "rank"         
##  [5] "fitted.values" "assign"        "qr"            "df.residual"  
##  [9] "na.action"     "xlevels"       "call"          "terms"        
## [13] "model"
\end{verbatim}

You can see any of these parts using the dollar sign

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  What does \texttt{fit\$residuals} return? Answer using equation
  \eqref{eq:fcmi}
\item
  What does \texttt{fit\$fitted.values} return? Answer using equation
  @ref(eq:fcmi
\end{enumerate}

You can use qplot to make a plot similar to the first plot of
\texttt{plot(fit)}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{qplot}\NormalTok{(fit}\OperatorTok{$}\NormalTok{fitted.values, fit}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{geom=}\KeywordTok{c}\NormalTok{(}\StringTok{'point'}\NormalTok{, }\StringTok{'smooth'}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## `geom_smooth()` using method = 'loess' and formula 'y ~ x'
\end{verbatim}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fit-fitted-1.pdf}

\section{Problems}\label{problems-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the chick data from Chapter 3. Compare the effects of
  nest\_temperature\_above\_ambient on day13\_mass by fitting two
  separate linear models 1) one using only the control group and one
  using the treated group. The grouping variable is playback\_treatment.
  These models were plotted in Chapter 3 so \texttt{lm} will return the
  linear model behind these plots.
\end{enumerate}

Report the results using the two effect estimates and a 95\% confidence
interval (we will learn in a later chapter a more sophisticated way of
comparing the effects between the groups)

\textbf{file name}: ``allDatasetsMarietteBuchanan2016.xls''

\textbf{source}: \url{https://datadryad.org//handle/10255/dryad.122315}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  (Grad students only) -- find a dataset using Dryad that has data that
  can be fit by a simple linear model with a single continuous \(X\)
  (its okay if the authors fit the data with a more complex model). Fit
  the data and report the results with a plot and text.
\end{enumerate}

\chapter{Least Squares Estimation}\label{least-squares-estimation}

The linear models in the last chapter and for much of this book are fit
to data using a method called ``ordinary least squares'' (OLS). This
chapter explores the meaning of OLS and related statistics, including
\(R^2\), as well as some alternative methods for bivariate regression.

\section{OLS regression}\label{ols-regression}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fake-data-ma-1.pdf}

The fake data illustrated in the scatterplot above (Figure
\ref{fig:fake-data-ma}) were modeled to look something like the squirrel
fecal cortisol metabolite data in the previous chapter. If a typical
student is asked to draw a regression line through the scatter, they
typically draw a line similar to that in Figure \ref{fig:fake-data-ma}.
This line is not the OLS regression line but the major axis of an elipse
that encloses the scatter of points--that students invariably draw this
line suggests that the brain interprets the major axis of an elliptical
scatter of points as a trend (This major axis line is an alternative
method for estimating a slope and is known as standard major-axis
regression. More about this at the end of this chapter.)

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/fake-data-ols-1.pdf}

The OLS regression line is the red line in Figure
\ref{fig:fake-data-ols} -- the standard major axis line is left for
comparison). The OLS regression line

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  passes through the bivariate mean (\(\bar{x}\), \(\bar{y}\)) of the
  scatter, and
\item
  minimizes the sum of the squared deviations from each point to it's
  modeled value \(\sum{(y_i - \hat{y}_i)^2}\)
\end{enumerate}

There are an infinite number of lines that pass through the bivariate
mean (think of anchoring a line at the bivariate mean and spinning it).
The OLS line is the line that minimizes the squared (vertical)
deviations (``least squares'').

For a bivariate regression, the slope (coefficient \(b_1\) of \(X\)) of
the OLS model fit is computed by

\begin{equation}
b_1 = \frac{\mathrm{COV}(X, Y)}{\mathrm{VAR}(X)}
\end{equation}

This equation is worth memorizing. We will generalize this into a more
flexible equation in a few chapters.

\section{\texorpdfstring{How well does the model fit the data? \(R^2\)
and ``variance
explained''}{How well does the model fit the data? R\^{}2 and variance explained}}\label{how-well-does-the-model-fit-the-data-r2-and-variance-explained}

Let's switch to real data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.056r5}
\item
  File: ``Diet-shift data.xls''
\end{enumerate}

Fish require arachidonic acid (ARA) and other highyly unsaturated fatty
acids in their diet and embryo and yolk-stage larvae obtain these from
yolk. Fuiman and Faulk (xxx) designed an experiment to investigate if
red drum (\emph{Sciaenops ocellatus}) mothers provision the yolk with
ARA from recent dietary intake or from stored sources in somatic
tissues. The data below are from experiment 8. The \emph{x}-axis is the
days since a diet shift to more and less ARA (\(days\)) and the
\emph{y}-axis is the ARA content of the eggs (\(ARA\)).

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/egg-data-1.pdf}

The statistic \(R^2\) is a measure of the fit of a model to data. The
\(R^2\) for the fit of the egg data is 0.42. \(R^2\) is the fraction of
two variances \(\frac{\mathrm{VAR}(Model)}{\mathrm{VAR}(Y)}\), or, the
fraction of the variance of \(Y\) ``explained by the model.'' The value
of \(R^2\) ranges from zero (the fit cannot be any worse) to one (the
fit is ``pefect'').

To understand \(R^2\), and its computation, a bit more, let's look at
three kinds of deviations.

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/ols-deviations-1.pdf}
\caption{\label{fig:ols-deviations}Three kinds of deviations from a fit
model. A. Deviations of the measured values from the mean. These are in
the numerator of the equation of the sample variance. The dashed line is
the mean ARA content. B. Deviations of the measured values from the
modeled values. The sum of these deviations squared is what is minimized
in an OLS fit. C. Deviations of the modeled values from the mean ARA
content. The measured values are in gray, the modeled values in black}
\end{figure}

Figure \ref{fig:ols-deviations}A shows the deviations from the measured
values to the mean value (dashed line). These are the deviations in the
numerator of the equation to compute the variance of \(ARA_EGG_MG\).
Figure \ref{fig:ols-deviations}B shows the deviations of the measured
values from the modeled values. The sum of these deviations squared is
what is minimized by the OLS fit. The bigger these deviations are, the
worse the model fit. Figure \ref{fig:ols-deviations}C shows the
deviations of the modeled values to the mean value. The bigger these
deviations are, the better the model fit.

The sums of the squares of these deviations (or ``sums of squares'')
have names:

\begin{equation}
\mathrm{SS(total)} = \sum{(y_i - \bar{y})^2}
\end{equation}

\begin{equation}
\mathrm{SS(error)} = \sum{(y_i - \hat{y_i})^2}
\end{equation}

\begin{equation}
\mathrm{SS(model)} = \sum{(\hat{y_i} - \bar{y})^2}
\end{equation}

Again, \(\mathrm{SS(total)}\) is the numerator of the equation for the
sample variance. It is called ``s-s-total'' because
\(\mathrm{SS(total)} = \mathrm{SS(model)} + \mathrm{SS(error)}\). That
is, the total sums of squares can be \textbf{decomposed} into two
\textbf{components}: the modeled sums of squares and the error sums of
squares. Given these components, it's easy to understand \(R^2\)

\begin{equation}
R^2 = \frac{SS(model)}{SS(total)}
\end{equation}

\(R^2\) is the fraction of the total sums of squares that is due to (or
``explained by'') the model sums of squares. Above I said that \(R^2\)
is the fraction of \emph{variance} explained by the model. Equation xxx
is a ratio of variance, but the \((n-1)^{-1}\) in both the numerator and
the denominator cancel out. Finally, many sources give the equation for
\(R^2\) as

\begin{equation}
R^2 = 1- \frac{SS(error)}{SS(total)}
\end{equation}

which is an obvious alternative given the decomposition. I prefer the
former equation because it emphasizes the model fit instead of model
ill-fit.

\chapter{\texorpdfstring{A linear model with a single, categorical
\emph{X}}{A linear model with a single, categorical X}}\label{a-linear-model-with-a-single-categorical-x}

\section{\texorpdfstring{A linear model with a single, categorical
\emph{X} is the engine behind a single factor (one-way) ANOVA and a
t-test is a special case of this
model.}{A linear model with a single, categorical X is the engine behind a single factor (one-way) ANOVA and a t-test is a special case of this model.}}\label{a-linear-model-with-a-single-categorical-x-is-the-engine-behind-a-single-factor-one-way-anova-and-a-t-test-is-a-special-case-of-this-model.}

To introduce modeling with a single, categorical \(X\) variable, I'll
use the vole data from

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Source: Dryad Digital Repository.
  \url{https://doi.org/10.5061/dryad.31cc4/1}
\item
  File: ``RSBL-2013-0432 vole data.xlsx''
\item
  Sheet: ``COLD VOLES LIFESPAN''
\end{enumerate}

Normal cellular metabolism creates reactive oxygen species (ROS) that
can disrupt cell function and potentially cause cell damage.
Anti-oxidants are molecules that bind ROS, inhibiting their ability to
disrupt cell activity. A working hypothesis for many years is that
supplemental anti-oxidants should improve cell function and, scaling up,
whole-animal function (such as lifespan). The vole data explores this
with supplemental Vitamins C and E, which are anti-oxidants, in the diet
of the short-tailed field vole (\emph{Microtus agrestis}).

The goal of the study is to measure the effect of anti-oxidants on
lifespan. The researchers randomly assigned the voles to one of thre
treatment levels: ``control'', ``vitamin E'', and ``vitamin C''. The
variable \(treatment\), is a single, categorical \(X\) variable.
Categorical variables are often called \textbf{factors} and the
treatment levels are often called \textbf{factor levels}. There are no
units to a categorical \(X\) variable (even though a certain amount of
each anti-oxidant was supplemented). The response (\(Y\)) is
\(lifespan\) measured in days.

The linear model with a categorical \(X\) variable with three levels is
not immediately obvious, and so I don't present the model until after
showing the table of model coefficients

\subsection{Table of model
coefficients}\label{table-of-model-coefficients}

Here is the table of coefficients from the linear model fit

\begin{tabular}{l|r|r|r|r}
\hline
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\hline
(Intercept) & 503.4 & 27.4 & 18.4 & 0.000\\
\hline
treatmentvitamin\_E & -89.9 & 52.5 & -1.7 & 0.090\\
\hline
treatmentvitamin\_C & -115.1 & 54.5 & -2.1 & 0.037\\
\hline
\end{tabular}

The table has estimates for three parameters. The first estimate (the
intercept) is the mean response in the reference level. Here the
reference level is the ``control'' group. The additional estimates are
the differences in the mean between each of the other treatment levels
and the reference level. These are the ``effects'' in the model. So
typically with categorical \(X\), when we speak of an \emph{effect} we
mean a difference in means. These estimates and their meaning are
illustrated in Figure \ref{fig:vole-mean-plot}.

(note. The default in R is to set the level that is first alphabetically
as the reference level. In the vole data, ``control'' comes before
``vitamin\_E'' and ``vitamin\_C'' alphabetically, and so by default, it
is the reference level. This makes sense for these data -- we want to
compare the lifespan of the vitamins E and C groups to that of the
control group. The reference level can be changed of course.)

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/vole-mean-plot-1.pdf}
\caption{\label{fig:vole-mean-plot}What the coefficients of a linear model
with a single categorical X mean. The means of the three treatment
levels for the vole data are shown with the filled circles. The length
of the double-headed arrows are differences in means. The intercept
(\(b_0\)) is the mean of the reference treatment level. The coefficients
(\(b_1\) and \(b_2\)) are the differences between the treatment level's
mean and the reference mean. As with a linear model with a continuous X,
the coefficients are effects.}
\end{figure}

\subsection{The linear model}\label{the-linear-model}

We can see an immediate difference between the coefficient table for a
linear model fit to a single, categorical \(X\) and that for a single,
continuous \(X\). For the latter, there is a single coefficient for
\(X\). For the former, there is a coefficient for each level of the
categorical \(X\) \emph{except} the ``reference'' level.

The linear model for a single, continuous \(X\) with three factor levels
is

\begin{equation}
lifespan = \beta_0 + \beta_1 vitamin\_E + \beta_2 vitamin\_C + \varepsilon
\end{equation}

and the estimates in the coefficient table are the coefficients of the
fit model

\begin{equation}
lifespan_i = b_0 + b_1 vitamin\_E + b_2 vitamin\_C + e_i
\end{equation}

Remember, \(b_0\) is the mean of the control group, \(b_1\) is the
difference in means between the vitamin E and control groups, and
\(b_2\) is the difference in means between the vitamin C and control
groups (Figure \ref{fig:vole-mean-plot}).

In this model, \(vitamin\_E\) and \(vitamin\_C\) are \textbf{dummy
variables} that contain a one, if the data is from that treatment level,
and zero otherwise. This is called dummy coding or treatment coding. The
\texttt{lm} function creates these dummy variables under the table, in
something called the \textbf{model matrix}, which we'll cover in the
next chapter. You won't see these columns in your data. But if you did,
it would look something like this

\begin{tabular}{r|l|r|r}
\hline
lifespan & treatment & vitamin\_E & vitamin\_C\\
\hline
621 & control & 0 & 0\\
\hline
865 & control & 0 & 0\\
\hline
583 & vitamin\_E & 1 & 0\\
\hline
561 & vitamin\_E & 1 & 0\\
\hline
315 & vitamin\_C & 0 & 1\\
\hline
157 & vitamin\_C & 0 & 1\\
\hline
\end{tabular}

There are alternative coding methods. Dummy coding is the default in R.
Note that the method of coding can make a difference in an ANOVA table,
and many published papers using R have published incorrect
interpretations of ANOVA table outputs. This is both getting ahead of
ourselves and somewhat moot, because I don't advocate publishing ANOVA
tables.

\subsection{Reporting results}\label{reporting-results-1}

What should be reported for the analyis of effects of anti-oxidant
supplements on vole lifespan? Best practice includes reporting the raw
data with a summary distribution and treatment effects with CIs. ``Raw
data'' means the individual lifespans as a function of treatment level.

\subsubsection{Harrel Plot of the data}\label{harrel-plot-of-the-data}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/harrellplot-1.pdf}
\caption{\label{fig:harrellplot}HarrellPlot of the raw data, distribution,
and effects of the vole lifespan data.}
\end{figure}

The raw data, the distributions within treatment level, and the effects
(difference in means) of treatment can be combined into a single plot
that I call a Harrell plot (Figure \ref{fig:voles}). Notice that the
\emph{x}-axis and \emph{y} axes are flipped so that \(lifespan\) is on
the \emph{x}-axis. It is still the ``response'' or ``Y'' variable! The
Harrell plot contains two parts

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The bottom contains a \textbf{strip chart} (often called a ``dot
  plot'') of the raw response measures grouped by factor level.
  Superimposed over the strip chart is a \textbf{box plot} summarizing
  the distribution of each factor level. The line in the center of a box
  is the median \(lifespan\) for that group, the left and right edges of
  the box are the 25\% and 75\% quantiles of \(lifespan\) for that grop,
  and the lines extending to the left and right of the box are the
  ``whiskers'', which are the smallest and largest value within
  \(1.5 IQR\) (inter-quartile range, which is the interval bounded by
  box).
\item
  The top is a \textbf{forest plot} of the effects and the 95\% CI of
  the effects. For categorical \(X\), the effects could be model
  coefficients or treatment \textbf{contrasts}, which are differences in
  means between treatment levels. Model coefficients are a subset of
  possible treatment contrasts.
\end{enumerate}

The Harrell plot above shows the effects as model coefficients, which
(again!) are differences between the mean of the response in a specific
treatment level and the mean of the response in a reference level. Here
the reference level is the control group.

\subsubsection{In-text reporting}\label{in-text-reporting}

``The mean lifespan of cold-reared voles supplmented with vitamin E was
-89.9 days shorter than the mean lifespan for the control group (95\%
CI: -194.1, 14.3). The mean lifespan of cold-reared voles supplmented
with vitamin C was -115.1 days shorter than the mean lifespan for the
control group (95\% CI: -223.2, -6.9).

\subsubsection{Correct interpretation of the Confidence Interval is
key}\label{correct-interpretation-of-the-confidence-interval-is-key}

Remember, that the CI contains the range of parameter values that are
consistent with the data (in the sense that a t-test wouldn't reject the
hypothesis test). This means that a true value at the low end or the
high end of the CI is consistent with the data. Your technical
report/manuscript should discuss the consequences of this. For example,
A small, increase in lifespan is consistant with the Vitamin E but not
Vitamin C supplementation, if we use the 95\% CI as a pretty good range
for inferring ``consistent with''. Both a 223 day and a 7 day decrease
in lifespan are consistant with the Vitamin C effect. 223 days seems
like a huge effect, especially for a short lived vole. 7 days is
certainly a much smaller effect, but this doesn't mean that it doesn't
have important ecological, behavioral, or fitness consequences.

\section{Working in R}\label{working-in-r-1}

\subsection{\texorpdfstring{Exploring the relationship between \emph{Y}
and
\emph{X}}{Exploring the relationship between Y and X}}\label{exploring-the-relationship-between-y-and-x}

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Import the vole data from the Dryad repository as the data.table
  \texttt{vole\_wide}. Replace the spaces in the column labels with the
  underscore ``\_``.
\end{enumerate}

The data are in ``wide'' format. A pretty good script for for converting
these to long format is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vole_long <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(vole_wide, }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"control"}\NormalTok{, }\StringTok{"vitamin_E"}\NormalTok{, }\StringTok{"vitamin_C"}\NormalTok{), }\DataTypeTok{variable.name=}\StringTok{"treatment"}\NormalTok{)}
\NormalTok{vole <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(vole_long)}
\CommentTok{# melt is from reshape2 package and is a workhorse in R}
\CommentTok{# the resulting data frame has 3 stacked copies of the same rows}
\CommentTok{# na.omit removes the superfluous two extra sets of rows created by melt}
\CommentTok{# the more compact way to do this is combine the steps:}
\CommentTok{# vole <- na.omit(melt(vole_wide, measure.vars=c("control", "vitamin_E", "vitamin_C"))}
\CommentTok{# but I suggest two steps so you can see what melt does. This isn't the best example of using melt.}
\end{Highlighting}
\end{Shaded}

Use the ggpubr package to create a box plot, grouped by treatment, with
superimposed strip chart (``dots'')

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Do the response as a function of group look fairly normally
  distributed or are there red flags such as skewness, outliers, bimodal
  clusters, etc.
\item
  Is the direction of the effect consistent with the expected direction?
\end{enumerate}

\subsection{Fitting the model}\label{fitting-the-model}

As with a single, continuous \(X\), we fit the model using the
\texttt{lm} function and with the model formula of the form
\texttt{y\ \textasciitilde{}\ x}. Note that the R formula can use the
single categorical variable \(treatment\). The code underneath lm will
note that \(treatment\) is a factor with three levels and will
automatically create the two dummy variables noted above in the linear
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lifespan }\OperatorTok{~}\StringTok{ }\NormalTok{treatment, }\DataTypeTok{data=}\NormalTok{vole)}
\end{Highlighting}
\end{Shaded}

All of the same scripts to access the information in \texttt{fit} that
we used with the continuous \(X\) analysis are the same. For example,
the base R \texttt{summary} function gives the same information as in
the continuous \(X\) example.

Questions

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Review different output in the \texttt{summary} function and list
  which are useful and which are not so useful and why.
\end{enumerate}

Other useful functions on the lm object (``fit'') are
\texttt{coefficients(summary())}, \texttt{coef()} or
\texttt{coefficients()} and \texttt{confint}. Assigning the output of
these functions to an R object allows you to increase reproducibility.
For example, if I assign the coefficients to \texttt{b}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{b <-}\StringTok{ }\KeywordTok{coef}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

then I can report these in R markdown text by embedding r code directly
in the text. For example, if I embed ``r
round(b{[}``treatmentvitamin\_E''{]}, 1)'' between a pair of single
accent characters, then r markdown inserts -89.9 into the rendered text.

\subsection{An introduction to
contrasts}\label{an-introduction-to-contrasts}

We often want to compare more than just the non-reference levels to the
reference level. For example, we might want to compare the effects of
the vitamin E supplementation to vitamin C supplementation. Or, we might
want to combine (or ``pool'') vitamin C and vitamin E levels effects
into a single ``anti-oxidant'' level and compare to the control. These
comparisons of means are called linear \textbf{contrasts}. The emmeans
package is a good package for obtaining contrasts for both simple linear
models computed with \texttt{lm} and for more complicated statistical
models. If you haven't already, download the emmeans package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit.em <-}\StringTok{ }\KeywordTok{emmeans}\NormalTok{(fit, }\DataTypeTok{spec=}\StringTok{"treatment"}\NormalTok{)}
\NormalTok{fit.em}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  treatment   emmean       SE df lower.CL upper.CL
##  control   503.3929 27.40978 93 448.9625 557.8233
##  vitamin_E 413.4762 44.75999 93 324.5917 502.3607
##  vitamin_C 388.3158 47.05685 93 294.8702 481.7614
## 
## Confidence level used: 0.95
\end{verbatim}

The \texttt{emmeans()} function returns various estimated means,
depending on what is specified with the \texttt{spec=} parameter. Here
the grouping variable ``treatment'' is specified, so the means returned
are estimates of \(\mathrm{E}(lifespan | treatment)\), the modeled means
for each level of treatment. For this simple analysis, the modeled means
are simply the group means. Note that the default value returned is a
table with the standard error and 95\% confidence limits of the
estimates.

Let's use the emmeans object to get the contrasts for all combinations
of treatment levels.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(}\KeywordTok{contrast}\NormalTok{(fit.em, }\DataTypeTok{method=}\StringTok{"revpairwise"}\NormalTok{, }\DataTypeTok{adjust=}\StringTok{"none"}\NormalTok{), }\DataTypeTok{infer=}\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  contrast                estimate       SE df  lower.CL   upper.CL t.ratio
##  vitamin_E - control    -89.91667 52.48574 93 -194.1429  14.309609  -1.713
##  vitamin_C - control   -115.07707 54.45772 93 -223.2193  -6.934834  -2.113
##  vitamin_C - vitamin_E  -25.16040 64.94462 93 -154.1275 103.806738  -0.387
##  p.value
##   0.0900
##   0.0373
##   0.6993
## 
## Confidence level used: 0.95
\end{verbatim}

I've sent to parameters to the contrast function and one to the summary
function

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  method=``revpairwise''. \texttt{contrast} can create different
  combinations of differences between means. Here I've specified all
  pairwise differences (the ``rev'' reverses the order of the
  subtraction). Notice that the statistics (estimate, SE, etc) are equal
  to the same statistics for \(b_1\) and \(b_2\) of the linear model. I
  said earlier that these coefficients are contrasts!
\item
  adjust=``none''. In classical frequentist hypothesis testing, the
  p-value of a contrast in what are called ``post-hoc tests'' is
  adjusted to reflect ``multiple testing'' (more than one p-value is
  being computed). This adjustment is almost standard in biology, but
  the practice is hugely controversial. The concept of multiple testing
  is important, and we will return to this in a future chapter, but here
  I have chosen to show the unadjusted p-value. The reason is that I
  want the unadjusted confidence interval and the adjustment would
  adjust these as well. If deleted \texttt{adjust="none"} from the
  script, the contrast function would default to the \textbf{Tukey HSD}
  (Honestly Significant Difference) test. There are literally dozens and
  dozens of post-hoc tests, which largely reflects the misplaced
  emphasis on ``better'' \(p\)-values rather than parameter estimates
  and their uncertainty.
\item
  infer=c(TRUE, TRUE). This parameter controls what kind of inference to
  put in the table. The first value specifies the inclusion of the CI
  (emmeans uses ``CL'' for confidence limit), the second value specifies
  the inclusion of \(t\) and \(p\)-values.
\end{enumerate}

\subsection{Harrell plot}\label{harrell-plot}

\subsubsection{Installing the harrellplot
package}\label{installing-the-harrellplot-package}

The harrellplot package is available on github but not a cran repository
and, therefore, takes a little more work to install. To install a
package from a github repository, 1. load library(devtools) -- this may
need to be installed first using the R Studio Tools \textgreater{}
Install Packages\ldots{} tool 2. install harrellplot from github. In the
console, type

\texttt{install\_github("middleprofessor/harrellplot")}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  load the harrellplot package
\item
  harrellplot requires other packages including broom, Hmisc, car, lme4,
  and lmerTest. If you haven't installed these do. load these with the
  library() function at the start of your notebook.
\end{enumerate}

\subsubsection{Using harrellplot to make a nice, publishable plot of
treatment
effects}\label{using-harrellplot-to-make-a-nice-publishable-plot-of-treatment-effects}

In the console type `?harrellplot to see the many parameters. Unlike
ggplot2, variable names need to be specified with quotes in the
harrellplot function. The harrellplot function is a list with several
elements.

Here is the default plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{vole.harrellplot <-}\StringTok{ }\KeywordTok{harrellplot}\NormalTok{(}\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"lifespan"}\NormalTok{, }\DataTypeTok{data=}\NormalTok{vole)}
\NormalTok{vole.harrellplot}\OperatorTok{$}\NormalTok{gg }\CommentTok{# gg is the plot object}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/unnamed-chunk-12-1.pdf}

\chapter{Inference}\label{inference}

\section{\texorpdfstring{\(p\)-values}{p-values}}\label{p-values}

\begin{tabular}{l|r|r|r|r|r|r|r}
\hline
contrast & estimate & SE & df & lower.CL & upper.CL & t.ratio & p.value\\
\hline
vitamin\_E - control & -89.9 & 52 & 93 & -194.1 & 14.3 & -1.7 & 0.090\\
\hline
vitamin\_C - control & -115.1 & 54 & 93 & -223.2 & -6.9 & -2.1 & 0.037\\
\hline
vitamin\_C - vitamin\_E & -25.2 & 65 & 93 & -154.1 & 103.8 & -0.4 & 0.699\\
\hline
\end{tabular}

Let's use the vole data to introduce the \(p\)-value. The table above
gives a \(t\) and \(p\)-value for each pairwise contrast among the three
treatment levels. Given the table above, a typical report might (one
with several misconceptions) read

``We found a significant effect of Vitamin C (\(t=\) -2.1, \(p=\) 0.037)
on lifespan, but no effect of vitamin E (\(t=\) -1.7, \(p=\) 0.09) on
lifespan.''

A \(p\) value \emph{is a continuous measures of evidence against the
null}. As long as the data approximate the assumptions of the null
hypothesis pretty well, a very small \(p\)-value, such as 0.002 or
0.0005, is pretty good evidence against the null hypothesis -- but does
not mean ``an effect exists''. To show an effect exists, we need to
rigorously probe the hypothesis with multiple experiments that challenge
the hypothesis in different ways. A small \(p\), then, is evidence for a
research program to move forward with this probing. A big \(p\)-value,
say 0.22 or 0.76, is pretty weak evidence against the null, but does not
mean ``there is no effect.'' If an experiment is well designed, a big
\(p\) could suggest abandoning any hypotheses that predict biologically
consequential effects. Unfortunately, a big \(p\) could also reflect a
weak experimental design. Between small and big \(p\) values, such as
0.009 or 0.083, problems arise. These intermediate \(p\)-values beg for
replication. A major problem of inference using \(p\) values is that
there is no sharp boundaries between these three regions. Instead
biologists typically use the \(p < 0.05\) as a sharp boundary to declare
that an effect exists or not.

Okay. so what \emph{is} a \(p\)-value? A pretty good definition of a
\(p\)-value is: the long-run frequency of observing a test-statistic as
large or larger than the observed statistic, if the null were true. A
more succinct way to state this is

\begin{equation}
p = \mathrm{prob}(t \ge t_o | H_o)
\end{equation}

where \(t\) is a hypothetically sampled \(t\)-value from a null
distribution, \(t_o\) is the observed \(t\)-value, and \(H_o\) is the
null hypothesis. Part of the null hypothesis is the expected value of
the parameter estimated is usually (but not always) zero -- this can be
called the nil null. For example, if there is no vitamin E effect on
lifespan, then the expected difference between the means of the control
and vitamin E treatment levels is zero. Or,

\begin{equation}
\mathrm{E}(\bar{vitamin_E} - \bar{control} | H_o) = 0.0
\end{equation}

let's plot the data and look at the group means. Below is a strip chart
of the vole data with superimposed treatment level means, using the
function \texttt{ggstripchart} from the ggpubr package (can you make
this?). I'm going to refer to this kind of chart as a ``dot plot'',
which is what most biology researchers call this type of chart.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggstripchart}\NormalTok{(}\DataTypeTok{data=}\NormalTok{vole, }\DataTypeTok{x=}\StringTok{"treatment"}\NormalTok{, }\DataTypeTok{y=}\StringTok{"lifespan"}\NormalTok{, }\DataTypeTok{add =} \KeywordTok{c}\NormalTok{(}\StringTok{"mean"}\NormalTok{), }\DataTypeTok{add.params=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{color=}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/inference-strip-chart-1.pdf}

\subsection{the Null Distribution}\label{the-null-distribution}

The mean lifespan in the vitamin\_E treatment is -89.9 days shorter than
the mean lifespan in the control treatment. And, the mean lifespan in
the vitamin\_E treatment is -115.1 days shorter than the mean lifespan
in the control treatment. These are the measured effects, or the
\textbf{observed differences in means}. How confident are we in these
effects? Certainly, if the researchers did the experiment with
\emph{two} control treatment groups, they would measure some difference
in their means simply because of finite sampling (more specifically, the
many, many random effects that contribute to lifespan will differ
between the two control groups). So let's reframe the question: are the
observed differences unusually large compared to a distribution of
differences that would occur if there were no effect? That is, if the
``null were true''. To answer this, we compare our observed difference
to this \textbf{null distribution}. This comparison gives the
probability (a long-run frequency) of ``sampling'' a random difference
from the null distribution of differences that is as large, or larger,
than the observed difference.

What is a null distribution? It is the distribution of a statistic (such
as a difference in means, or better, a \(t\)-value) if the null were
true. Here, I am generating a null distribution that is relevant to the
cold vole data. See if you can understand the script before reading the
explanation below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seed <-}\StringTok{ }\DecValTok{1}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5} \CommentTok{# number of iterations}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan]) }
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan])}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{((vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{,]))}
\NormalTok{sample1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples (each size n)}
\NormalTok{sample2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples}
\NormalTok{null_dis <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, mean) }\OperatorTok{-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample1, }\DecValTok{2}\NormalTok{, mean)}
\KeywordTok{qplot}\NormalTok{(null_dis)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/null-distribution-1.pdf}
\caption{\label{fig:null-distribution}Null distribution for an infinitely
large data set that looks curiously like the lifespans of the cold-rear
voles from the control treatment.}
\end{figure}

What have we done above? We've simulated an infinitely large population
of voles that have a distribution of lifespans similar to that of the
cold-reared voles assigned to the control group. The mean \(\mu\) and
standard deviation \(\sigma\) of the simulated lifespan are equal to the
observed mean and standard deviation of the lifespans of the control
voles. Then, the script:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  randomly sample 56 values from this population of simulated lifespans
  and assign to sample1. We sample 56 values because that is the sample
  size of our control in the experiment.
\item
  randomly sample 56 values from this population of simulated lifespans
  and assign to sample2.
\item
  compute the difference \(\bar{Y}_{sample2} - \bar{Y}_{sample1}\).
\item
  repeat 1-3 100,000 times, each time saving the difference in means.
\item
  plot the distribution of the 100,000 differences using a histogram
\end{enumerate}

The distribution of the differences is a null distribution. Notice that
the mode of the null distribution is at zero, and the mean (-0.11584) is
close to zero (if we had set n to infinity, the mean would be precisely
zero). \emph{The expected difference between the means of two random
samples from the same population is, of course, zero}. Don't gloss over
this statement if that is not obvious. The tails extend out to a little
more than +100 and -100. What this means is that it would be rare to
randomly sample two sets of data from the same population with mean
\(\mu\) and standard deviation \(\sigma\) and find a difference of, say,
-257. In fact, in the 100,000 runs, there were no difference as large as
\textbar{}-257\textbar{} (the absolute value of -257). The minimum and
maximum differences sampled over the 100,000 iterations was -187 days
and 201 days.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{le_vitamin_E <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(null_dis}\OperatorTok{<=-}\KeywordTok{abs}\NormalTok{(b[}\StringTok{"treatmentvitamin_E"}\NormalTok{]))}
\NormalTok{ge_vitamin_E <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(null_dis}\OperatorTok{>=}\KeywordTok{abs}\NormalTok{(b[}\StringTok{"treatmentvitamin_E"}\NormalTok{]))}
\NormalTok{p_vitamin_E <-}\StringTok{ }\NormalTok{((le_vitamin_E }\OperatorTok{+}\StringTok{ }\NormalTok{ge_vitamin_E)}\OperatorTok{+}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(n_iter }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

How do our observed differences compare? Let's focus on vitamin E. The
vitamin\_E effect is -89.9 days. There are 2110 sampled differences less
than the observed value and 2126 greater than the absolute value of the
observed value. Together this is 4236 so the frequency of differences
from the simulated null distribution that as larger or larger than the
observed difference is 0.042 (this compuation includes the observed
value in both the numerator and denominator).

\subsection{\texorpdfstring{\(t\)-tests}{t-tests}}\label{t-tests}

A \(t\)-test is a test of differences between two values. These could be

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the difference between the means of two samples (a ``two-sample''
  \(t\)-test)
\item
  the difference between a mean of a sample and some pre-specified value
  (a ``one-sample'' \(t\)-test)
\item
  the difference between a coefficient from a linear model and a value
  (often zero)
\end{enumerate}

A \(t\)-test compares an observed \(t\)-value to a \(t\)-distribution.
The null distribution introduced above was a distribution of mean
differences. This isn't generally useful, since the distribution of
expected mean differences is a function of sample variability (standard
deviations) in addition to sample size and, therefore, a mean-difference
null distribution will be unique to every study. A \(t\)-distribution is
a distribution of \(t\)-values under the null (statistical jargon for
``given the null is true''), where a \(t\)-value is a difference
standardized by its standard error. Standardizing by a standard
deviation (remember that a standard error is an estimate of the
statistic's standard deviation) removes the effect of sample variability
on the distribution. A \(t\)-distribution, then, is only a function of
sample size (or ``degrees of freedom''). As \(n\) increases a \(t\)
distribution becomes converges on the standard, normal distribution.

The difference between the mean of the vitamin\_E treatment and the
control treatment is -89.9. A two-sample \(t\)-test with equal variance
of this difference is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ttest.E <-}\StringTok{ }\KeywordTok{t.test}\NormalTok{(}\DataTypeTok{x=}\NormalTok{vole[treatment}\OperatorTok{==}\StringTok{'vitamin_E'}\NormalTok{, lifespan], }\DataTypeTok{y=}\NormalTok{vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan], }\DataTypeTok{var.equal=}\OtherTok{TRUE}\NormalTok{)}
\NormalTok{ttest.E}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  vole[treatment == "vitamin_E", lifespan] and vole[treatment == "control", lifespan]
## t = -1.6275, df = 75, p-value = 0.1078
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -199.97362   20.14029
## sample estimates:
## mean of x mean of y 
##  413.4762  503.3929
\end{verbatim}

The \(p\)-value comes from comparing the observed \(t\) to the \(t\)
distribution and ``counting'' the values that are bigger than the
observed \(t\) (these are counted in both tails). We can simulate this
with a finite, instead of infinite, null distribution using the
t-distribution instead of the distribution of mean differences, as
above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seed <-}\StringTok{ }\DecValTok{1}
\NormalTok{n_iter <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5} \CommentTok{# number of iterations}
\NormalTok{mu <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan]) }
\NormalTok{sigma <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{, lifespan])}
\NormalTok{n <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{((vole[treatment}\OperatorTok{==}\StringTok{'control'}\NormalTok{,]))}
\NormalTok{sample1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples (each size n)}
\NormalTok{sample2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rnorm}\NormalTok{(n}\OperatorTok{*}\NormalTok{n_iter, }\DataTypeTok{mean=}\NormalTok{mu, }\DataTypeTok{sd=}\NormalTok{sigma), }\DataTypeTok{nrow=}\NormalTok{n) }\CommentTok{# 100,000 samples}
\NormalTok{mean_diffs <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, mean) }\OperatorTok{-}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample1, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{se_mean_diffs <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, sd)}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{n }\OperatorTok{+}\StringTok{ }\KeywordTok{apply}\NormalTok{(sample2, }\DecValTok{2}\NormalTok{, sd)}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\NormalTok{n)}
\NormalTok{t_dis <-}\StringTok{ }\NormalTok{mean_diffs}\OperatorTok{/}\NormalTok{se_mean_diffs}
\KeywordTok{qplot}\NormalTok{(t_dis)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Walker-elementary-statistical-modeling-draft_files/figure-latex/null-distribution-t-1.pdf}

Hey that looks pretty good! Compare the observed \(t\) (`r
round(ttest.E\$statistic, 1)) to this distribution. What is the
probability of observing this value given this distribution?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p_vitamin_E <-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\KeywordTok{abs}\NormalTok{(t_dis) }\OperatorTok{>=}\StringTok{ }\KeywordTok{abs}\NormalTok{(ttest.E}\OperatorTok{$}\NormalTok{statistic)) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{/}\NormalTok{(n_iter }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
\NormalTok{p_vitamin_E}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1090389
\end{verbatim}

This value is very (very!) close to that computed from the observed
\(t\)-test.

\section{Statistical modeling instead of hypothesis
testing}\label{statistical-modeling-instead-of-hypothesis-testing}

This chapter is an introduction to a \(p\)-value by way of \(t\)-tests.
I advocate that you analyze \(t\)-test like questions using statistical
modeling instead of null hypothesis significance testing. The reason is
that we learn much more from an estimate of the effect and a CI than
from a \(t\) and \(p\)-value. But, it is also good to know that a \(t\)
test is a special case of a linear model, and you can get that \(t\) and
\(p\) using a statistical modeling approach should your boss want them
(and you cannot convince them otherwise). Let's explore this.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the emmeans package, compute the effects (differences in means)
  of vitamin E and vitamin C on lifespan, relative to the control, with
  their 95\% CI and the \(t\) and \(p\) values for the cold-reared vole
  data.
\item
  Compute a separate \(t\)-test of vitamin-E vs.~control and vitamin C
  vs.~control.
\end{enumerate}

Are the \(t\) and \(p\) values the same? No! The reason is that the
statistical model had three groups and the SE of the difference was
computed from the sample standard deviation of all three groups. Each
t-test computes the SE of the difference from only the two groups being
compared. In general, the SE computed from all three groups is better
because it uses more information. This is one reason to prefer the
linear model instead of the separate t-tests.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  To convince yourself that a \(t\)-test is a special case as of a
  linear model, compute the effects of the vitamin E treatment (relative
  to control) \textbf{but exclude the vitamin C data from the model
  fit}. Now compare the \(t\) and \(p\) values with the \(t\)-test.
  These should be the same.
\item
  Now use the default t.test function by deleting ``var.equal=TRUE''
  from the function. Are \(t\) and \(p\) still equal to those from the
  statistical model? No! the reason is because the default t.test
  function uses a modification of the t-test called ``Welsch's t-test''.
  This test allows for heterogenity of variances. Several sources argue
  that one should always uses Welsch's test since it simplifies to the
  classical t-test when the sample variances are equal. This is true,
  but only relevant if you're into \(t\)-tests. And, we can model
  heterogenous variances using a statistical model. We'll do this in a
  later chapter.
\item
  Use the function \texttt{pairwise.t.test} to compute all pairwise
  t.tests among the three treatment levels. Is the \(p\)-value for the
  vitamin\_E - control contrast the same as that if using t.test (with
  var.equal=TRUE) or the statistical model with vitamin\_C data
  excluded? No! The reason is that pairwise.t.test adjusts the p-values
  for multiple testing as a default.
\end{enumerate}

Pro tip: Before you use a new R function like t.test or pairwise.t.test,
it is really advisable to read the help page and look at the defaults
for the parameters! Researchers publish errors because they failed to
look closely at what the R function was doing and they think the
function is doing something else. Ooops!

\chapter*{Appendix 1: Getting Started with
R}\label{appendix-1-getting-started-with-r}
\addcontentsline{toc}{chapter}{Appendix 1: Getting Started with R}

\section{Get your computer ready}\label{get-your-computer-ready}

\subsection{Install R}\label{install-r}

R is the core software

\href{https://cran.r-project.org}{Download R for your OS}

\subsection{Install R Studio}\label{install-r-studio}

R Studio is a slick (very slick) GUI interface for developing R projects

\href{https://www.rstudio.com/products/rstudio/download/}{Download R
Studio Desktop}

\subsection{Resources for installing R and R
Studio}\label{resources-for-installing-r-and-r-studio}

\href{https://medium.com/@GalarnykMichael/install-r-and-rstudio-on-windows-5f503f708027}{On
Windows}

\href{https://medium.com/@GalarnykMichael/install-r-and-rstudio-on-mac-e911606ce4f4}{On
a Mac}

\subsection{Install LaTeX}\label{install-latex}

LaTeX (``la-tek'') is necessary to use the pdf output of R Markdown.

\href{https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-windows-10-952b0c48bfa9}{On
Windows}

\href{https://medium.com/@sorenlind/create-pdf-reports-using-r-r-markdown-latex-and-knitr-on-macos-high-sierra-e7b5705c9fd}{On
a Mac}

\section{Start learning}\label{start-learning}

\subsection{Start with Data Camp Introduction to
R}\label{start-with-data-camp-introduction-to-r}

\href{https://www.datacamp.com/courses/free-introduction-to-r}{Data
Camp: Introduction to R (free online course)}

\subsection{Then Move to Introduction to R
Studio}\label{then-move-to-introduction-to-r-studio}

\href{https://www.rstudio.com/resources/webinars/rstudio-essentials-webinar-series-part-1/}{R
Studio Essentials, Programming Part 1 (Writing code in RStudio)}

\subsection{Develop your project with an R Studio
Notebook}\label{develop-your-project-with-an-r-studio-notebook}

\href{https://www.rstudio.com/resources/webinars/getting-started-with-r-markdown/}{Getting
Started with R Markdown}

\href{https://www.rstudio.com/resources/webinars/introducing-notebooks-with-r-markdown/}{Introducing
Notebooks with R Markdown}

\section{Getting Data into R}\label{getting-data-into-r}

\href{https://www.rstudio.com/resources/webinars/getting-your-data-into-r/}{Getting
your data into R}

\section{Additional R learning
resources}\label{additional-r-learning-resources}

\href{https://bookdown.org/chesterismay/rbasics/}{Getting used to R,
RStudio, and R Markdown}

\href{https://www.rstudio.com/resources/webinars/}{Link to list of R
Studio webinars}

\href{https://www.rstudio.com/resources/cheatsheets/}{Link to set of R
package cheat sheets (amazing!)}

\href{https://bookdown.org}{Bookdown online books}

\section{Packages used extensively in this
text}\label{packages-used-extensively-in-this-text}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ggplot2
\item
  data.table
\item
  mvtnorm
\item
  lme4
\item
  nlme
\item
  emmeans
\item
  readxl
\item
  reshape2
\end{enumerate}

\href{http://r4ds.had.co.nz/data-visualisation.html}{Data Visualisation
chapter from \emph{R for Data Science}}

\href{http://r4ds.had.co.nz/graphics-for-communication.html}{Graphics
for communication chapter from \emph{R for Data Science}}

Youtube: \href{https://www.youtube.com/watch?v=pc1ARG6kbAM}{An
Introduction to The data.table Package}

Coursera:
\href{https://www.coursera.org/learn/data-cleaning/lecture/trMZ7/the-data-table-package}{The
data.table Package}

\chapter*{Appendix 2: Online Resources for Getting Started with Linear
Modeling in
R}\label{appendix-2-online-resources-for-getting-started-with-linear-modeling-in-r}
\addcontentsline{toc}{chapter}{Appendix 2: Online Resources for Getting
Started with Linear Modeling in R}

\href{https://leanpub.com/regmods}{Regression Models for Data Science in
R by Brian Caffo}

\href{https://bookdown.org/roback/bookdown-bysh/}{Broadening Your
Statistical Horizons: Generalized Linear Models and Multilevel Models by
J. Legler and P. Roback}

\href{https://bookdown.org/rdpeng/artofdatascience/}{The Art of Data
Science by Roger D. Peng and Elizabeth Matsui}

\bibliography{book.bib,packages.bib}


\end{document}
