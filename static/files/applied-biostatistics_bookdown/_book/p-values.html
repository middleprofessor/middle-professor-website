<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 P-values | Applied Statistics for Experimental Biology</title>
  <meta name="description" content="An introduction to statistical modeling for experimental biology researchers" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 P-values | Applied Statistics for Experimental Biology" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introduction to statistical modeling for experimental biology researchers" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 P-values | Applied Statistics for Experimental Biology" />
  
  <meta name="twitter:description" content="An introduction to statistical modeling for experimental biology researchers" />
  

<meta name="author" content="Copyright 2018 Jeffrey A. Walker" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="uncertainty.html"/>
<link rel="next" href="errors-in-inference.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    background-color: #ffffff;
    color: #a0a0a0;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #a0a0a0;  padding-left: 4px; }
div.sourceCode
  { color: #1f1c1b; background-color: #ffffff; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #1f1c1b; } /* Normal */
code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
code span.an { color: #ca60ca; } /* Annotation */
code span.at { color: #0057ae; } /* Attribute */
code span.bn { color: #b08000; } /* BaseN */
code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
code span.ch { color: #924c9d; } /* Char */
code span.cn { color: #aa5500; } /* Constant */
code span.co { color: #898887; } /* Comment */
code span.cv { color: #0095ff; } /* CommentVar */
code span.do { color: #607880; } /* Documentation */
code span.dt { color: #0057ae; } /* DataType */
code span.dv { color: #b08000; } /* DecVal */
code span.er { color: #bf0303; text-decoration: underline; } /* Error */
code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
code span.fl { color: #b08000; } /* Float */
code span.fu { color: #644a9b; } /* Function */
code span.im { color: #ff5500; } /* Import */
code span.in { color: #b08000; } /* Information */
code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
code span.op { color: #1f1c1b; } /* Operator */
code span.ot { color: #006e28; } /* Other */
code span.pp { color: #006e28; } /* Preprocessor */
code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
code span.sc { color: #3daee9; } /* SpecialChar */
code span.ss { color: #ff5500; } /* SpecialString */
code span.st { color: #bf0303; } /* String */
code span.va { color: #0057ae; } /* Variable */
code span.vs { color: #bf0303; } /* VerbatimString */
code span.wa { color: #bf0303; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Elements of Applied Biostatistics</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="index.html#why-bother-with-linear-models-arent-t-tests-and-anova-good-enough" id="toc-why-bother-with-linear-models-arent-t-tests-and-anova-good-enough">Why bother with linear models – aren’t <em>t</em>-tests and ANOVA good enough?</a></li>
<li><a href="index.html#what-is-unusual-about-this-book" id="toc-what-is-unusual-about-this-book">What is unusual about this book?</a></li>
</ul></li>
<li><a href="a-table-of-models.html#a-table-of-models" id="toc-a-table-of-models">A Table of Models</a>
<ul>
<li><a href="a-table-of-models.html#including-mapping-between-linear-models-and-classical-tests" id="toc-including-mapping-between-linear-models-and-classical-tests">including mapping between linear models and classical tests</a></li>
</ul></li>
<li><a href="ask1-intro.html#ask1-intro" id="toc-ask1-intro"><span class="toc-section-number">1</span> Analyzing experimental data with a linear model</a>
<ul>
<li><a href="ask1-intro.html#this-text-is-about-using-linear-models-to-estimate-treatment-effects-and-the-uncertainty-in-our-estimates.-this-raises-the-question-what-is-an-effect" id="toc-this-text-is-about-using-linear-models-to-estimate-treatment-effects-and-the-uncertainty-in-our-estimates.-this-raises-the-question-what-is-an-effect"><span class="toc-section-number">1.1</span> This text is about using linear models to estimate treatment effects and the uncertainty in our estimates. This, raises the question, what is “an effect”?</a></li>
</ul></li>
<li><a href="part-i-getting-started.html#part-i-getting-started" id="toc-part-i-getting-started">Part I: Getting Started</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#getting-started-r-projects-and-r-markdown" id="toc-getting-started-r-projects-and-r-markdown"><span class="toc-section-number">2</span> Getting Started – R Projects and R Markdown</a>
<ul>
<li><a href="getting-started-r-projects-and-r-markdown.html#r-vs-r-studio" id="toc-r-vs-r-studio"><span class="toc-section-number">2.1</span> R vs R Studio</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#download-and-install-r-and-r-studio" id="toc-download-and-install-r-and-r-studio"><span class="toc-section-number">2.2</span> Download and install R and R studio</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#open-r-studio-and-modify-the-workspace-preference" id="toc-open-r-studio-and-modify-the-workspace-preference"><span class="toc-section-number">2.3</span> Open R Studio and modify the workspace preference</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#if-you-didnt-modify-the-workspace-preferences-from-the-previous-section-go-back-and-do-it" id="toc-if-you-didnt-modify-the-workspace-preferences-from-the-previous-section-go-back-and-do-it"><span class="toc-section-number">2.4</span> If you didn’t modify the workspace preferences from the previous section, go back and do it</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#r-markdown-in-a-nutshell" id="toc-r-markdown-in-a-nutshell"><span class="toc-section-number">2.5</span> R Markdown in a nutshell</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#install-r-markdown" id="toc-install-r-markdown"><span class="toc-section-number">2.6</span> Install R Markdown</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#importing-packages" id="toc-importing-packages"><span class="toc-section-number">2.7</span> Importing Packages</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#setup-create-project" id="toc-setup-create-project"><span class="toc-section-number">2.8</span> Create an R Studio Project for this textbook</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#working-on-a-project-in-a-nutshell" id="toc-working-on-a-project-in-a-nutshell"><span class="toc-section-number">2.9</span> Working on a project, in a nutshell</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#setup-create-rmd" id="toc-setup-create-rmd"><span class="toc-section-number">2.10</span> Create and setup an R Markdown document (Rmd)</a>
<ul>
<li><a href="getting-started-r-projects-and-r-markdown.html#modify-the-yaml-header" id="toc-modify-the-yaml-header"><span class="toc-section-number">2.10.1</span> Modify the yaml header</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#modify-the-setup-chunk" id="toc-modify-the-setup-chunk"><span class="toc-section-number">2.10.2</span> Modify the “setup” chunk</a></li>
</ul></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#lets-play-around-with-an-r-markdown-file" id="toc-lets-play-around-with-an-r-markdown-file"><span class="toc-section-number">2.11</span> Let’s play around with an R Markdown file</a>
<ul>
<li><a href="getting-started-r-projects-and-r-markdown.html#create-a-fake-data-chunk" id="toc-create-a-fake-data-chunk"><span class="toc-section-number">2.11.1</span> Create a “fake-data” chunk</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#create-a-plot-chunk" id="toc-create-a-plot-chunk"><span class="toc-section-number">2.11.2</span> Create a “plot” chunk</a></li>
<li><a href="getting-started-r-projects-and-r-markdown.html#knit-the-rmd" id="toc-knit-the-rmd"><span class="toc-section-number">2.11.3</span> Knit the Rmd</a></li>
</ul></li>
</ul></li>
<li><a href="part-ii-r-fundamentals.html#part-ii-r-fundamentals" id="toc-part-ii-r-fundamentals">Part II: R fundamentals</a></li>
<li><a href="data.html#data" id="toc-data"><span class="toc-section-number">3</span> Data – Reading, Wrangling, and Writing</a>
<ul>
<li><a href="data.html#data-long" id="toc-data-long"><span class="toc-section-number">3.1</span> Long data format – the way data should be</a></li>
<li><a href="data.html#data-here" id="toc-data-here"><span class="toc-section-number">3.2</span> Use the here function to construct the file path</a></li>
<li><a href="data.html#learning-from-this-chapter" id="toc-learning-from-this-chapter"><span class="toc-section-number">3.3</span> Learning from this chapter</a></li>
<li><a href="data.html#data-working-in-r" id="toc-data-working-in-r"><span class="toc-section-number">3.4</span> Working in R</a>
<ul>
<li><a href="data.html#importing-data" id="toc-importing-data"><span class="toc-section-number">3.4.1</span> Importing data</a></li>
</ul></li>
<li><a href="data.html#data-wrangling" id="toc-data-wrangling"><span class="toc-section-number">3.5</span> Data wrangling</a>
<ul>
<li><a href="data.html#reshaping-data-wide-to-long" id="toc-reshaping-data-wide-to-long"><span class="toc-section-number">3.5.1</span> Reshaping data – Wide to long</a></li>
<li><a href="data.html#reshaping-data-transpose-turning-the-columns-into-rows" id="toc-reshaping-data-transpose-turning-the-columns-into-rows"><span class="toc-section-number">3.5.2</span> Reshaping data – Transpose (turning the columns into rows)</a></li>
<li><a href="data.html#combining-data" id="toc-combining-data"><span class="toc-section-number">3.5.3</span> Combining data</a></li>
<li><a href="data.html#data-subset" id="toc-data-subset"><span class="toc-section-number">3.5.4</span> Subsetting data</a></li>
<li><a href="data.html#wrangling-columns" id="toc-wrangling-columns"><span class="toc-section-number">3.5.5</span> Wrangling columns</a></li>
<li><a href="data.html#missing-data" id="toc-missing-data"><span class="toc-section-number">3.5.6</span> Missing data</a></li>
</ul></li>
<li><a href="data.html#saving-data" id="toc-saving-data"><span class="toc-section-number">3.6</span> Saving data</a></li>
<li><a href="data.html#exercises" id="toc-exercises"><span class="toc-section-number">3.7</span> Exercises</a></li>
</ul></li>
<li><a href="plotting-models.html#plotting-models" id="toc-plotting-models"><span class="toc-section-number">4</span> Plotting Models</a>
<ul>
<li><a href="plotting-models.html#pretty-good-plots-show-the-model-and-the-data" id="toc-pretty-good-plots-show-the-model-and-the-data"><span class="toc-section-number">4.1</span> Pretty good plots show the model and the data</a>
<ul>
<li><a href="plotting-models.html#pretty-good-plot-component-1-modeled-effects-plot" id="toc-pretty-good-plot-component-1-modeled-effects-plot"><span class="toc-section-number">4.1.1</span> Pretty good plot component 1: Modeled effects plot</a></li>
<li><a href="plotting-models.html#pretty-good-plot-component-2-modeled-mean-and-ci-plot" id="toc-pretty-good-plot-component-2-modeled-mean-and-ci-plot"><span class="toc-section-number">4.1.2</span> Pretty good plot component 2: Modeled mean and CI plot</a></li>
<li><a href="plotting-models.html#combining-effects-and-modeled-mean-and-ci-plots-an-effects-and-response-plot." id="toc-combining-effects-and-modeled-mean-and-ci-plots-an-effects-and-response-plot."><span class="toc-section-number">4.1.3</span> Combining Effects and Modeled mean and CI plots – an Effects and response plot.</a></li>
<li><a href="plotting-models.html#some-comments-on-plot-components" id="toc-some-comments-on-plot-components"><span class="toc-section-number">4.1.4</span> Some comments on plot components</a></li>
</ul></li>
<li><a href="plotting-models.html#working-in-r" id="toc-working-in-r"><span class="toc-section-number">4.2</span> Working in R</a>
<ul>
<li><a href="plotting-models.html#source-data" id="toc-source-data"><span class="toc-section-number">4.2.1</span> Source data</a></li>
<li><a href="plotting-models.html#how-to-plot-the-model" id="toc-how-to-plot-the-model"><span class="toc-section-number">4.2.2</span> How to plot the model</a></li>
<li><a href="plotting-models.html#be-sure-ggplot_the_model-is-in-your-r-folder" id="toc-be-sure-ggplot_the_model-is-in-your-r-folder"><span class="toc-section-number">4.2.3</span> Be sure ggplot_the_model is in your R folder</a></li>
<li><a href="plotting-models.html#how-to-use-the-plot-the-model-functions" id="toc-how-to-use-the-plot-the-model-functions"><span class="toc-section-number">4.2.4</span> How to use the Plot the Model functions</a></li>
<li><a href="plotting-models.html#how-to-generate-a-response-plot-using-ggpubr" id="toc-how-to-generate-a-response-plot-using-ggpubr"><span class="toc-section-number">4.2.5</span> How to generate a Response Plot using ggpubr</a></li>
<li><a href="plotting-models.html#how-to-generate-a-response-plot-with-a-grid-of-treatments-using-ggplot2" id="toc-how-to-generate-a-response-plot-with-a-grid-of-treatments-using-ggplot2"><span class="toc-section-number">4.2.6</span> How to generate a Response Plot with a grid of treatments using ggplot2</a></li>
<li><a href="plotting-models.html#how-to-generate-an-effects-plot" id="toc-how-to-generate-an-effects-plot"><span class="toc-section-number">4.2.7</span> How to generate an Effects Plot</a></li>
<li><a href="plotting-models.html#how-to-combine-the-response-and-effects-plots" id="toc-how-to-combine-the-response-and-effects-plots"><span class="toc-section-number">4.2.8</span> How to combine the response and effects plots</a></li>
<li><a href="plotting-models.html#how-to-add-the-interaction-effect-to-response-and-effects-plots" id="toc-how-to-add-the-interaction-effect-to-response-and-effects-plots"><span class="toc-section-number">4.2.9</span> How to add the interaction effect to response and effects plots</a></li>
</ul></li>
</ul></li>
<li><a href="part-iii-some-fundamentals-of-statistical-modeling.html#part-iii-some-fundamentals-of-statistical-modeling" id="toc-part-iii-some-fundamentals-of-statistical-modeling">Part III: Some Fundamentals of Statistical Modeling</a></li>
<li><a href="uncertainty.html#uncertainty" id="toc-uncertainty"><span class="toc-section-number">5</span> Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)</a>
<ul>
<li><a href="uncertainty.html#standard-errors-are-used-to-compute-p-values-and-confidence-intervals" id="toc-standard-errors-are-used-to-compute-p-values-and-confidence-intervals"><span class="toc-section-number">5.1</span> Standard errors are used to compute <em>p</em>-values and confidence intervals</a></li>
<li><a href="uncertainty.html#background" id="toc-background"><span class="toc-section-number">5.2</span> Background</a>
<ul>
<li><a href="uncertainty.html#sample-standard-deviation" id="toc-sample-standard-deviation"><span class="toc-section-number">5.2.1</span> Sample standard deviation</a></li>
<li><a href="uncertainty.html#standard-error-of-the-mean" id="toc-standard-error-of-the-mean"><span class="toc-section-number">5.2.2</span> Standard error of the mean</a></li>
</ul></li>
<li><a href="uncertainty.html#simulations-using-fake-data-as-an-intuition-pump" id="toc-simulations-using-fake-data-as-an-intuition-pump"><span class="toc-section-number">5.3</span> Simulations – using fake data as an intuition pump</a>
<ul>
<li><a href="uncertainty.html#using-google-sheets-to-generate-fake-data-to-explore-the-standard-error" id="toc-using-google-sheets-to-generate-fake-data-to-explore-the-standard-error"><span class="toc-section-number">5.3.1</span> Using Google Sheets to generate fake data to explore the standard error</a></li>
<li><a href="uncertainty.html#using-r-to-generate-fake-data-to-explore-the-standard-error" id="toc-using-r-to-generate-fake-data-to-explore-the-standard-error"><span class="toc-section-number">5.3.2</span> Using R to generate fake data to explore the standard error</a></li>
</ul></li>
<li><a href="uncertainty.html#bootstrap" id="toc-bootstrap"><span class="toc-section-number">5.4</span> Bootstrapped standard errors</a>
<ul>
<li><a href="uncertainty.html#an-example-of-bootstrapped-standard-errors" id="toc-an-example-of-bootstrapped-standard-errors"><span class="toc-section-number">5.4.1</span> An example of bootstrapped standard errors</a></li>
</ul></li>
<li><a href="uncertainty.html#confidence-intervals" id="toc-confidence-intervals"><span class="toc-section-number">5.5</span> Confidence Intervals</a>
<ul>
<li><a href="uncertainty.html#just-the-math" id="toc-just-the-math"><span class="toc-section-number">5.5.1</span> Just the math</a></li>
<li><a href="uncertainty.html#interpretation-of-a-confidence-interval" id="toc-interpretation-of-a-confidence-interval"><span class="toc-section-number">5.5.2</span> Interpretation of a confidence interval</a></li>
<li><a href="uncertainty.html#bootstrap-confidence-interval" id="toc-bootstrap-confidence-interval"><span class="toc-section-number">5.5.3</span> Bootstrap confidence interval</a></li>
<li><a href="uncertainty.html#a-plot-of-a-parametric-ci-vs.-bootstrap-ci-of-the-means" id="toc-a-plot-of-a-parametric-ci-vs.-bootstrap-ci-of-the-means"><span class="toc-section-number">5.5.4</span> A plot of a “parametric” CI vs. bootstrap CI of the means</a></li>
</ul></li>
<li><a href="uncertainty.html#standard-error-of-a-difference-between-means" id="toc-standard-error-of-a-difference-between-means"><span class="toc-section-number">5.6</span> Standard error of a difference between means</a>
<ul>
<li><a href="uncertainty.html#the-standard-error-of-a-difference-between-means-is-the-standard-deviation-of-the-sampling-distribution-of-the-difference" id="toc-the-standard-error-of-a-difference-between-means-is-the-standard-deviation-of-the-sampling-distribution-of-the-difference"><span class="toc-section-number">5.6.1</span> The standard error of a difference between means is the standard deviation of the sampling distribution of the difference</a></li>
</ul></li>
<li><a href="uncertainty.html#confidence-limits-of-a-difference-between-means" id="toc-confidence-limits-of-a-difference-between-means"><span class="toc-section-number">5.7</span> Confidence limits of a difference between means</a>
<ul>
<li><a href="uncertainty.html#of-95-cis-of-the-difference-include-the-true-difference" id="toc-of-95-cis-of-the-difference-include-the-true-difference"><span class="toc-section-number">5.7.1</span> 95% of 95% CIs of the difference include the true difference</a></li>
</ul></li>
<li><a href="uncertainty.html#hidden-code" id="toc-hidden-code"><span class="toc-section-number">5.8</span> Hidden code</a>
<ul>
<li><a href="uncertainty.html#import-exp3b" id="toc-import-exp3b"><span class="toc-section-number">5.8.1</span> Import exp3b</a></li>
</ul></li>
</ul></li>
<li><a href="p-values.html#p-values" id="toc-p-values"><span class="toc-section-number">6</span> P-values</a>
<ul>
<li><a href="p-values.html#a-p-value-is-the-probability-of-sampling-a-value-as-or-more-extreme-than-the-test-statistic-if-sampling-from-a-null-distribution" id="toc-a-p-value-is-the-probability-of-sampling-a-value-as-or-more-extreme-than-the-test-statistic-if-sampling-from-a-null-distribution"><span class="toc-section-number">6.1</span> A <em>p</em>-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution</a></li>
<li><a href="p-values.html#pump-your-intuition-creating-a-null-distribution" id="toc-pump-your-intuition-creating-a-null-distribution"><span class="toc-section-number">6.2</span> Pump your intuition – Creating a null distribution</a></li>
<li><a href="p-values.html#a-null-distribution-of-t-values-the-t-distribution" id="toc-a-null-distribution-of-t-values-the-t-distribution"><span class="toc-section-number">6.3</span> A null distribution of <em>t</em>-values – the <em>t</em> distribution</a></li>
<li><a href="p-values.html#p-values-from-the-perspective-of-permutation" id="toc-p-values-from-the-perspective-of-permutation"><span class="toc-section-number">6.4</span> P-values from the perspective of permutation</a></li>
<li><a href="p-values.html#parametric-vs.-non-parametric-statistics" id="toc-parametric-vs.-non-parametric-statistics"><span class="toc-section-number">6.5</span> Parametric vs. non-parametric statistics</a></li>
<li><a href="p-values.html#frequentist-probability-and-the-interpretation-of-p-values" id="toc-frequentist-probability-and-the-interpretation-of-p-values"><span class="toc-section-number">6.6</span> frequentist probability and the interpretation of p-values</a>
<ul>
<li><a href="p-values.html#background-1" id="toc-background-1"><span class="toc-section-number">6.6.1</span> Background</a></li>
<li><a href="p-values.html#this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability." id="toc-this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability."><span class="toc-section-number">6.6.2</span> This book covers frequentist approaches to statistical modeling and when a probability arises, such as the <em>p</em>-value of a test statistic, this will be a frequentist probability.</a></li>
<li><a href="p-values.html#two-interpretations-of-the-p-value" id="toc-two-interpretations-of-the-p-value"><span class="toc-section-number">6.6.3</span> Two interpretations of the <em>p</em>-value</a></li>
<li><a href="p-values.html#nhst" id="toc-nhst"><span class="toc-section-number">6.6.4</span> NHST</a></li>
</ul></li>
<li><a href="p-values.html#some-major-misconceptions-of-the-p-value" id="toc-some-major-misconceptions-of-the-p-value"><span class="toc-section-number">6.7</span> Some major misconceptions of the <em>p</em>-value</a>
<ul>
<li><a href="p-values.html#misconception-p-0.05-means-there-is-no-effect-of-treatment" id="toc-misconception-p-0.05-means-there-is-no-effect-of-treatment"><span class="toc-section-number">6.7.1</span> Misconception: <span class="math inline">\(p &gt; 0.05\)</span> means there is no effect of treatment</a></li>
<li><a href="p-values.html#misconception-a-p-value-is-repeatable" id="toc-misconception-a-p-value-is-repeatable"><span class="toc-section-number">6.7.2</span> Misconception: a <em>p</em>-value is repeatable</a></li>
<li><a href="p-values.html#misconception-0.05-is-the-lifetime-rate-of-false-discoveries" id="toc-misconception-0.05-is-the-lifetime-rate-of-false-discoveries"><span class="toc-section-number">6.7.3</span> Misconception: 0.05 is the lifetime rate of false discoveries</a></li>
<li><a href="p-values.html#misconception-a-low-p-value-indicates-an-important-effect" id="toc-misconception-a-low-p-value-indicates-an-important-effect"><span class="toc-section-number">6.7.4</span> Misconception: a low <em>p</em>-value indicates an important effect</a></li>
<li><a href="p-values.html#misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity" id="toc-misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity"><span class="toc-section-number">6.7.5</span> Misconception: a low <em>p</em>-value indicates high model fit or high predictive capacity</a></li>
</ul></li>
<li><a href="p-values.html#what-the-p-value-does-not-mean" id="toc-what-the-p-value-does-not-mean"><span class="toc-section-number">6.8</span> What the <em>p</em>-value does not mean</a></li>
<li><a href="p-values.html#on-using-p-values-in-experimental-biology" id="toc-on-using-p-values-in-experimental-biology"><span class="toc-section-number">6.9</span> On using <em>p</em>-values in experimental biology</a></li>
<li><a href="p-values.html#better-reproducibility" id="toc-better-reproducibility"><span class="toc-section-number">6.10</span> Better reproducibility</a></li>
<li><a href="p-values.html#multiple-testing-and-controlling-for-false-positives" id="toc-multiple-testing-and-controlling-for-false-positives"><span class="toc-section-number">6.11</span> Multiple testing and controlling for false positives</a>
<ul>
<li><a href="p-values.html#controlling-the-family-wise-error-rate-when-all-tests-are-testing-the-same-hypothesis" id="toc-controlling-the-family-wise-error-rate-when-all-tests-are-testing-the-same-hypothesis"><span class="toc-section-number">6.11.1</span> Controlling the family-wise error rate when all tests are testing the same hypothesis</a></li>
</ul></li>
<li><a href="p-values.html#recommendations" id="toc-recommendations"><span class="toc-section-number">6.12</span> Recommendations</a>
<ul>
<li><a href="p-values.html#primary-sources-for-recommendations" id="toc-primary-sources-for-recommendations"><span class="toc-section-number">6.12.1</span> Primary sources for recommendations</a></li>
</ul></li>
<li><a href="p-values.html#problems" id="toc-problems"><span class="toc-section-number">6.13</span> Problems</a></li>
</ul></li>
<li><a href="errors-in-inference.html#errors-in-inference" id="toc-errors-in-inference"><span class="toc-section-number">7</span> Errors in inference</a>
<ul>
<li><a href="errors-in-inference.html#classical-nhst-concepts-of-wrong" id="toc-classical-nhst-concepts-of-wrong"><span class="toc-section-number">7.1</span> Classical NHST concepts of wrong</a>
<ul>
<li><a href="errors-in-inference.html#type-i-error" id="toc-type-i-error"><span class="toc-section-number">7.1.1</span> Type I error</a></li>
<li><a href="errors-in-inference.html#power" id="toc-power"><span class="toc-section-number">7.1.2</span> Power</a></li>
</ul></li>
<li><a href="errors-in-inference.html#a-non-neyman-pearson-concept-of-power" id="toc-a-non-neyman-pearson-concept-of-power"><span class="toc-section-number">7.2</span> A non-Neyman-Pearson concept of power</a>
<ul>
<li><a href="errors-in-inference.html#estimation-error" id="toc-estimation-error"><span class="toc-section-number">7.2.1</span> Estimation error</a></li>
<li><a href="errors-in-inference.html#coverage" id="toc-coverage"><span class="toc-section-number">7.2.2</span> Coverage</a></li>
<li><a href="errors-in-inference.html#type-s-error" id="toc-type-s-error"><span class="toc-section-number">7.2.3</span> Type S error</a></li>
<li><a href="errors-in-inference.html#type-m-error" id="toc-type-m-error"><span class="toc-section-number">7.2.4</span> Type M error</a></li>
</ul></li>
</ul></li>
<li><a href="part-iv-introduction-to-linear-models.html#part-iv-introduction-to-linear-models" id="toc-part-iv-introduction-to-linear-models">Part IV: Introduction to Linear Models</a></li>
<li><a href="intro-linear-models.html#intro-linear-models" id="toc-intro-linear-models"><span class="toc-section-number">8</span> An introduction to linear models</a>
<ul>
<li><a href="intro-linear-models.html#lm-specifications" id="toc-lm-specifications"><span class="toc-section-number">8.1</span> Two specifications of a linear model</a>
<ul>
<li><a href="intro-linear-models.html#the-error-draw-specification" id="toc-the-error-draw-specification"><span class="toc-section-number">8.1.1</span> The “error draw” specification</a></li>
<li><a href="intro-linear-models.html#the-conditional-draw-specification" id="toc-the-conditional-draw-specification"><span class="toc-section-number">8.1.2</span> The “conditional draw” specification</a></li>
<li><a href="intro-linear-models.html#comparing-the-error-draw-and-conditional-draw-ways-of-specifying-the-linear-model" id="toc-comparing-the-error-draw-and-conditional-draw-ways-of-specifying-the-linear-model"><span class="toc-section-number">8.1.3</span> Comparing the error-draw and conditional-draw ways of specifying the linear model</a></li>
<li><a href="intro-linear-models.html#anova-notation-of-a-linear-model" id="toc-anova-notation-of-a-linear-model"><span class="toc-section-number">8.1.4</span> ANOVA notation of a linear model</a></li>
</ul></li>
<li><a href="intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables" id="toc-a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables"><span class="toc-section-number">8.2</span> A linear model can be fit to data with continuous, discrete, or categorical <span class="math inline">\(X\)</span> variables</a>
<ul>
<li><a href="intro-linear-models.html#fitting-linear-models-to-experimental-data-in-which-the-x-variable-is-continuous-or-discrete" id="toc-fitting-linear-models-to-experimental-data-in-which-the-x-variable-is-continuous-or-discrete"><span class="toc-section-number">8.2.1</span> Fitting linear models to experimental data in which the <span class="math inline">\(X\)</span> variable is continuous or discrete</a></li>
<li><a href="intro-linear-models.html#slope" id="toc-slope"><span class="toc-section-number">8.2.2</span> Fitting linear models to experimental data in which the <span class="math inline">\(X\)</span> variable is categorical</a></li>
</ul></li>
<li><a href="intro-linear-models.html#statistical-models-are-used-for-prediction-explanation-and-description" id="toc-statistical-models-are-used-for-prediction-explanation-and-description"><span class="toc-section-number">8.3</span> Statistical models are used for prediction, explanation, and description</a></li>
<li><a href="intro-linear-models.html#what-is-the-interpretation-of-a-regression-coefficient" id="toc-what-is-the-interpretation-of-a-regression-coefficient"><span class="toc-section-number">8.4</span> What is the interpretation of a regression coefficient?</a></li>
<li><a href="intro-linear-models.html#what-do-we-call-the-x-and-y-variables" id="toc-what-do-we-call-the-x-and-y-variables"><span class="toc-section-number">8.5</span> What do we call the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> variables?</a></li>
<li><a href="intro-linear-models.html#modeling-strategy" id="toc-modeling-strategy"><span class="toc-section-number">8.6</span> Modeling strategy</a></li>
<li><a href="intro-linear-models.html#predictions-from-the-model" id="toc-predictions-from-the-model"><span class="toc-section-number">8.7</span> Predictions from the model</a></li>
<li><a href="intro-linear-models.html#inference-from-the-model" id="toc-inference-from-the-model"><span class="toc-section-number">8.8</span> Inference from the model</a>
<ul>
<li><a href="intro-linear-models.html#assumptions-for-inference-with-a-statistical-model" id="toc-assumptions-for-inference-with-a-statistical-model"><span class="toc-section-number">8.8.1</span> Assumptions for inference with a statistical model</a></li>
<li><a href="intro-linear-models.html#specific-assumptions-for-inference-with-a-linear-model" id="toc-specific-assumptions-for-inference-with-a-linear-model"><span class="toc-section-number">8.8.2</span> Specific assumptions for inference with a linear model</a></li>
</ul></li>
<li><a href="intro-linear-models.html#linear-modelregression-model-or-statistical-model" id="toc-linear-modelregression-model-or-statistical-model"><span class="toc-section-number">8.9</span> “linear model,”regression model”, or “statistical model”?</a></li>
</ul></li>
<li><a href="regression.html#regression" id="toc-regression"><span class="toc-section-number">9</span> Linear models with a single, continuous <em>X</em> (“regression”)</a>
<ul>
<li><a href="regression.html#a-linear-model-with-a-single-continuous-x-is-classical-regression" id="toc-a-linear-model-with-a-single-continuous-x-is-classical-regression"><span class="toc-section-number">9.1</span> A linear model with a single, continuous <em>X</em> is classical “regression”</a>
<ul>
<li><a href="regression.html#analysis-of-green-down-data" id="toc-analysis-of-green-down-data"><span class="toc-section-number">9.1.1</span> Analysis of “green-down” data</a></li>
<li><a href="regression.html#learning-from-the-green-down-example" id="toc-learning-from-the-green-down-example"><span class="toc-section-number">9.1.2</span> Learning from the green-down example</a></li>
<li><a href="regression.html#using-a-regression-model-for-explanation-causal-models" id="toc-using-a-regression-model-for-explanation-causal-models"><span class="toc-section-number">9.1.3</span> Using a regression model for “explanation” – causal models</a></li>
<li><a href="regression.html#using-a-regression-model-for-prediction-prediction-models" id="toc-using-a-regression-model-for-prediction-prediction-models"><span class="toc-section-number">9.1.4</span> Using a regression model for prediction – prediction models</a></li>
<li><a href="regression.html#using-a-regression-model-for-creating-a-new-response-variable-comparing-slopes-of-longitudinal-data" id="toc-using-a-regression-model-for-creating-a-new-response-variable-comparing-slopes-of-longitudinal-data"><span class="toc-section-number">9.1.5</span> Using a regression model for creating a new response variable – comparing slopes of longitudinal data</a></li>
<li><a href="regression.html#using-a-regression-model-for-for-calibration" id="toc-using-a-regression-model-for-for-calibration"><span class="toc-section-number">9.1.6</span> Using a regression model for for calibration</a></li>
</ul></li>
<li><a href="regression.html#working-in-r-1" id="toc-working-in-r-1"><span class="toc-section-number">9.2</span> Working in R</a>
<ul>
<li><a href="plotting-models.html#fit-the-model" id="toc-fit-the-model"><span class="toc-section-number">9.2.1</span> Fitting the linear model</a></li>
<li><a href="regression.html#getting-to-know-the-linear-model-the-summary-function" id="toc-getting-to-know-the-linear-model-the-summary-function"><span class="toc-section-number">9.2.2</span> Getting to know the linear model: the <code>summary</code> function</a></li>
<li><a href="regression.html#inference-the-coefficient-table" id="toc-inference-the-coefficient-table"><span class="toc-section-number">9.2.3</span> Inference – the coefficient table</a></li>
<li><a href="regression.html#how-good-is-our-model-model-checking" id="toc-how-good-is-our-model-model-checking"><span class="toc-section-number">9.2.4</span> How good is our model? – Model checking</a></li>
<li><a href="regression.html#plotting-models-with-continuous-x" id="toc-plotting-models-with-continuous-x"><span class="toc-section-number">9.2.5</span> Plotting models with continuous <em>X</em></a></li>
<li><a href="regression.html#creating-a-table-of-predicted-values-and-95-prediction-intervals" id="toc-creating-a-table-of-predicted-values-and-95-prediction-intervals"><span class="toc-section-number">9.2.6</span> Creating a table of predicted values and 95% prediction intervals</a></li>
</ul></li>
<li><a href="regression.html#hidden-code-1" id="toc-hidden-code-1"><span class="toc-section-number">9.3</span> Hidden code</a>
<ul>
<li><a href="regression.html#import-and-plot-of-fig2c-ecosystem-warming-experimental-data" id="toc-import-and-plot-of-fig2c-ecosystem-warming-experimental-data"><span class="toc-section-number">9.3.1</span> Import and plot of fig2c (ecosystem warming experimental) data</a></li>
<li><a href="regression.html#import-and-plot-efig_3d-ecosysem-warming-observational-data" id="toc-import-and-plot-efig_3d-ecosysem-warming-observational-data"><span class="toc-section-number">9.3.2</span> Import and plot efig_3d (Ecosysem warming observational) data</a></li>
<li><a href="regression.html#import-and-plot-of-fig1f-methionine-restriction-data" id="toc-import-and-plot-of-fig1f-methionine-restriction-data"><span class="toc-section-number">9.3.3</span> Import and plot of fig1f (methionine restriction) data</a></li>
</ul></li>
<li><a href="regression.html#try-it" id="toc-try-it"><span class="toc-section-number">9.4</span> Try it</a>
<ul>
<li><a href="regression.html#a-prediction-model-from-the-literature" id="toc-a-prediction-model-from-the-literature"><span class="toc-section-number">9.4.1</span> A prediction model from the literature</a></li>
</ul></li>
<li><a href="regression.html#intuition-pumps" id="toc-intuition-pumps"><span class="toc-section-number">9.5</span> Intuition pumps</a>
<ul>
<li><a href="regression.html#correlation-and-r2" id="toc-correlation-and-r2"><span class="toc-section-number">9.5.1</span> Correlation and $R^2</a></li>
</ul></li>
</ul></li>
<li><a href="oneway.html#oneway" id="toc-oneway"><span class="toc-section-number">10</span> Linear models with a single, categorical <em>X</em> (“t-tests” and “ANOVA”)</a>
<ul>
<li><a href="oneway.html#oneway-data" id="toc-oneway-data"><span class="toc-section-number">10.1</span> A linear model with a single, categorical <em>X</em> variable estimates the effects of the levels of <em>X</em> on the response</a>
<ul>
<li><a href="oneway.html#oneway-example1" id="toc-oneway-example1"><span class="toc-section-number">10.1.1</span> Example 1 (fig3d) – two treatment levels (“groups”)</a></li>
<li><a href="oneway.html#understanding-the-analysis-with-two-treatment-levels" id="toc-understanding-the-analysis-with-two-treatment-levels"><span class="toc-section-number">10.1.2</span> Understanding the analysis with two treatment levels</a></li>
<li><a href="oneway.html#oneway-example2" id="toc-oneway-example2"><span class="toc-section-number">10.1.3</span> Example 2 – three treatment levels (“groups”)</a></li>
<li><a href="oneway.html#understanding-the-analysis-with-three-or-more-treatment-levels" id="toc-understanding-the-analysis-with-three-or-more-treatment-levels"><span class="toc-section-number">10.1.4</span> Understanding the analysis with three (or more) treatment levels</a></li>
</ul></li>
<li><a href="oneway.html#working-in-r-2" id="toc-working-in-r-2"><span class="toc-section-number">10.2</span> Working in R</a>
<ul>
<li><a href="oneway.html#fit-the-model-1" id="toc-fit-the-model-1"><span class="toc-section-number">10.2.1</span> Fit the model</a></li>
<li><a href="oneway.html#controlling-the-output-in-tables-using-the-coefficient-table-as-an-example" id="toc-controlling-the-output-in-tables-using-the-coefficient-table-as-an-example"><span class="toc-section-number">10.2.2</span> Controlling the output in tables using the coefficient table as an example</a></li>
<li><a href="oneway.html#using-the-emmeans-function" id="toc-using-the-emmeans-function"><span class="toc-section-number">10.2.3</span> Using the emmeans function</a></li>
<li><a href="oneway.html#using-the-contrast-function" id="toc-using-the-contrast-function"><span class="toc-section-number">10.2.4</span> Using the contrast function</a></li>
<li><a href="oneway.html#how-to-generate-anova-tables" id="toc-how-to-generate-anova-tables"><span class="toc-section-number">10.2.5</span> How to generate ANOVA tables</a></li>
</ul></li>
<li><a href="oneway.html#hidden-code-2" id="toc-hidden-code-2"><span class="toc-section-number">10.3</span> Hidden Code</a>
<ul>
<li><a href="oneway.html#importing-and-wrangling-the-fig_3d-data-for-example-1" id="toc-importing-and-wrangling-the-fig_3d-data-for-example-1"><span class="toc-section-number">10.3.1</span> Importing and wrangling the fig_3d data for example 1</a></li>
<li><a href="oneway.html#importing-and-wrangling-the-fig2a-data-for-example-2" id="toc-importing-and-wrangling-the-fig2a-data-for-example-2"><span class="toc-section-number">10.3.2</span> Importing and wrangling the fig2a data for example 2</a></li>
</ul></li>
</ul></li>
<li><a href="model-checking.html#model-checking" id="toc-model-checking"><span class="toc-section-number">11</span> Model Checking</a>
<ul>
<li><a href="model-checking.html#check-check" id="toc-check-check"><span class="toc-section-number">11.1</span> All statistical analyses should be followed by model checking</a></li>
<li><a href="model-checking.html#linear-model-assumptions" id="toc-linear-model-assumptions"><span class="toc-section-number">11.2</span> Linear model assumptions</a>
<ul>
<li><a href="model-checking.html#a-bit-about-iid" id="toc-a-bit-about-iid"><span class="toc-section-number">11.2.1</span> A bit about IID</a></li>
</ul></li>
<li><a href="model-checking.html#diagnostic-plots-use-the-residuals-from-the-model-fit" id="toc-diagnostic-plots-use-the-residuals-from-the-model-fit"><span class="toc-section-number">11.3</span> Diagnostic plots use the residuals from the model fit</a>
<ul>
<li><a href="model-checking.html#residuals" id="toc-residuals"><span class="toc-section-number">11.3.1</span> Residuals</a></li>
<li><a href="model-checking.html#normal-qq" id="toc-normal-qq"><span class="toc-section-number">11.3.2</span> A Normal Q-Q plot is used to check for characteristic departures from Normality</a></li>
<li><a href="model-checking.html#mapping-qq-plot-departures-from-normality" id="toc-mapping-qq-plot-departures-from-normality"><span class="toc-section-number">11.3.3</span> Mapping QQ-plot departures from Normality</a></li>
<li><a href="model-checking.html#model-checking-spread-level" id="toc-model-checking-spread-level"><span class="toc-section-number">11.3.4</span> Model checking homoskedasticity</a></li>
</ul></li>
<li><a href="model-checking.html#using-r" id="toc-using-r"><span class="toc-section-number">11.4</span> Using R</a></li>
<li><a href="model-checking.html#hidden-code-3" id="toc-hidden-code-3"><span class="toc-section-number">11.5</span> Hidden Code</a>
<ul>
<li><a href="model-checking.html#normal-q-q-plots" id="toc-normal-q-q-plots"><span class="toc-section-number">11.5.1</span> Normal Q-Q plots</a></li>
</ul></li>
</ul></li>
<li><a href="violations.html#violations" id="toc-violations"><span class="toc-section-number">12</span> Violations of independence, homogeneity, or Normality</a>
<ul>
<li><a href="violations.html#oneway-paired-t" id="toc-oneway-paired-t"><span class="toc-section-number">12.1</span> Lack of independence</a>
<ul>
<li><a href="violations.html#violations-paired-t" id="toc-violations-paired-t"><span class="toc-section-number">12.1.1</span> Example 1 (exp1b) – a paired t-test is a special case of a linear mixed model</a></li>
<li><a href="violations.html#violations-rmanova" id="toc-violations-rmanova"><span class="toc-section-number">12.1.2</span> Example 2 (diHOME exp2a) – A repeated measures ANOVA is a special case of a linear mixed model</a></li>
</ul></li>
<li><a href="violations.html#oneway-welch" id="toc-oneway-welch"><span class="toc-section-number">12.2</span> Heterogeneity of variances</a>
<ul>
<li><a href="violations.html#when-groups-of-the-focal-test-have-variance" id="toc-when-groups-of-the-focal-test-have-variance"><span class="toc-section-number">12.2.1</span> When groups of the focal test have &gt;&gt; variance</a></li>
</ul></li>
<li><a href="violations.html#the-conditional-response-isnt-normal" id="toc-the-conditional-response-isnt-normal"><span class="toc-section-number">12.3</span> The conditional response isn’t Normal</a>
<ul>
<li><a href="violations.html#violations-6f" id="toc-violations-6f"><span class="toc-section-number">12.3.1</span> Example 1 (fig6f) – Linear models for non-normal count data</a></li>
<li><a href="violations.html#my-data-arent-normal-what-is-the-best-practice" id="toc-my-data-arent-normal-what-is-the-best-practice"><span class="toc-section-number">12.3.2</span> My data aren’t normal, what is the best practice?</a></li>
</ul></li>
<li><a href="violations.html#hidden-code-4" id="toc-hidden-code-4"><span class="toc-section-number">12.4</span> Hidden Code</a>
<ul>
<li><a href="violations.html#importing-and-wrangling-the-exp1b-data" id="toc-importing-and-wrangling-the-exp1b-data"><span class="toc-section-number">12.4.1</span> Importing and wrangling the exp1b data</a></li>
<li><a href="violations.html#importing-and-wrangling-the-exp2a-data" id="toc-importing-and-wrangling-the-exp2a-data"><span class="toc-section-number">12.4.2</span> Importing and wrangling the exp2a data</a></li>
<li><a href="violations.html#importing-and-wrangling-the-fig6f-data" id="toc-importing-and-wrangling-the-fig6f-data"><span class="toc-section-number">12.4.3</span> Importing and wrangling the fig6f data</a></li>
</ul></li>
</ul></li>
<li><a href="issues.html#issues" id="toc-issues"><span class="toc-section-number">13</span> Issues in inference</a>
<ul>
<li><a href="issues.html#replicated-experiments-include-textttexperiment-as-a-random-factor-better-than-one-way-anova-of-means" id="toc-replicated-experiments-include-textttexperiment-as-a-random-factor-better-than-one-way-anova-of-means"><span class="toc-section-number">13.1</span> Replicated experiments – include <span class="math inline">\(\texttt{Experiment}\)</span> as a random factor (better than one-way ANOVA of means)</a>
<ul>
<li><a href="issues.html#issues-exp4d" id="toc-issues-exp4d"><span class="toc-section-number">13.1.1</span> Multiple experiments Example 1 (wound healing Exp4d)</a></li>
<li><a href="issues.html#issues-combined-experiments" id="toc-issues-combined-experiments"><span class="toc-section-number">13.1.2</span> Models for combining replicated experiments</a></li>
<li><a href="issues.html#understanding-model-exp4d_m1" id="toc-understanding-model-exp4d_m1"><span class="toc-section-number">13.1.3</span> Understanding Model <code>exp4d_m1</code></a></li>
<li><a href="issues.html#the-univariate-model-is-equivalent-to-a-linear-mixed-model-of-the-aggregated-data-model-exp4d_m2" id="toc-the-univariate-model-is-equivalent-to-a-linear-mixed-model-of-the-aggregated-data-model-exp4d_m2"><span class="toc-section-number">13.1.4</span> The univariate model is equivalent to a linear mixed model of the aggregated data (Model exp4d_m2)</a></li>
<li><a href="issues.html#a-linear-mixed-model-of-the-full-data" id="toc-a-linear-mixed-model-of-the-full-data"><span class="toc-section-number">13.1.5</span> A linear mixed model of the full data</a></li>
<li><a href="issues.html#analysis-of-the-experiment-means-has-less-precision-and-power" id="toc-analysis-of-the-experiment-means-has-less-precision-and-power"><span class="toc-section-number">13.1.6</span> Analysis of the experiment means has less precision and power</a></li>
<li><a href="issues.html#dont-do-this-a-t-testfixed-effect-anova-of-the-full-data" id="toc-dont-do-this-a-t-testfixed-effect-anova-of-the-full-data"><span class="toc-section-number">13.1.7</span> Don’t do this – a t-test/fixed-effect ANOVA of the full data</a></li>
</ul></li>
<li><a href="issues.html#issues-pre-post" id="toc-issues-pre-post"><span class="toc-section-number">13.2</span> Comparing change from baseline (pre-post)</a>
<ul>
<li><a href="issues.html#pre-post-example-1-dpp4-fig4c" id="toc-pre-post-example-1-dpp4-fig4c"><span class="toc-section-number">13.2.1</span> Pre-post example 1 (DPP4 fig4c)</a></li>
<li><a href="issues.html#what-if-the-data-in-example-1-were-from-from-an-experiment-where-the-treatment-was-applied-prior-to-the-baseline-measure" id="toc-what-if-the-data-in-example-1-were-from-from-an-experiment-where-the-treatment-was-applied-prior-to-the-baseline-measure"><span class="toc-section-number">13.2.2</span> What if the data in example 1 were from from an experiment where the treatment was applied prior to the baseline measure?</a></li>
<li><a href="issues.html#pre-post-example-2-xx-males-fig1c" id="toc-pre-post-example-2-xx-males-fig1c"><span class="toc-section-number">13.2.3</span> Pre-post example 2 (XX males fig1c)</a></li>
<li><a href="issues.html#issues-regression-to-mean" id="toc-issues-regression-to-mean"><span class="toc-section-number">13.2.4</span> Regression to the mean</a></li>
</ul></li>
<li><a href="issues.html#longitudinal-designs-with-more-than-one-post-baseline-measure" id="toc-longitudinal-designs-with-more-than-one-post-baseline-measure"><span class="toc-section-number">13.3</span> Longitudinal designs with more than one-post baseline measure</a>
<ul>
<li><a href="issues.html#issues-auc" id="toc-issues-auc"><span class="toc-section-number">13.3.1</span> Area under the curve (AUC)</a></li>
</ul></li>
<li><a href="issues.html#normalization-the-analysis-of-ratios" id="toc-normalization-the-analysis-of-ratios"><span class="toc-section-number">13.4</span> Normalization – the analysis of ratios</a>
<ul>
<li><a href="issues.html#kinds-of-ratios-in-experimental-biology" id="toc-kinds-of-ratios-in-experimental-biology"><span class="toc-section-number">13.4.1</span> Kinds of ratios in experimental biology</a></li>
<li><a href="issues.html#issues-offset" id="toc-issues-offset"><span class="toc-section-number">13.4.2</span> Example 1 – The ratio is a density (number of something per area)</a></li>
<li><a href="issues.html#issues-size" id="toc-issues-size"><span class="toc-section-number">13.4.3</span> Example 2 – The ratio is normalizing for size differences</a></li>
</ul></li>
<li><a href="issues.html#dont-do-this-stuff" id="toc-dont-do-this-stuff"><span class="toc-section-number">13.5</span> Don’t do this stuff</a>
<ul>
<li><a href="issues.html#normalize-the-response-so-that-all-control-values-are-equal-to-1." id="toc-normalize-the-response-so-that-all-control-values-are-equal-to-1."><span class="toc-section-number">13.5.1</span> Normalize the response so that all control values are equal to 1.</a></li>
</ul></li>
<li><a href="issues.html#a-difference-in-significance-is-not-necessarily-significant" id="toc-a-difference-in-significance-is-not-necessarily-significant"><span class="toc-section-number">13.6</span> A difference in significance is not necessarily significant</a></li>
<li><a href="issues.html#researcher-degrees-of-freedom" id="toc-researcher-degrees-of-freedom"><span class="toc-section-number">13.7</span> Researcher degrees of freedom</a></li>
<li><a href="issues.html#hidden-code-5" id="toc-hidden-code-5"><span class="toc-section-number">13.8</span> Hidden code</a>
<ul>
<li><a href="issues.html#import-exp4d-vimentin-cell-count-data-replicate-experiments-example" id="toc-import-exp4d-vimentin-cell-count-data-replicate-experiments-example"><span class="toc-section-number">13.8.1</span> Import exp4d vimentin cell count data (replicate experiments example)</a></li>
<li><a href="issues.html#import-fig4c-data" id="toc-import-fig4c-data"><span class="toc-section-number">13.8.2</span> Import Fig4c data</a></li>
<li><a href="issues.html#xx-males-fig1c" id="toc-xx-males-fig1c"><span class="toc-section-number">13.8.3</span> XX males fig1c</a></li>
<li><a href="issues.html#generation-of-fake-data-to-illustrate-regression-to-the-mean" id="toc-generation-of-fake-data-to-illustrate-regression-to-the-mean"><span class="toc-section-number">13.8.4</span> Generation of fake data to illustrate regression to the mean</a></li>
<li><a href="issues.html#import-fig3f" id="toc-import-fig3f"><span class="toc-section-number">13.8.5</span> Import fig3f</a></li>
<li><a href="issues.html#issues-import-exp3b" id="toc-issues-import-exp3b"><span class="toc-section-number">13.8.6</span> Import exp3b</a></li>
<li><a href="issues.html#issues-exp3b-plot" id="toc-issues-exp3b-plot"><span class="toc-section-number">13.8.7</span> Plot the model of exp3b (glm offset data)</a></li>
</ul></li>
</ul></li>
<li><a href="part-v-more-than-one-x-multivariable-models.html#part-v-more-than-one-x-multivariable-models" id="toc-part-v-more-than-one-x-multivariable-models">Part V: More than one <span class="math inline">\(X\)</span> – Multivariable Models</a></li>
<li><a href="covariates.html#covariates" id="toc-covariates"><span class="toc-section-number">14</span> Linear models with added covariates (“ANCOVA”)</a>
<ul>
<li><a href="covariates.html#adding-covariates-can-increases-the-precision-of-the-effect-of-interest" id="toc-adding-covariates-can-increases-the-precision-of-the-effect-of-interest"><span class="toc-section-number">14.1</span> Adding covariates can increases the precision of the effect of interest</a></li>
<li><a href="covariates.html#understanding-a-linear-model-with-an-added-covariate-heart-necrosis-data" id="toc-understanding-a-linear-model-with-an-added-covariate-heart-necrosis-data"><span class="toc-section-number">14.2</span> Understanding a linear model with an added covariate – heart necrosis data</a>
<ul>
<li><a href="covariates.html#fit-the-model-5" id="toc-fit-the-model-5"><span class="toc-section-number">14.2.1</span> Fit the model</a></li>
<li><a href="covariates.html#plot-the-model-2" id="toc-plot-the-model-2"><span class="toc-section-number">14.2.2</span> Plot the model</a></li>
<li><a href="covariates.html#interpretation-of-the-model-coefficients" id="toc-interpretation-of-the-model-coefficients"><span class="toc-section-number">14.2.3</span> Interpretation of the model coefficients</a></li>
<li><a href="covariates.html#everything-adds-up" id="toc-everything-adds-up"><span class="toc-section-number">14.2.4</span> Everything adds up</a></li>
<li><a href="covariates.html#interpretation-of-the-estimated-marginal-means" id="toc-interpretation-of-the-estimated-marginal-means"><span class="toc-section-number">14.2.5</span> Interpretation of the estimated marginal means</a></li>
<li><a href="covariates.html#interpretation-of-the-contrasts" id="toc-interpretation-of-the-contrasts"><span class="toc-section-number">14.2.6</span> Interpretation of the contrasts</a></li>
<li><a href="covariates.html#adding-the-covariate-improves-inference" id="toc-adding-the-covariate-improves-inference"><span class="toc-section-number">14.2.7</span> Adding the covariate improves inference</a></li>
</ul></li>
<li><a href="covariates.html#understanding-interaction-effects-with-covariates" id="toc-understanding-interaction-effects-with-covariates"><span class="toc-section-number">14.3</span> Understanding interaction effects with covariates</a>
<ul>
<li><a href="covariates.html#fit-the-model-6" id="toc-fit-the-model-6"><span class="toc-section-number">14.3.1</span> Fit the model</a></li>
<li><a href="covariates.html#plot-the-model-with-interaction-effect" id="toc-plot-the-model-with-interaction-effect"><span class="toc-section-number">14.3.2</span> Plot the model with interaction effect</a></li>
<li><a href="covariates.html#interpretation-of-the-model-coefficients-1" id="toc-interpretation-of-the-model-coefficients-1"><span class="toc-section-number">14.3.3</span> Interpretation of the model coefficients</a></li>
<li><a href="covariates.html#what-is-the-effect-of-a-treatment-if-interactions-are-modeled-it-depends." id="toc-what-is-the-effect-of-a-treatment-if-interactions-are-modeled-it-depends."><span class="toc-section-number">14.3.4</span> What is the effect of a treatment, if interactions are modeled? – it depends.</a></li>
<li><a href="covariates.html#which-model-do-we-use-mathcalm_1-or-mathcalm_2" id="toc-which-model-do-we-use-mathcalm_1-or-mathcalm_2"><span class="toc-section-number">14.3.5</span> Which model do we use, <span class="math inline">\(\mathcal{M}_1\)</span> or <span class="math inline">\(\mathcal{M}_2\)</span>?</a></li>
</ul></li>
<li><a href="covariates.html#understanding-ancova-tables" id="toc-understanding-ancova-tables"><span class="toc-section-number">14.4</span> Understanding ANCOVA tables</a></li>
<li><a href="covariates.html#working-in-r-3" id="toc-working-in-r-3"><span class="toc-section-number">14.5</span> Working in R</a>
<ul>
<li><a href="covariates.html#importing-the-heart-necrosis-data" id="toc-importing-the-heart-necrosis-data"><span class="toc-section-number">14.5.1</span> Importing the heart necrosis data</a></li>
<li><a href="covariates.html#fitting-the-model" id="toc-fitting-the-model"><span class="toc-section-number">14.5.2</span> Fitting the model</a></li>
<li><a href="covariates.html#using-the-emmeans-function-1" id="toc-using-the-emmeans-function-1"><span class="toc-section-number">14.5.3</span> Using the emmeans function</a></li>
<li><a href="covariates.html#ancova-tables" id="toc-ancova-tables"><span class="toc-section-number">14.5.4</span> ANCOVA tables</a></li>
<li><a href="covariates.html#plotting-the-model" id="toc-plotting-the-model"><span class="toc-section-number">14.5.5</span> Plotting the model</a></li>
</ul></li>
<li><a href="covariates.html#best-practices" id="toc-best-practices"><span class="toc-section-number">14.6</span> Best practices</a>
<ul>
<li><a href="covariates.html#do-not-use-a-ratio-of-partwhole-as-a-response-variable-instead-add-the-denominator-as-a-covariate" id="toc-do-not-use-a-ratio-of-partwhole-as-a-response-variable-instead-add-the-denominator-as-a-covariate"><span class="toc-section-number">14.6.1</span> Do not use a ratio of part:whole as a response variable – instead add the denominator as a covariate</a></li>
<li><a href="covariates.html#do-not-use-change-from-baseline-as-a-response-variable-instead-add-the-baseline-measure-as-a-covariate" id="toc-do-not-use-change-from-baseline-as-a-response-variable-instead-add-the-baseline-measure-as-a-covariate"><span class="toc-section-number">14.6.2</span> Do not use change from baseline as a response variable – instead add the baseline measure as a covariate</a></li>
<li><a href="covariates.html#do-not-test-for-balance-of-baseline-measures" id="toc-do-not-test-for-balance-of-baseline-measures"><span class="toc-section-number">14.6.3</span> Do not “test for balance” of baseline measures</a></li>
</ul></li>
<li><a href="covariates.html#best-practices-2-use-a-covariate-instead-of-normalizing-a-response" id="toc-best-practices-2-use-a-covariate-instead-of-normalizing-a-response"><span class="toc-section-number">14.7</span> Best practices 2: Use a covariate instead of normalizing a response</a></li>
</ul></li>
<li><a href="factorial.html#factorial" id="toc-factorial"><span class="toc-section-number">15</span> Linear models with two categorical <span class="math inline">\(X\)</span> – Factorial linear models (“two-way ANOVA”)</a>
<ul>
<li><a href="factorial.html#a-linear-model-with-crossed-factors-estimates-interaction-effects" id="toc-a-linear-model-with-crossed-factors-estimates-interaction-effects"><span class="toc-section-number">15.1</span> A linear model with crossed factors estimates interaction effects</a>
<ul>
<li><a href="factorial.html#an-interaction-is-a-difference-in-simple-effects" id="toc-an-interaction-is-a-difference-in-simple-effects"><span class="toc-section-number">15.1.1</span> An interaction is a difference in simple effects</a></li>
<li><a href="factorial.html#a-linear-model-with-crossed-factors-includes-interaction-effects" id="toc-a-linear-model-with-crossed-factors-includes-interaction-effects"><span class="toc-section-number">15.1.2</span> A linear model with crossed factors includes interaction effects</a></li>
<li><a href="factorial.html#factorial-experiments-are-frequently-analyzed-as-flattened-linear-models-in-the-experimental-biology-literature" id="toc-factorial-experiments-are-frequently-analyzed-as-flattened-linear-models-in-the-experimental-biology-literature"><span class="toc-section-number">15.1.3</span> factorial experiments are frequently analyzed as flattened linear models in the experimental biology literature</a></li>
</ul></li>
<li><a href="factorial.html#example-1-estimation-of-a-treatment-effect-relative-to-a-control-effect-something-different-experiment-2j-glucose-uptake-data" id="toc-example-1-estimation-of-a-treatment-effect-relative-to-a-control-effect-something-different-experiment-2j-glucose-uptake-data"><span class="toc-section-number">15.2</span> Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data)</a>
<ul>
<li><a href="factorial.html#twoway-understand1" id="toc-twoway-understand1"><span class="toc-section-number">15.2.1</span> Understand the experimental design</a></li>
<li><a href="factorial.html#fit-the-linear-model" id="toc-fit-the-linear-model"><span class="toc-section-number">15.2.2</span> Fit the linear model</a></li>
<li><a href="factorial.html#twoway-exp2j-inf" id="toc-twoway-exp2j-inf"><span class="toc-section-number">15.2.3</span> Inference</a></li>
<li><a href="factorial.html#plot-the-model-3" id="toc-plot-the-model-3"><span class="toc-section-number">15.2.4</span> Plot the model</a></li>
</ul></li>
<li><a href="factorial.html#understanding-the-linear-model-with-crossed-factors-1" id="toc-understanding-the-linear-model-with-crossed-factors-1"><span class="toc-section-number">15.3</span> Understanding the linear model with crossed factors 1</a>
<ul>
<li><a href="factorial.html#twoway-what-coefs-are" id="toc-twoway-what-coefs-are"><span class="toc-section-number">15.3.1</span> What the coefficients are</a></li>
<li><a href="factorial.html#the-interaction-effect-is-something-different" id="toc-the-interaction-effect-is-something-different"><span class="toc-section-number">15.3.2</span> The interaction effect is something different</a></li>
<li><a href="factorial.html#why-we-want-to-compare-the-treatment-effect-to-a-control-effect" id="toc-why-we-want-to-compare-the-treatment-effect-to-a-control-effect"><span class="toc-section-number">15.3.3</span> Why we want to compare the treatment effect to a control effect</a></li>
<li><a href="factorial.html#the-order-of-the-factors-in-the-model-tells-the-same-story-differently" id="toc-the-order-of-the-factors-in-the-model-tells-the-same-story-differently"><span class="toc-section-number">15.3.4</span> The order of the factors in the model tells the same story differently</a></li>
<li><a href="factorial.html#power-for-the-interaction-effect-is-less-than-that-for-simple-effects" id="toc-power-for-the-interaction-effect-is-less-than-that-for-simple-effects"><span class="toc-section-number">15.3.5</span> Power for the interaction effect is less than that for simple effects</a></li>
<li><a href="factorial.html#planned-comparisons-vs.-post-hoc-tests" id="toc-planned-comparisons-vs.-post-hoc-tests"><span class="toc-section-number">15.3.6</span> Planned comparisons vs. post-hoc tests</a></li>
</ul></li>
<li><a href="factorial.html#example-2-estimation-of-the-effect-of-background-condition-on-an-effect-it-depends-experiment-3e-lesian-area-data" id="toc-example-2-estimation-of-the-effect-of-background-condition-on-an-effect-it-depends-experiment-3e-lesian-area-data"><span class="toc-section-number">15.4</span> Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data)</a>
<ul>
<li><a href="factorial.html#understand-the-experimental-design" id="toc-understand-the-experimental-design"><span class="toc-section-number">15.4.1</span> Understand the experimental design</a></li>
<li><a href="factorial.html#fit-the-linear-model-1" id="toc-fit-the-linear-model-1"><span class="toc-section-number">15.4.2</span> Fit the linear model</a></li>
<li><a href="factorial.html#check-the-model-1" id="toc-check-the-model-1"><span class="toc-section-number">15.4.3</span> Check the model</a></li>
<li><a href="factorial.html#inference-from-the-model-2" id="toc-inference-from-the-model-2"><span class="toc-section-number">15.4.4</span> Inference from the model</a></li>
<li><a href="factorial.html#plot-the-model-4" id="toc-plot-the-model-4"><span class="toc-section-number">15.4.5</span> Plot the model</a></li>
</ul></li>
<li><a href="factorial.html#understanding-the-linear-model-with-crossed-factors-2" id="toc-understanding-the-linear-model-with-crossed-factors-2"><span class="toc-section-number">15.5</span> Understanding the linear model with crossed factors 2</a>
<ul>
<li><a href="factorial.html#twoway-marginal-means" id="toc-twoway-marginal-means"><span class="toc-section-number">15.5.1</span> Conditional and marginal means</a></li>
<li><a href="factorial.html#simple-conditional-effects" id="toc-simple-conditional-effects"><span class="toc-section-number">15.5.2</span> Simple (conditional) effects</a></li>
<li><a href="factorial.html#twoway-marginal-effects" id="toc-twoway-marginal-effects"><span class="toc-section-number">15.5.3</span> Marginal effects</a></li>
<li><a href="factorial.html#the-additive-model" id="toc-the-additive-model"><span class="toc-section-number">15.5.4</span> The additive model</a></li>
<li><a href="factorial.html#twoway-reduce" id="toc-twoway-reduce"><span class="toc-section-number">15.5.5</span> Reduce models for the right reason</a></li>
<li><a href="factorial.html#the-marginal-means-of-an-additive-linear-model-with-two-factors-can-be-weird" id="toc-the-marginal-means-of-an-additive-linear-model-with-two-factors-can-be-weird"><span class="toc-section-number">15.5.6</span> The marginal means of an additive linear model with two factors can be weird</a></li>
</ul></li>
<li><a href="factorial.html#example-3-estimation-of-synergy-more-than-the-sum-of-the-parts-experiment-1c-ja-data" id="toc-example-3-estimation-of-synergy-more-than-the-sum-of-the-parts-experiment-1c-ja-data"><span class="toc-section-number">15.6</span> Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data)</a>
<ul>
<li><a href="factorial.html#examine-the-data" id="toc-examine-the-data"><span class="toc-section-number">15.6.1</span> Examine the data</a></li>
<li><a href="factorial.html#fit-the-model-7" id="toc-fit-the-model-7"><span class="toc-section-number">15.6.2</span> Fit the model</a></li>
<li><a href="factorial.html#model-check" id="toc-model-check"><span class="toc-section-number">15.6.3</span> Model check</a></li>
<li><a href="factorial.html#inference-from-the-model-3" id="toc-inference-from-the-model-3"><span class="toc-section-number">15.6.4</span> Inference from the model</a></li>
<li><a href="factorial.html#plot-the-model-5" id="toc-plot-the-model-5"><span class="toc-section-number">15.6.5</span> Plot the model</a></li>
<li><a href="factorial.html#alternative-plot" id="toc-alternative-plot"><span class="toc-section-number">15.6.6</span> Alternative plot</a></li>
</ul></li>
<li><a href="factorial.html#understanding-the-linear-model-with-crossed-factors-3" id="toc-understanding-the-linear-model-with-crossed-factors-3"><span class="toc-section-number">15.7</span> Understanding the linear model with crossed factors 3</a>
<ul>
<li><a href="factorial.html#thinking-about-the-coefficients-of-the-linear-model" id="toc-thinking-about-the-coefficients-of-the-linear-model"><span class="toc-section-number">15.7.1</span> Thinking about the coefficients of the linear model</a></li>
</ul></li>
<li><a href="factorial.html#issues-in-inference" id="toc-issues-in-inference"><span class="toc-section-number">15.8</span> Issues in inference</a>
<ul>
<li><a href="factorial.html#for-pairwise-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model" id="toc-for-pairwise-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model"><span class="toc-section-number">15.8.1</span> For pairwise contrasts, it doesn’t matter if you fit a factorial or a flattened linear model</a></li>
<li><a href="factorial.html#for-interaction-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model" id="toc-for-interaction-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model"><span class="toc-section-number">15.8.2</span> For interaction contrasts, it doesn’t matter if you fit a factorial or a flattened linear model</a></li>
<li><a href="factorial.html#twoway-multiple-tests" id="toc-twoway-multiple-tests"><span class="toc-section-number">15.8.3</span> Adjusting <em>p</em>-values for multiple tests</a></li>
</ul></li>
<li><a href="factorial.html#two-way-anova" id="toc-two-way-anova"><span class="toc-section-number">15.9</span> Two-way ANOVA</a>
<ul>
<li><a href="factorial.html#how-to-read-a-two-way-anova-table" id="toc-how-to-read-a-two-way-anova-table"><span class="toc-section-number">15.9.1</span> How to read a two-way ANOVA table</a></li>
<li><a href="factorial.html#what-do-the-main-effects-in-an-anova-table-mean" id="toc-what-do-the-main-effects-in-an-anova-table-mean"><span class="toc-section-number">15.9.2</span> What do the main effects in an ANOVA table mean?</a></li>
</ul></li>
<li><a href="factorial.html#more-issues-in-inference" id="toc-more-issues-in-inference"><span class="toc-section-number">15.10</span> More issues in inference</a>
<ul>
<li><a href="factorial.html#longitudinal-experiments-include-time-as-a-random-factor-better-than-repeated-measures-anova" id="toc-longitudinal-experiments-include-time-as-a-random-factor-better-than-repeated-measures-anova"><span class="toc-section-number">15.10.1</span> Longitudinal experiments – include Time as a random factor (better than repeated measures ANOVA)</a></li>
</ul></li>
<li><a href="factorial.html#working-in-r-4" id="toc-working-in-r-4"><span class="toc-section-number">15.11</span> Working in R</a>
<ul>
<li><a href="factorial.html#model-formula" id="toc-model-formula"><span class="toc-section-number">15.11.1</span> Model formula</a></li>
<li><a href="factorial.html#using-the-emmeans-function-2" id="toc-using-the-emmeans-function-2"><span class="toc-section-number">15.11.2</span> Using the emmeans function</a></li>
<li><a href="factorial.html#contrasts" id="toc-contrasts"><span class="toc-section-number">15.11.3</span> Contrasts</a></li>
<li><a href="factorial.html#practice-safe-anova" id="toc-practice-safe-anova"><span class="toc-section-number">15.11.4</span> Practice safe ANOVA</a></li>
<li><a href="factorial.html#better-to-avoid-these" id="toc-better-to-avoid-these"><span class="toc-section-number">15.11.5</span> Better to avoid these</a></li>
</ul></li>
<li><a href="factorial.html#hidden-code-6" id="toc-hidden-code-6"><span class="toc-section-number">15.12</span> Hidden Code</a>
<ul>
<li><a href="factorial.html#import-exp2j-example-1" id="toc-import-exp2j-example-1"><span class="toc-section-number">15.12.1</span> Import exp2j (Example 1)</a></li>
<li><a href="factorial.html#import-exp3e-lesian-area-data-example-2" id="toc-import-exp3e-lesian-area-data-example-2"><span class="toc-section-number">15.12.2</span> Import exp3e lesian area data (Example 2)</a></li>
<li><a href="factorial.html#import-exp1c-ja-data-example-3" id="toc-import-exp1c-ja-data-example-3"><span class="toc-section-number">15.12.3</span> Import Exp1c JA data (Example 3)</a></li>
</ul></li>
</ul></li>
<li><a href="part-vi-expanding-the-linear-model.html#part-vi-expanding-the-linear-model" id="toc-part-vi-expanding-the-linear-model">Part VI – Expanding the Linear Model</a></li>
<li><a href="lmm.html#lmm" id="toc-lmm"><span class="toc-section-number">16</span> Models with random factors – linear mixed models</a>
<ul>
<li><a href="lmm.html#lmm-demo" id="toc-lmm-demo"><span class="toc-section-number">16.1</span> Example 1 – A random intercepts and slopes explainer (demo1)</a>
<ul>
<li><a href="lmm.html#batched-measurements-result-in-clustered-residuals" id="toc-batched-measurements-result-in-clustered-residuals"><span class="toc-section-number">16.1.1</span> Batched measurements result in clustered residuals</a></li>
<li><a href="lmm.html#lmm-corerr" id="toc-lmm-corerr"><span class="toc-section-number">16.1.2</span> Clustered residuals result in correlated error</a></li>
<li><a href="lmm.html#in-blocked-designs-clustered-residuals-adds-a-variance-component-that-masks-treatment-effects" id="toc-in-blocked-designs-clustered-residuals-adds-a-variance-component-that-masks-treatment-effects"><span class="toc-section-number">16.1.3</span> In blocked designs, clustered residuals adds a variance component that masks treatment effects</a></li>
<li><a href="lmm.html#linear-mixed-models-are-linear-models-with-added-random-factors" id="toc-linear-mixed-models-are-linear-models-with-added-random-factors"><span class="toc-section-number">16.1.4</span> Linear mixed models are linear models with added random factors</a></li>
<li><a href="lmm.html#what-the-random-effects-are" id="toc-what-the-random-effects-are"><span class="toc-section-number">16.1.5</span> What the random effects are</a></li>
<li><a href="lmm.html#in-a-blocked-design-a-linear-model-with-added-random-effects-increases-precision-of-treatment-effects" id="toc-in-a-blocked-design-a-linear-model-with-added-random-effects-increases-precision-of-treatment-effects"><span class="toc-section-number">16.1.6</span> In a blocked design, a linear model with added random effects increases precision of treatment effects</a></li>
<li><a href="lmm.html#lmm-varcorr" id="toc-lmm-varcorr"><span class="toc-section-number">16.1.7</span> The correlation among random intercepts and slopes</a></li>
<li><a href="lmm.html#clustered-residuals-create-heterogeneity-among-treatments" id="toc-clustered-residuals-create-heterogeneity-among-treatments"><span class="toc-section-number">16.1.8</span> Clustered residuals create heterogeneity among treatments</a></li>
<li><a href="lmm.html#linear-mixed-models-are-flexible" id="toc-linear-mixed-models-are-flexible"><span class="toc-section-number">16.1.9</span> Linear mixed models are flexible</a></li>
<li><a href="lmm.html#a-random-intercept-only-model" id="toc-a-random-intercept-only-model"><span class="toc-section-number">16.1.10</span> A random intercept only model</a></li>
<li><a href="lmm.html#a-model-including-an-interaction-intercept" id="toc-a-model-including-an-interaction-intercept"><span class="toc-section-number">16.1.11</span> A model including an interaction intercept</a></li>
<li><a href="lmm.html#lmm-demo-aic" id="toc-lmm-demo-aic"><span class="toc-section-number">16.1.12</span> AIC and model selection – which model to report?</a></li>
<li><a href="lmm.html#the-specification-of-random-effects-matters" id="toc-the-specification-of-random-effects-matters"><span class="toc-section-number">16.1.13</span> The specification of random effects matters</a></li>
<li><a href="lmm.html#mixed-effect-and-repeated-measures-anova" id="toc-mixed-effect-and-repeated-measures-anova"><span class="toc-section-number">16.1.14</span> Mixed Effect and Repeated Measures ANOVA</a></li>
<li><a href="lmm.html#pseudoreplication" id="toc-pseudoreplication"><span class="toc-section-number">16.1.15</span> Pseudoreplication</a></li>
</ul></li>
<li><a href="lmm.html#lmm-example2" id="toc-lmm-example2"><span class="toc-section-number">16.2</span> Example 2 – experiments without subsampling replication (exp6g)</a>
<ul>
<li><a href="lmm.html#understand-the-data" id="toc-understand-the-data"><span class="toc-section-number">16.2.1</span> Understand the data</a></li>
<li><a href="lmm.html#model-fit-and-inference" id="toc-model-fit-and-inference"><span class="toc-section-number">16.2.2</span> Model fit and inference</a></li>
<li><a href="lmm.html#the-model-exp6g_m1-adds-a-random-intercept-but-not-a-random-slope" id="toc-the-model-exp6g_m1-adds-a-random-intercept-but-not-a-random-slope"><span class="toc-section-number">16.2.3</span> The model exp6g_m1 adds a random intercept but not a random slope</a></li>
<li><a href="lmm.html#the-fixed-effect-coefficients-of-model-exp6g_m1" id="toc-the-fixed-effect-coefficients-of-model-exp6g_m1"><span class="toc-section-number">16.2.4</span> The fixed effect coefficients of model exp6g_m1</a></li>
<li><a href="lmm.html#the-random-intercept-coefficients-of-exp6g_m1" id="toc-the-random-intercept-coefficients-of-exp6g_m1"><span class="toc-section-number">16.2.5</span> The random intercept coefficients of exp6g_m1</a></li>
<li><a href="lmm.html#the-random-and-residual-variance-and-the-intraclass-correlation-of-model-exp6g_m1" id="toc-the-random-and-residual-variance-and-the-intraclass-correlation-of-model-exp6g_m1"><span class="toc-section-number">16.2.6</span> The random and residual variance and the intraclass correlation of model exp6g_m1</a></li>
<li><a href="lmm.html#the-linear-mixed-model-exp6g_m1-increases-precision-of-treatment-effects-relative-to-a-fixed-effects-model" id="toc-the-linear-mixed-model-exp6g_m1-increases-precision-of-treatment-effects-relative-to-a-fixed-effects-model"><span class="toc-section-number">16.2.7</span> The linear mixed model exp6g_m1 increases precision of treatment effects, relative to a fixed effects model</a></li>
<li><a href="lmm.html#lmm-exp6g-alt" id="toc-lmm-exp6g-alt"><span class="toc-section-number">16.2.8</span> Alternative models for exp6g</a></li>
<li><a href="lmm.html#paired-t-tests-and-repeated-measures-anova-are-special-cases-of-linear-mixed-models" id="toc-paired-t-tests-and-repeated-measures-anova-are-special-cases-of-linear-mixed-models"><span class="toc-section-number">16.2.9</span> Paired t-tests and repeated measures ANOVA are special cases of linear mixed models</a></li>
<li><a href="lmm.html#lmm-exp6g-rmanova" id="toc-lmm-exp6g-rmanova"><span class="toc-section-number">16.2.10</span> Classical (“univariate model”) repeated measures ANOVA of exp6g</a></li>
<li><a href="lmm.html#lmm-exp6g-rmanova-multi" id="toc-lmm-exp6g-rmanova-multi"><span class="toc-section-number">16.2.11</span> “Multivariate model” repeated measures ANOVA</a></li>
<li><a href="lmm.html#linear-mixed-models-vs-repeated-measures-anova" id="toc-linear-mixed-models-vs-repeated-measures-anova"><span class="toc-section-number">16.2.12</span> Linear mixed models vs repeated measures ANOVA</a></li>
<li><a href="lmm.html#modeling-textttmouse_id-as-a-fixed-effect" id="toc-modeling-textttmouse_id-as-a-fixed-effect"><span class="toc-section-number">16.2.13</span> Modeling <span class="math inline">\(\texttt{mouse_id}\)</span> as a fixed effect</a></li>
</ul></li>
<li><a href="lmm.html#example-3-factorial-experiments-and-no-subsampling-replicates-exp5c" id="toc-example-3-factorial-experiments-and-no-subsampling-replicates-exp5c"><span class="toc-section-number">16.3</span> Example 3 – Factorial experiments and no subsampling replicates (exp5c)</a>
<ul>
<li><a href="lmm.html#understand-the-data-1" id="toc-understand-the-data-1"><span class="toc-section-number">16.3.1</span> Understand the data</a></li>
<li><a href="lmm.html#examine-the-data-1" id="toc-examine-the-data-1"><span class="toc-section-number">16.3.2</span> Examine the data</a></li>
<li><a href="lmm.html#lmm-exp5c-m1" id="toc-lmm-exp5c-m1"><span class="toc-section-number">16.3.3</span> Model fit and inference</a></li>
<li><a href="lmm.html#why-we-care-about-modeling-batch-in-exp5c" id="toc-why-we-care-about-modeling-batch-in-exp5c"><span class="toc-section-number">16.3.4</span> Why we care about modeling batch in exp5c</a></li>
<li><a href="lmm.html#the-linear-mixed-model-exp5c_m1-adds-two-random-intercepts" id="toc-the-linear-mixed-model-exp5c_m1-adds-two-random-intercepts"><span class="toc-section-number">16.3.5</span> The linear mixed model exp5c_m1 adds two random intercepts</a></li>
<li><a href="lmm.html#the-fixed-effect-coefficients-of-model-exp5c_m1" id="toc-the-fixed-effect-coefficients-of-model-exp5c_m1"><span class="toc-section-number">16.3.6</span> The fixed effect coefficients of model exp5c_m1</a></li>
<li><a href="lmm.html#the-random-effect-coefficients-of-model-exp5c_m1" id="toc-the-random-effect-coefficients-of-model-exp5c_m1"><span class="toc-section-number">16.3.7</span> The random effect coefficients of model exp5c_m1</a></li>
<li><a href="lmm.html#lmm-exp5c-alt" id="toc-lmm-exp5c-alt"><span class="toc-section-number">16.3.8</span> Alternative models for exp5c</a></li>
<li><a href="lmm.html#lmm-rmanova-5c" id="toc-lmm-rmanova-5c"><span class="toc-section-number">16.3.9</span> Classical (“univariate model”) repeated measures ANOVA</a></li>
<li><a href="lmm.html#lmm-rmanova-5c-multi" id="toc-lmm-rmanova-5c-multi"><span class="toc-section-number">16.3.10</span> “Multivariate model” repeated measures ANOVA of exp5c</a></li>
<li><a href="lmm.html#modeling-textttdonor-as-a-fixed-effect" id="toc-modeling-textttdonor-as-a-fixed-effect"><span class="toc-section-number">16.3.11</span> Modeling <span class="math inline">\(\texttt{donor}\)</span> as a fixed effect</a></li>
</ul></li>
<li><a href="lmm.html#lmm-example4" id="toc-lmm-example4"><span class="toc-section-number">16.4</span> Example 4 – Experiments with subsampling replication (exp1g)</a>
<ul>
<li><a href="lmm.html#understand-the-data-2" id="toc-understand-the-data-2"><span class="toc-section-number">16.4.1</span> Understand the data</a></li>
<li><a href="lmm.html#examine-the-data-2" id="toc-examine-the-data-2"><span class="toc-section-number">16.4.2</span> Examine the data</a></li>
<li><a href="lmm.html#fit-the-model-9" id="toc-fit-the-model-9"><span class="toc-section-number">16.4.3</span> Fit the model</a></li>
<li><a href="lmm.html#inference-from-the-model-6" id="toc-inference-from-the-model-6"><span class="toc-section-number">16.4.4</span> Inference from the model</a></li>
<li><a href="lmm.html#plot-the-model-6" id="toc-plot-the-model-6"><span class="toc-section-number">16.4.5</span> Plot the model</a></li>
<li><a href="lmm.html#alternaplot-the-model-2" id="toc-alternaplot-the-model-2"><span class="toc-section-number">16.4.6</span> Alternaplot the model</a></li>
<li><a href="lmm.html#understanding-the-alternative-models" id="toc-understanding-the-alternative-models"><span class="toc-section-number">16.4.7</span> Understanding the alternative models</a></li>
<li><a href="lmm.html#the-varcorr-matrix-of-models-exp1g_m1a-and-exp1g_m1b" id="toc-the-varcorr-matrix-of-models-exp1g_m1a-and-exp1g_m1b"><span class="toc-section-number">16.4.8</span> The VarCorr matrix of models exp1g_m1a and exp1g_m1b</a></li>
<li><a href="lmm.html#the-linear-mixed-model-has-more-precision-and-power-than-the-fixed-effect-model-of-batch-means" id="toc-the-linear-mixed-model-has-more-precision-and-power-than-the-fixed-effect-model-of-batch-means"><span class="toc-section-number">16.4.9</span> The linear mixed model has more precision and power than the fixed effect model of batch means</a></li>
<li><a href="lmm.html#fixed-effect-models-and-pseudoreplication" id="toc-fixed-effect-models-and-pseudoreplication"><span class="toc-section-number">16.4.10</span> Fixed effect models and pseudoreplication</a></li>
<li><a href="lmm.html#mixed-effect-anova" id="toc-mixed-effect-anova"><span class="toc-section-number">16.4.11</span> Mixed-effect ANOVA</a></li>
</ul></li>
<li><a href="lmm.html#working-in-r-5" id="toc-working-in-r-5"><span class="toc-section-number">16.5</span> Working in R</a>
<ul>
<li><a href="lmm.html#fitting-linear-mixed-models" id="toc-fitting-linear-mixed-models"><span class="toc-section-number">16.5.1</span> Fitting linear mixed models</a></li>
<li><a href="lmm.html#plotting-models-fit-to-batched-data" id="toc-plotting-models-fit-to-batched-data"><span class="toc-section-number">16.5.2</span> Plotting models fit to batched data</a></li>
<li><a href="lmm.html#repeated-measures-anova-randomized-complete-block-with-no-subsampling" id="toc-repeated-measures-anova-randomized-complete-block-with-no-subsampling"><span class="toc-section-number">16.5.3</span> Repeated measures ANOVA (randomized complete block with no subsampling)</a></li>
</ul></li>
<li><a href="lmm.html#hidden-code-7" id="toc-hidden-code-7"><span class="toc-section-number">16.6</span> Hidden code</a>
<ul>
<li><a href="lmm.html#import-exp5c" id="toc-import-exp5c"><span class="toc-section-number">16.6.1</span> Import exp5c</a></li>
<li><a href="lmm.html#import-exp1g" id="toc-import-exp1g"><span class="toc-section-number">16.6.2</span> Import exp1g</a></li>
</ul></li>
</ul></li>
<li><a href="pre-post.html#pre-post" id="toc-pre-post"><span class="toc-section-number">17</span> Linear models for longitudinal experiments – I. pre-post designs</a>
<ul>
<li><a href="pre-post.html#best-practice-models" id="toc-best-practice-models"><span class="toc-section-number">17.1</span> Best practice models</a></li>
<li><a href="pre-post.html#common-alternatives-that-are-not-recommended" id="toc-common-alternatives-that-are-not-recommended"><span class="toc-section-number">17.2</span> Common alternatives that are not recommended</a></li>
<li><a href="pre-post.html#advanced-models" id="toc-advanced-models"><span class="toc-section-number">17.3</span> Advanced models</a></li>
<li><a href="pre-post.html#understanding-the-alternative-models-1" id="toc-understanding-the-alternative-models-1"><span class="toc-section-number">17.4</span> Understanding the alternative models</a>
<ul>
<li><a href="pre-post.html#m1-linear-model-with-the-baseline-measure-as-the-covariate-ancova-model" id="toc-m1-linear-model-with-the-baseline-measure-as-the-covariate-ancova-model"><span class="toc-section-number">17.4.1</span> (M1) Linear model with the baseline measure as the covariate (ANCOVA model)</a></li>
<li><a href="pre-post.html#m2-linear-model-of-the-change-score-change-score-model" id="toc-m2-linear-model-of-the-change-score-change-score-model"><span class="toc-section-number">17.4.2</span> (M2) Linear model of the change score (change-score model)</a></li>
<li><a href="pre-post.html#m3-linear-model-of-post-baseline-values-without-the-baseline-as-a-covariate-post-model" id="toc-m3-linear-model-of-post-baseline-values-without-the-baseline-as-a-covariate-post-model"><span class="toc-section-number">17.4.3</span> (M3) Linear model of post-baseline values without the baseline as a covariate (post model)</a></li>
<li><a href="pre-post.html#m4-linear-model-with-factorial-fixed-effects-fixed-effects-model" id="toc-m4-linear-model-with-factorial-fixed-effects-fixed-effects-model"><span class="toc-section-number">17.4.4</span> (M4) Linear model with factorial fixed effects (fixed-effects model)</a></li>
<li><a href="pre-post.html#m5-repeated-measures-anova" id="toc-m5-repeated-measures-anova"><span class="toc-section-number">17.4.5</span> (M5) Repeated measures ANOVA</a></li>
<li><a href="pre-post.html#m6-linear-mixed-model" id="toc-m6-linear-mixed-model"><span class="toc-section-number">17.4.6</span> (M6) Linear mixed model</a></li>
<li><a href="pre-post.html#m7-linear-model-with-correlated-error" id="toc-m7-linear-model-with-correlated-error"><span class="toc-section-number">17.4.7</span> (M7) Linear model with correlated error</a></li>
<li><a href="pre-post.html#m8-constrained-fixed-effects-model-with-correlated-error-clda-model" id="toc-m8-constrained-fixed-effects-model-with-correlated-error-clda-model"><span class="toc-section-number">17.4.8</span> (M8) Constrained fixed effects model with correlated error (cLDA model)</a></li>
<li><a href="pre-post.html#comparison-table" id="toc-comparison-table"><span class="toc-section-number">17.4.9</span> Comparison table</a></li>
</ul></li>
<li><a href="pre-post.html#example-1-a-single-post-baseline-measure-pre-post-design" id="toc-example-1-a-single-post-baseline-measure-pre-post-design"><span class="toc-section-number">17.5</span> Example 1 – a single post-baseline measure (pre-post design)</a></li>
<li><a href="pre-post.html#working-in-r-6" id="toc-working-in-r-6"><span class="toc-section-number">17.6</span> Working in R</a></li>
<li><a href="pre-post.html#hidden-code-8" id="toc-hidden-code-8"><span class="toc-section-number">17.7</span> Hidden code</a>
<ul>
<li><a href="pre-post.html#import-and-wrangle-mouse-sociability-data" id="toc-import-and-wrangle-mouse-sociability-data"><span class="toc-section-number">17.7.1</span> Import and wrangle mouse sociability data</a></li>
</ul></li>
</ul></li>
<li><a href="counts.html#counts" id="toc-counts"><span class="toc-section-number">18</span> Linear models for counts, binary responses, skewed responses, and ratios – Generalized Linear Models</a>
<ul>
<li><a href="counts.html#glm-counts" id="toc-glm-counts"><span class="toc-section-number">18.1</span> Introducing Generalized Linear Models using count data examples</a>
<ul>
<li><a href="counts.html#the-generalized-linear-model-glm" id="toc-the-generalized-linear-model-glm"><span class="toc-section-number">18.1.1</span> The Generalized Linear Model (GLM)</a></li>
<li><a href="counts.html#kinds-of-data-that-are-modeled-by-a-glm" id="toc-kinds-of-data-that-are-modeled-by-a-glm"><span class="toc-section-number">18.1.2</span> Kinds of data that are modeled by a GLM</a></li>
</ul></li>
<li><a href="counts.html#glm-1" id="toc-glm-1"><span class="toc-section-number">18.2</span> Example 1 – GLM models for count responses (“angiogenic sprouts” exp3a)</a>
<ul>
<li><a href="counts.html#understand-the-data-3" id="toc-understand-the-data-3"><span class="toc-section-number">18.2.1</span> Understand the data</a></li>
<li><a href="counts.html#model-fit-and-inference-1" id="toc-model-fit-and-inference-1"><span class="toc-section-number">18.2.2</span> Model fit and inference</a></li>
</ul></li>
<li><a href="counts.html#understanding-example-1" id="toc-understanding-example-1"><span class="toc-section-number">18.3</span> Understanding Example 1</a>
<ul>
<li><a href="counts.html#modeling-strategy-1" id="toc-modeling-strategy-1"><span class="toc-section-number">18.3.1</span> Modeling strategy</a></li>
<li><a href="counts.html#model-checking-fits-to-count-data" id="toc-model-checking-fits-to-count-data"><span class="toc-section-number">18.3.2</span> Model checking fits to count data</a></li>
<li><a href="counts.html#biological-count-data-are-rarely-fit-well-by-a-poisson-glm.-instead-fit-a-quasi-poisson-or-negative-binomial-glm-model." id="toc-biological-count-data-are-rarely-fit-well-by-a-poisson-glm.-instead-fit-a-quasi-poisson-or-negative-binomial-glm-model."><span class="toc-section-number">18.3.3</span> Biological count data are rarely fit well by a Poisson GLM. Instead, fit a quasi-poisson or negative binomial GLM model.</a></li>
<li><a href="counts.html#a-glm-is-a-linear-model-on-the-link-scale" id="toc-a-glm-is-a-linear-model-on-the-link-scale"><span class="toc-section-number">18.3.4</span> A GLM is a linear model on the link scale</a></li>
<li><a href="counts.html#coeffecients-of-a-generalized-linear-model-with-a-log-link-function-are-on-the-link-scale." id="toc-coeffecients-of-a-generalized-linear-model-with-a-log-link-function-are-on-the-link-scale."><span class="toc-section-number">18.3.5</span> Coeffecients of a Generalized Linear Model with a log-link function are on the link scale.</a></li>
<li><a href="counts.html#modeled-means-in-the-emmeans-table-of-a-generalized-linear-model-can-be-on-the-link-scale-or-response-scale-report-the-response-scale" id="toc-modeled-means-in-the-emmeans-table-of-a-generalized-linear-model-can-be-on-the-link-scale-or-response-scale-report-the-response-scale"><span class="toc-section-number">18.3.6</span> Modeled means in the emmeans table of a Generalized Linear Model can be on the link scale or response scale – Report the response scale</a></li>
<li><a href="counts.html#some-consequences-of-fitting-a-linear-model-to-count-data" id="toc-some-consequences-of-fitting-a-linear-model-to-count-data"><span class="toc-section-number">18.3.7</span> Some consequences of fitting a linear model to count data</a></li>
</ul></li>
<li><a href="counts.html#example-2-use-a-glm-with-an-offset-instead-of-a-ratio-of-some-measurement-per-area-dna-damage-data-exp3b" id="toc-example-2-use-a-glm-with-an-offset-instead-of-a-ratio-of-some-measurement-per-area-dna-damage-data-exp3b"><span class="toc-section-number">18.4</span> Example 2 – Use a GLM with an offset instead of a ratio of some measurement per area (“dna damage” data exp3b)</a>
<ul>
<li><a href="counts.html#exp3b-dna-damage-data" id="toc-exp3b-dna-damage-data"><span class="toc-section-number">18.4.1</span> exp3b (“dna damage”) data</a></li>
<li><a href="counts.html#understand-the-data-4" id="toc-understand-the-data-4"><span class="toc-section-number">18.4.2</span> Understand the data</a></li>
<li><a href="counts.html#model-fit-and-inference-2" id="toc-model-fit-and-inference-2"><span class="toc-section-number">18.4.3</span> Model fit and inference</a></li>
</ul></li>
<li><a href="counts.html#understanding-example-2" id="toc-understanding-example-2"><span class="toc-section-number">18.5</span> Understanding Example 2</a>
<ul>
<li><a href="counts.html#an-offset-is-an-added-covariate-with-a-coefficient-fixed-at-1" id="toc-an-offset-is-an-added-covariate-with-a-coefficient-fixed-at-1"><span class="toc-section-number">18.5.1</span> An offset is an added covariate with a coefficient fixed at 1</a></li>
<li><a href="counts.html#glm-rate" id="toc-glm-rate"><span class="toc-section-number">18.5.2</span> A count GLM with an offset models the area-normalized means</a></li>
<li><a href="counts.html#compare-an-offset-to-an-added-covariate-with-an-estimated-coefficient" id="toc-compare-an-offset-to-an-added-covariate-with-an-estimated-coefficient"><span class="toc-section-number">18.5.3</span> Compare an offset to an added covariate with an estimated coefficient</a></li>
<li><a href="counts.html#issues-with-plotting" id="toc-issues-with-plotting"><span class="toc-section-number">18.5.4</span> Issues with plotting</a></li>
</ul></li>
<li><a href="counts.html#example-3-glm-models-for-binary-responses" id="toc-example-3-glm-models-for-binary-responses"><span class="toc-section-number">18.6</span> Example 3 – GLM models for binary responses</a></li>
<li><a href="counts.html#working-in-r-7" id="toc-working-in-r-7"><span class="toc-section-number">18.7</span> Working in R</a>
<ul>
<li><a href="counts.html#fitting-glms-to-count-data" id="toc-fitting-glms-to-count-data"><span class="toc-section-number">18.7.1</span> Fitting GLMs to count data</a></li>
<li><a href="counts.html#fitting-a-glm-to-a-continuous-conditional-response-with-right-skew." id="toc-fitting-a-glm-to-a-continuous-conditional-response-with-right-skew."><span class="toc-section-number">18.7.2</span> Fitting a GLM to a continuous conditional response with right skew.</a></li>
<li><a href="counts.html#fitting-a-glm-to-a-binary-success-or-failure-presence-or-absence-survived-or-died-response" id="toc-fitting-a-glm-to-a-binary-success-or-failure-presence-or-absence-survived-or-died-response"><span class="toc-section-number">18.7.3</span> Fitting a GLM to a binary (success or failure, presence or absence, survived or died) response</a></li>
<li><a href="counts.html#fitting-generalized-linear-mixed-models" id="toc-fitting-generalized-linear-mixed-models"><span class="toc-section-number">18.7.4</span> Fitting Generalized Linear Mixed Models</a></li>
</ul></li>
<li><a href="counts.html#model-checking-glms" id="toc-model-checking-glms"><span class="toc-section-number">18.8</span> Model checking GLMs</a></li>
<li><a href="counts.html#hidden-code-9" id="toc-hidden-code-9"><span class="toc-section-number">18.9</span> Hidden code</a>
<ul>
<li><a href="counts.html#import-example-1-data-exp3a-angiogenic-sprouts" id="toc-import-example-1-data-exp3a-angiogenic-sprouts"><span class="toc-section-number">18.9.1</span> Import Example 1 data (exp3a – “angiogenic sprouts”)</a></li>
</ul></li>
</ul></li>
<li><a href="gls.html#gls" id="toc-gls"><span class="toc-section-number">19</span> Linear models with heterogenous variance</a></li>
<li><a href="appendix-an-example-set-of-analyses-of-experimental-data-with-linear-models.html#appendix-an-example-set-of-analyses-of-experimental-data-with-linear-models" id="toc-appendix-an-example-set-of-analyses-of-experimental-data-with-linear-models">Appendix: An example set of analyses of experimental data with linear models</a></li>
<li><a href="ask1-bio.html#ask1-bio" id="toc-ask1-bio"><span class="toc-section-number">20</span> Background physiology to the experiments in Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity”</a></li>
<li><a href="ask1-report.html#ask1-report" id="toc-ask1-report"><span class="toc-section-number">21</span> Analyses for Figure 2 of “ASK1 inhibits browning of white adipose tissue in obesity”</a>
<ul>
<li><a href="ask1-report.html#setup" id="toc-setup"><span class="toc-section-number">21.1</span> Setup</a></li>
<li><a href="ask1-report.html#data-source" id="toc-data-source"><span class="toc-section-number">21.2</span> Data source</a></li>
<li><a href="ask1-report.html#control-the-color-palette" id="toc-control-the-color-palette"><span class="toc-section-number">21.3</span> control the color palette</a></li>
<li><a href="ask1-report.html#useful-functions" id="toc-useful-functions"><span class="toc-section-number">21.4</span> useful functions</a></li>
<li><a href="ask1-report.html#figure-2b-effect-of-ask1-deletion-on-growth-body-weight" id="toc-figure-2b-effect-of-ask1-deletion-on-growth-body-weight"><span class="toc-section-number">21.5</span> figure 2b – effect of ASK1 deletion on growth (body weight)</a>
<ul>
<li><a href="ask1-report.html#figure-2b-import" id="toc-figure-2b-import"><span class="toc-section-number">21.5.1</span> figure 2b – import</a></li>
<li><a href="ask1-report.html#figure-2b-exploratory-plots" id="toc-figure-2b-exploratory-plots"><span class="toc-section-number">21.5.2</span> figure 2b – exploratory plots</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2c-effect-of-ask1-deletion-on-final-body-weight" id="toc-figure-2c-effect-of-ask1-deletion-on-final-body-weight"><span class="toc-section-number">21.6</span> Figure 2c – Effect of ASK1 deletion on final body weight</a>
<ul>
<li><a href="ask1-report.html#figure-2c-import" id="toc-figure-2c-import"><span class="toc-section-number">21.6.1</span> Figure 2c – import</a></li>
<li><a href="ask1-report.html#figure-2c-check-own-computation-of-weight-change-v-imported-value" id="toc-figure-2c-check-own-computation-of-weight-change-v-imported-value"><span class="toc-section-number">21.6.2</span> Figure 2c – check own computation of weight change v imported value</a></li>
<li><a href="ask1-report.html#figure-2c-exploratory-plots" id="toc-figure-2c-exploratory-plots"><span class="toc-section-number">21.6.3</span> Figure 2c – exploratory plots</a></li>
<li><a href="ask1-report.html#figure-2c-fit-the-model-m1-lm" id="toc-figure-2c-fit-the-model-m1-lm"><span class="toc-section-number">21.6.4</span> Figure 2c – fit the model: m1 (lm)</a></li>
<li><a href="ask1-report.html#figure-2c-check-the-model-m1" id="toc-figure-2c-check-the-model-m1"><span class="toc-section-number">21.6.5</span> Figure 2c – check the model: m1</a></li>
<li><a href="ask1-report.html#figure-2c-fit-the-model-m2-gamma-glm" id="toc-figure-2c-fit-the-model-m2-gamma-glm"><span class="toc-section-number">21.6.6</span> Figure 2c – fit the model: m2 (gamma glm)</a></li>
<li><a href="ask1-report.html#figure-2c-check-the-model-m2" id="toc-figure-2c-check-the-model-m2"><span class="toc-section-number">21.6.7</span> Figure 2c – check the model, m2</a></li>
<li><a href="ask1-report.html#figure-2c-inference-from-the-model" id="toc-figure-2c-inference-from-the-model"><span class="toc-section-number">21.6.8</span> Figure 2c – inference from the model</a></li>
<li><a href="ask1-report.html#figure-2c-plot-the-model" id="toc-figure-2c-plot-the-model"><span class="toc-section-number">21.6.9</span> Figure 2c – plot the model</a></li>
<li><a href="ask1-report.html#figure-2c-report" id="toc-figure-2c-report"><span class="toc-section-number">21.6.10</span> Figure 2c – report</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2d-effect-of-ask1-ko-on-glucose-tolerance-whole-curve" id="toc-figure-2d-effect-of-ask1-ko-on-glucose-tolerance-whole-curve"><span class="toc-section-number">21.7</span> Figure 2d – Effect of ASK1 KO on glucose tolerance (whole curve)</a>
<ul>
<li><a href="ask1-report.html#figure-2d-import" id="toc-figure-2d-import"><span class="toc-section-number">21.7.1</span> Figure 2d – Import</a></li>
<li><a href="ask1-report.html#figure-2d-exploratory-plots" id="toc-figure-2d-exploratory-plots"><span class="toc-section-number">21.7.2</span> Figure 2d – exploratory plots</a></li>
<li><a href="ask1-report.html#figure-2d-fit-the-model" id="toc-figure-2d-fit-the-model"><span class="toc-section-number">21.7.3</span> Figure 2d – fit the model</a></li>
<li><a href="ask1-report.html#figure-2d-check-the-model" id="toc-figure-2d-check-the-model"><span class="toc-section-number">21.7.4</span> Figure 2d – check the model</a></li>
<li><a href="ask1-report.html#figure-2d-inference" id="toc-figure-2d-inference"><span class="toc-section-number">21.7.5</span> Figure 2d – inference</a></li>
<li><a href="ask1-report.html#figure-2d-plot-the-model" id="toc-figure-2d-plot-the-model"><span class="toc-section-number">21.7.6</span> Figure 2d – plot the model</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2e-effect-of-ask1-deletion-on-glucose-tolerance-summary-measure" id="toc-figure-2e-effect-of-ask1-deletion-on-glucose-tolerance-summary-measure"><span class="toc-section-number">21.8</span> Figure 2e – Effect of ASK1 deletion on glucose tolerance (summary measure)</a>
<ul>
<li><a href="ask1-report.html#figure-2e-message-the-data" id="toc-figure-2e-message-the-data"><span class="toc-section-number">21.8.1</span> Figure 2e – message the data</a></li>
<li><a href="ask1-report.html#figure-2e-exploratory-plots" id="toc-figure-2e-exploratory-plots"><span class="toc-section-number">21.8.2</span> Figure 2e – exploratory plots</a></li>
<li><a href="ask1-report.html#figure-2e-fit-the-model" id="toc-figure-2e-fit-the-model"><span class="toc-section-number">21.8.3</span> Figure 2e – fit the model</a></li>
<li><a href="ask1-report.html#figure-2e-check-the-model" id="toc-figure-2e-check-the-model"><span class="toc-section-number">21.8.4</span> Figure 2e – check the model</a></li>
<li><a href="ask1-report.html#figure-2e-inference-from-the-model" id="toc-figure-2e-inference-from-the-model"><span class="toc-section-number">21.8.5</span> Figure 2e – inference from the model</a></li>
<li><a href="ask1-report.html#figure-2e-plot-the-model" id="toc-figure-2e-plot-the-model"><span class="toc-section-number">21.8.6</span> Figure 2e – plot the model</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2f-effect-of-ask1-deletion-on-glucose-infusion-rate" id="toc-figure-2f-effect-of-ask1-deletion-on-glucose-infusion-rate"><span class="toc-section-number">21.9</span> Figure 2f – Effect of ASK1 deletion on glucose infusion rate</a>
<ul>
<li><a href="ask1-report.html#figure-2f-import" id="toc-figure-2f-import"><span class="toc-section-number">21.9.1</span> Figure 2f – import</a></li>
<li><a href="ask1-report.html#figure-2f-exploratory-plots" id="toc-figure-2f-exploratory-plots"><span class="toc-section-number">21.9.2</span> Figure 2f – exploratory plots</a></li>
<li><a href="ask1-report.html#figure-2f-fit-the-model" id="toc-figure-2f-fit-the-model"><span class="toc-section-number">21.9.3</span> Figure 2f – fit the model</a></li>
<li><a href="ask1-report.html#figure-2f-check-the-model" id="toc-figure-2f-check-the-model"><span class="toc-section-number">21.9.4</span> Figure 2f – check the model</a></li>
<li><a href="ask1-report.html#figure-2f-inference" id="toc-figure-2f-inference"><span class="toc-section-number">21.9.5</span> Figure 2f – inference</a></li>
<li><a href="ask1-report.html#figure-2f-plot-the-model" id="toc-figure-2f-plot-the-model"><span class="toc-section-number">21.9.6</span> Figure 2f – plot the model</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2g-effect-of-ask1-deletion-on-tissue-specific-glucose-uptake" id="toc-figure-2g-effect-of-ask1-deletion-on-tissue-specific-glucose-uptake"><span class="toc-section-number">21.10</span> Figure 2g – Effect of ASK1 deletion on tissue-specific glucose uptake</a>
<ul>
<li><a href="ask1-report.html#figure-2g-import" id="toc-figure-2g-import"><span class="toc-section-number">21.10.1</span> Figure 2g – import</a></li>
<li><a href="ask1-report.html#figure-2g-exploratory-plots" id="toc-figure-2g-exploratory-plots"><span class="toc-section-number">21.10.2</span> Figure 2g – exploratory plots</a></li>
<li><a href="ask1-report.html#figure-2g-fit-the-model" id="toc-figure-2g-fit-the-model"><span class="toc-section-number">21.10.3</span> Figure 2g – fit the model</a></li>
<li><a href="ask1-report.html#figure-2g-check-the-model" id="toc-figure-2g-check-the-model"><span class="toc-section-number">21.10.4</span> Figure 2g – check the model</a></li>
<li><a href="ask1-report.html#figure-2g-inference" id="toc-figure-2g-inference"><span class="toc-section-number">21.10.5</span> Figure 2g – inference</a></li>
<li><a href="ask1-report.html#figure-2g-plot-the-model" id="toc-figure-2g-plot-the-model"><span class="toc-section-number">21.10.6</span> Figure 2g – plot the model</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2h" id="toc-figure-2h"><span class="toc-section-number">21.11</span> Figure 2h</a></li>
<li><a href="ask1-report.html#figure-2i-effect-of-ask1-deletion-on-liver-tg" id="toc-figure-2i-effect-of-ask1-deletion-on-liver-tg"><span class="toc-section-number">21.12</span> Figure 2i – Effect of ASK1 deletion on liver TG</a>
<ul>
<li><a href="ask1-report.html#figure-2i-fit-the-model" id="toc-figure-2i-fit-the-model"><span class="toc-section-number">21.12.1</span> Figure 2i – fit the model</a></li>
<li><a href="ask1-report.html#figure-2i-check-the-model" id="toc-figure-2i-check-the-model"><span class="toc-section-number">21.12.2</span> Figure 2i – check the model</a></li>
<li><a href="ask1-report.html#figure-2i-inference" id="toc-figure-2i-inference"><span class="toc-section-number">21.12.3</span> Figure 2i – inference</a></li>
<li><a href="ask1-report.html#figure-2i-plot-the-model" id="toc-figure-2i-plot-the-model"><span class="toc-section-number">21.12.4</span> Figure 2i – plot the model</a></li>
<li><a href="ask1-report.html#figure-2i-report-the-model" id="toc-figure-2i-report-the-model"><span class="toc-section-number">21.12.5</span> Figure 2i – report the model</a></li>
</ul></li>
<li><a href="ask1-report.html#figure-2j" id="toc-figure-2j"><span class="toc-section-number">21.13</span> Figure 2j</a></li>
</ul></li>
<li><a href="count-sim.html#count-sim" id="toc-count-sim"><span class="toc-section-number">22</span> Simulations – Count data (alternatives to a t-test)</a>
<ul>
<li><a href="count-sim.html#use-data-similar-to-figure-6f-from-example-1" id="toc-use-data-similar-to-figure-6f-from-example-1"><span class="toc-section-number">22.1</span> Use data similar to Figure 6f from Example 1</a></li>
<li><a href="count-sim.html#functions" id="toc-functions"><span class="toc-section-number">22.2</span> Functions</a></li>
<li><a href="count-sim.html#simulations" id="toc-simulations"><span class="toc-section-number">22.3</span> Simulations</a>
<ul>
<li><a href="count-sim.html#type-i-pseudo-normal-distribution" id="toc-type-i-pseudo-normal-distribution"><span class="toc-section-number">22.3.1</span> Type I, Pseudo-Normal distribution</a></li>
<li><a href="count-sim.html#type-i-neg-binom-equal-n" id="toc-type-i-neg-binom-equal-n"><span class="toc-section-number">22.3.2</span> Type I, neg binom, equal n</a></li>
<li><a href="count-sim.html#type-i-neg-binom-equal-n-small-theta" id="toc-type-i-neg-binom-equal-n-small-theta"><span class="toc-section-number">22.3.3</span> Type I, neg binom, equal n, small theta</a></li>
<li><a href="count-sim.html#type-i-neg-binom-unequal-n" id="toc-type-i-neg-binom-unequal-n"><span class="toc-section-number">22.3.4</span> Type I, neg binom, unequal n</a></li>
<li><a href="count-sim.html#power-pseudo-normal-distribution-equal-n" id="toc-power-pseudo-normal-distribution-equal-n"><span class="toc-section-number">22.3.5</span> Power, Pseudo-Normal distribution, equal n</a></li>
<li><a href="count-sim.html#power-neg-binom-equal-n" id="toc-power-neg-binom-equal-n"><span class="toc-section-number">22.3.6</span> Power, neg binom, equal n</a></li>
<li><a href="count-sim.html#power-neg-binom-small-theta" id="toc-power-neg-binom-small-theta"><span class="toc-section-number">22.3.7</span> Power, neg binom, small theta</a></li>
<li><a href="count-sim.html#power-neg-binom-unequal-n" id="toc-power-neg-binom-unequal-n"><span class="toc-section-number">22.3.8</span> Power, neg binom, unequal n</a></li>
<li><a href="count-sim.html#power-neg-binom-unequal-n-unequal-theta" id="toc-power-neg-binom-unequal-n-unequal-theta"><span class="toc-section-number">22.3.9</span> Power, neg binom, unequal n, unequal theta</a></li>
<li><a href="count-sim.html#type-1-neg-binom-equal-n-unequal-theta" id="toc-type-1-neg-binom-equal-n-unequal-theta"><span class="toc-section-number">22.3.10</span> Type 1, neg binom, equal n, unequal theta</a></li>
</ul></li>
<li><a href="count-sim.html#save-it-read-it" id="toc-save-it-read-it"><span class="toc-section-number">22.4</span> Save it, Read it</a></li>
<li><a href="count-sim.html#analysis" id="toc-analysis"><span class="toc-section-number">22.5</span> Analysis</a></li>
</ul></li>
<li><a href="appendix-1-getting-started-with-r.html#appendix-1-getting-started-with-r" id="toc-appendix-1-getting-started-with-r">Appendix 1: Getting Started with R</a>
<ul>
<li><a href="appendix-1-getting-started-with-r.html#get-your-computer-ready" id="toc-get-your-computer-ready"><span class="toc-section-number">22.6</span> Get your computer ready</a>
<ul>
<li><a href="appendix-1-getting-started-with-r.html#start-here" id="toc-start-here"><span class="toc-section-number">22.6.1</span> Start here</a></li>
<li><a href="appendix-1-getting-started-with-r.html#install-r" id="toc-install-r"><span class="toc-section-number">22.6.2</span> Install R</a></li>
<li><a href="appendix-1-getting-started-with-r.html#install-r-studio" id="toc-install-r-studio"><span class="toc-section-number">22.6.3</span> Install R Studio</a></li>
<li><a href="appendix-1-getting-started-with-r.html#install-r-markdown-1" id="toc-install-r-markdown-1"><span class="toc-section-number">22.6.4</span> Install R Markdown</a></li>
<li><a href="appendix-1-getting-started-with-r.html#optional-alternative-latex-installations" id="toc-optional-alternative-latex-installations"><span class="toc-section-number">22.6.5</span> (optional) Alternative LaTeX installations</a></li>
</ul></li>
<li><a href="appendix-1-getting-started-with-r.html#start-learning-r-studio" id="toc-start-learning-r-studio"><span class="toc-section-number">22.7</span> Start learning R Studio</a></li>
</ul></li>
<li><a href="appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r.html#appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r" id="toc-appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r">Appendix 2: Online Resources for Getting Started with Statistical Modeling in R</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Statistics for Experimental Biology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="p-values" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> P-values</h1>
<p><em>A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm.</em><a href="appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r.html#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>A <em>p</em>-value <em>is a measure of the compatibility between observed data and the null model</em>. Here, “compatibility” is a probability, specifically, the probability of sampling a test-statistic as or more extreme than the observed test statistic, if all the assumptions used to compute the <em>p</em>-value are true.</p>
<p>To deconstruct what this means, and the implications of the meaning, let’s use the exp2i data from Figure 2i in the study on the browning of white adipose tissue in mice that was introduced in the introductory chapter # <a href="ask1-intro.html#ask1-intro">Analyzing experimental data with a linear model</a>.</p>
<p>Data source: <a href="https://www.nature.com/articles/s41467-020-15483-7" target="_blank">ASK1 inhibits browning of white adipose tissue in obesity</a></p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="p-values.html#cb166-1" aria-hidden="true" tabindex="-1"></a>fig_2i_m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(liver_tg <span class="sc">~</span> treatment, <span class="at">data =</span> fig_2i)</span></code></pre></div>
<p>The chunk above fits a linear model with <code>liver_tg</code> as the response variable and <code>treatment</code> as the single <span class="math inline">\(X\)</span>-varaiable. Figure <a href="p-values.html#fig:p-value-fig2i-fig">6.1</a> is the plot of the modeled means, 95% confidence intervals of the mean, and <em>p</em>-value of the significance test of the effect of treatment on liver triacylglycerol.</p>
<div class="figure"><span style="display:block;" id="fig:p-value-fig2i-fig"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/p-value-fig2i-fig-1.png" alt="UCP1 expression, relative to the mean level in the control group. Mean (circle) and 95% confidence interval (line) are shown. Unadjusted p-values are from linear model with sh-RNA treatment and LPS treatment fully crossed." width="576" />
<p class="caption">
Figure 6.1: UCP1 expression, relative to the mean level in the control group. Mean (circle) and 95% confidence interval (line) are shown. Unadjusted p-values are from linear model with sh-RNA treatment and LPS treatment fully crossed.
</p>
</div>
<p>The coefficients of the model, and the standard error, 95% confidence interval, test-statistic, and <em>p</em>-value of each coefficient are shown in Table <a href="#tab:pvalue-coef-table"><strong>??</strong></a> Recall from Chapter <a href="#analyzing-experimental-data-with-a-linear-model"><strong>??</strong></a> that, with this model, the coefficient for the intercept term is the mean <code>liver_tg</code> for the control group, which is the estimate of the true mean for a mice with functional ASK1 protein. And the coefficient for the treatmentASK1Δadipo term is the difference in means between the knockout and control group, which is the estimate for the true effect of knocking out ASK1 in the adipose tissue. The <em>p</em>-value for this “effect” term is 0.012. How do we interpret this number?</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
t value
</th>
<th style="text-align:right;">
Pr(&gt;|t|)
</th>
<th style="text-align:right;">
2.5 %
</th>
<th style="text-align:right;">
97.5 %
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Intercept)
</td>
<td style="text-align:right;">
61.5
</td>
<td style="text-align:right;">
4.98
</td>
<td style="text-align:right;">
12.3
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
50.4
</td>
<td style="text-align:right;">
72.6
</td>
</tr>
<tr>
<td style="text-align:left;">
treatmentASK1Δadipo
</td>
<td style="text-align:right;">
-21.6
</td>
<td style="text-align:right;">
7.05
</td>
<td style="text-align:right;">
-3.1
</td>
<td style="text-align:right;">
0.012
</td>
<td style="text-align:right;">
-37.3
</td>
<td style="text-align:right;">
-5.9
</td>
</tr>
</tbody>
</table>
<div id="a-p-value-is-the-probability-of-sampling-a-value-as-or-more-extreme-than-the-test-statistic-if-sampling-from-a-null-distribution" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> A <em>p</em>-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution</h2>
<p>The test statistic in the table above is a <em>t</em>-value. For this specific model, this <em>t</em>-value is precisely the <em>t</em>-value one would get if they executed a classical Student <em>t</em>-test on the two groups of liver TG values. Importantly, this is not generally true. For many of the models in this text, a <em>t</em>-value is computed but this is <em>not</em> the <em>t</em>-value that would be computed in a classical <em>t</em>-test.</p>
<p>When we do a <em>t</em>-test, we get a <em>p</em>-value. The probability returned in a <em>t</em>-test is <span class="math inline">\(p = \mathrm{prob}(t \ge t_{obs} | H_0)\)</span>. Read this as “the p-value is the probability of observing a t-value that is greater than or equal to the observed t-value, given the null model is true.” Probability, in this text, is a long run frequency of sampling. The specific probability associated with the effect of treatment on liver TG is the long-run frequency of observing a <em>t</em>-value as big or bigger than the observed <em>t</em>-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a <em>t</em>-value as big or bigger than the observed <em>t</em>-value” and “null is true”.</p>
<p>A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a <em>t</em>-value. Unfortunately, google sheets doesn’t have a <em>t</em>-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now “fill right” or copy and paste these functions into 999 new columns. You now have 1000 <em>t</em>-tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected <em>t</em>-value will also be zero (why?) and the distribution of these 1000 <em>t</em>-values will look normal but the tails are a little fuller. This row of <em>t</em>-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a <em>t</em>-value in your head, say 0.72 (remember that <em>t</em>-values will largely range from about -3 to +3 although the theoretical range is <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. What is the probability of observing a <em>t</em> of 0.72 <em>or bigger</em> if the null is true? Look at the row of <em>t</em>-values! Count the number of <span class="math inline">\(t \ge 0.72\)</span> and then divide by the total number of <em>t</em>-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and <em>t</em>-values).</p>
<p>Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact <em>t</em> is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding <em>t</em> as big, or bigger, than our observed <em>t</em>. Second, the <em>t</em>-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the <span class="math inline">\(t \ge 0.72\)</span> and the <span class="math inline">\(t \le -0.72\)</span> and then add these two counts to compute the frequency of <em>as extreme or more extreme</em> values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count <span class="math inline">\(t \ge 0.72\)</span> but take advantage of the beautiful mathematical properties of the theoretical <em>t</em> distribution, which allows us to compute the frequentist probability (expected long range frequency) given the <em>t</em>-value and the degrees of freedom using the <em>t</em>-distribution.</p>
<p>Now what do I mean with the phrase “null is true”? Most people equate “null is true” with “no difference in means” but the phrase entails much more than this. Effectively, the phrase is short for “all assumptions used to compute” the <em>p</em>-value (see the Sander Greenland quote at the start of this chapter). A <em>p</em>-value is based on modeling the real data with a theoretical sample in which all the values were randomly sampled from the same distribution and the assignment of the individual values to treatment was random. Random sampling from the same distribution has three important consequences. First, random assignment to treatment group means that the expected means of each group are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment <em>also</em> means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. <strong>Here is what is super important about this</strong>: a low <em>p</em>-value is more likely if <em>any</em> one of these consequences is untrue about our data. A low <em>p</em>-value could arise from a difference in true means, or it could arise from a difference in true variances, or it could arise if the <span class="math inline">\(Y\)</span> values are not independent of each other. This is why we need certain assumptions to make a <em>p</em>-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low <em>p</em>-value is evidence of unequal means.</p>
<p>Let’s summarize: A pretty good definition of a <em>p</em>-value is: the long-run frequency of observing a test-statistic as large or larger than the observed statistic, if the null were true. A more succinct way to state this is</p>
<p><span class="math display">\[\begin{equation}
p = \mathrm{prob}(t \ge t_o | H_o)
\end{equation}\]</span></p>
<p>where <em>t</em> is a hypothetically sampled <em>t</em>-value from a null distribution, <span class="math inline">\(t_o\)</span> is the observed <em>t</em>-value, and <span class="math inline">\(H_o\)</span> is the null hypothesis. Part of the null hypothesis is the expected value of the parameter estimated is usually (but not always) zero – this can be called the nil null. For example, if there is no ASK1 deletion effect on liver TG levels, then the expected difference between the means of the control and knockout mice is zero. Or,</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(\bar{Y}_{knockout} - \bar{Y}_{control} | H_o) = 0.0
\end{equation}\]</span></p>
</div>
<div id="pump-your-intuition-creating-a-null-distribution" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Pump your intuition – Creating a null distribution</h2>
<p>The mean <code>liver_tg</code> in the knockout treatment is 21.6 µmol less than the mean <code>liver_tg</code> in the control treatment. This is the measured effect, or the <strong>observed differences in means</strong>. How confident are we in this effect? Certainly, if the researchers did the experiment comparing two control (ASK1F/F) groups, instead of a control and treatment group, they would measure some difference in their means simply because of sampling (that is which mice were sampled for the experiment). So let’s reframe the question: are the observed differences unusually large compared to a distribution of differences that would occur if there were no effect? That is, if the “null were true”. To answer this, we compare our observed difference to this <strong>null distribution</strong>. This comparison gives the probability (a long-run frequency) of “sampling” a random difference from the null distribution of differences that is as large, or larger, than the observed difference.</p>
<p>What is a null distribution? It is the distribution of a statistic (such as a difference in means, or better, a <em>t</em>-value) if the null were true. Here, I hope to pump your intuition by generating a null distribution that is relevant to the ASK1 liver TG data. See if you can understand the script before reading the explanation below.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="p-values.html#cb167-1" aria-hidden="true" tabindex="-1"></a>seed <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb167-2"><a href="p-values.html#cb167-2" aria-hidden="true" tabindex="-1"></a>n_rep <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="co"># number of replicate experiments</span></span>
<span id="cb167-3"><a href="p-values.html#cb167-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-4"><a href="p-values.html#cb167-4" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">mean</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>, liver_tg]) </span>
<span id="cb167-5"><a href="p-values.html#cb167-5" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sd</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>, liver_tg])</span>
<span id="cb167-6"><a href="p-values.html#cb167-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-7"><a href="p-values.html#cb167-7" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>,])</span>
<span id="cb167-8"><a href="p-values.html#cb167-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-9"><a href="p-values.html#cb167-9" aria-hidden="true" tabindex="-1"></a>d_null <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_rep)</span>
<span id="cb167-10"><a href="p-values.html#cb167-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(rep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep){</span>
<span id="cb167-11"><a href="p-values.html#cb167-11" aria-hidden="true" tabindex="-1"></a>  sample_1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb167-12"><a href="p-values.html#cb167-12" aria-hidden="true" tabindex="-1"></a>  sample_2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb167-13"><a href="p-values.html#cb167-13" aria-hidden="true" tabindex="-1"></a>  d_null[rep] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_2) <span class="sc">-</span> <span class="fu">mean</span>(sample_1)</span>
<span id="cb167-14"><a href="p-values.html#cb167-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb167-15"><a href="p-values.html#cb167-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb167-16"><a href="p-values.html#cb167-16" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(d_null)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pvalue-d-null"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/pvalue-d-null-1.png" alt="Null distribution for the difference in means of two samples from the same, inifinitely large population with a true mean and standard deviation equal to the observed mean and standard deviation of the ASK1 liver TG data." width="576" />
<p class="caption">
Figure 6.2: Null distribution for the difference in means of two samples from the same, inifinitely large population with a true mean and standard deviation equal to the observed mean and standard deviation of the ASK1 liver TG data.
</p>
</div>
<p>What have we done above? using the <code>rnorm</code> function, we’ve simulated an infinitely large population of mice that have a distribution of liver TG values similar to that of the mice assigned to the control (ASK1F/F) group. The true mean (<span class="math inline">\(\mu\)</span>) and standard deviation (<span class="math inline">\(\sigma\)</span>) of the simulated TG level are equal to the observed mean and standard deviation of the TG levels of the control mice.</p>
<ol style="list-style-type: decimal">
<li>randomly sample 6 values from this population of simulated liver TG values and assign to <span class="math inline">\(\texttt{sample_1}\)</span>. We sample 6 values because that is the sample size of our control in the experiment.</li>
<li>randomly sample 6 values from <strong>the same</strong> population of simulated liver TG values and assign to <span class="math inline">\(\texttt{sample_1}\)</span>.</li>
<li>compute the difference of the means: <span class="math inline">\(\bar{Y}_{sample\_2} - \bar{Y}_{sample\_1}\)</span>.</li>
<li>repeat 1-3 100,000 times, each time saving the difference in means.</li>
<li>plot the distribution of the 100,000 differences using a histogram</li>
</ol>
<p>The distribution of the differences is a null distribution. Notice that the mode of the null distribution is at zero, and the mean (0.01768) is close to zero (if we had set <span class="math inline">\(n\)</span> to infinity, the mean would be precisely zero). <em>The expected difference between the means of two random samples from the same population is, of course, zero</em>. Don’t gloss over this statement if that is not obvious. The tails extend out to a little more than +20 and -20. What this means is that it would be uncommon to randomly sample a value from this distribution of differences <em>as or more extreme</em> than our observed difference, -21.6. By “more extreme”, I mean any value more negative than -21.6 or more postive than 21.6. So it would be uncommon to sample a value from this distribution whose <em>absolute value</em> is as or more extreme than 21.6. How uncommon would this be?</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="p-values.html#cb168-1" aria-hidden="true" tabindex="-1"></a>diff_obs <span class="ot">&lt;-</span> fig_2i_m1_coef[<span class="st">&quot;treatmentASK1Δadipo&quot;</span>, <span class="st">&quot;Estimate&quot;</span>]</span>
<span id="cb168-2"><a href="p-values.html#cb168-2" aria-hidden="true" tabindex="-1"></a>null_diff_extreme <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">abs</span>(d_null) <span class="sc">&gt;</span> <span class="fu">abs</span>(diff_obs))</span>
<span id="cb168-3"><a href="p-values.html#cb168-3" aria-hidden="true" tabindex="-1"></a>n_extreme <span class="ot">&lt;-</span> <span class="fu">length</span>(null_diff_extreme)</span>
<span id="cb168-4"><a href="p-values.html#cb168-4" aria-hidden="true" tabindex="-1"></a>(<span class="at">p_d_null =</span> n_extreme<span class="sc">/</span>n_rep)</span></code></pre></div>
<pre><code>## [1] 0.00566</code></pre>
<p>In the 100,000 runs, only 566 generated data with an absolute difference as large or larger than 21.6 (an “absolute difference” is the absoute value of the difference). The frequency of differences as large or larger than our observed difference is 0.00566. This frequency is the <em>probability</em> of sampling a difference as or more extreme than the observed difference “under the null”. It is a <em>p</em>-value, but it is not the <em>p</em>-value in the coefficient table. This is because the <em>p</em>-value in the coefficient table is computed from a distribution of <em>t</em>-values, not raw differences. This raises the question, what is a <em>t</em>-distribution, and a <em>t</em>-value, more generally?</p>
</div>
<div id="a-null-distribution-of-t-values-the-t-distribution" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> A null distribution of <em>t</em>-values – the <em>t</em> distribution</h2>
<p>A <em>t</em>-test is a test of differences between two values. These could be</p>
<ol style="list-style-type: decimal">
<li>the difference between the means of two samples (a “two-sample” <em>t</em>-test)</li>
<li>the difference between a mean of a sample and some pre-specified value (a “one-sample” <em>t</em>-test)</li>
<li>the difference between a coefficient from a linear model and zero</li>
</ol>
<p>A <em>t</em>-test compares an observed <em>t</em>-value to a <em>t</em>-distribution. The null distribution introduced above was a distribution of mean differences under the null. A distribution of mean differences under the null is very specific to the mean and standard deviation of the population modeled and the sample size of the experiment. This isn’t generally useful, since it will be unique to every study (at least it wasn’t generally useful prior to the time of fast computers. One could, and some statisticians do, compute <em>p</em>-values using the algorithm above). A <em>t</em>-distribution is a way of transforming a null distribution of mean differences, which is unique to the study, into a distribution that is a function of sample size only.</p>
<p>A <em>t</em>-distribution is a distribution of <em>t</em>-values under the null, where a <em>t</em>-value is a difference <em>standardized by its standard error</em>. For a two-sample <em>t</em>-test, this is</p>
<p><span class="math display" id="eq:t">\[\begin{equation}
t = \frac{\bar{y}_2 - \bar{y}_1}{SE_{\bar{y}_2 - \bar{y}_1}}
\tag{6.1}
\end{equation}\]</span></p>
<p>The numerator is <strong>the effect</strong> while the denominator is the precision of the estimate. Like many test statistics, a <em>t</em>-value is a signal-to-noise ratio – the effect is the signal and the SE of the difference is the noise.</p>
<p>A <em>t</em> distribution looks like a standard, normal distribution, except the tails are heavy, meaning there are more large-ish values than the normal. Like the standard normal distribution, large <em>t</em>-values are unlikely under the null and, therefore, a large <em>t</em> has a low probability – or <em>p</em>-value – under the null.</p>
<p>Looking at the equation for the two-sample <em>t</em>-test above, it is easy to see that three features of an experiment are associated with large <em>t</em> and small <em>p</em>-values: 1) big effect size (the numerator of the equation), 2) small sample standard deviations (which results in small standard errors of the difference, the denominator of equation <a href="p-values.html#eq:t">(6.1)</a>, and 3) large sample size (which results in small standard errors of the difference). As a quick-and-dirty generalization, absolute t-values greater than 3 are uncommon if the null is true.</p>
<p>The <em>p</em>-value for a <em>t</em>-test comes from comparing the observed <em>t</em> to a null <em>t</em> distribution and “counting” the values that are more extreme than the observed <em>t</em>. The <em>p</em>-value is the relative frequency of these more extreme values (relative to the total number of <em>t</em>-values in the distribution). I have “counting” in quotes because nothing is really counted – there are an infinite number of <em>t</em>-values in the <em>t</em>-distribution. Instead, the t-distribution function is integrated to compute the fraction of the total area under the curve with <em>t</em>-values more extreme than the observed value. In a <strong>two-tailed test</strong>, this fraction includes both tails (positive <em>t</em>-values more positive than <span class="math inline">\(|t|_{observed}\)</span> and negative <em>t</em>-values more negative than <span class="math inline">\(-|t|_{observed}\)</span>.</p>
<p>Let’s repeat the simulation of a null distribution of mean differences above but add the computation of the t-value for each replicate comparison in order to generate a null distribution of <em>t</em>-values. Importantly, I’ve also changed bits of the code to more properly think about what a computed <em>t</em>-value is. These changes are:</p>
<ol style="list-style-type: decimal">
<li>I want to think of the first sample as being assigned to “WT” and the second sample as being assigned to “KO”. But, the KO sample is drawn from the same distribution (the same hat of numbers) as the WT sample – this guarantees that there is no difference in expected mean.</li>
<li>I want to think of the observed variances of the WT and KO samples as sampled variances from this fake distribution. Therefore, I give the variance of the fake distribution the average of the observed WT and KO samples. The true (population) standard deviation (<span class="math inline">\(\sigma\)</span>) of the simulated data, then, is the square root of this averaged variance.</li>
</ol>
<p>I show the script, but don’t just cut and paste the code. Spend time thinking about what the each line does. Explore it by copying parts and pasting into console.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="p-values.html#cb170-1" aria-hidden="true" tabindex="-1"></a>seed <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb170-2"><a href="p-values.html#cb170-2" aria-hidden="true" tabindex="-1"></a>n_rep <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">5</span> <span class="co"># number of iterations</span></span>
<span id="cb170-3"><a href="p-values.html#cb170-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-4"><a href="p-values.html#cb170-4" aria-hidden="true" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">mean</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>, liver_tg])</span>
<span id="cb170-5"><a href="p-values.html#cb170-5" aria-hidden="true" tabindex="-1"></a>sd_control <span class="ot">&lt;-</span> <span class="fu">sd</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>, liver_tg])</span>
<span id="cb170-6"><a href="p-values.html#cb170-6" aria-hidden="true" tabindex="-1"></a>sd_knockout <span class="ot">&lt;-</span> <span class="fu">sd</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1Δadipo&quot;</span>, liver_tg])</span>
<span id="cb170-7"><a href="p-values.html#cb170-7" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((sd_control<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> sd_knockout<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">2</span>)</span>
<span id="cb170-8"><a href="p-values.html#cb170-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-9"><a href="p-values.html#cb170-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(fig_2i[treatment <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>,])</span>
<span id="cb170-10"><a href="p-values.html#cb170-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-11"><a href="p-values.html#cb170-11" aria-hidden="true" tabindex="-1"></a>treatment <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;WT&quot;</span>, <span class="st">&quot;KO&quot;</span>), <span class="at">each =</span> n) <span class="sc">%&gt;%</span></span>
<span id="cb170-12"><a href="p-values.html#cb170-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;WT&quot;</span>, <span class="st">&quot;KO&quot;</span>)) <span class="co"># need this for for-loop</span></span>
<span id="cb170-13"><a href="p-values.html#cb170-13" aria-hidden="true" tabindex="-1"></a>t_null <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb170-14"><a href="p-values.html#cb170-14" aria-hidden="true" tabindex="-1"></a>t_null_manual <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb170-15"><a href="p-values.html#cb170-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(rep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_rep){</span>
<span id="cb170-16"><a href="p-values.html#cb170-16" aria-hidden="true" tabindex="-1"></a>  wt_sample <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb170-17"><a href="p-values.html#cb170-17" aria-hidden="true" tabindex="-1"></a>  ko_sample <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma)</span>
<span id="cb170-18"><a href="p-values.html#cb170-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-19"><a href="p-values.html#cb170-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># way no.1 - compute the t-tests using the linear model</span></span>
<span id="cb170-20"><a href="p-values.html#cb170-20" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">c</span>(wt_sample, ko_sample)</span>
<span id="cb170-21"><a href="p-values.html#cb170-21" aria-hidden="true" tabindex="-1"></a>  m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> treatment)</span>
<span id="cb170-22"><a href="p-values.html#cb170-22" aria-hidden="true" tabindex="-1"></a>  t_null[rep] <span class="ot">&lt;-</span> <span class="fu">coef</span>(<span class="fu">summary</span>(m1))[<span class="st">&quot;treatmentKO&quot;</span>, <span class="st">&quot;t value&quot;</span>]</span>
<span id="cb170-23"><a href="p-values.html#cb170-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb170-24"><a href="p-values.html#cb170-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># way no. 2 - compute the t-tests manually!</span></span>
<span id="cb170-25"><a href="p-values.html#cb170-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># check to make sure these are the same as t_null !!!</span></span>
<span id="cb170-26"><a href="p-values.html#cb170-26" aria-hidden="true" tabindex="-1"></a>  diff <span class="ot">&lt;-</span> <span class="fu">mean</span>(ko_sample) <span class="sc">-</span> <span class="fu">mean</span>(wt_sample)</span>
<span id="cb170-27"><a href="p-values.html#cb170-27" aria-hidden="true" tabindex="-1"></a>  se_diff <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sd</span>(ko_sample)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n <span class="sc">+</span> <span class="fu">sd</span>(wt_sample)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>n)</span>
<span id="cb170-28"><a href="p-values.html#cb170-28" aria-hidden="true" tabindex="-1"></a>  t_null_manual[rep] <span class="ot">&lt;-</span> diff<span class="sc">/</span>se_diff</span>
<span id="cb170-29"><a href="p-values.html#cb170-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-30"><a href="p-values.html#cb170-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb170-31"><a href="p-values.html#cb170-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-32"><a href="p-values.html#cb170-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb170-33"><a href="p-values.html#cb170-33" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the null distribution of t-values</span></span>
<span id="cb170-34"><a href="p-values.html#cb170-34" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(t_null)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pvalue-t-null"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/pvalue-t-null-1.png" alt="Null distribution of t-values. The simulation generated 10,000 t-tests with a true null." width="576" />
<p class="caption">
Figure 6.3: Null distribution of t-values. The simulation generated 10,000 t-tests with a true null.
</p>
</div>
<p>Now let’s use this null distribution of <em>t</em>-values to compute a <em>p</em>-value</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="p-values.html#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># what is the p-value?</span></span>
<span id="cb171-2"><a href="p-values.html#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the p-value is the number of t-values in t_null_2 that are as large</span></span>
<span id="cb171-3"><a href="p-values.html#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="co"># or larger than the observed t. Large, negative t-values</span></span>
<span id="cb171-4"><a href="p-values.html#cb171-4" aria-hidden="true" tabindex="-1"></a><span class="co"># are as unlikely under the null as large, positive t-values.</span></span>
<span id="cb171-5"><a href="p-values.html#cb171-5" aria-hidden="true" tabindex="-1"></a><span class="co"># To account for this, we want to use absolute values in our counts</span></span>
<span id="cb171-6"><a href="p-values.html#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="co"># this is a &quot;two-tail test&quot;</span></span>
<span id="cb171-7"><a href="p-values.html#cb171-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-8"><a href="p-values.html#cb171-8" aria-hidden="true" tabindex="-1"></a><span class="co"># first assign the observed t-value</span></span>
<span id="cb171-9"><a href="p-values.html#cb171-9" aria-hidden="true" tabindex="-1"></a>t_obs <span class="ot">&lt;-</span> fig_2i_m1_coef[<span class="st">&quot;treatmentASK1Δadipo&quot;</span>, <span class="st">&quot;t value&quot;</span>]</span>
<span id="cb171-10"><a href="p-values.html#cb171-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-11"><a href="p-values.html#cb171-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-12"><a href="p-values.html#cb171-12" aria-hidden="true" tabindex="-1"></a><span class="co"># now count the number of t-values in t_dis as big or bigger than this</span></span>
<span id="cb171-13"><a href="p-values.html#cb171-13" aria-hidden="true" tabindex="-1"></a><span class="co"># include the observed value as one of these (so add 1 to the count)</span></span>
<span id="cb171-14"><a href="p-values.html#cb171-14" aria-hidden="true" tabindex="-1"></a>count <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">abs</span>(t_null) <span class="sc">&gt;=</span> <span class="fu">abs</span>(t_obs))</span>
<span id="cb171-15"><a href="p-values.html#cb171-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-16"><a href="p-values.html#cb171-16" aria-hidden="true" tabindex="-1"></a><span class="co"># the p-value is the frequency of t_dis &gt;= t_obs, so divide</span></span>
<span id="cb171-17"><a href="p-values.html#cb171-17" aria-hidden="true" tabindex="-1"></a><span class="co"># count by the total number of t-values in the distribution.</span></span>
<span id="cb171-18"><a href="p-values.html#cb171-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Again add one since the observed value counts as a sample</span></span>
<span id="cb171-19"><a href="p-values.html#cb171-19" aria-hidden="true" tabindex="-1"></a>(p_ASK1Δadipo <span class="ot">&lt;-</span> count<span class="sc">/</span>(n_rep))</span></code></pre></div>
<pre><code>## [1] 0.01173</code></pre>
<p>Hey that looks pretty good! Compare this to the <em>p</em>-value in the coefficient table above.</p>
<p>A <em>p</em>-value can be computed by counting the number of simulated <em>t</em>-values, <em>including the observed value</em>, that are equal to or more extreme (in either the positive or negative direction) than the observed <em>t</em>. Including the observed <em>t</em>, there are 1173 values that are more extreme than that observed. An approximate measure of <em>p</em> is this count divided by 100,001 (why is 1 added to the denominator?), which is 0.01173. This simulation-based <em>p</em>-value is very (very!) close to that computed from the observed <em>t</em>-test.</p>
</div>
<div id="p-values-from-the-perspective-of-permutation" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> P-values from the perspective of permutation</h2>
<p>A very intuitive way to think about <em>p</em>-values as a frequency is <strong>random permutation</strong>. A permutation is a re-arrangement of items. If there is an effect of ASK1 deletion on liver TG, then the arrangement of the values in the <code>treatment</code> column matters. If there is no effect of ASK1 deletion on liver TG, then the arrangement of the values in the <code>treatment</code> column does not matter.</p>
<p>Think about the structure of the liver TG data: there are two columns, <code>treatment</code>, which contains the assigned treatment, and <code>liver_tg</code>. The values in the <code>treatment</code> column were randomly assigned prior to the start of the experiment. If there is a negative effect of ASK1 deletion on liver TG, then assginment matters – the values in the <code>liver_tg</code> column for the ASK1Δadipo rows will be smaller than, on average, the values in the ASK1F/F rows. That is, a specific value of a<code>liver_tg</code> is what it is because of the value of <code>treatement</code> in the same row. Assignment to ASK1F/F or ASK1Δadipo changes the expected value of <code>liver_tg</code>. But, if there were no true effect, then assignment to ASK1F/F or ASK1Δadipo does not change the expected value of <code>liver_tg</code>. The expected value of every cell in the <code>liver_tg</code> column would be the same regardless of what is in the <code>treatment</code> column.</p>
<p>In our thought experiment, let’s leave the values in the <code>treatment</code> column be, and just randomly re-arrange or permute the values in the <code>liver_tg</code> column. What is the new expected diference in liver TG between the rows assigned to ASK1F/F and the rows assigned to ASK1Δadipo? The expected difference is Zero. Because the <code>liver_tg</code> values were randomly re-arranged, they <em>cannot</em> be caused by treatment assignment.</p>
<p>A permutation is a random re-arrangement of values in a column. Consider the many thousands of permutations of the values in the <code>liver_tg</code> column. A difference in means can be computed from each of these permuations and a distribution of differences can be generated. Is the observed difference extreme relative to the other values in this distribution? This is a permutation test – it compares an observed statistic to a distribution of the statistic computed over many thousands of permutations.</p>
<p>Let’s create a script for a permutation test</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="p-values.html#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb173-2"><a href="p-values.html#cb173-2" aria-hidden="true" tabindex="-1"></a>n_iter <span class="ot">&lt;-</span> <span class="dv">5000</span> <span class="co"># number of random permutations</span></span>
<span id="cb173-3"><a href="p-values.html#cb173-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-4"><a href="p-values.html#cb173-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> fig_2i[, liver_tg]</span>
<span id="cb173-5"><a href="p-values.html#cb173-5" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> fig_2i[, treatment]</span>
<span id="cb173-6"><a href="p-values.html#cb173-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-7"><a href="p-values.html#cb173-7" aria-hidden="true" tabindex="-1"></a>d_dist_perm <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_iter)</span>
<span id="cb173-8"><a href="p-values.html#cb173-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-9"><a href="p-values.html#cb173-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(iter <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_iter){</span>
<span id="cb173-10"><a href="p-values.html#cb173-10" aria-hidden="true" tabindex="-1"></a>  xbar1 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y[x <span class="sc">==</span> <span class="st">&quot;ASK1F/F&quot;</span>])</span>
<span id="cb173-11"><a href="p-values.html#cb173-11" aria-hidden="true" tabindex="-1"></a>  xbar2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y[x <span class="sc">==</span> <span class="st">&quot;ASK1Δadipo&quot;</span>])</span>
<span id="cb173-12"><a href="p-values.html#cb173-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-13"><a href="p-values.html#cb173-13" aria-hidden="true" tabindex="-1"></a>  d_dist_perm[iter] <span class="ot">&lt;-</span> xbar2 <span class="sc">-</span> xbar1</span>
<span id="cb173-14"><a href="p-values.html#cb173-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb173-15"><a href="p-values.html#cb173-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># permute y</span></span>
<span id="cb173-16"><a href="p-values.html#cb173-16" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> <span class="fu">sample</span>(y, <span class="at">replace=</span><span class="cn">FALSE</span>)</span>
<span id="cb173-17"><a href="p-values.html#cb173-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># note that, when i=1, the first &quot;permutation&quot; is the original arrangement</span></span>
<span id="cb173-18"><a href="p-values.html#cb173-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb173-19"><a href="p-values.html#cb173-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-20"><a href="p-values.html#cb173-20" aria-hidden="true" tabindex="-1"></a><span class="fu">qplot</span>(d_dist_perm)</span></code></pre></div>
<p><img src="Walker-elementary-statistical-modeling-draft_files/figure-html/pvalue-d-dist-perm-1.png" width="576" /></p>
<p>From this distribution of distances generated by random permuation of the response, we can compute a permutation <em>p</em>-value.</p>
<p><code>{rpvalue-d-dist-perm-p } (p_permute &lt;- sum(abs(d_dist_perm) &gt;= abs(d_dist_perm[1]))/n_iter)</code></p>
</div>
<div id="parametric-vs.-non-parametric-statistics" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Parametric vs. non-parametric statistics</h2>
<p>A statistic such as the difference in mean liver TG between ASK1Δadipo and ASK1F/F groups does not have “a” <em>p</em>-value. A <em>p</em>-value is the probability of observing an event <em>given a model of how the event was generated</em>. For the <em>p</em>-value in the coefficient table above, the event is sampling a <em>t</em>-value from a modeled <em>t</em> distribution that is as or more extreme than the observed <em>t</em>-value. The model generating the null distribution of <em>t</em>-values includes random sampling from a distribution that is defined by specific parameters (in this case, a mean and a variance), these parameters define the location and shape of the distribution of values that could be sampled. A <em>p</em>-value computed from a distribution that is defined by a set of parameters is a <strong>parametric</strong> <em>p</em>-value.</p>
<p>For the <em>p</em>-value computed using the permutation test, the event is the probability of of computing a difference of means <em>from a randomly permuted set of <span class="math inline">\(Y\)</span></em> as or more extreme than the observed difference of means. The distribution of differences from the permutated <span class="math inline">\(Y\)</span> data sets was not generated by any of the known distributions (normal, Poisson, binomial, etc.) given a specific value of parameters. Consequently, the permutation <em>p</em>-value is <strong>non-parametric</strong>.</p>
<p>The validity of all <em>p</em>-values depends on a set of model assumptions, which differ from model to model. The permutation <em>p</em>-value has fewer assumptions than a parametric <em>p</em>-value because no distribution is assumed (the permutation <em>p</em>-value is <strong>distribution-free</strong>).</p>
</div>
<div id="frequentist-probability-and-the-interpretation-of-p-values" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> frequentist probability and the interpretation of p-values</h2>
<div id="background-1" class="section level3" number="6.6.1">
<h3><span class="header-section-number">6.6.1</span> Background</h3>
<p>There are at least three different meanings of <strong>probability</strong>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>subjective probability</strong> is the probability that an individual assigns to an event based on prior knowledge and the kinds of information considered reliable evidence. For example, if I asked a sample of students, what is the probability that a 30c homeopathic medicine could clear a <em>Streptococcus</em> infection from your respiratory system, their answers would differ because of variation in their knowledge of basic science, including chemistry and physics, their knowledge of what homeopathic medicines are, and how they weight different kinds of evidence.</p></li>
<li><p><strong>classical probability</strong> is simply one divided by the number of possible unique events. For example, with a six-sided die, there are six possible unique events. The probability of rolling a 2 is <span class="math inline">\(\frac{1}{6}\)</span> and the probability of rolling an odd number is <span class="math inline">\(\frac{1}{2}\)</span>.</p></li>
<li><p><strong>frequentist probability</strong> is based on the concept of <strong>long run frequency</strong>. If I roll a die 10 times, the frequency of rolling a 2 will be approximately <span class="math inline">\(\frac{1}{6}\)</span>. If I roll the die 100 times, the frequency of rolling a two will be closer to <span class="math inline">\(\frac{1}{6}\)</span>. If I roll the die 1000 times, the frequency of rolling the die will be even closer to <span class="math inline">\(\frac{1}{6}\)</span>. So the frequentist definition is the expected frequency given an infinite number of rolls. For events with continous outcomes, a frequentist probability is the long run frquency of <em>observing an outcome equal to or more extreme that that observed</em>.</p></li>
</ol>
</div>
<div id="this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability." class="section level3" number="6.6.2">
<h3><span class="header-section-number">6.6.2</span> This book covers frequentist approaches to statistical modeling and when a probability arises, such as the <em>p</em>-value of a test statistic, this will be a frequentist probability.</h3>
<p>When we do a <em>t</em>-test, we get a <em>p</em>-value. There are several ways to think about this probability. The most compact way is <span class="math inline">\(P(data | null)\)</span>, which is literally read as the probability of the data given the null (or “conditional” on the null), but is really short for <em>the probability of the data, or something more extreme than the data, given that the null hypothesis is true</em>. The “probability of the data” is kinda vague. More specifically, we mean the probability of some statistic about the data such as the difference in means between group A and group B or the <em>t</em>-value associated with this difference. So, a bit more formally, the probability returned in a <em>t</em>-test is <span class="math inline">\(\mathrm{prob}(t \ge t_{obs} | H_0)\)</span>. This is the long run frequency of observing a <em>t</em>-value as big or bigger than the observed <em>t</em>-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a <em>t</em>-value as big or bigger than the observed <em>t</em>-value” and “null is true”.</p>
<p>A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a <em>t</em>-value. Unfortunately, google sheets doesn’t have a <em>t</em>-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now ``fill right’’ or copy and paste these functions into 999 new columns. You now have 1000 <em>t</em> tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected <em>t</em>-value will also be zero (why?) and the distribution of these 1000 <em>t</em>-values will look normal but the tails are a little fuller. This row of <em>t</em>-values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a <em>t</em>-value in your head, say 0.72 (remember that <em>t</em>-values will largely range from about -3 to +3 although the theoretical range is <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. What is the probability of observing a <em>t</em> of 0.72 <em>or bigger</em> if the null is true? Look at the row of <em>t</em>-values! Count the number of <span class="math inline">\(t \ge 0.72\)</span> and then divide by the total number of <em>t</em>-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and <em>t</em>-values).</p>
<p>Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact <em>t</em> is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding <em>t</em> as big, or bigger, than our observed <em>t</em>. Second, the <em>t</em>-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the <span class="math inline">\(t \ge 0.72\)</span> and the <span class="math inline">\(t \le -0.72\)</span> and then add these two counts to compute the frequency of <em>as extreme or more extreme</em> values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count <span class="math inline">\(t \ge 0.72\)</span> but take advantage of the beautiful mathematical properties of the theoretical <em>t</em> distribution, which allows us to compute the frequentist probability (expected long range frequency) given the <em>t</em>-value and the degrees of freedom using the <em>t</em>-distribution.</p>
<p>Now what do I mean with the phrase “null is true”? Most people equate “null is true” with ``no difference in means’’ but the phrase entails much more than this. Effectively, the phrase means that the <em>p</em>-value is based on modeling the real data with a theoretical sample in which all the points were randomly sampled from the same distribution and that the assignment of the individual points to treatment was random. This model means the theoretical sample has three properties: First, random assignment to treatment after sampling from the same distribution means that the expected means are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment after sampling from the same distribution <em>also</em> means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. <strong>Here is what is super important about this</strong>: if we get a really low <em>p</em>-value, any one of these consequences may be untrue about our data, for example it could be that the true means of the two treatment groups really are different, or it could mean it is the variances that differ between the two groups, or it could mean that the data (or technically, the errors) are not independent of each other. This is why we need certain assumptions to make a <em>p</em>-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low <em>p</em>-value is evidence of unequal means.</p>
</div>
<div id="two-interpretations-of-the-p-value" class="section level3" number="6.6.3">
<h3><span class="header-section-number">6.6.3</span> Two interpretations of the <em>p</em>-value</h3>
<p>Since we want to be working scientists who want to use <em>p</em>-values as a tool, we need to know how to interpret (or use) the <em>p</em>-value to make reasonable inferences and how to avoid mis-interpreting the <em>p</em>-value and making unreasonable or even incorrect inferences. Two different interpretations of the <em>p</em>-value arose during the development of frequentist statistics. Ronald Fisher (who developed the bulk of the framework of frequentist statistics) thought of the <em>p</em>-value as a quantitative measure of evidence against the null hypothesis. Jerzy Neyman and Egon Pearson (Neyman-Pearson) thought of the <em>p</em>-value as a qualitative, threshold metric used for decision making – to act as if there is an effect. Modern researchers in biology typically use an interpretation that is an odd hybrid of the two, which often leads to illogical inference. Regardless, understanding the distinction between Fisher and Neyman-Pearson will inform how we write up our results in a manuscript.</p>
<p>Fisher was working in the context of an agricultural experiments, the goal of which was to discover better agricultural practices – how do the yields in these five varieties of crop differ under this agricultural practice? Fisher thought of <em>p</em> as evidence against the null; the smaller the <em>p</em>, the stronger the evidence that the mean of the two sampling distributions differ <em>given all model assumptions are true</em>. Fisher never thought of a single experiment as definitive. Any decision following an experiment is only partly informed by the <em>p</em>-value and Fisher offered no formal rule about what <em>p</em>-value lies on the threshold of this decision.</p>
<p>Neyman-Pearson thought of <em>p</em> as the necessary and sufficient information <em>to make a decision</em> between accepting the null (or at least not rejecting the null) or rejecting the null and accepting an alternative hypothesis. This decision balances two sorts of errors: Type I (false positives), which they called <span class="math inline">\(\alpha\)</span>, and Type II (false negatives), which they called <span class="math inline">\(\beta\)</span>. A false positive occurs when the null is rejected (because <span class="math inline">\(p &lt; \alpha\)</span>) but there is no effect of treatment (the null is true). A false negative occurs when the test fails to reject the null (because <span class="math inline">\(p &gt; \alpha\)</span>) but there actually is an effect (the null is false). <span class="math inline">\(\alpha\)</span> is set by the experimenter and is the long-term frequency (or “rate”) of false positives <strong>when the null is true</strong> that the experimenters are willing to accept.</p>
<p>After setting <span class="math inline">\(\alpha\)</span>, the experimenter designs the experiment to achieve an acceptable rate of <span class="math inline">\(\beta\)</span>. Since <span class="math inline">\(\beta\)</span> is the false negative rate, <span class="math inline">\(1-\beta\)</span> is the rate of <em>not</em> making a false negative error. Or, stated without the double negative, <span class="math inline">\(1-\beta\)</span> is the rate of rejecting the null (“finding an effect”) when there really is an effect. This is called the <strong>power</strong> of the experiment. An experiment with high power will have a low probability of a Type II error. An experiment with low power will have a high probability of a Type II error. Power is partly determined by sample size, the bigger the sample the smaller the <em>p</em>-value, all other things equal (think about why in the context of the formula for the <em>t</em>-value). Power is a function of error variance, both the natural variance and the component added because of measurement error (think about why in the context of the formula for the <em>t</em>-value). Power is also a function of <span class="math inline">\(\alpha\)</span>. If we set a low <span class="math inline">\(\alpha\)</span> (say, <span class="math inline">\(\alpha=0.01\)</span>), the test is conservative. We are more likely to fail to reject the null even if the null is false. A researcher can increase power by increasing sample size, using clever strategies to reduce measurement error, or increasing alpha.</p>
<p>An experimenter sets <span class="math inline">\(\alpha\)</span>, computes the sample size needed to achieve a certain level of power (<span class="math inline">\(1-\beta\)</span>), and then does the experiment. A thoughtful researcher will set <span class="math inline">\(\alpha\)</span> after considering and weighing the pros and cons of different levels of <span class="math inline">\(\alpha\)</span>. If false positives have costly consequences (expense, time, deleterious side-effects), then set <span class="math inline">\(\alpha\)</span> to a low value, such as 0.01 or 0.001. For example, if an initial screen has identified a previously unknown candidate that potentially functions in the focal system of the researcher, then a researcher might decide to set a low <span class="math inline">\(\alpha\)</span> (0.001) in the initial tests of this candidate to avoid devoting time, personnel, and expense to chasing a phantom (a false-positive candidate). If false positives have trivial consequences, then set <span class="math inline">\(\alpha\)</span> to a high value, such as 0.05, or 0.1, or even 0.2. For example, if the initial tests of a candidate in a functional system are cheap and fast to construct, then a researcher might choose to set a high <span class="math inline">\(\alpha\)</span> for the screen that identifies candidates. False positive candidates don’t cost the lab much effort to identify them as false, but missing positive candidates because of a small <span class="math inline">\(\alpha\)</span> (which results in low power) at the screen stage costs the researcher the discovery of a potentially exciting component of the functional system.</p>
<p>In Fisher’s interpretation, there is no <span class="math inline">\(\alpha\)</span>, no <span class="math inline">\(\beta\)</span>, no alternative hypothesis, and no sharp decision rule. Instead, in Fisher, <em>p</em> is a continuous measure of evidence against the null and its value is interpreted subjectively by an informed and knowledgeable expert using additional information to make decisions. Neyman-Pearson rejected Fisher’s conception of <em>p</em> as evidence against the null arguing that a single experimental <em>p</em>-value is too noisy without embedding it into a more formal system of of decision making that maintains long-term type I error rates at <span class="math inline">\(\alpha\)</span>, given a certain power. In Neyman-Pearson, <em>p</em> is compared to a threshold, <span class="math inline">\(\alpha\)</span> and this alone makes the decision. In Neyman-Pearson, <em>p</em> <strong>is not treated as continuous information</strong>. <span class="math inline">\(p=0.00000001\)</span> is no more evidence to use to reject the null than <span class="math inline">\(p=0.049\)</span>.</p>
</div>
<div id="nhst" class="section level3" number="6.6.4">
<h3><span class="header-section-number">6.6.4</span> NHST</h3>
<p>Most biology researchers today interpret <em>p</em> using a combination of Fisher and Neyman-Pearson concepts in what has become known as Null Hypothesis Significance Testing (NHST).</p>
<ol style="list-style-type: decimal">
<li>Nearly all papers in biology either explicitly state something like “P values &lt; 0.05 were considered to be statistically significant” or implicitly use 0.05 as the “level of significance” (<span class="math inline">\(\alpha\)</span>). Comparing a <em>p</em>-value to a pre-defined <span class="math inline">\(\alpha\)</span> is Neyman-Pearson.</li>
<li>Unlike Neyman-Pearson, there is little evidence that researchers are thoughtfully considering the level of <span class="math inline">\(\alpha\)</span> for each experiment. Instead, researchers mindlessly choose <span class="math inline">\(\alpha=0.05\)</span> because this is what everyone else uses.</li>
<li>Unlike Neyman-Pearson, but somewhat in the spirit of Fisher, researchers, journals, and textbooks, advocate polychotomizing a statistically significant <em>p</em> into “significance bins” – three asterisks for <span class="math inline">\(p &lt; 0.001\)</span>, two asterisks for <span class="math inline">\(0.001 &lt; p &lt; 0.01\)</span>, and one asterisk for <span class="math inline">\(0.01 &lt; p &lt; 0.05\)</span>). This is not Neyman-Pearson. Again, Neyman-Pearson developed a system to control the long-run frequency of Type I error, which is controlled by a strict use of <span class="math inline">\(\alpha\)</span>. If an observed <em>p</em>-value is in the *** bin or the * bin is meaningless in a system using Neyman-Pearson. There is only “accept” (<span class="math inline">\(p \ge \alpha\)</span>) or “reject” (<span class="math inline">\(p &lt; \alpha\)</span>).</li>
<li>Many researchers report exact <em>p</em>-values when <span class="math inline">\(p &lt; 0.05\)</span> but “n.s.” (not significant) when <span class="math inline">\(p &gt; 0.05\)</span>. Reporting exact <em>p</em>-values is Fisher. Reporting n.s. is Neyman-Pearson.</li>
<li>Many researchers further polychomotomize the <em>p</em>-value space just above 0.05 by using language such as “marginally significant”.</li>
</ol>
</div>
</div>
<div id="some-major-misconceptions-of-the-p-value" class="section level2" number="6.7">
<h2><span class="header-section-number">6.7</span> Some major misconceptions of the <em>p</em>-value</h2>
<p>Setting the type I error rate <span class="math inline">\(\alpha\)</span> to 0.05 is so pervasive that I’m going to simply use “0.05” instead of “alpha” in discussing misconceptions.</p>
<div id="misconception-p-0.05-means-there-is-no-effect-of-treatment" class="section level3" number="6.7.1">
<h3><span class="header-section-number">6.7.1</span> Misconception: <span class="math inline">\(p &gt; 0.05\)</span> means there is no effect of treatment</h3>
<p>Many researchers believe that if <span class="math inline">\(p &gt; 0.05\)</span> then “there is no effect.” A frequentist hypothesis test cannot show that an effect doesn’t exist, only that the null has a low probablity of producing a test statistic as extreme or more extreme than the observed effect. Even if there is a true effect of treatment, a high <em>p</em>-value can occur because of</p>
<ol style="list-style-type: decimal">
<li>a low signal:noise ratio, where the signal is the true effect size (the magnitude of the true difference in response) and the noise is the combination of intrinsic (biological) and extrinsic (experimental) error.</li>
<li>a small sample size, where small is relative to the sample size necessary for high power.</li>
</ol>
<p>The statement “There is no effect of knockout on glucose tolerance” is not a valid conclusion of a frequentist hypothesis test. The similar statement “We found no effect of knockout on glucose tolerance” is misleading because a frequentist hypothesis test can neither find an effect nor find no effect.</p>
</div>
<div id="misconception-a-p-value-is-repeatable" class="section level3" number="6.7.2">
<h3><span class="header-section-number">6.7.2</span> Misconception: a <em>p</em>-value is repeatable</h3>
<p>Many researchers believe that a <em>p</em>-value is a precise measure – that if the experiment were replicated, a similar <em>p</em> would result. This belief requires at least two misconceptions. First, if the null were true, then <em>any</em> <em>p</em>-value is equally likely. <span class="math inline">\(p=0.00137\)</span> is just as likely as <span class="math inline">\(p=0.492\)</span>. In other words, if the null were true, the <em>p</em>-value is not replicable at all! Second, the <em>p</em>-value is highly dependent on the sample, and can be highly variable among replications, but there is no true <em>p</em>-value, so there can be no estimate or standard error. Let’s explore these.</p>
<div id="the-incredible-inconsistency-of-the-p-value" class="section level4" number="6.7.2.1">
<h4><span class="header-section-number">6.7.2.1</span> The incredible inconsistency of the <em>p</em>-value</h4>
<p>How replicable is the conclusion of an experiment if the <em>p</em>-value for a <em>t</em>-test is 0.03? If our conclusion is based on <span class="math inline">\(p &lt; 0.05\)</span>, then the conclusion is not very replicable. The simulation below shows the results of 15 replicates of an experiment with true power of 40%. There are five “significant” results (one less than expected) but several replicates have very high <em>p</em>-values.</p>
<div class="figure"><span style="display:block;" id="fig:dance-fig"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/dance-fig-1.png" alt="Variability of *p*-values when the power is 0.4" width="576" />
<p class="caption">
Figure 6.4: Variability of <em>p</em>-values when the power is 0.4
</p>
</div>
</div>
<div id="what-is-the-distribution-of-p-values-under-the-null" class="section level4" number="6.7.2.2">
<h4><span class="header-section-number">6.7.2.2</span> What is the distribution of <em>p</em>-values under the null?</h4>
<p>I often ask students, “if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what is the most likely <em>p</em>-value?”. A common answer (although answers are uncommon) is <span class="math inline">\(p = 0.5\)</span>. Sometimes I rephrase the question, if there is no true effect (no difference in means), and we were to repeat an experiment thousands of times, what do you think the distribution of <em>p</em>-values would look like?” The typical answer to this is a the distribtion will look like a normal curve with the peak at 0.5, (presumably the tails abruptly stop at 0 and 1).</p>
<p><img src="Walker-elementary-statistical-modeling-draft_files/figure-html/uniform-1.png" width="576" /></p>
</div>
</div>
<div id="misconception-0.05-is-the-lifetime-rate-of-false-discoveries" class="section level3" number="6.7.3">
<h3><span class="header-section-number">6.7.3</span> Misconception: 0.05 is the lifetime rate of false discoveries</h3>
<p>An important and widespread misconception is that if a researcher consistently uses <span class="math inline">\(\alpha=0.05\)</span>, then the frequency of incorrectly concluding an effect exists, or “discovering” an effect, over the lifetime of the researcher, will be 5%. This is incorrect. <span class="math inline">\(\alpha\)</span> is the rate of false positives <em>in the subset of tests in which the null hypothesis is true</em>. <span class="math inline">\(\alpha\)</span> is the Type I error rate.</p>
<p>Our mental conception of “lifetime rate of false discoveries” is the <strong>False Discovery Rate</strong>, and is the frequency of false positives divided by the frequency of positives (the sum of false and true positives).</p>
<p>To pump or intution about the differences between the Type I error rate and the False Discovery Rate, imagine we test</p>
<ol style="list-style-type: decimal">
<li>1000 null hypotheses over a lifetime</li>
<li>60% are true nulls, this means there are 600 true nulls and 400 true effects</li>
<li>alpha is 5%. This means we expect to find <span class="math inline">\(p \le 0.05\)</span> 30 times (<span class="math inline">\(0.05 \times 600\)</span>) when the null is true</li>
<li>power is 25%. This means we expect to find <span class="math inline">\(p \le 0.05\)</span> 100 times (<span class="math inline">\(0.25 \times 400\)</span>) when the null is false</li>
<li>We have made <span class="math inline">\(30 + 100=130\)</span> “discoveries” (all experiments with <span class="math inline">\(p \le 0.05\)</span>), but</li>
<li>30 of the 130, or 23%, are “false discoveries”. This is the false discovery rate.</li>
</ol>
<p>Think about this. If the null is never true, you cannot have a false discovery – every <span class="math inline">\(p \le 0.05\)</span> is a true discovery (the false discovery rate is 0%). And if the null is always true, every <span class="math inline">\(p &lt; 0.05\)</span> is a false discovery (the false discovery rate is 100%).</p>
</div>
<div id="misconception-a-low-p-value-indicates-an-important-effect" class="section level3" number="6.7.4">
<h3><span class="header-section-number">6.7.4</span> Misconception: a low <em>p</em>-value indicates an important effect</h3>
<p>Many researchers write results as if they believe that a small <em>p</em>-value means the effect is big or important. This may misconception may arise because of the ubiquitous use of “significant” to indicate a small p-value and “very” or “extremely” or “wicked” significant to indicate a really small p-value. Regardless, this is a misconception. A small p-value will usually result when there is high power (but can occur even if power is low) and power is a function of effect size, variability (the standard deviation), and sample size. A small <em>p</em> could result from a large effect size but can also result with a small effect size if the sample size is big enough.</p>
<p>This is easy to simulate (see script below). Let’s model the effect of the genotype of a gene on height</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="p-values.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb174-2"><a href="p-values.html#cb174-2" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb174-3"><a href="p-values.html#cb174-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">4</span></span>
<span id="cb174-4"><a href="p-values.html#cb174-4" aria-hidden="true" tabindex="-1"></a>genotype <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;+/+&quot;</span>, <span class="st">&quot;+/-&quot;</span>, <span class="st">&quot;-/-&quot;</span>)</span>
<span id="cb174-5"><a href="p-values.html#cb174-5" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb174-6"><a href="p-values.html#cb174-6" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> Sigma[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> rho</span>
<span id="cb174-7"><a href="p-values.html#cb174-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">sigma=</span>Sigma)</span>
<span id="cb174-8"><a href="p-values.html#cb174-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</span>
<span id="cb174-9"><a href="p-values.html#cb174-9" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.05</span>)</span>
<span id="cb174-10"><a href="p-values.html#cb174-10" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>beta <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb174-11"><a href="p-values.html#cb174-11" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)</span>
<span id="cb174-12"><a href="p-values.html#cb174-12" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(<span class="fu">summary</span>(fit))</span></code></pre></div>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01
## XX1         0.044304824 0.01154709 3.8368830 1.253725e-04
## XX2         0.048228101 0.01170855 4.1190490 3.835033e-05</code></pre>
</div>
<div id="misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity" class="section level3" number="6.7.5">
<h3><span class="header-section-number">6.7.5</span> Misconception: a low <em>p</em>-value indicates high model fit or high predictive capacity</h3>
<p>On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point averages.” This is incorrect. A p-value is not a measure of the predictive capability of a model because the p-value is a function of the signal, noise (unmodeled error), and <em>sample size</em> while predictive ability is a function of just the signal:noise ratio. If the signal:noise ratio is tiny, the predictive ability is small but the p-value can be tiny if the sample size is large. This is easy to simulate (see script below). The whole-model p-value is exceptionally small (0.00001002) but the relative predictive ability, measured by the <span class="math inline">\(R^2\)</span>, is near zero (0.002).</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="p-values.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb176-2"><a href="p-values.html#cb176-2" aria-hidden="true" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb176-3"><a href="p-values.html#cb176-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">4</span></span>
<span id="cb176-4"><a href="p-values.html#cb176-4" aria-hidden="true" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="dv">2</span>)</span>
<span id="cb176-5"><a href="p-values.html#cb176-5" aria-hidden="true" tabindex="-1"></a>Sigma[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> Sigma[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> rho</span>
<span id="cb176-6"><a href="p-values.html#cb176-6" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(n, <span class="at">mean=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">sigma=</span>Sigma)</span>
<span id="cb176-7"><a href="p-values.html#cb176-7" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(X) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)</span>
<span id="cb176-8"><a href="p-values.html#cb176-8" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="sc">-</span><span class="fl">0.05</span>)</span>
<span id="cb176-9"><a href="p-values.html#cb176-9" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> X<span class="sc">%*%</span>beta <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb176-10"><a href="p-values.html#cb176-10" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)</span>
<span id="cb176-11"><a href="p-values.html#cb176-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6449 -0.6857  0.0148  0.6756  3.6510 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.007473   0.010079   0.741 0.458466    
## XX1          0.044305   0.011547   3.837 0.000125 ***
## XX2         -0.051772   0.011709  -4.422  9.9e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.008 on 9997 degrees of freedom
## Multiple R-squared:  0.0023, Adjusted R-squared:  0.002101 
## F-statistic: 11.52 on 2 and 9997 DF,  p-value: 1.002e-05</code></pre>
</div>
</div>
<div id="what-the-p-value-does-not-mean" class="section level2" number="6.8">
<h2><span class="header-section-number">6.8</span> What the <em>p</em>-value does not mean</h2>
<ol style="list-style-type: decimal">
<li><em>p</em> is not the probability of the null being true. More formally, this probability is <span class="math inline">\(Prob(null | data)\)</span> but our <em>p</em>-value is <span class="math inline">\(P(data | null)\)</span>. These are not the same. <span class="math inline">\(P(null | data)\)</span> is the probability of the null being true given the data. <span class="math inline">\(P(data | null)\)</span> is the probability of our data, or something more extreme than our data, conditional on a true null.</li>
<li><span class="math inline">\(1-p\)</span> is not the probability of the alternative</li>
<li><em>p</em> is not a measure of effect size.</li>
<li><em>p</em> in one experiment is not the same level of evidence against the null as in another experiment</li>
<li><em>p</em> is not a great indicator of which is more likely, H0 or H1.</li>
<li>If one treatment level has <span class="math inline">\(p &lt; 0.05\)</span> and another treatment level has <span class="math inline">\(p &gt; 0.05\)</span>, this is not evidence that the treatment levels have different effects on the outcome.</li>
</ol>
</div>
<div id="on-using-p-values-in-experimental-biology" class="section level2" number="6.9">
<h2><span class="header-section-number">6.9</span> On using <em>p</em>-values in experimental biology</h2>
<p>The flow of experiments in the article <a href="https://www.nature.com/articles/s41586-019-1911-y" target="_blank">GDF15 mediates the effects of metformin on body weight and energy balance</a> is typical of that in many experimental biology studies.</p>
<ol style="list-style-type: decimal">
<li>(Figure 1) A cited observational studying showing an association between metformin and increased blood levels of the peptide hormone GDF15 in humans led to experiments to confirm this association in humans and mice. The positive results inferred from <span class="math inline">\(p &lt; 0.05\)</span> led to follow-up experiments in Figures 2, 3, and 4.</li>
<li>(Figure 2) Experiments with <em>GDF15</em> knockout and <em>GFRAL</em> (the GDF15 receptor) knockout to probe if GDF15 is necessary for the metformin-associated weight loss, food intake reduction, and energy expenditure increase. The positive results inferred from <span class="math inline">\(p &lt; 0.05\)</span> led to the conclusion that GDF15 signaling is a mediator of metformin-induced weight loss via GDF15 signaling of both food intake and energy expenditure.</li>
<li>(Figure 3) Experiments with <em>GDF15</em> knockout and <em>GFRAL</em> (the GDF15 receptor) knockout to probe if GDF15 is necessary for metformin-associated regulation of glucose homeostasis. The negative results inferred from <span class="math inline">\(p &gt; 0.05\)</span> led to the conclusion that GDF15 signaling is a not a mediator of metformin-induced decreases in fasting glucose and insulin.</li>
<li>(Figure 4) Experiments to probe which tissues respond to metformin by upregulating <em>GDF15</em> expression. The positive results inferred from <span class="math inline">\(p &lt; 0.05\)</span> led to follup up, confirmatory experiments using alternative methods and tissue sources. The positive results inferred from <span class="math inline">\(p &lt; 0.05\)</span> in these original and follow up experiments led to the conclusion that metformin upregulates <em>GDF15</em> expression in the small intestine, colon, rectum, and kidney.</li>
</ol>
<p>The researchers are using the <em>p</em>-values as a tool not just to draw conclusions about effects and lack of effects but, importantly to identify follow-up experiments. And positive results (<span class="math inline">\(p &lt; 0.05\)</span>) in this publication will motivate this research group and others to execute follow-up experiments in future studies. Here, and in effectively all other experimental biology papers, <em>p</em>-values are not used in a Neyman-Pearson framework despite the ubiquitous statement “we consider <span class="math inline">\(p &lt; 0.05\)</span> to be significant” in the methods. Instead, smaller p-values seem to give the researchers greater confidence in the conclusion. And, the decision strategy seems to be, if the <em>p</em>-value is at least close to 0.05 and we expect a treatment effect given available information, any <em>p</em> value close to but not less than 0.05 is good enough to act as if there is an effect.</p>
<ul>
<li>abandon p-values and emphasize effect sizes and uncertainty. Problem is biological consequences of effect size is unknown and some assays use units in which effect sizes are meaningless.</li>
<li>abandon p-values in favor or model selection or model averaging</li>
<li>abandon frequentist statistics in favor of Bayesian methods. Not going to happen. Decisions still have to be made.</li>
<li>abandon decisions based on p-values for decisions based on Bayes factors</li>
<li>abandon “signficance”. Worry is that researchers will then claim effects when p = 0.1 or whatev. Who cares?</li>
<li>cost probably more to optimistic p-values due to pseudoreplication or not controlling for FDR or not sufficient replication, so continue as is but do better statistics.</li>
</ul>
</div>
<div id="better-reproducibility" class="section level2" number="6.10">
<h2><span class="header-section-number">6.10</span> Better reproducibility</h2>
<ol style="list-style-type: decimal">
<li>true randomization and selection, blind scoring</li>
<li>knowledge of statistics helps with experimental design</li>
<li>use best practice statistics especially accounting for pseudoreplication and blocking</li>
<li>combine experiments properly</li>
<li>experiments used to make decisions to move forward should 1) replicate experiments and 2) use conservative p-values (ideally results shouldn’t need a formal test)</li>
</ol>
<p>xxx finish section</p>
</div>
<div id="multiple-testing-and-controlling-for-false-positives" class="section level2" number="6.11">
<h2><span class="header-section-number">6.11</span> Multiple testing and controlling for false positives</h2>
<p>Bench biologists compute <em>a bunch</em> of <em>p</em>-values in every paper. For example, in a 2 x 2 factorial experiment (e.g. WT-Chow, WT-HFD, KO-Chow, KO-HFD), there are six differences between means (pairwise comparisons) plus an interaction effect. Researchers typically compute <em>p</em>-values for multiple measured outcomes per experiment. If there are 5 outcomes from a 2 x 2 factorial experiment, researchers might report 35 p-values. The number of potential p-values, can rise very quickly above this if one or more of the variables are measured at multiple time points, such as weekly body weight measurements over 12 weeks, or multiple glucose measurements over the 120 minute duration of a glucose tolerance test. And this is just Figure 1!</p>
<p>If these <em>p</em>-values are used to claim an effect (a discovery) or used to pursue a line of follow-up experiments, there will be multiple false positives. Here, I’ll define a false positive as either False positives lead to non-reproducible research. Researchers want to limit false positives because it is costly in both time and money to pursue lines of research based on a mistaken model of how something works. Exciting, positive results from one study spawn follow-up experiments not just by the original research team but also by other research groups. So, researchers want other researchers to limit false positives. And of course, funding agencies want researchers to limit false positives.</p>
<div id="controlling-the-family-wise-error-rate-when-all-tests-are-testing-the-same-hypothesis" class="section level3" number="6.11.1">
<h3><span class="header-section-number">6.11.1</span> Controlling the family-wise error rate when all tests are testing the same hypothesis</h3>
</div>
</div>
<div id="recommendations" class="section level2" number="6.12">
<h2><span class="header-section-number">6.12</span> Recommendations</h2>
<ol style="list-style-type: decimal">
<li>Simply report the exact <em>p</em>-value, along with a CI of the estimate.</li>
</ol>
<ul>
<li><em>P</em>-values are noisy, there is little reason to report more than two significant digits (report “<span class="math inline">\(p = 0.011\)</span>” not “<span class="math inline">\(p = 0.0108\)</span>”) although some journals recommend more than two significant digits.</li>
<li>For high <em>p</em>-values, report “<span class="math inline">\(p = 0.23\)</span>” not “<span class="math inline">\(p = n.s.\)</span>”.</li>
<li>For small <em>p</em>-values, there is little reason to report more than one significant digit (report “<span class="math inline">\(p = 0.0002\)</span>” not “<span class="math inline">\(p = 0.00018\)</span>”).</li>
<li>For really small p-values, there is little reason to report the exact <em>p</em>-value (report “<span class="math inline">\(p &lt; 0.0001\)</span>” and not “<span class="math inline">\(p = 2.365E-11\)</span>”). Recognize that “really small” is entirely arbitrary. Rafael Irizarry suggested that <em>p</em>-values less than something like the probability of being killed by lightning strike should be reported as “<span class="math inline">\(p &lt; m\)</span>”, where <span class="math inline">\(m\)</span> is the probability of being killed by lightning strike<a href="appendix-2-online-resources-for-getting-started-with-statistical-modeling-in-r.html#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. According Google University, this is 0.00000142 in one year or 0.00033 in one lifetime. This text will use <span class="math inline">\(p &lt; ls\)</span>” for <em>p</em>-values less than 0.0001 – the lifetime probability of being killed by lightning strike in someone that spends too much time indoors analyzing data.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>If <span class="math inline">\(p &lt; 0.05\)</span> (or some other <span class="math inline">\(\alpha\)</span>) do not report this as “significant” – in fact, avoid the word “significant”. In the english language, “significant” implies big or important. Small <em>p</em>-values can result even with trivially small effects if <span class="math inline">\(n\)</span> is big or sample variation is small. The phrase “ASK1 knockout had a significant effect on reducing liver TG (<span class="math inline">\(p = 0.011\)</span>)” is</li>
</ol>
<ul>
<li>potentially misleading, if we interpret “significant” to mean “having a large effect on the regulation of liver TG”,</li>
<li>wrong, if we interpret “significant” to mean “there is an ASK1 knockout effect”.
A low <em>p</em>-value is evidence that the effect of ASK1 knockout is not zero, but I would wager that knocking out any gene expressed in white adipose cells will have <em>some</em> effect (however small) on liver TG.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>If a decision needs to be made (“do we devote time, expense, and personel to pursue this further?”), then a <em>p</em>-value is a useful tool. If <em>p</em> is smaller than say 0.001, this is pretty good evidence that the data is not a fluke of sampling, <strong>as long as we are justifiably confident in all the assumptions that went into computing this <em>p</em>-value</strong>. A replicate experiment with a small <em>p</em>-value is better evidence. If <em>p</em> is closer to 0.01 or 0.05, this is only weak evidence of a fluke because of the sampling variability of <em>p</em>. A replicate experiment with a small <em>p</em>-value is <em>much</em> better evidence.</li>
</ol>
<div id="primary-sources-for-recommendations" class="section level3" number="6.12.1">
<h3><span class="header-section-number">6.12.1</span> Primary sources for recommendations</h3>
<ol style="list-style-type: decimal">
<li><a href="https://link.springer.com/article/10.1007/s10654-016-0149-3">Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations</a>.</li>
<li>“Q: Why do so many colleges and grad schools teach p = 0.05? A: Because that’s still what the scientific community and journal editors use. Q: Why do so many people still use p = 0.05? A: Because that’s what they were taught in college or grad school.” – <a href="https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108">ASA Statement on Statistical Significance and P-Values</a></li>
<li>“We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences.” – <a href="https://www.tandfonline.com/doi/abs/10.1080/00031305.2018.1527253">Abandon Statistical Significance</a></li>
<li>“We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “<span class="math inline">\(p&lt;0.05\)</span>,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way.” – <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913">Moving to a World Beyond “p &lt; 0.05”</a></li>
<li>“We agree, and call for the entire concept of statistical significance to be abandoned.”–<a href="https://www.nature.com/articles/d41586-019-00857-9">Scientists rise up against statistical significance</a></li>
</ol>
</div>
</div>
<div id="problems" class="section level2" number="6.13">
<h2><span class="header-section-number">6.13</span> Problems</h2>
<p>Problem 1 – simulate the distribution of <em>p</em> under the null. There are many ways to do this but a straightforard approach is to</p>
<ol style="list-style-type: decimal">
<li>Create a <span class="math inline">\(2n \times m\)</span> matrix of random normal deviates with mean 0 and sd 1</li>
<li>Do a <em>t</em>-test on each column, with the first <span class="math inline">\(n\)</span> values assigned to one group and the remaining <span class="math inline">\(n\)</span> values assigned to the second group. Save the <em>p</em>-value from each.</li>
<li>Plot a histogram of the <em>p</em>-values.</li>
<li>What is the distribution? What is the most likely value of <em>p</em>?</li>
</ol>
<p>Problem 2 – simulate power. Again, many ways to do this but following up on Problem 1.
1. Create a <span class="math inline">\(2n \times m\)</span> matrix of random normal deviates with mean 0 and sd 1
2. Add an effect to the first <span class="math inline">\(n\)</span> values of each column. Things to think about
a. what is a good effect size to add? The effect/sd ratio, known as Cohen’s d, is a relative (or standardized) measure of effect size. Cohen suggest 0.2, 0.5, and 0.8 as small, medium, and large standardized effects.
b. should the same effect be added to each individual? Yes! It is the random component that captures the individual variation in the response.
3. Do a <em>t</em>-test on each column of the matrix, using the first <span class="math inline">\(n\)</span> values in group 1 and the remaining <span class="math inline">\(n\)</span> values in group 2. Save the p-values for each.
4. Compute the power, the relative frequency <span class="math inline">\(p \le 0.05\)</span>.
5. Repeat with different values of <span class="math inline">\(n\)</span>, effect size, and sd, but only vary one at a time. How does power vary with these three parameters?</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="uncertainty.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="errors-in-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapters/17-p-values.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Walker-elementary-statistical-modeling-draft.pdf", "Walker-elementary-statistical-modeling-draft.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
