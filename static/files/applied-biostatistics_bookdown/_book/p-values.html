<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Elementary Statistical Modeling for Applied Biostatistics</title>
  <meta name="description" content="A first course in statistical modeling for biology students">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Elementary Statistical Modeling for Applied Biostatistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A first course in statistical modeling for biology students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Elementary Statistical Modeling for Applied Biostatistics" />
  
  <meta name="twitter:description" content="A first course in statistical modeling for biology students" />
  

<meta name="author" content="Copyright 2018 Jeffrey A. Walker">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="a-linear-model-with-a-single-categorical-x.html">
<link rel="next" href="two-or-more-categorical-x-factorial-designs.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Elements of Applied Biostatistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html"><i class="fa fa-check"></i><b>1</b> Linear models as statistical models</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#two-specifications-of-a-linear-model-i-the-linear-model-way"><i class="fa fa-check"></i><b>1.1</b> Two specifications of a linear model I: the “linear model” way</a></li>
<li class="chapter" data-level="1.2" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#two-specifications-of-a-linear-model-ii-the-statistical-model-way"><i class="fa fa-check"></i><b>1.2</b> Two specifications of a linear model II: the “statistical model” way</a></li>
<li class="chapter" data-level="1.3" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#statistical-models-are-used-for-prediction-explanation-and-description"><i class="fa fa-check"></i><b>1.3</b> Statistical models are used for prediction, explanation, and description</a></li>
<li class="chapter" data-level="1.4" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#model-fitting"><i class="fa fa-check"></i><b>1.4</b> Model fitting</a></li>
<li class="chapter" data-level="1.5" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#a-mean-is-the-simplest-model"><i class="fa fa-check"></i><b>1.5</b> A mean is the simplest model</a></li>
<li class="chapter" data-level="1.6" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#assumptions-for-inference-with-a-statistical-model"><i class="fa fa-check"></i><b>1.6</b> Assumptions for inference with a statistical model</a></li>
<li class="chapter" data-level="1.7" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#specific-assumptions-for-inference-with-a-linear-model"><i class="fa fa-check"></i><b>1.7</b> Specific assumptions for inference with a linear model</a></li>
<li class="chapter" data-level="1.8" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#statistical-model-or-regression-model"><i class="fa fa-check"></i><b>1.8</b> “Statistical model” or “regression model”?</a></li>
<li class="chapter" data-level="1.9" data-path="linear-models-as-statistical-models.html"><a href="linear-models-as-statistical-models.html#glm-vs.glm-vs.gls"><i class="fa fa-check"></i><b>1.9</b> GLM vs. GLM vs. GLS</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html"><i class="fa fa-check"></i><b>2</b> Organization – R Projects and R Notebooks</a><ul>
<li class="chapter" data-level="2.1" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#importing-packages"><i class="fa fa-check"></i><b>2.1</b> Importing Packages</a></li>
<li class="chapter" data-level="2.2" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#create-an-r-studio-project-for-this-class"><i class="fa fa-check"></i><b>2.2</b> Create an R Studio Project for this Class</a></li>
<li class="chapter" data-level="2.3" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#r-notebooks"><i class="fa fa-check"></i><b>2.3</b> R Notebooks</a><ul>
<li class="chapter" data-level="2.3.1" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#create-an-r-notebook-for-this-chapter"><i class="fa fa-check"></i><b>2.3.1</b> Create an R Notebook for this Chapter</a></li>
<li class="chapter" data-level="2.3.2" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#create-a-setup-chunk"><i class="fa fa-check"></i><b>2.3.2</b> Create a “setup” chunk</a></li>
<li class="chapter" data-level="2.3.3" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#create-a-simple-plot-chunk"><i class="fa fa-check"></i><b>2.3.3</b> Create a “simple plot” chunk</a></li>
<li class="chapter" data-level="2.3.4" data-path="organization-r-projects-and-r-notebooks.html"><a href="organization-r-projects-and-r-notebooks.html#create-more-r-chunks-and-explore-options-and-play-with-r-code"><i class="fa fa-check"></i><b>2.3.4</b> Create more R chunks and explore options and play with R code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html"><i class="fa fa-check"></i><b>3</b> Data – Reading, Writing, and Fake</a><ul>
<li class="chapter" data-level="3.1" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#create-new-notebook-for-this-chapter"><i class="fa fa-check"></i><b>3.1</b> Create new notebook for this chapter</a></li>
<li class="chapter" data-level="3.2" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#importing-data"><i class="fa fa-check"></i><b>3.2</b> Importing Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#excel-file"><i class="fa fa-check"></i><b>3.2.1</b> Excel File</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#text-file"><i class="fa fa-check"></i><b>3.2.2</b> Text File</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#creating-fake-data"><i class="fa fa-check"></i><b>3.3</b> Creating Fake Data</a><ul>
<li class="chapter" data-level="3.3.1" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#continuous-x-fake-observational-data"><i class="fa fa-check"></i><b>3.3.1</b> Continuous X (fake observational data)</a></li>
<li class="chapter" data-level="3.3.2" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#categorical-x-fake-experimental-data"><i class="fa fa-check"></i><b>3.3.2</b> Categorical X (fake experimental data)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#saving-data"><i class="fa fa-check"></i><b>3.4</b> Saving Data</a></li>
<li class="chapter" data-level="3.5" data-path="data-reading-writing-and-fake.html"><a href="data-reading-writing-and-fake.html#problems"><i class="fa fa-check"></i><b>3.5</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><i class="fa fa-check"></i><b>4</b> Variability and Uncertainty (Standard Deviations and Standard Errors)</a><ul>
<li class="chapter" data-level="4.1" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#the-sample-standard-deviation-vs.the-standard-error-of-the-mean"><i class="fa fa-check"></i><b>4.1</b> The sample standard deviation vs. the standard error of the mean</a><ul>
<li class="chapter" data-level="4.1.1" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#sample-standard-deviation"><i class="fa fa-check"></i><b>4.1.1</b> Sample standard deviation</a></li>
<li class="chapter" data-level="4.1.2" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>4.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#using-google-sheets-to-generate-fake-data-to-explore-uncertainty"><i class="fa fa-check"></i><b>4.2</b> Using Google Sheets to generate fake data to explore uncertainty</a><ul>
<li class="chapter" data-level="4.2.1" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#steps"><i class="fa fa-check"></i><b>4.2.1</b> Steps</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#using-r-to-generate-fake-data-to-explore-uncertainty"><i class="fa fa-check"></i><b>4.3</b> Using R to generate fake data to explore uncertainty</a><ul>
<li class="chapter" data-level="4.3.1" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#part-i"><i class="fa fa-check"></i><b>4.3.1</b> part I</a></li>
<li class="chapter" data-level="4.3.2" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#part-ii---means"><i class="fa fa-check"></i><b>4.3.2</b> part II - means</a></li>
<li class="chapter" data-level="4.3.3" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#part-iii---how-do-sd-and-se-change-as-sample-size-n-increases"><i class="fa fa-check"></i><b>4.3.3</b> part III - how do SD and SE change as sample size (n) increases?</a></li>
<li class="chapter" data-level="4.3.4" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#part-iv-generating-fake-data-with-for-loops"><i class="fa fa-check"></i><b>4.3.4</b> Part IV – Generating fake data with “for loops”</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="variability-and-uncertainty-standard-deviations-and-standard-errors.html"><a href="variability-and-uncertainty-standard-deviations-and-standard-errors.html#bootstrapped-standard-errors"><i class="fa fa-check"></i><b>4.4</b> Bootstrapped standard errors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="plotting.html"><a href="plotting.html"><i class="fa fa-check"></i><b>5</b> Plotting</a><ul>
<li class="chapter" data-level="5.1" data-path="plotting.html"><a href="plotting.html#plots-should-be-the-center-of-your-papers-universe"><i class="fa fa-check"></i><b>5.1</b> Plots should be the center of your paper’s universe</a></li>
<li class="chapter" data-level="5.2" data-path="plotting.html"><a href="plotting.html#pretty-good-plots-show-the-data"><i class="fa fa-check"></i><b>5.2</b> Pretty good plots show the data</a></li>
<li class="chapter" data-level="5.3" data-path="plotting.html"><a href="plotting.html#even-better-plots"><i class="fa fa-check"></i><b>5.3</b> Even better plots…</a><ul>
<li class="chapter" data-level="5.3.1" data-path="plotting.html"><a href="plotting.html#let-interaction-plots-be-interaction-plots"><i class="fa fa-check"></i><b>5.3.1</b> Let interaction plots be interaction plots</a></li>
<li class="chapter" data-level="5.3.2" data-path="plotting.html"><a href="plotting.html#even-better-plots-continuedshow-the-model"><i class="fa fa-check"></i><b>5.3.2</b> Even better plots (continued)…Show the model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html"><i class="fa fa-check"></i><b>6</b> A linear model with a single, continous <em>X</em></a><ul>
<li class="chapter" data-level="6.1" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#a-linear-model-with-a-single-continous-x-is-classical-regression"><i class="fa fa-check"></i><b>6.1</b> A linear model with a single, continous <em>X</em> is classical “regression”</a><ul>
<li class="chapter" data-level="6.1.1" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#using-a-linear-model-to-estimate-explanatory-effects"><i class="fa fa-check"></i><b>6.1.1</b> Using a linear model to estimate explanatory effects</a></li>
<li class="chapter" data-level="6.1.2" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#using-a-linear-model-for-prediction"><i class="fa fa-check"></i><b>6.1.2</b> Using a linear model for prediction</a></li>
<li class="chapter" data-level="6.1.3" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#reporting-results"><i class="fa fa-check"></i><b>6.1.3</b> Reporting results</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#working-in-r"><i class="fa fa-check"></i><b>6.2</b> Working in R</a><ul>
<li class="chapter" data-level="6.2.1" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#exploring-the-bivariate-relationship-between-y-and-x"><i class="fa fa-check"></i><b>6.2.1</b> Exploring the bivariate relationship between <em>Y</em> and <em>X</em></a></li>
<li class="chapter" data-level="6.2.2" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#fitting-the-linear-model"><i class="fa fa-check"></i><b>6.2.2</b> Fitting the linear model</a></li>
<li class="chapter" data-level="6.2.3" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#getting-to-know-the-linear-model-the-summary-function"><i class="fa fa-check"></i><b>6.2.3</b> Getting to know the linear model: the <code>summary</code> function</a></li>
<li class="chapter" data-level="6.2.4" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#display-an-alternative-to-summary"><i class="fa fa-check"></i><b>6.2.4</b> display: An alternative to summary</a></li>
<li class="chapter" data-level="6.2.5" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.5</b> Confidence intervals</a></li>
<li class="chapter" data-level="6.2.6" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#how-good-is-our-model"><i class="fa fa-check"></i><b>6.2.6</b> How good is our model?</a></li>
<li class="chapter" data-level="6.2.7" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#exploring-a-lm-object"><i class="fa fa-check"></i><b>6.2.7</b> exploring a lm object</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="a-linear-model-with-a-single-continous-x.html"><a href="a-linear-model-with-a-single-continous-x.html#problems-1"><i class="fa fa-check"></i><b>6.3</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="least-squares-estimation-and-the-decomposition-of-variance.html"><a href="least-squares-estimation-and-the-decomposition-of-variance.html"><i class="fa fa-check"></i><b>7</b> Least Squares Estimation and the Decomposition of Variance</a><ul>
<li class="chapter" data-level="7.1" data-path="least-squares-estimation-and-the-decomposition-of-variance.html"><a href="least-squares-estimation-and-the-decomposition-of-variance.html#ols-regression"><i class="fa fa-check"></i><b>7.1</b> OLS regression</a></li>
<li class="chapter" data-level="7.2" data-path="least-squares-estimation-and-the-decomposition-of-variance.html"><a href="least-squares-estimation-and-the-decomposition-of-variance.html#how-well-does-the-model-fit-the-data-r2-and-variance-explained"><i class="fa fa-check"></i><b>7.2</b> How well does the model fit the data? <span class="math inline">\(R^2\)</span> and “variance explained”</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html"><i class="fa fa-check"></i><b>8</b> A linear model with a single, categorical <em>X</em></a><ul>
<li class="chapter" data-level="8.1" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#a-linear-model-with-a-single-categorical-x-is-the-engine-behind-a-single-factor-one-way-anova-and-a-t-test-is-a-special-case-of-this-model."><i class="fa fa-check"></i><b>8.1</b> A linear model with a single, categorical <em>X</em> is the engine behind a single factor (one-way) ANOVA and a t-test is a special case of this model.</a><ul>
<li class="chapter" data-level="8.1.1" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#table-of-model-coefficients"><i class="fa fa-check"></i><b>8.1.1</b> Table of model coefficients</a></li>
<li class="chapter" data-level="8.1.2" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#the-linear-model"><i class="fa fa-check"></i><b>8.1.2</b> The linear model</a></li>
<li class="chapter" data-level="8.1.3" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#reporting-results-1"><i class="fa fa-check"></i><b>8.1.3</b> Reporting results</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#working-in-r-1"><i class="fa fa-check"></i><b>8.2</b> Working in R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#exploring-the-relationship-between-y-and-x"><i class="fa fa-check"></i><b>8.2.1</b> Exploring the relationship between <em>Y</em> and <em>X</em></a></li>
<li class="chapter" data-level="8.2.2" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#fitting-the-model"><i class="fa fa-check"></i><b>8.2.2</b> Fitting the model</a></li>
<li class="chapter" data-level="8.2.3" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#an-introduction-to-contrasts"><i class="fa fa-check"></i><b>8.2.3</b> An introduction to contrasts</a></li>
<li class="chapter" data-level="8.2.4" data-path="a-linear-model-with-a-single-categorical-x.html"><a href="a-linear-model-with-a-single-categorical-x.html#harrell-plot"><i class="fa fa-check"></i><b>8.2.4</b> Harrell plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="p-values.html"><a href="p-values.html"><i class="fa fa-check"></i><b>9</b> P-values</a><ul>
<li class="chapter" data-level="9.1" data-path="p-values.html"><a href="p-values.html#p-values"><i class="fa fa-check"></i><b>9.1</b> <span class="math inline">\(p\)</span>-values</a></li>
<li class="chapter" data-level="9.2" data-path="p-values.html"><a href="p-values.html#creating-a-null-distribution."><i class="fa fa-check"></i><b>9.2</b> Creating a null distribution.</a><ul>
<li class="chapter" data-level="9.2.1" data-path="p-values.html"><a href="p-values.html#the-null-distribution"><i class="fa fa-check"></i><b>9.2.1</b> the Null Distribution</a></li>
<li class="chapter" data-level="9.2.2" data-path="p-values.html"><a href="p-values.html#t-tests"><i class="fa fa-check"></i><b>9.2.2</b> <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="9.2.3" data-path="p-values.html"><a href="p-values.html#p-values-from-the-perspective-of-permutation"><i class="fa fa-check"></i><b>9.2.3</b> P-values from the perspective of permutation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="p-values.html"><a href="p-values.html#statistical-modeling-instead-of-hypothesis-testing"><i class="fa fa-check"></i><b>9.3</b> Statistical modeling instead of hypothesis testing</a></li>
<li class="chapter" data-level="9.4" data-path="p-values.html"><a href="p-values.html#frequentist-probability-and-the-interpretation-of-p-values"><i class="fa fa-check"></i><b>9.4</b> frequentist probability and the interpretation of p-values</a><ul>
<li class="chapter" data-level="9.4.1" data-path="p-values.html"><a href="p-values.html#background"><i class="fa fa-check"></i><b>9.4.1</b> Background</a></li>
<li class="chapter" data-level="9.4.2" data-path="p-values.html"><a href="p-values.html#this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability."><i class="fa fa-check"></i><b>9.4.2</b> This book covers frequentist approaches to statistical modeling and when a probability arises, such as the <span class="math inline">\(p\)</span>-value of a test statistic, this will be a frequentist probability.</a></li>
<li class="chapter" data-level="9.4.3" data-path="p-values.html"><a href="p-values.html#two-interpretations-of-the-p-value"><i class="fa fa-check"></i><b>9.4.3</b> Two interpretations of the <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="9.4.4" data-path="p-values.html"><a href="p-values.html#nhst"><i class="fa fa-check"></i><b>9.4.4</b> NHST</a></li>
<li class="chapter" data-level="9.4.5" data-path="p-values.html"><a href="p-values.html#some-major-misconceptions-of-the-p-value"><i class="fa fa-check"></i><b>9.4.5</b> Some major misconceptions of the <span class="math inline">\(p\)</span>-value</a></li>
<li class="chapter" data-level="9.4.6" data-path="p-values.html"><a href="p-values.html#recommendations"><i class="fa fa-check"></i><b>9.4.6</b> Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="p-values.html"><a href="p-values.html#problems-2"><i class="fa fa-check"></i><b>9.5</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html"><i class="fa fa-check"></i><b>10</b> Two (or more) Categorical <span class="math inline">\(X\)</span> – Factorial designs</a><ul>
<li class="chapter" data-level="10.1" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#factorial-experiments"><i class="fa fa-check"></i><b>10.1</b> Factorial experiments</a><ul>
<li class="chapter" data-level="10.1.1" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#model-coefficients-an-interaction-effect-is-what-is-leftover-after-adding-the-treatment-effects-to-the-control"><i class="fa fa-check"></i><b>10.1.1</b> Model coefficients: an interaction effect is what is leftover after adding the treatment effects to the control</a></li>
<li class="chapter" data-level="10.1.2" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#what-is-the-biological-meaning-of-an-interaction-effect"><i class="fa fa-check"></i><b>10.1.2</b> What is the biological meaning of an interaction effect?</a></li>
<li class="chapter" data-level="10.1.3" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#what-about-models-with-more-than-two-factors"><i class="fa fa-check"></i><b>10.1.3</b> What about models with more than two factors?</a></li>
<li class="chapter" data-level="10.1.4" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#the-additive-model"><i class="fa fa-check"></i><b>10.1.4</b> The additive model</a></li>
<li class="chapter" data-level="10.1.5" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#contrasts-simple-vs.main-effects"><i class="fa fa-check"></i><b>10.1.5</b> Contrasts – simple vs. main effects</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#reporting-results-2"><i class="fa fa-check"></i><b>10.2</b> Reporting results</a><ul>
<li class="chapter" data-level="10.2.1" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#text-results"><i class="fa fa-check"></i><b>10.2.1</b> Text results</a></li>
<li class="chapter" data-level="10.2.2" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#harrellplot"><i class="fa fa-check"></i><b>10.2.2</b> Harrellplot</a></li>
<li class="chapter" data-level="10.2.3" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#interaction-plots"><i class="fa fa-check"></i><b>10.2.3</b> Interaction plots</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#recommendations-1"><i class="fa fa-check"></i><b>10.3</b> Recommendations</a></li>
<li class="chapter" data-level="10.4" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#working-in-r-2"><i class="fa fa-check"></i><b>10.4</b> Working in R</a></li>
<li class="chapter" data-level="10.5" data-path="two-or-more-categorical-x-factorial-designs.html"><a href="two-or-more-categorical-x-factorial-designs.html#problems-3"><i class="fa fa-check"></i><b>10.5</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="anova-tables.html"><a href="anova-tables.html"><i class="fa fa-check"></i><b>11</b> ANOVA Tables</a><ul>
<li class="chapter" data-level="11.1" data-path="anova-tables.html"><a href="anova-tables.html#summary-of-usage"><i class="fa fa-check"></i><b>11.1</b> Summary of usage</a></li>
<li class="chapter" data-level="11.2" data-path="anova-tables.html"><a href="anova-tables.html#example-a-one-way-anova-using-the-vole-data"><i class="fa fa-check"></i><b>11.2</b> Example: a one-way ANOVA using the vole data</a></li>
<li class="chapter" data-level="11.3" data-path="anova-tables.html"><a href="anova-tables.html#example-a-two-way-anova-using-the-urchin-data"><i class="fa fa-check"></i><b>11.3</b> Example: a two-way ANOVA using the urchin data</a><ul>
<li class="chapter" data-level="11.3.1" data-path="anova-tables.html"><a href="anova-tables.html#how-to-read-an-anova-table"><i class="fa fa-check"></i><b>11.3.1</b> How to read an ANOVA table</a></li>
<li class="chapter" data-level="11.3.2" data-path="anova-tables.html"><a href="anova-tables.html#how-to-read-anova-results-reported-in-the-text"><i class="fa fa-check"></i><b>11.3.2</b> How to read ANOVA results reported in the text</a></li>
<li class="chapter" data-level="11.3.3" data-path="anova-tables.html"><a href="anova-tables.html#better-practice-estimates-and-their-uncertainty"><i class="fa fa-check"></i><b>11.3.3</b> Better practice – estimates and their uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="anova-tables.html"><a href="anova-tables.html#unbalanced-designs"><i class="fa fa-check"></i><b>11.4</b> Unbalanced designs</a><ul>
<li class="chapter" data-level="11.4.1" data-path="anova-tables.html"><a href="anova-tables.html#what-is-going-on-in-unbalanced-anova-type-i-ii-iii-sum-of-squares"><i class="fa fa-check"></i><b>11.4.1</b> What is going on in unbalanced ANOVA? – Type I, II, III sum of squares</a></li>
<li class="chapter" data-level="11.4.2" data-path="anova-tables.html"><a href="anova-tables.html#back-to-interpretation-of-main-effects"><i class="fa fa-check"></i><b>11.4.2</b> Back to interpretation of main effects</a></li>
<li class="chapter" data-level="11.4.3" data-path="anova-tables.html"><a href="anova-tables.html#the-anova-tables-for-type-i-ii-and-iii-sum-of-squares-are-the-same-if-the-design-is-balanced."><i class="fa fa-check"></i><b>11.4.3</b> The anova tables for Type I, II, and III sum of squares are the same if the design is balanced.</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="anova-tables.html"><a href="anova-tables.html#working-in-r-3"><i class="fa fa-check"></i><b>11.5</b> Working in R</a><ul>
<li class="chapter" data-level="11.5.1" data-path="anova-tables.html"><a href="anova-tables.html#type-i-sum-of-squares-in-r"><i class="fa fa-check"></i><b>11.5.1</b> Type I sum of squares in R</a></li>
<li class="chapter" data-level="11.5.2" data-path="anova-tables.html"><a href="anova-tables.html#type-ii-and-iii-sum-of-squares"><i class="fa fa-check"></i><b>11.5.2</b> Type II and III Sum of Squares</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html"><i class="fa fa-check"></i><b>12</b> Adding covariates to a linear model I: ANCOVA</a><ul>
<li class="chapter" data-level="12.1" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#adding-covariates-can-increases-the-precision-of-the-effect-of-interest"><i class="fa fa-check"></i><b>12.1</b> Adding covariates can increases the precision of the effect of interest</a><ul>
<li class="chapter" data-level="12.1.1" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#interaction-effects-with-covariates"><i class="fa fa-check"></i><b>12.1.1</b> Interaction effects with covariates</a></li>
<li class="chapter" data-level="12.1.2" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#add-only-covariates-that-were-measured-before-peaking-at-the-data"><i class="fa fa-check"></i><b>12.1.2</b> Add only covariates that were measured before peaking at the data</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#regression-to-the-mean"><i class="fa fa-check"></i><b>12.2</b> Regression to the mean</a><ul>
<li class="chapter" data-level="12.2.1" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#do-not-use-percent-change-believing-that-percents-account-for-effects-of-initial-weights"><i class="fa fa-check"></i><b>12.2.1</b> Do not use percent change, believing that percents account for effects of initial weights</a></li>
<li class="chapter" data-level="12.2.2" data-path="adding-covariates-to-a-linear-model-i-ancova.html"><a href="adding-covariates-to-a-linear-model-i-ancova.html#do-not-test-for-balance-of-baseline-measures"><i class="fa fa-check"></i><b>12.2.2</b> Do not “test for balance” of baseline measures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html"><i class="fa fa-check"></i><b>13</b> Generalized linear models I: Count data</a><ul>
<li class="chapter" data-level="13.1" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#the-generalized-linear-model"><i class="fa fa-check"></i><b>13.1</b> The generalized linear model</a></li>
<li class="chapter" data-level="13.2" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#count-data-example-number-of-trematode-worm-larvae-in-eyes-of-threespine-stickleback-fish"><i class="fa fa-check"></i><b>13.2</b> Count data example – number of trematode worm larvae in eyes of threespine stickleback fish</a><ul>
<li class="chapter" data-level="13.2.1" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#modeling-strategy"><i class="fa fa-check"></i><b>13.2.1</b> Modeling strategy</a></li>
<li class="chapter" data-level="13.2.2" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#checking-the-model-i-a-normal-q-q-plot"><i class="fa fa-check"></i><b>13.2.2</b> Checking the model I – a Normal Q-Q plot</a></li>
<li class="chapter" data-level="13.2.3" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#checking-the-model-ii-scale-location-plot-for-checking-homoskedasticity"><i class="fa fa-check"></i><b>13.2.3</b> Checking the model II – scale-location plot for checking homoskedasticity</a></li>
<li class="chapter" data-level="13.2.4" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#two-distributions-for-count-data-poisson-and-negative-binomial"><i class="fa fa-check"></i><b>13.2.4</b> Two distributions for count data – Poisson and Negative Binomial</a></li>
<li class="chapter" data-level="13.2.5" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#fitting-a-glm-with-a-poisson-distribution-to-the-worm-data"><i class="fa fa-check"></i><b>13.2.5</b> Fitting a GLM with a Poisson distribution to the worm data</a></li>
<li class="chapter" data-level="13.2.6" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#model-checking-fits-to-count-data"><i class="fa fa-check"></i><b>13.2.6</b> Model checking fits to count data</a></li>
<li class="chapter" data-level="13.2.7" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#fitting-a-glm-with-a-negative-binomial-distribution-to-the-worm-data"><i class="fa fa-check"></i><b>13.2.7</b> Fitting a GLM with a Negative Binomial distribution to the worm data</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#working-in-r-4"><i class="fa fa-check"></i><b>13.3</b> Working in R</a></li>
<li class="chapter" data-level="13.4" data-path="generalized-linear-models-i-count-data.html"><a href="generalized-linear-models-i-count-data.html#problems-4"><i class="fa fa-check"></i><b>13.4</b> Problems</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html"><i class="fa fa-check"></i>Appendix 1: Getting Started with R</a><ul>
<li class="chapter" data-level="13.5" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#get-your-computer-ready"><i class="fa fa-check"></i><b>13.5</b> Get your computer ready</a><ul>
<li class="chapter" data-level="13.5.1" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#install-r"><i class="fa fa-check"></i><b>13.5.1</b> Install R</a></li>
<li class="chapter" data-level="13.5.2" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#install-r-studio"><i class="fa fa-check"></i><b>13.5.2</b> Install R Studio</a></li>
<li class="chapter" data-level="13.5.3" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#resources-for-installing-r-and-r-studio"><i class="fa fa-check"></i><b>13.5.3</b> Resources for installing R and R Studio</a></li>
<li class="chapter" data-level="13.5.4" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#install-latex"><i class="fa fa-check"></i><b>13.5.4</b> Install LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#start-learning"><i class="fa fa-check"></i><b>13.6</b> Start learning</a><ul>
<li class="chapter" data-level="13.6.1" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#start-with-data-camp-introduction-to-r"><i class="fa fa-check"></i><b>13.6.1</b> Start with Data Camp Introduction to R</a></li>
<li class="chapter" data-level="13.6.2" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#then-move-to-introduction-to-r-studio"><i class="fa fa-check"></i><b>13.6.2</b> Then Move to Introduction to R Studio</a></li>
<li class="chapter" data-level="13.6.3" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#develop-your-project-with-an-r-studio-notebook"><i class="fa fa-check"></i><b>13.6.3</b> Develop your project with an R Studio Notebook</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#getting-data-into-r"><i class="fa fa-check"></i><b>13.7</b> Getting Data into R</a></li>
<li class="chapter" data-level="13.8" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#additional-r-learning-resources"><i class="fa fa-check"></i><b>13.8</b> Additional R learning resources</a></li>
<li class="chapter" data-level="13.9" data-path="appendix-1-getting-started-with-r.html"><a href="appendix-1-getting-started-with-r.html#packages-used-extensively-in-this-text"><i class="fa fa-check"></i><b>13.9</b> Packages used extensively in this text</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-2-online-resources-for-getting-started-with-linear-modeling-in-r.html"><a href="appendix-2-online-resources-for-getting-started-with-linear-modeling-in-r.html"><i class="fa fa-check"></i>Appendix 2: Online Resources for Getting Started with Linear Modeling in R</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Elementary Statistical Modeling for Applied Biostatistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="p-values" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> P-values</h1>
<div id="p-values" class="section level2">
<h2><span class="header-section-number">9.1</span> <span class="math inline">\(p\)</span>-values</h2>
<table>
<thead>
<tr class="header">
<th align="left">contrast</th>
<th align="right">estimate</th>
<th align="right">SE</th>
<th align="right">df</th>
<th align="right">lower.CL</th>
<th align="right">upper.CL</th>
<th align="right">t.ratio</th>
<th align="right">p.value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">vitamin_E - control</td>
<td align="right">-89.9</td>
<td align="right">52</td>
<td align="right">93</td>
<td align="right">-194.1</td>
<td align="right">14.3</td>
<td align="right">-1.7</td>
<td align="right">0.090</td>
</tr>
<tr class="even">
<td align="left">vitamin_C - control</td>
<td align="right">-115.1</td>
<td align="right">54</td>
<td align="right">93</td>
<td align="right">-223.2</td>
<td align="right">-6.9</td>
<td align="right">-2.1</td>
<td align="right">0.037</td>
</tr>
<tr class="odd">
<td align="left">vitamin_C - vitamin_E</td>
<td align="right">-25.2</td>
<td align="right">65</td>
<td align="right">93</td>
<td align="right">-154.1</td>
<td align="right">103.8</td>
<td align="right">-0.4</td>
<td align="right">0.699</td>
</tr>
</tbody>
</table>
<p>Let’s use the vole data to introduce the <span class="math inline">\(p\)</span>-value. The table above gives a SE, <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span>-value for each pairwise contrast among the three treatment levels. A typical report (one with several misconceptions) might read</p>
<p>“We found a significant effect of Vitamin C (<span class="math inline">\(t=\)</span> -2.1, <span class="math inline">\(p=\)</span> 0.037) on lifespan, but no effect of vitamin E (<span class="math inline">\(t=\)</span> -1.7, <span class="math inline">\(p=\)</span> 0.09) on lifespan.”</p>
<p>A <span class="math inline">\(p\)</span> value <em>is a continuous measure of evidence against the null</em>. As long as the data approximate the assumptions of the null hypothesis pretty well, a very small <span class="math inline">\(p\)</span>-value, such as 0.002 or 0.0005, is pretty good evidence against the null hypothesis – but does not mean “an effect exists”. To show an effect exists, we should have small <span class="math inline">\(p\)</span>-values in multiple replicates <em>and</em> we should rigorously probe the hypothesis with different experiments that challenge the hypothesis in different ways. A small <span class="math inline">\(p\)</span> is evidence for a research program to move forward with replication and probing. A big <span class="math inline">\(p\)</span>-value, say 0.22 or 0.76, is pretty weak evidence against the null, but does not mean “there is no effect.” If an experiment is well designed, a big <span class="math inline">\(p\)</span> could suggest abandoning any hypotheses that predict biologically consequential effects. Unfortunately, a big <span class="math inline">\(p\)</span> could also reflect a weak experimental design. Between small and big <span class="math inline">\(p\)</span> values, such as 0.009 or 0.011, problems arise. These intermediate <span class="math inline">\(p\)</span>-values beg for replication. A major problem of inference using <span class="math inline">\(p\)</span> values is that there is no sharp boundaries between these three regions. Instead biologists typically use the <span class="math inline">\(p &lt; 0.05\)</span> as a sharp boundary to declare that an effect exists or not.</p>
<p>Okay. so what <em>is</em> a <span class="math inline">\(p\)</span>-value? When we do a <span class="math inline">\(t\)</span>-test, we get a <span class="math inline">\(p\)</span>-value. There are several ways to think about this probability. The most compact way is <span class="math inline">\(P(data | null)\)</span>, which is literally read as the probability of the data given the null (or “conditional” on the null), but is really short for <em>the probability of the data, or something more extreme than the data, given that the null hypothesis is true</em>. The “probability of the data” is kinda vague. More specifically, we mean the probability of some statistic about the data such as the difference in means between group A and group B or the <span class="math inline">\(t\)</span>-value associated with this difference. So, a bit more formally, the probability returned in a <span class="math inline">\(t\)</span>-test is <span class="math inline">\(\mathrm{prob}(t \ge t_{obs} | H_0)\)</span>. This is the long run frequency of observing a <span class="math inline">\(t\)</span>-value as big or bigger than the observed <span class="math inline">\(t\)</span>-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a <span class="math inline">\(t\)</span>-value as big or bigger than the observed <span class="math inline">\(t\)</span>-value” and “null is true”.</p>
<p>A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a <span class="math inline">\(t\)</span>-value. Unfortunately, google sheets doesn’t have a <span class="math inline">\(t\)</span>-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now ``fill right’’ or copy and paste these functions into 999 new columns. You now have 1000 <span class="math inline">\(t\)</span> tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected <span class="math inline">\(t\)</span>-value will also be zero (why?) and the distribution of these 1000 <span class="math inline">\(t\)</span> values will look normal but the tails are a little fuller. This row of <span class="math inline">\(t\)</span> values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a <span class="math inline">\(t\)</span>-value in your head, say 0.72 (remember that <span class="math inline">\(t\)</span> values will largely range from about -3 to +3 although the theoretical range is <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. What is the probability of observing a <span class="math inline">\(t\)</span> of 0.72 <em>or bigger</em> if the null is true? Look at the row of <span class="math inline">\(t\)</span>-values! Count the number of <span class="math inline">\(t \ge 0.72\)</span> and then divide by the total number of <span class="math inline">\(t\)</span>-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and <span class="math inline">\(t\)</span>-values).</p>
<p>Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact <span class="math inline">\(t\)</span> is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding <span class="math inline">\(t\)</span> as big, or bigger, than our observed <span class="math inline">\(t\)</span>. Second, the <span class="math inline">\(t\)</span>-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the <span class="math inline">\(t \ge 0.72\)</span> and the <span class="math inline">\(t \le -0.72\)</span> and then add these two counts to compute the frequency of <em>as extreme or more extreme</em> values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count <span class="math inline">\(t \ge 0.72\)</span> but take advantage of the beautiful mathematical properties of the theoretical <span class="math inline">\(t\)</span> distribution, which allows us to compute the frequentist probability (expected long range frequency) given the <span class="math inline">\(t\)</span>-value and the degrees of freedom using the <span class="math inline">\(t\)</span>-distribution.</p>
<p>Now what do I mean with the phrase “null is true”? Most people equate “null is true” with ``no difference in means’’ but the phrase entails much more than this. Effectively, the phrase means that the <span class="math inline">\(p\)</span>-value is based on modeling the real data with a theoretical sample in which all the points were randomly sampled from the same distribution and that the assignment of the individual points to treatment was random. This model means the theoretical sample has three properties: First, random assignment to treatment after sampling from the same distribution means that the expected means are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment after sampling from the same distribution <em>also</em> means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. <strong>Here is what is super important about this</strong>: if we get a really low <span class="math inline">\(p\)</span>-value, any one of these consequences may be untrue about our data, for example it could be that the true means of the two treatment groups really are different, or it could mean it is the variances that differ between the two groups, or it could mean that the data (or technically, the errors) are not independent of each other. This is why we need certain assumptions to make a <span class="math inline">\(p\)</span>-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low <span class="math inline">\(p\)</span> value is evidence of unequal means.</p>
</div>
<div id="creating-a-null-distribution." class="section level2">
<h2><span class="header-section-number">9.2</span> Creating a null distribution.</h2>
<p>Let’s repeat: A pretty good definition of a <span class="math inline">\(p\)</span>-value is: the long-run frequency of observing a test-statistic as large or larger than the observed statistic, if the null were true. A more succinct way to state this is</p>
<span class="math display">\[\begin{equation}
p = \mathrm{prob}(t \ge t_o | H_o)
\end{equation}\]</span>
<p>where <span class="math inline">\(t\)</span> is a hypothetically sampled <span class="math inline">\(t\)</span>-value from a null distribution, <span class="math inline">\(t_o\)</span> is the observed <span class="math inline">\(t\)</span>-value, and <span class="math inline">\(H_o\)</span> is the null hypothesis. Part of the null hypothesis is the expected value of the parameter estimated is usually (but not always) zero – this can be called the nil null. For example, if there is no vitamin E effect on lifespan, then the expected difference between the means of the control and vitamin E treatment levels is zero. Or,</p>
<span class="math display">\[\begin{equation}
\mathrm{E}(\bar{vitamin_E} - \bar{control} | H_o) = 0.0
\end{equation}\]</span>
<p>let’s plot the data and look at the group means. Below is a strip chart of the vole data with superimposed treatment level means, using the function <code>ggstripchart</code> from the ggpubr package (can you make this?). I’m going to refer to this kind of chart as a “dot plot”, which is what most biology researchers call this type of chart.</p>
<p><img src="Walker-elementary-statistical-modeling-draft_files/figure-html/inference-strip-chart-1.png" width="576" /></p>
<div id="the-null-distribution" class="section level3">
<h3><span class="header-section-number">9.2.1</span> the Null Distribution</h3>
<p>The mean lifespan in the vitamin_E treatment is -89.9 days shorter than the mean lifespan in the control treatment. And, the mean lifespan in the vitamin_E treatment is -115.1 days shorter than the mean lifespan in the control treatment. These are the measured effects, or the <strong>observed differences in means</strong>. How confident are we in these effects? Certainly, if the researchers did the experiment with <em>two</em> control treatment groups, they would measure some difference in their means simply because of finite sampling (more specifically, the many, many random effects that contribute to lifespan will differ between the two control groups). So let’s reframe the question: are the observed differences unusually large compared to a distribution of differences that would occur if there were no effect? That is, if the ``null were true’’. To answer this, we compare our observed difference to this <strong>null distribution</strong>. This comparison gives the probability (a long-run frequency) of “sampling” a random difference from the null distribution of differences that is as large, or larger, than the observed difference.</p>
<p>What is a null distribution? It is the distribution of a statistic (such as a difference in means, or better, a <span class="math inline">\(t\)</span>-value) if the null were true. Here, I am generating a null distribution that is relevant to the cold vole data. See if you can understand the script before reading the explanation below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">seed &lt;-<span class="st"> </span><span class="dv">1</span>
n_iter &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">5</span> <span class="co"># number of iterations</span>
mu &lt;-<span class="st"> </span><span class="kw">mean</span>(vole[treatment<span class="op">==</span><span class="st">&#39;control&#39;</span>, lifespan]) 
sigma &lt;-<span class="st"> </span><span class="kw">sd</span>(vole[treatment<span class="op">==</span><span class="st">&#39;control&#39;</span>, lifespan])
n &lt;-<span class="st"> </span><span class="kw">nrow</span>((vole[treatment<span class="op">==</span><span class="st">&#39;control&#39;</span>,]))
sample1 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>n_iter, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma), <span class="dt">nrow=</span>n) <span class="co"># 100,000 samples (each size n)</span>
sample2 &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>n_iter, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma), <span class="dt">nrow=</span>n) <span class="co"># 100,000 samples</span>
null_dis &lt;-<span class="st"> </span><span class="kw">apply</span>(sample2, <span class="dv">2</span>, mean) <span class="op">-</span><span class="st"> </span><span class="kw">apply</span>(sample1, <span class="dv">2</span>, mean)
<span class="kw">qplot</span>(null_dis)</code></pre></div>
<div class="figure"><span id="fig:null-distribution"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/null-distribution-1.png" alt="Null distribution for an infinitely large data set that looks curiously like the lifespans of the cold-rear voles from the control treatment." width="576" />
<p class="caption">
Figure 9.1: Null distribution for an infinitely large data set that looks curiously like the lifespans of the cold-rear voles from the control treatment.
</p>
</div>
<p>What have we done above? We’ve simulated an infinitely large population of voles that have a distribution of lifespans similar to that of the cold-reared voles assigned to the control group. The mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> of the simulated lifespan are equal to the observed mean and standard deviation of the lifespans of the control voles. Then, the script:</p>
<ol style="list-style-type: decimal">
<li>randomly sample 56 values from this population of simulated lifespans and assign to sample1. We sample 56 values because that is the sample size of our control in the experiment.</li>
<li>randomly sample 56 values from this population of simulated lifespans and assign to sample2.</li>
<li>compute the difference <span class="math inline">\(\bar{Y}_{sample2} - \bar{Y}_{sample1}\)</span>.</li>
<li>repeat 1-3 100,000 times, each time saving the difference in means.</li>
<li>plot the distribution of the 100,000 differences using a histogram</li>
</ol>
<p>The distribution of the differences is a null distribution. Notice that the mode of the null distribution is at zero, and the mean (-0.11584) is close to zero (if we had set <span class="math inline">\(n\)</span> to infinity, the mean would be precisely zero). <em>The expected difference between the means of two random samples from the same population is, of course, zero</em>. Don’t gloss over this statement if that is not obvious. The tails extend out to a little more than +100 and -100. What this means is that it would be rare to randomly sample two sets of data from the same population with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> and find a difference of, say, -257. In fact, in the 100,000 runs, there were no difference as large as |-257| (the absolute value of -257). The minimum and maximum differences sampled over the 100,000 iterations was -187 days and 201 days.</p>
<p>How do our observed differences compare? Let’s focus on vitamin E. The vitamin_E effect is -89.9 days. There are 2110 sampled differences less than the observed value and 2126 greater than the absolute value of the observed value. Together this is 4236 so the frequency of differences from the simulated null distribution that as larger or larger than the observed difference is 0.042 (this compuation includes the observed value in both the numerator and denominator).</p>
</div>
<div id="t-tests" class="section level3">
<h3><span class="header-section-number">9.2.2</span> <span class="math inline">\(t\)</span>-tests</h3>
<p>A <span class="math inline">\(t\)</span>-test is a test of differences between two values. These could be</p>
<ol style="list-style-type: decimal">
<li>the difference between the means of two samples (a “two-sample” <span class="math inline">\(t\)</span>-test)</li>
<li>the difference between a mean of a sample and some pre-specified value (a “one-sample” <span class="math inline">\(t\)</span>-test)</li>
<li>the difference between a coefficient from a linear model and a value (often zero)</li>
</ol>
<p>A <span class="math inline">\(t\)</span>-test compares an observed <span class="math inline">\(t\)</span>-value to a <span class="math inline">\(t\)</span>-distribution. The null distribution introduced above was a distribution of mean differences. This isn’t generally useful, since the distribution of expected mean differences is a function of sample variability (standard deviations) in addition to sample size and, therefore, a mean-difference null distribution will be unique to every study. A <span class="math inline">\(t\)</span>-distribution is a distribution of <span class="math inline">\(t\)</span>-values under the null (statistical jargon for “given the null is true”), where a <span class="math inline">\(t\)</span>-value is a difference standardized by its standard error. Standardizing by a standard deviation (remember that a standard error is an estimate of the statistic’s standard deviation) removes the effect of sample variability on the distribution. A <span class="math inline">\(t\)</span>-distribution, then, is only a function of sample size (or “degrees of freedom”). As <span class="math inline">\(n\)</span> increases a <span class="math inline">\(t\)</span> distribution becomes converges on the standard, normal distribution.</p>
<p>The difference between the mean of the vitamin_E treatment and the control treatment is -89.9. A two-sample <span class="math inline">\(t\)</span>-test of this difference is</p>
<pre><code>## 
##  Two Sample t-test
## 
## data:  vole[treatment == &quot;vitamin_E&quot;, lifespan] and vole[treatment == &quot;control&quot;, lifespan]
## t = -1.6275, df = 75, p-value = 0.1078
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -199.97362   20.14029
## sample estimates:
## mean of x mean of y 
##  413.4762  503.3929</code></pre>
<p>The <span class="math inline">\(p\)</span>-value comes from comparing the observed <span class="math inline">\(t\)</span> to a null <span class="math inline">\(t\)</span> distribution and “counting” the values that are bigger than the observed <span class="math inline">\(t\)</span>. These are counted in both tails, because <span class="math inline">\(p\)</span> is the probability of a <span class="math inline">\(t\)</span> more extreme than the observed value, and <span class="math inline">\(t\)</span> can be more extreme in the negative direction and in the positive direction. We can simulate this with a finite, instead of infinite, null distribution using the t-distribution instead of the distribution of mean differences, as above. I hide the script, but its the same as above, except that the <span class="math inline">\(t\)</span>-value is computed for each simulated experiment and not just the difference in means.</p>
<div class="figure"><span id="fig:null-distribution-t"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/null-distribution-t-1.png" alt="Null distribution of t-values. The simulation generated 100,000 t-tests with a true null." width="576" />
<p class="caption">
Figure 9.2: Null distribution of t-values. The simulation generated 100,000 t-tests with a true null.
</p>
</div>
<p>Hey that looks pretty good! A <span class="math inline">\(p\)</span> value can be computed by counting the number of simulated <span class="math inline">\(t\)</span>-values, <em>including the observed value</em>, that are more extreme (in either the positive or negative direction) than the observed <span class="math inline">\(t\)</span>. Including the observed <span class="math inline">\(t\)</span>, there are 10904 values that are more extreme than that observed. An approximate measure of <span class="math inline">\(p\)</span> is this count divided by 100,001 (why is 1 added to the denominator?), which is 0.1090389. This simulation-based <span class="math inline">\(p\)</span>-value is very (very!) close to that computed from the observed <span class="math inline">\(t\)</span>-test.</p>
</div>
<div id="p-values-from-the-perspective-of-permutation" class="section level3">
<h3><span class="header-section-number">9.2.3</span> P-values from the perspective of permutation</h3>
<p>A very intuitive way to think about <span class="math inline">\(p\)</span>-values is with a permutation test. Consider two of the treatment levels in the vole data, say vitamin E and the vitamin C (I’m bored with the control!). Think about the structure of the dataset: there are two columns, “Treatment”, which contains the assigned treatment, and “Lifespan”. The values in the Treatment column were randomly assigned prior to the start of the experiment. If there is an effect of treatment on lifespan, then assginment matters – the values in the lifespan column for the vitamin E rows will be more or less, on average, than the values in the lifepan column for the vitamin C rows. Or, the lifespan values are what they are <em>because</em> of the values in the treatment column.</p>
<p>Now let’s leave the values in the treatment column be, and just randomly re-arrange or permute the lifespan values. What is the new expected diference in lifespan between the two treatments? Zero, of course! That is, because the lifespans were randomly re-arranged, they cannot be caused by treatment assignment!</p>
<p>A permutation is a random re-arrangement of values in a column. Consider the many thousands of permutations of the values in the lifespan column. A difference in means can be computed from each of these permuations and a distribution of differences can be generated. Is the observed difference extreme relative to the other values in this distribution? This is a permutation test – it compares an observed statistic to a distributin of the statistic computed over many thousands of permutations.</p>
</div>
</div>
<div id="statistical-modeling-instead-of-hypothesis-testing" class="section level2">
<h2><span class="header-section-number">9.3</span> Statistical modeling instead of hypothesis testing</h2>
<p>This chapter is an introduction to a <span class="math inline">\(p\)</span>-value by way of <span class="math inline">\(t\)</span>-tests. I advocate that you analyze <span class="math inline">\(t\)</span>-test like questions using statistical modeling instead of null hypothesis significance testing. The reason is that we learn much more from an estimate of the effect and a CI than from a <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span>-value. But, it is also good to know that a <span class="math inline">\(t\)</span> test is a special case of a linear model, and you can get that <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span> using a statistical modeling approach should your boss want them (and you cannot convince them otherwise). Let’s explore this.</p>
<ol style="list-style-type: decimal">
<li>Using the emmeans package, compute the effects (differences in means) of vitamin E and vitamin C on lifespan, relative to the control, with their 95% CI and the <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span> values for the cold-reared vole data.</li>
<li>Compute a separate <span class="math inline">\(t\)</span>-test of vitamin-E vs. control and vitamin C vs. control.</li>
</ol>
<p>Are the <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span> values the same? No! The reason is that the statistical model had three groups and the SE of the difference was computed from the sample standard deviation of all three groups. Each t-test computes the SE of the difference from only the two groups being compared. In general, the SE computed from all three groups is better because it uses more information. This is one reason to prefer the linear model instead of the separate t-tests.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>To convince yourself that a <span class="math inline">\(t\)</span>-test is a special case as of a linear model, compute the effects of the vitamin E treatment (relative to control) <strong>but exclude the vitamin C data from the model fit</strong>. Now compare the <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span> values with the <span class="math inline">\(t\)</span>-test. These should be the same.</p></li>
<li><p>Now use the default t.test function by deleting “var.equal=TRUE” from the function. Are <span class="math inline">\(t\)</span> and <span class="math inline">\(p\)</span> still equal to those from the statistical model? No! the reason is because the default t.test function uses a modification of the t-test called “Welsch’s t-test”. This test allows for heterogenity of variances. Several sources argue that one should always uses Welsch’s test since it simplifies to the classical t-test when the sample variances are equal. This is true, but only relevant if you’re into <span class="math inline">\(t\)</span>-tests. And, we can model heterogenous variances using a statistical model. We’ll do this in a later chapter.</p></li>
<li><p>Use the function <code>pairwise.t.test</code> to compute all pairwise t.tests among the three treatment levels. Is the <span class="math inline">\(p\)</span>-value for the vitamin_E - control contrast the same as that if using t.test (with var.equal=TRUE) or the statistical model with vitamin_C data excluded? No! The reason is that pairwise.t.test adjusts the p-values for multiple testing as a default.</p></li>
</ol>
<p>Pro tip: Before you use a new R function like t.test or pairwise.t.test, it is really advisable to read the help page and look at the defaults for the parameters! Researchers publish errors because they failed to look closely at what the R function was doing and they think the function is doing something else. Ooops!</p>
</div>
<div id="frequentist-probability-and-the-interpretation-of-p-values" class="section level2">
<h2><span class="header-section-number">9.4</span> frequentist probability and the interpretation of p-values</h2>
<div id="background" class="section level3">
<h3><span class="header-section-number">9.4.1</span> Background</h3>
<p>There are at least three different meanings of <strong>probability</strong>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>subjective probability</strong> is the probability that an individual assigns to an event based on prior knowledge and the kinds of information considered reliable evidence. For example, if I asked a sample of students, what is the probability that a 30c homeopathic medicine could clear a <em>Streptococcus</em> infection from your respiratory system, their answers would differ because of variation in their knowledge of basic science, including chemistry and physics, their knowledge of what homeopathic medicines are, and how they weight different kinds of evidence.</p></li>
<li><p><strong>classical probability</strong> is simply one divided by the number of possible unique events. For example, with a six-sided die, there are six possible unique events. The probability of rolling a 2 is <span class="math inline">\(\frac{1}{6}\)</span> and the probability of rolling an odd number is <span class="math inline">\(\frac{1}{2}\)</span>.</p></li>
<li><p><strong>frequentist probability</strong> is based on the concept of . If I roll a die 10 times, the frequency of rolling a 2 will be approximately <span class="math inline">\(\frac{1}{6}\)</span>. If I roll the die 100 times, the frequency of rolling a two will be closer, but to <span class="math inline">\(\frac{1}{6}\)</span>. If I roll the die 1000 times, the frequency of rolling the die will be even closer to <span class="math inline">\(\frac{1}{6}\)</span>. So the frequentist definition is the expected frequency given an infinite number of rolls. For events with continous outcomes, a frequentist probability is the long run frquency of <em>observing the outcome or one more extreme</em>.</p></li>
</ol>
</div>
<div id="this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability." class="section level3">
<h3><span class="header-section-number">9.4.2</span> This book covers frequentist approaches to statistical modeling and when a probability arises, such as the <span class="math inline">\(p\)</span>-value of a test statistic, this will be a frequentist probability.</h3>
<p>When we do a <span class="math inline">\(t\)</span>-test, we get a <span class="math inline">\(p\)</span>-value. There are several ways to think about this probability. The most compact way is <span class="math inline">\(P(data | null)\)</span>, which is literally read as the probability of the data given the null (or “conditional” on the null), but is really short for <em>the probability of the data, or something more extreme than the data, given that the null hypothesis is true</em>. The “probability of the data” is kinda vague. More specifically, we mean the probability of some statistic about the data such as the difference in means between group A and group B or the <span class="math inline">\(t\)</span>-value associated with this difference. So, a bit more formally, the probability returned in a <span class="math inline">\(t\)</span>-test is <span class="math inline">\(\mathrm{prob}(t \ge t_{obs} | H_0)\)</span>. This is the long run frequency of observing a <span class="math inline">\(t\)</span>-value as big or bigger than the observed <span class="math inline">\(t\)</span>-value (the one you actually got with your data) if the null is true. Let’s parse this into “long run frequency of observing a <span class="math inline">\(t\)</span>-value as big or bigger than the observed <span class="math inline">\(t\)</span>-value” and “null is true”.</p>
<p>A thought experiment: You open a google sheet and insert 12 standard, normal random deviates (so the true mean is zero and the true variance is one) in Column A, rows 1-12. You arbitrarily assign the first six values (rows 1-6) to treatment A and the second six values (rows 7-12) to treatment B. You use the space immediately below these data to compute the mean of treatment A, the mean of treatment B, the difference in means (A - B), and a <span class="math inline">\(t\)</span>-value. Unfortunately, google sheets doesn’t have a <span class="math inline">\(t\)</span>-value function so you’d have to compute this yourself. Or not, since this is a thought experiment. Now ``fill right’’ or copy and paste these functions into 999 new columns. You now have 1000 <span class="math inline">\(t\)</span> tests. The expected value of the difference in means is zero (why?) but the actual values will form a normal distribution about zero. Most will be close to zero (either in the negative or positive direction) but some will be further from zero. The expected <span class="math inline">\(t\)</span>-value will also be zero (why?) and the distribution of these 1000 <span class="math inline">\(t\)</span> values will look normal but the tails are a little fuller. This row of <span class="math inline">\(t\)</span> values is a null distribution, because in generating the data we used the exact same formula for the values assigned to A and the values assigned to B. Now think of a <span class="math inline">\(t\)</span>-value in your head, say 0.72 (remember that <span class="math inline">\(t\)</span> values will largely range from about -3 to +3 although the theoretical range is <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. What is the probability of observing a <span class="math inline">\(t\)</span> of 0.72 <em>or bigger</em> if the null is true? Look at the row of <span class="math inline">\(t\)</span>-values! Count the number of <span class="math inline">\(t \ge 0.72\)</span> and then divide by the total number of <span class="math inline">\(t\)</span>-values in the row (1000) and you have a probability computed as a frequency. But remember the frequentist definition is the long run frequency, or the expected frequency at the limit (when you’ve generated not 1000 or even 1,000,000 but an infinite number of columns and <span class="math inline">\(t\)</span>-values).</p>
<p>Some asides to the thought experiment: First, why “as big or bigger” and not just the probability of the value itself? The reason is that the probability of finding the exact <span class="math inline">\(t\)</span> is 1/infinity, which doesn’t do us much good. So instead we compute the probability of finding <span class="math inline">\(t\)</span> as big, or bigger, than our observed <span class="math inline">\(t\)</span>. Second, the <span class="math inline">\(t\)</span>-test probability described above is a “one-tail probability”. Because a difference can be both in the positive direction and the negative direction, we usually want to count all the <span class="math inline">\(t \ge 0.72\)</span> and the <span class="math inline">\(t \le -0.72\)</span> and then add these two counts to compute the frequency of <em>as extreme or more extreme</em> values. This is called a “two-tailed probability” because we find extremes at both tails of the distribution. Third, we don’t really count <span class="math inline">\(t \ge 0.72\)</span> but take advantage of the beautiful mathematical properties of the theoretical <span class="math inline">\(t\)</span> distribution, which allows us to compute the frequentist probability (expected long range frequency) given the <span class="math inline">\(t\)</span>-value and the degrees of freedom using the <span class="math inline">\(t\)</span>-distribution.</p>
<p>Now what do I mean with the phrase “null is true”? Most people equate “null is true” with ``no difference in means’’ but the phrase entails much more than this. Effectively, the phrase means that the <span class="math inline">\(p\)</span>-value is based on modeling the real data with a theoretical sample in which all the points were randomly sampled from the same distribution and that the assignment of the individual points to treatment was random. This model means the theoretical sample has three properties: First, random assignment to treatment after sampling from the same distribution means that the expected means are the same, or put differently, the expected difference in means between the assigned groups is zero. Second, random assignment to treatment after sampling from the same distribution <em>also</em> means that the expected variances of the two groups are equal. And third, random sampling means that the values of each point are independent – we cannot predict the value of one point knowing information about any other point. <strong>Here is what is super important about this</strong>: if we get a really low <span class="math inline">\(p\)</span>-value, any one of these consequences may be untrue about our data, for example it could be that the true means of the two treatment groups really are different, or it could mean it is the variances that differ between the two groups, or it could mean that the data (or technically, the errors) are not independent of each other. This is why we need certain assumptions to make a <span class="math inline">\(p\)</span>-value meaningful for empirical data. By assuming independent error and homogenous (equal) variances in our two samples, a low <span class="math inline">\(p\)</span> value is evidence of unequal means.</p>
</div>
<div id="two-interpretations-of-the-p-value" class="section level3">
<h3><span class="header-section-number">9.4.3</span> Two interpretations of the <span class="math inline">\(p\)</span>-value</h3>
<p>Since we want to be working scientists who want to use <span class="math inline">\(p\)</span>-values as a tool, we need to know how to interpret (or use) the <span class="math inline">\(p\)</span>-value to make reasonable inferences and how to avoid mis-interpreting the <span class="math inline">\(p\)</span>-value and making unreasonable or even incorrect inferences. Ronald Fisher, the inventor of frequentist statistics, developed an interpretation of the <span class="math inline">\(p\)</span>-value that is probably most useful for academic and applied research programs. Neyman and Pearson (Neyman-Pearson) gave the <span class="math inline">\(p\)</span>-value a different interpretation, one that is probably most useful for industrial quality control. Today’s biology researchers use an interpretation that is an odd hybrid of the two, which often leads to silly inference. Regardless, understanding the distinction between Fisher and Neyman-Pearson will inform how we write up our results in a manuscript. I’ll describe these in the context of the two-sample <span class="math inline">\(t\)</span>-test.</p>
<div id="fishers-interpretation" class="section level4">
<h4><span class="header-section-number">9.4.3.1</span> Fisher’s interpretation</h4>
<p>Fisher was working in the context of an agricultural field station, the goal of which is to discover better agricultural practices. Does this new fertilizer work better than our old fertilizer? This is the context of much of modern biosciences and clinical medicine. Fisher thought of <span class="math inline">\(p\)</span> as evidence against the null; the smaller the <span class="math inline">\(p\)</span> the better the evidence that the means differ, which, in an experimental context, implies a treatment effect. If an experiment results in a large <span class="math inline">\(p\)</span>-value, we can move on and test other fertilizers. If an experiment results in a small <span class="math inline">\(p\)</span>-value, we want to pursue this new fertilizer more. Do more experiments! Fisher never thought of a single experiment as definitive. The decision to move on or pursue is only partly informed by the <span class="math inline">\(p\)</span>-value and Fisher offered no rule about what <span class="math inline">\(p\)</span>-value lies on the threshold of this decision. When pressed, Fisher might say that <span class="math inline">\(p=0.05\)</span> is a reasonable threshold.</p>
</div>
<div id="neyman-pearson-interpretation" class="section level4">
<h4><span class="header-section-number">9.4.3.2</span> Neyman-Pearson interpretation</h4>
<p>Neyman-Pearson thought of <span class="math inline">\(p\)</span> as the necessary and sufficient information to make a decision between accepting the null (or at least not rejecting the null) or rejecting the null and accepting an alternative hypothesis. This decision balances two sorts of errors: Type I (false positives), which they called <span class="math inline">\(\alpha\)</span>, and Type II (false negatives), which they called <span class="math inline">\(\beta\)</span>. A false positive means the null was rejected but there really is no effect. A false negative means that the null was not rejected but there actually is an effect. <span class="math inline">\(\alpha\)</span> is set by the experimenter and is the long-term frequency (or ``rate’’) of false positives <strong>when the null is true</strong> that the experimenters are willing to accept. This is easily understood in the context of manufacturing. I’ve just made a batch of beer that I now need to ship. I sample 10 cans and test the quality against a norm. If <span class="math inline">\(p &lt; \alpha\)</span>, we reject the null in favor of the alternative – something may be wrong with the batch, it differs from the norm. We throw the beer away. If <span class="math inline">\(p &gt; \alpha\)</span>, we do not reject the null, nor the beer! We ship it.</p>
<p>After setting <span class="math inline">\(\alpha\)</span>, the experimenter designs the experiment to achieve an acceptable rate of <span class="math inline">\(\beta\)</span>. Since <span class="math inline">\(\beta\)</span> is the false negative rate then <span class="math inline">\(1-\beta\)</span> is the rate of not making a false negative error, that is, the rate of rejecting the null when there really is an effect. This is called the <strong>power</strong> of the experiment. An experiment with high power will have a low probability of a Type II error. An experiment with low power will have a high probability of a Type II error. Power is partly determined by sample size, the bigger the sample the smaller the <span class="math inline">\(p\)</span>-value, all other things equal (think about why in the context of the formula for the <span class="math inline">\(t\)</span>-value). Power is also a function of <span class="math inline">\(\alpha\)</span>. If we set a low <span class="math inline">\(\alpha\)</span> (say, <span class="math inline">\(\alpha=0.01\)</span>), the test is conservative. We are more likely to fail to reject the null even if the null is false. This is the balance. We want to make sure that we test our batch of beer using enough cans to find a bad batch if it exists, but we don’t want to test too many cans because this is a waste of money. An experimenter sets <span class="math inline">\(\alpha\)</span>, computes the sample size needed to achieve a certain level of power (<span class="math inline">\(1-\beta\)</span>), and then does the experiment.</p>
<p>In Fisher’s interpretation, there is no <span class="math inline">\(\alpha\)</span>, no <span class="math inline">\(\beta\)</span>, no alternative hypothesis, and no sharp decision rule. Instead, in Fisher, <span class="math inline">\(p\)</span> is a continuous measure of evidence against the null and its value is interpreted subjectively by an informed and knowledgeable expert using additional information to make decisions. Neyman-Pearson rejected Fisher’s conception of <span class="math inline">\(p\)</span> as evidence against the null and used <span class="math inline">\(p\)</span> as a tool to make a decision that maintains long-term type I error rates at alpha given a certain power. In Neyman-Pearson, <span class="math inline">\(p\)</span> is compared to a threshold, <span class="math inline">\(\alpha\)</span> and this alone makes the decision. In Neyman-Pearson, <span class="math inline">\(p\)</span> is not treated as continuous information. <span class="math inline">\(p=0.00000001\)</span> is no more evidence to use to reject the null than <span class="math inline">\(p=0.049\)</span>.</p>
</div>
</div>
<div id="nhst" class="section level3">
<h3><span class="header-section-number">9.4.4</span> NHST</h3>
<p>Modern researchers interpret <span class="math inline">\(p\)</span> using a combination of Fisher and Neyman-Pearson concepts in what has become known as Null Hypothesis Significance Testing (NHST). Similar to Neyman-Pearson, a <span class="math inline">\(p\)</span>-value is compared to <span class="math inline">\(\alpha\)</span> but similar to Fisher, many researchers, and many textbooks and statistics software (including base R) trichtomize a statistically significant <span class="math inline">\(p\)</span> into “significance levels” (three asterisks for <span class="math inline">\(p &lt; 0.001\)</span>, two asterisks for <span class="math inline">\(0.001 &lt; p &lt; 0.01\)</span>, and one asterisk for <span class="math inline">\(0.01 &lt; p &lt; 0.05\)</span>) but many researchers also casuallly partition non-significant <span class="math inline">\(p\)</span> values into “marginally signifiant” (or similar) and “not significant”.</p>
</div>
<div id="some-major-misconceptions-of-the-p-value" class="section level3">
<h3><span class="header-section-number">9.4.5</span> Some major misconceptions of the <span class="math inline">\(p\)</span>-value</h3>
<p>Setting the type I error rate <span class="math inline">\(\alpha\)</span> to 0.05 is so pervasive that I’m going to simply use “0.05” instead of “alpha” in discussing misconceptions.</p>
<div id="misconception-p-is-the-probability-that-the-null-is-true-and-1-p-is-probability-that-the-alternative-is-true" class="section level4">
<h4><span class="header-section-number">9.4.5.1</span> Misconception: <span class="math inline">\(p\)</span> is the probability that the null is true <em>and</em> <span class="math inline">\(1-p\)</span> is probability that the alternative is true</h4>
<p>Many researchers believe that if <span class="math inline">\(p &gt; 0.05\)</span> then “there is no effect.” A frequentist hypothesis test cannot show that an effect doesn’t exist, only that the null has a low probablity of producing a test statistic as extreme or more extreme than the observed effect.</p>
<p>Many researchers believe that if <span class="math inline">\(p &lt; 0.05\)</span> then “there is an effect.” Again, a frequentist hypothesis test cannot show that an effect exists, only that the null has a low probablity of producing a test statistic as extreme or more extreme than the observed effect.</p>
<ol style="list-style-type: decimal">
<li>The statement “There is no effect of predators on feeding behavior” is not a valid conclusion of a frequentist hypothesis test.</li>
<li>The statement “We found no effect of predators on feeding behavior” is misleading because a frequentist hypothesis test can neither find an effect nor find no effect.</li>
</ol>
<p>The two errors above are gross misconceptions that are pervasive in the biology literature. A more subtle issue is the belief that a low <span class="math inline">\(p\)</span>-value shows that the researcher’s explanatory hypothesis is correct. For example, researchers believe the result “the prey fish fed 14.2 (95% CI: 9.2, 19.2) minutes shorter in the presence of the predator fish” confirms their hypothesis that prey modulate feeding duration as a function of their ability to assess the risk of predation. Some alternative explanations:</p>
<ol style="list-style-type: decimal">
<li>The predator fish also competes with the prey fish for the prey fish’s food and with less food the prey fish spends less time feeding because it gives up when food density drops below some amount.</li>
<li>The predator fish is introduced to the prey tank by hand and odorant molecules from the researcher’s hands are detected by the prey and the prey reduces feeding duration because of these odorants.</li>
</ol>
<p>Importantly, no single experiment confirms an explanatory hypothesis. Instead, alternative explanations require multiple experiments with different controls to “rigrously probe” the preferred hypothesis.</p>
</div>
<div id="misconception-a-p-value-is-repeatable" class="section level4">
<h4><span class="header-section-number">9.4.5.2</span> Misconception: a <span class="math inline">\(p\)</span>-value is repeatable</h4>
<p>Many researchers believe that a <span class="math inline">\(p\)</span>-value is a precise measure – that if the experiment were replicated, a similar <span class="math inline">\(p\)</span> would result. This belief requires at least two misconceptions. First, if the null were true, then <em>any</em> <span class="math inline">\(p\)</span>-value is equally likely. <span class="math inline">\(p=0.00137\)</span> is just as likely as <span class="math inline">\(p=0.492\)</span>. In other words, if the null were true, the <span class="math inline">\(p\)</span>-value is not replicable at all! Second, the <span class="math inline">\(p\)</span> value is highly dependent on the sample, and can be highly variable among replications, but there is no true <span class="math inline">\(p\)</span>-value, so there can be no estimate or standard error. Let’s explore these.</p>
<p><strong>What is the distribution of <span class="math inline">\(p\)</span>-values under the null?</strong> I often ask students, “if the null were true, what is the most likely <span class="math inline">\(p\)</span>-value?” or “if the null were true, what kind of <span class="math inline">\(p\)</span>-values would we expect, that is what is the expected distribution”. A common answer is <span class="math inline">\(p=0.5\)</span> is the most likely value and something like a normal curve, except the tails abruptly stop at 0 and 1, is the expected distribution.</p>
<p><img src="Walker-elementary-statistical-modeling-draft_files/figure-html/uniform-1.png" width="576" /></p>
<p><strong>The incredible inconsistency of the <span class="math inline">\(p\)</span>-value</strong></p>
<p>How replicable is the conclusion of an experiment if the <span class="math inline">\(p\)</span>-value for a <span class="math inline">\(t\)</span>-test is 0.03? If our conclusion is based on <span class="math inline">\(p &lt; 0.05\)</span>, then the conclusion is not very replicable. The simulation below shows the results of 15 replicates of an experiment with true power of 40%. There are five “significant” results (one less than expected) but several replicates have very high <span class="math inline">\(p\)</span>-values.</p>
<div class="figure"><span id="fig:dance-fig"></span>
<img src="Walker-elementary-statistical-modeling-draft_files/figure-html/dance-fig-1.png" alt="Variability of $p$-values when the power is 0.4" width="576" />
<p class="caption">
Figure 9.3: Variability of <span class="math inline">\(p\)</span>-values when the power is 0.4
</p>
</div>
</div>
<div id="misconception-0.05-is-the-lifetime-rate-of-false-discoveries" class="section level4">
<h4><span class="header-section-number">9.4.5.3</span> Misconception: 0.05 is the lifetime rate of false discoveries</h4>
<p>An important and widespread misconception is that if a researcher consistently uses <span class="math inline">\(\alpha=0.05\)</span>, then the frequency of incorrectly concluding an effect exists, or “discovering” an effect, over the lifetime of the researcher, will be 5%. This is incorrect. This “false discovery rate” is the frequency of false positives divided by the frequency of positives (the sum of false and true positives). This differs from the the Type I error rate, which is the frequency of false positives divided by the frequency of tests <em>in which the null is true</em>.</p>
<p>Imagine we test</p>
<ol style="list-style-type: decimal">
<li>1000 null hypotheses over a lifetime</li>
<li>60% are true nulls, this means there are 600 true nulls and 400 true effects</li>
<li>alpha is 5%. This means we expect to find <span class="math inline">\(p \le 0.05\)</span> 30 times (<span class="math inline">\(0.05 \times 600\)</span>) when the null is true</li>
<li>power is 25%. This means we expect to find <span class="math inline">\(p \le 0.05\)</span> 100 times (<span class="math inline">\(0.25 \times 400\)</span>) when the null is false</li>
<li>We have made <span class="math inline">\(30 + 100=130\)</span> “discoveries” (all experiments with <span class="math inline">\(p \le 0.05\)</span>), but</li>
<li>30 of the 130, or 23%, are “false discoveries”.</li>
</ol>
<p>That is 11.7% is the “false discovery rate”.</p>
<p>Think about this. If the null is never true, you cannot have a false discovery–every <span class="math inline">\(p \le 0.05\)</span> is a true discovery (the false discovery rate is 0%). And if the null is always true, every <span class="math inline">\(p &lt; 0.05\)</span> is a false discovery (the false discovery rate is 100%).</p>
</div>
<div id="misconception-a-low-p-value-indicates-an-important-effect" class="section level4">
<h4><span class="header-section-number">9.4.5.4</span> Misconception: a low <span class="math inline">\(p\)</span>-value indicates an important effect</h4>
<p>Many researchers write results as if they believe that a small <span class="math inline">\(p\)</span>-value means the effect is big or important. This may misconception may arise because of the ubiquitous use of “significant” to indicate a small p-value and “very” or “extremely” or “wicked” significant to indicate a really small p-value. Regardless, this is a misconception. A small p-value will usually result when there is high power (but can occur even if power is low) and power is a function of effect size, variability (the standard deviation), and sample size. A small <span class="math inline">\(p\)</span> could result from a large effect size but can also result with a small effect size if the sample size is big enough.</p>
<p>This is easy to simulate (see script below). Let’s model the effect of the genotype of a gene on height</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
rho &lt;-<span class="st"> </span><span class="fl">0.5</span>
n &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>
genotype &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;+/+&quot;</span>, <span class="st">&quot;+/-&quot;</span>, <span class="st">&quot;-/-&quot;</span>)
Sigma &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="dv">2</span>)
Sigma[<span class="dv">1</span>,<span class="dv">2</span>] &lt;-<span class="st"> </span>Sigma[<span class="dv">2</span>,<span class="dv">1</span>] &lt;-<span class="st"> </span>rho
X &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(n, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">sigma=</span>Sigma)
<span class="kw">colnames</span>(X) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)
beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.05</span>)
y &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">coefficients</span>(<span class="kw">summary</span>(fit))</code></pre></div>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01
## XX1         0.044304824 0.01154709 3.8368830 1.253725e-04
## XX2         0.048228101 0.01170855 4.1190490 3.835033e-05</code></pre>
</div>
<div id="misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity" class="section level4">
<h4><span class="header-section-number">9.4.5.5</span> Misconception: a low <span class="math inline">\(p\)</span>-value indicates high model fit or high predictive capacity</h4>
<p>On page 606, of Lock et al “Statistics: Unlocking the Power of Data”, the authors state in item D “The p-value from the ANOVA table is 0.000 so the model as a whole is effective at predicting grade point averages.” This is incorrect. A p-value is not a measure of the predictive capacity of a model because the p-value is a function of 1) signal, 2) noise (unmodeled error), and 3) sample size while predictive capacity is a function of the signal:noise ratio. If the signal:noise ratio is tiny, the predictive capacity is small but the p-value can be tiny if the sample size is large. This is easy to simulate (see script below). The whole-model p-value is exceptionally small (0.00001002) but the relative predictive ability, measured by the <span class="math inline">\(R^2\)</span>, is near zero (0.002).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mvtnorm)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
rho &lt;-<span class="st"> </span><span class="fl">0.5</span>
n &lt;-<span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">4</span>
Sigma &lt;-<span class="st"> </span><span class="kw">diag</span>(<span class="dv">2</span>)
Sigma[<span class="dv">1</span>,<span class="dv">2</span>] &lt;-<span class="st"> </span>Sigma[<span class="dv">2</span>,<span class="dv">1</span>] &lt;-<span class="st"> </span>rho
X &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(n, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">sigma=</span>Sigma)
<span class="kw">colnames</span>(X) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>)
beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.05</span>, <span class="op">-</span><span class="fl">0.05</span>)
y &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>X)
<span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6449 -0.6857  0.0148  0.6756  3.6510 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.007473   0.010079   0.741 0.458466    
## XX1          0.044305   0.011547   3.837 0.000125 ***
## XX2         -0.051772   0.011709  -4.422  9.9e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.008 on 9997 degrees of freedom
## Multiple R-squared:  0.0023, Adjusted R-squared:  0.002101 
## F-statistic: 11.52 on 2 and 9997 DF,  p-value: 1.002e-05</code></pre>
<div id="what-the-p-value-does-not-mean" class="section level5">
<h5><span class="header-section-number">9.4.5.5.1</span> What the <span class="math inline">\(p\)</span>-value does not mean</h5>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p\)</span> is not the probability of the null being true. More formally, this probability is <span class="math inline">\(P(null | data)\)</span> but our <span class="math inline">\(p\)</span>-value is <span class="math inline">\(P(data | null)\)</span>. These are not the same. <span class="math inline">\(P(null | data)\)</span> is the probability of the null being true given the data. <span class="math inline">\(P(data | null)\)</span> is the probability of our data, or something more extreme than our data, conditional on a true null.</li>
<li><span class="math inline">\(1-p\)</span> is not the probability of the alternative</li>
<li><span class="math inline">\(p\)</span> is not a measure of effect size.</li>
<li><span class="math inline">\(p\)</span> in one experiment is not the same level of evidence against the null as in another experiment</li>
<li><span class="math inline">\(p\)</span> is not a great indicator of which is more likely, H0 or H1.</li>
<li>If one treatment level has <span class="math inline">\(p &lt; 0.05\)</span> and another treatment level has <span class="math inline">\(p &gt; 0.05\)</span>, this is not evidence that the treatment levels have different effects on the outcome.</li>
</ol>
</div>
</div>
</div>
<div id="recommendations" class="section level3">
<h3><span class="header-section-number">9.4.6</span> Recommendations</h3>
<p><strong>If you are working on basic science research</strong> simply report the exact <span class="math inline">\(p\)</span>-value, along with a CI. If <span class="math inline">\(p &lt; 0.05\)</span> (or some other <span class="math inline">\(\alpha\)</span>) do not report this as “significant” – in fact, avoid the word “significant”. In the english language, “significant” implies big or important. Small <span class="math inline">\(p\)</span>-values can result even with trivially small effects if <span class="math inline">\(n\)</span> is big or sample variation is small. If <span class="math inline">\(p\)</span> is smaller than say 0.001, then this is pretty good evidence that the data is not a fluke of sampling. But if <span class="math inline">\(p\)</span> is closer to 0.01 or 0.05, this is only weak evidence of a fluke because of the sampling variability of <span class="math inline">\(p\)</span>.</p>
<p><strong>If you are working on quality control</strong> then a <span class="math inline">\(p\)</span> value is a useful tool, but is only relevant compared to a decision rule with well-reasoned values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> – exact values of <span class="math inline">\(p\)</span> are not very meaningful.</p>
</div>
</div>
<div id="problems-2" class="section level2">
<h2><span class="header-section-number">9.5</span> Problems</h2>
<p>Problem 1 – simulate the distribution of <span class="math inline">\(p\)</span> under the null. There are many ways to do this but a straightforard approach is to</p>
<ol style="list-style-type: decimal">
<li>Create a <span class="math inline">\(2n \times m\)</span> matrix of random normal deviates with mean 0 and sd 1</li>
<li>Do a <span class="math inline">\(t\)</span>-test on each column, with the first <span class="math inline">\(n\)</span> values assigned to one group and the remaining <span class="math inline">\(n\)</span> values assigned to the second group. Save the <span class="math inline">\(p\)</span>-value from each.</li>
<li>Plot a histogram of the <span class="math inline">\(p\)</span>-values.</li>
<li>What is the distribution? What is the most likely value of <span class="math inline">\(p\)</span>?</li>
</ol>
<p>Problem 2 – simulate power. Again, many ways to do this but following up on Problem 1. 1. Create a <span class="math inline">\(2n \times m\)</span> matrix of random normal deviates with mean 0 and sd 1 2. Add an effect to the first <span class="math inline">\(n\)</span> values of each column. Things to think about a. what is a good effect size to add? The effect/sd ratio, known as Cohen’s d, is a relative (or standardized) measure of effect size. Cohen suggest 0.2, 0.5, and 0.8 as small, medium, and large standardized effects. b. should the same effect be added to each individual? Yes! It is the random component that captures the individual variation in the response. 3. Do a <span class="math inline">\(t\)</span>-test on each column of the matrix, using the first <span class="math inline">\(n\)</span> values in group 1 and the remaining <span class="math inline">\(n\)</span> values in group 2. Save the p-values for each. 4. Compute the power, the relative frequency <span class="math inline">\(p \le 0.05\)</span>. 5. Repeat with different values of <span class="math inline">\(n\)</span>, effect size, and sd, but only vary one at a time. How does power vary with these three parameters?</p>
<p>Problem 3 – write a script for a permutation test of the vitamin E and vitamin C levels of the vole data. Compare this to the <span class="math inline">\(t\)</span>-test.</p>
<p>Problem 4 – grad students only. Simulate the false discovery rate. Explore the parameters: the frequency of true nulls and the power.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-linear-model-with-a-single-categorical-x.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="two-or-more-categorical-x-factorial-designs.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/chapters/24-p-values.Rmd",
"text": "Edit"
},
"download": ["Walker-elementary-statistical-modeling-draft.pdf", "Walker-elementary-statistical-modeling-draft.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
