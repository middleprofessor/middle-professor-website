[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"index.html","id":"linear-models","chapter":"Preface","heading":"Linear Models","text":"cynically, one also well ask “medicine adopted frequentist inference, even though everyone presents P-values hypothesis tests?” answer : frequentist inference, like Bayesian inference, taught. Instead everyone gets taught misleading pseudo-frequentism: set rituals misinterpretations caricaturing frequentist inference, leading kinds misunderstandings. – Sander GreenlandWe use statistics learn data uncertainty. Traditional introductory textbooks biostatistics implicitly explicitly train students researchers “discover p-value” using hypothesis tests (Chapter 6). course many chapters, student learns use look-table flowchart make series binary decisions data (“independent variable continous categorical”, “response variable normal ”), choose correct “test” based decisions (“use Mann-Whitney U test”), compute test statistic data, compute p-value based test statistic, compare p-value 0.05. Textbooks typically give little guidance can concluded \\(p < 0.05\\) \\(p > 0.05\\), many researchers conclude, incorrectly, “discovered” effect \\(p < 0.05\\) found “effect” \\(p > 0.05\\).book introduction statistical analysis data biological experiments focus estimation treatment effects measures uncertainty theses estimates. Instead flowchart “statistical test”, book emphasizes regression modeling approach using linear models extensions linear models.“? learned post-doc lab regression data continuous independent variable t-tests ANOVA data categorical independent variables.” ! misconception roots history regression vs. ANOVA reinforced introductory biostatistics textbooks, instructors, choose teach statistics.Classical linear regression, t-tests ANOVA special cases linear model. different linear models text variations equation line \\(Y = mX + b\\) using slightly different notation:\\[\\begin{equation}\n\\mathrm{E}(Y | X) = \\beta_0 + \\beta_1 X\n\\tag{0.1}\n\\end{equation}\\]chapter introduction linear models explains meaning notation. , just recognize regression model, modern statistics, use estimate effects continuous \\(X\\) variable response (classical regression) also estimation effects categorical treatment variable response (classical t-tests ANOVA). regression model categorical treatment variable possible treatment variable recoded numeric indicator variable indicating group membership (“wildtype” “knockout”). Classical t-tests ANOVA equivalent special cases regression models linear model usually presented different way, one allows simple “paper pencil math” (addition, subtraction, multiplication, division). linear model underneath classical t-tests/ANOVA variation \\[\\begin{equation}\n\\overline{Y}_k = \\mu + \\alpha_k\n\\tag{0.2}\n\\end{equation}\\]\\(\\mu\\) grand mean \\(\\alpha_k\\) difference mean treatment k grand mean.text, use linear model model looks like Equation (0.1) ANOVA model model looks like Equation (0.2) (many statistics textbooks call “cell-mean models”). correct call models look like Equation (0.1) regression model word “regression” lot baggage taught – method data continuous \\(X\\) (“independent”) variables. said, “book regression models analyzing experimental data”, might quite reasonably, incorrectly, assume book wasn’t really relevant experiments independent variables categorical (“WT” vs.”KO”) continuous.Table 0.1 compact summary linear models introduced text, covers large fraction kinds models researchers need experimental data.","code":""},{"path":"index.html","id":"arent-t-tests-and-anova-good-enough","chapter":"Preface","heading":"Aren’t t-tests and ANOVA good enough?","text":"many experiments, linear models advocated give p-value t-test ANOVA, raises question, bother linear models? answers includeFlexibility. Linear models extensions linear models variations \\(Y = \\beta_0 + \\beta_1 X\\). Generalizations basic model (see Table 0.1) include linear models added covariates multiple factors, generalized least squares models heterogeneity variance correlated error, linear mixed models correlated error hierarchical grouping, generalized linear models counts biological data don’t normal distributions, generalized additive models responses vary non-linearly time, causal graphical models inferring causal effects observational data, multivariate models multiple responses, machine learning models. book comprehensive source methods introduction common foundations .Possibility. Issues inference linear models, including t-test/ANOVA, occur data violate assumptions independence, homogeneity variances, normal conditional response. Linear (regression!) models expanded different ways specifically model violations. Many models ANOVA equivalent – consequently, researchers using ANOVA forced use wrong model kludgy alternatives, non-parametric tests.Gateway drug. Many statistical models used genomics variations linear models introduced . steep learning curve methods statistical training consists t-tests, ANOVA, Mann-Whitney non-parametric tests.Biologically meaningful focus. Linear models encourage looking , thinking , reporting estimates size treatment effect uncertainty estimate. estimated treatment effect difference response two treatments. mean plasma glucose concentration period glucose tolerance test 15.9 mmol/l knockout group 18.9 mmol/l wildtype group, estimated effect -3.0 mmol/l. magnitude effect measure difference glucose tolerance two treatments. physiological consequence difference? big difference excite NIH trivial difference encourages us pursue line research? don’t know answers questions– ’m metabolic physiologist. Researchers metabolic physiology know answers, , don’t indicate literature. Effect sizes rarely reported experimental bench-biology literature.reported p-values. Small p-values give researchers confidence “effect exists” abundance small p-values series experiments rigorously probed system give researchers confidence discovered knowledge biological mechanism. Extremely small p-values give researchers confidence effect large important. confidence unwarranted. P-values useful measure effect size.P-values neither necessary sufficient good data analysis. , p-value useful tool data analysis toolkit. conduction experiment analysis results closely approximate model underlying computation p-value, p-value dampens frequency fooled randomness gives researcher confidence direction (positive negative) effect. Importantly, estimation effects uncertainty computation p-value alternatives. Throughout text, linear models used compute p-value addition estimates effects uncertainty.NHST Blues – emphasis p-values measure report consequence “statistical test?” strategy data analysis. practice, known Null-Hypothesis Significance Testing (NHST), criticized statisticians many, many decades. Nevertheless, introductory biostatistics textbooks written biologists statisticians continue organize textbooks around collection hypothesis tests, great deal emphasis “statistical test?” much less emphasis estimation uncertainty. NHST/-statistical-test strategy learning statistics easy requires little understanding statistical model underneath tests assumptions, limitations, behavior. NHST strategy combination point--click software enables mindless statistics encourages belief statistics tool like word processor tool, , rigorous analysis one’s data requires little getting p-values creating bar plots. Indeed, many PhD programs biosciences require statistics coursework training available students graduate students postdocs lab. consequence, biological sciences literature filled error bars imply data negative values p-values little relationship probability data given experimental design. importantly science, reported statistics often study researchers journal editors think .","code":""},{"path":"index.html","id":"what-is-unusual-about-this-book","chapter":"Preface","heading":"What is unusual about this book?","text":"Real data sets recent experimental biology literature. data text articles researchers followed open science best practices made data freely available archiving data article. let’s explain modern statistics relevant experimental biologist’s experiments using data collected experimental biologists also allows connect statistical explanation goals experiment. potential downside using real data understanding analysis requires level understanding underlying biology underlying methods data collection. try give enough background biology understand motivation experiment experimental methodology understand data.scan many, many articles experimental biology looking good archived data use examples text, ’ve developed appreciation variation experimental biologists think statistics. consequence recognition many ways researchers use non-best practices occasionally worst practices. Much material chapter one entire chapter (Issues inference) devoted addressing consequences non-best worst practices.","code":""},{"path":"a-table-of-models.html","id":"a-table-of-models","chapter":"A Table of Models","heading":"A Table of Models","text":"","code":""},{"path":"a-table-of-models.html","id":"including-mapping-between-linear-models-and-classical-tests","chapter":"A Table of Models","heading":"including mapping between linear models and classical tests","text":"\nTable 0.1: Linear models extensions linear models covered text.\n","code":""},{"path":"ask1-intro.html","id":"ask1-intro","chapter":"1 Analyzing experimental data with a linear model","heading":"1 Analyzing experimental data with a linear model","text":"","code":""},{"path":"ask1-intro.html","id":"this-text-is-about-using-linear-models-to-estimate-treatment-effects-and-the-uncertainty-in-our-estimates.-this-raises-the-question-what-is-an-effect","chapter":"1 Analyzing experimental data with a linear model","heading":"1.1 This text is about using linear models to estimate treatment effects and the uncertainty in our estimates. This, raises the question, what is “an effect”?","text":"end text, provide example set analyses multiple, related experiments. example goal target; ’s working towards learn text. data analysis come multiple experiments presented Figure 2 article ASK1 inhibits browning white adipose tissue obesity. chapter preceding analysis just enough biology help understand biological importance experiment. data Figure 2 set experiments exploring consequences adipose-tissue specific deletion ASK1 signaling protein multiple, adverse effects high-fat diet mice, including weight gain, glucose intolerance, increased liver triacylglycerol levels. chose data Figure 2 paper diversity analyses plot types. analyses plots differ slightly researchers implemented better practices – stuff text., use one panel Figure 2 outline statistical analysis experimental data . Much outline repeated “introduction linear models” chapter.goal experiments estimate effect adipose-specific ASK1 deletion. understand mean “effect”, understand can estimate effect fiting linear model data, let’s peak results fit model.\nFigure 1.1: effect adipose-specific ASK1 knockout liver triglyceride (TG).\nexperiment, researchers want know knocking ASK1 gene adipose tissue cells lowers liver triglyceride (TG) level mice fed high-fat diet. investigate , researchers compared TG levels group knockout mice (“ASK1Δadipo” – Δ del operator refers deletion genetics) TG levels group control mice (“ASK1F/F”). Specifically, measured difference mean TG control group mean TG knockout group.\\[\n\\overline{\\texttt{TG}}_\\texttt{ASK1Δadipo} - \\overline{\\texttt{TG}}_\\texttt{ASK1F/F}\n\\]measured difference means estimate effect ASK1 deletion liver TG levels. estimate -21.6 µmol per g liver. effect units variable compared (µmol per g liver), magnitude (21.6 units), direction (negative).version Figure 2i, shows results experiment, Fig. 1.1 . direction magnitude estimated effect (measured difference means) can mentally reconstructed comparing position two group means lower part Fig. 1.1. upper part Fig. 1.1 explicitly shows estimate shows uncertainty estimate using 95% confidence intervals.numbers make plot come coefficient table fit model, shown Table 1.1. second row table statistics effect knockout TG levels. value “Estimate” column estimated effect (measured difference means). value “Std. Error” column standard error difference means (SE), measure uncertainty estimate. use SE compute statistic t-test, get p-value, probability observing t-value large larger observed value, null true. “null true” means true effect zero also assumes long list conditions, , random treatment assignment homogeneity variance. also use SE compute 95% confidence intervals difference. Understanding standard error interpret confidence intervals paramount practicing good statistics. covered chapter Variability Uncertainty (Standard Deviations, Standard Errors, Confidence Intervals).\nTable 1.1: Coefficient table linear model fit exp2i data.\nhard overemphasize measure experiments estimated effects. true effect may larger, smaller, even reverse direction. text can infer true effects statistical analysis experimental data. inference requires also measure uncertainty estimate effect.measured means group computed random sample mice. cared six mice group experiment, need fit linear model data estimate effect, simply compute group’s mean subtract control mean knockout mean. care something dozen mice trying discover something general ASK1 regulation TG levels mice, generally (even mammals, especially humans, generally). make leap inference, use model claim sample mean estimate respective population mean. Given model, can compute standard error mean standard error difference means. standard error measure sampling variance statistic , therefore, measure precision estimate. standard error, , measure uncertainty estimate. think precision uncertainty: repeat experiment many, many times, generate long list mean TG levels control mice long list mean TG levels knockout mice. less variable means list, precise. using model, need repeat experiment many times get standard error.model going fit Figure 2i data \\[\\begin{align}\ny &= \\beta_0 + \\beta_1 x + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align}\\]model Figure 2i data generated. model, \\(y\\) liver TG level fictional, randomly generated mouse \\(x\\) variable indicates condition ask1 gene randomly generated mouse – value 0 given mice functional ASK1 gene value 1 given mice knocked gene.\\(\\beta_0\\) “true” mean TG mice fed high-fat diet functional ASK1 gene. “true”, mean mean computed measure TG infinite number mice (exactly “mice” means good topic campfire discussion). observed mean ASK1F/F group estimate \\(\\beta_0\\). sum \\(\\beta_0\\) + \\(\\beta_1\\) true mean TG mice fed high-fat diet knocked ASK1 gene. means \\(\\beta_1\\) true difference means, true effect. observed difference means ASK1Δadipo ASK1F/F groups estimate \\(\\beta_1\\). difference estimated effect.sum \\(\\beta_0 + \\beta_1 x\\) expectation TG, expected value TG generated mouse given generating model. sum equals true mean infinite set normal mice \\(x = 0\\) equals true mean infinite set ASK1 knockout mice \\(x = 1\\). generated control mice expected value TG. generated knockout mice expectated value TG.\\(\\varepsilon\\) error randomly generated mouse. random number sampled Normal distribution mean zero variance \\(\\sigma^2\\). variation generated mice systematic component due variation \\(x\\) random (stochastic) component due variation \\(\\varepsilon\\).fitting model data estimate parameters \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\). estimation \\(\\sigma\\) allows us compute measure uncertainty (standard error) estimates means (\\(\\beta_0\\) \\(\\beta_0 + \\beta_1\\)) difference means (\\(\\beta_1\\)).Let’s fit model Figure 2i data using R.Robust inference model (generalizing sample population, including measures uncertainty estimates, requires data approximates kind data ’d expect data generating model specified . rigorous analysis use specific model checks evaluate . First, “normality check” – use quantile-quantile (QQ) plot see data approximate ’d see sampled normal distribution.95% quantiles computed fake samples generating model inside two dashed lines plot. measured quantiles open circles. quantiles sample looks like sampled normal distribution, “looks Normal”, approximately linear mostly lie within dashed lines. sample looks good. Regardless, inference pretty robust moderate departure Normal.Second, “homogeneity check” – use spread level plot see pattern variance, example spread residuals noticeably bigger one group another, spread increases fitted value.looks pretty good. Given checks, lets move look table model coefficients.two values column “Estimate” estimates \\(\\beta_0\\) \\(\\beta_1\\). top value (61.5) mean control mice (units µmol/g). mean knockout mice sum two values (39.9 µmol/g). effect ASK1 deletion TG levels simply second value (-21.6 µmol/g). standard error effect 7.05 µmol/g.can use standard error compute t-value (-3.1, column “t value”). t-value test statistic. probability (“p value”) significance test 0.012. probability sampling t-value large larger observed t-value, sample null distribution t-values (distribution sampled t values true value \\(\\beta_1\\) 0). can also use standard error compute 95% confidence interval effect. lower bound interval -37.3 µmol/g upper bound -5.9 µmol/g. confidence interval another way communicating uncertainty, way advocated text. 95% confidence interval, 95% similarly constructed intervals (hypothetical sampling six mice ASK1 normal population six mice ASK1 knockout population) contain true mean. Another way think confidence interval , range true differences compatible data, compatible means “rejected” t-test (t-test estimated effect number inside interval return p-value greater 0.05).might report result paper:Mean TG level ASK1Δadipo mice high-fat diet 21.6 µmol/g less ASK1F/F mice high-fat diet (95% CI: -37.3, -5.9, \\(p = 0.012\\)) (Fig.@ref(fig:ask1-exp2i-ggplot_the_model)).","code":"\nexp2i_m1 <- lm(liver_tg ~ treatment, data = exp2i)\nset.seed(1)\nqqPlot(exp2i_m1, id=FALSE)\nspreadLevelPlot(exp2i_m1, id=FALSE)## \n## Suggested power transformation:  1.294553\nexp2i_m1_coef <- cbind(coef(summary(exp2i_m1)),\n                        confint(exp2i_m1))"},{"path":"part-i-getting-started.html","id":"part-i-getting-started","chapter":"Part I: Getting Started","heading":"Part I: Getting Started","text":"","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"getting-started-r-projects-and-r-markdown","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2 Getting Started – R Projects and R Markdown","text":"typical statistical modeling project consist :importing data Excel text (.csv .txt) filescleaning datainitial exploratory plotsanalysismodel checkinggenerating plotsgenerating tableswriting text describe project, methods, analysis, interpretation results (plots tables)best practice reproducible research use software tools steps possible. many research projects reproducible data cleaned Excel, different parts data separately imported GUI statistics software analysis, output statistics software transcribed Excel make table. parts analysis used create plot plotting software. tables plots pasted Microsoft Word create report. change step process require researcher remember downstream parts dependent change re-analysis, table, plot. team member project making asks particular variable transformed andR studio encourages best practices creating project folder contains project documents implementing version markdown called R Markdown. R Markdown document can explicitly link parts workflow changes earlier steps automatically flow later steps. completion project, researcher can choose “run ” menu data read, cleaned, analyzed, plotted, tabled, put report text.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"r-vs-r-studio","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.1 R vs R Studio","text":"R programming language. runs hood. never see . use R, need another piece software provides user interface. software use R Studio. R Studio slick (slick) graphical user interface (GUI) developing R projects.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"download-and-install-r-and-r-studio","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.2 Download and install R and R studio","text":"Download R OSDownload R Studio DesktopIf need help installing R R studio, Andy Field’s Installing R RStudio video tutorial)","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"open-r-studio-and-modify-the-workspace-preference","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.3 Open R Studio and modify the workspace preference","text":"Open R StudioClick R Studio > Preferences toClick General left menudiable “Restore .RData workspace startup”Click “Save workspace .RData exit” popup menu choose “Never”’s going ? workspace contains values objects created R code ’ve run working R session. Nothing good comes . want start R session clean slate, blank workspace. means start new R session, need re-run code chunks start left-close last R session. seems tedious , warned, bad things happen save workspace last session re-load startup. Trust . Just don’t .","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"if-you-didnt-modify-the-workspace-preferences-from-the-previous-section-go-back-and-do-it","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.4 If you didn’t modify the workspace preferences from the previous section, go back and do it","text":"","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"r-markdown-in-a-nutshell","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.5 R Markdown in a nutshell","text":"text, write code analyze data using R Markdown. R markdown version Markdown. Markdown tool creating document containing text (like microsoft Word), images, tables, code can output, knitted, three modern output formats: html (web pages), pdf (reports documents), microsoft word (okay, isn’t modern widely used).R Markdown, .Rmd, document contains three components:YAML header, specifies formatting styles knitted documentthe code “chunks”, blocks code somethingthe space code chunks contains text output images tables code chunks.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"install-r-markdown","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.6 Install R Markdown","text":"Directions installing R MarkdownR Markdown can output pdf files. mechanism first create LaTeX (“la-tek”) file. LaTeX amazing tool creating professional pdf documents. need PDF output text, encourage download install tinytex distribution, created especially R Markdown R Studio.tinytex distribution .","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"importing-packages","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.7 Importing Packages","text":"R scripts write include functions packages included Base R. packages need downloaded internet server computer. need (although redo time update R). , time start new R session, need load package using library() function. Now good time import packages useOpen R Studio choose menu item “Tools” > “Install Packages”. “packages” input box, insert names packages install package. names can separated spaces commas, example “data.table, emmeans, ggplot2”. Make sure “install dependencies” clicked click “Install”. Packages use book areImport wrangling packagesdevtools – use install packages CRANhere – use read write correct folderjanitor – use function clean_names packagereadxl – elegant importing microsoft Excel spreadsheetsdata.table - use data.table way wrangle data text.magrittr – instead nesting functions, use pipe operator packagestringr – use wrangle character variablesforcats – use wrangle factor variablesanalysis packagesemmeans – use compute modeled means contrastsnlme – use gls modelslme4 – use linear mixed modelslmerTest – use inference linear mixed modelsglmmTMB – use generalized linear modelsMASS – use glm.nb packageafex – use classic ANOVA linear modelscar – use model checkingDHARMa – use model checking generalized linear modelsinsight – use learn modelsgraphing tabling packagesggplot2 – use plottingggsci – use color palettesggthemes – use colorblind paletteggpubr – use make ggplots bit easierggforce – use improved jitter plotsdabestr – use make several plot typescowplot – use combine plotsknitr – use make kable tableskableExtra – use improve kable tableslazyWeave – use pretty p-valuesOnce installed, don’t need although additional packages might install. simply need use library() function start markdown script.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"setup-create-project","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.8 Create an R Studio Project for this textbook","text":"Create project folder within Documents folder (Mac OS) Documents folder (Windows OS). files associated book reside inside folder. name project folder something meaningful, “Applied Biostatistics” name class (students Applied Biostatistics class, folder named “BIO_413”).Within project folder, create new folders named\n“Rmd” – R markdown files stored\n“R” – additional R script files stored\n“data” – data download public archives stored\n“output” – store fake data generated class\n“images” – image files stored\n“Rmd” – R markdown files stored“R” – additional R script files stored“data” – data download public archives stored“output” – store fake data generated class“images” – image files storedOpen R Studio click menu item File > New Project…Choose “Existing Directory” navigate project folderChoose “Create Project”Check “.Rproj” file project folderDownload move file ggplot_the_model.R R folder.\nFigure 2.1: Project folder .Rproj file main folders located first level project\nproject directory look like Figure 2.1. Importantly, project file (“Applied Biostatistics.Rproj”) main folders located first level within project folder.Bug alert .Rproj file somewhere else (desktop, data folder, etc.), bad things happen.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"working-on-a-project-in-a-nutshell","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.9 Working on a project, in a nutshell","text":"Wake , brush teeth, open project double-clicking .Rproj icon. Alternatively, open R Studio use File > Open Project open project. name project top-right R Studio window. always want work within open project first workflow guarantees . Open R Studio open .Rmd file, working within another project project . Bad things happen.Run previous code chunks, order (top bottom). Write new code code chunks run. run code, add R objects workspace. workspace contains values objects created R code run working session. save .Rmd file, values saved, text code chunks R Markdown document. feature, bug.finished session, quit R Studio. get popup window asking want save workspace, click “”. immediately go back section “Open R Studio modify workspace preference” follow directions.","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"setup-create-rmd","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.10 Create and setup an R Markdown document (Rmd)","text":"top-left icon R Studio little plus sign within green circle. Click choose “R Markdown” pull-menu.Give file meaningful title.Add name Author text book. first time , R Studio default name.R Studio opens demo document. Delete text first code chunk, starting header “## R Markdown”","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"modify-the-yaml-header","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.10.1 Modify the yaml header","text":"Replace “output: html_document” yaml header following order create table content (toc) left side page enable code folding","code":"output:\n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: hide"},{"path":"getting-started-r-projects-and-r-markdown.html","id":"modify-the-setup-chunk","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.10.2 Modify the “setup” chunk","text":"R Markdown documents start setup chunk loads packages containing R functions Base R defines key R objects, name data folder. good practice load packages used chunks document. student researcher new R R studio, can confusing unlikely aware functions belong package , therefore, packages loaded., disregard best practice offer general, -purpose setup chunk users text.echo = TRUE tells knitr display code within code chunk R markdown file knitted. knitr::opts_chunk$set(echo = TRUE) sets echo = TRUE code chunks containing .Rmd file (R markdown files project!).<- ::makes sure uses function package package. Huh? Let’s back – R open source project packages written independent programmers scientists employees central company. someone develops package, create functions stuff. Sometimes developers different packages create functions name. function name conflict load two packages name. R session use function last loaded package function assigned name. want name used function previously loaded package, need either re-order library() statements, simply re-assign name function want. <- ::. script takes function package “” assigns object .","code":"\nknitr::opts_chunk$set(echo = TRUE)\n\n# wrangling packages\nlibrary(here) # here makes a project transportable\nlibrary(janitor) # clean_names\nlibrary(readxl) # read excel, duh!\nlibrary(data.table) # magical data frames\nlibrary(magrittr) # pipes\nlibrary(stringr) # string functions\nlibrary(forcats) # factor functions\n\n# analysis packages\nlibrary(emmeans) # the workhorse for inference\nlibrary(nlme) # gls and some lmm\nlibrary(lme4) # linear mixed models\nlibrary(lmerTest) # linear mixed model inference\nlibrary(afex) # ANOVA linear models\nlibrary(glmmTMB) # generalized linear models\nlibrary(MASS) # negative binomial and some other functions\nlibrary(car) # model checking and ANOVA\nlibrary(DHARMa) # model checking\n\n# graphing packages\nlibrary(ggsci) # color palettes\nlibrary(ggpubr) # publication quality plots\nlibrary(ggforce) # better jitter\nlibrary(cowplot) # combine plots\nlibrary(knitr) # kable tables\nlibrary(kableExtra) # kable_styling tables\n\n# ggplot_the_model.R packages not loaded above\nlibrary(insight)\nlibrary(lazyWeave)\n\n# use here from the here package\nhere <- here::here\n# use clean_names from the janitor package\nclean_names <- janitor::clean_names\n\n# load functions used by this text written by me\n# ggplot_the_model.R needs to be in the folder \"R\"\n# if you didn't download this and add to your R folder in your\n# project, then this line will cause an error\nsource_path <- here(\"R\", \"ggplot_the_model.R\")\nsource(source_path)\n\ndata_folder <- \"data\"\nimage_folder <- \"images\"\noutput_folder <- \"output\""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"lets-play-around-with-an-r-markdown-file","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.11 Let’s play around with an R Markdown file","text":"","code":""},{"path":"getting-started-r-projects-and-r-markdown.html","id":"create-a-fake-data-chunk","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.11.1 Create a “fake-data” chunk","text":"Create new chunk label “fake-data”. Insert following R script click chunk’s run buttonThis chunk creates fake neutrophil counts two different experiments. comment (#) sign View(fake_data) “comments ” line code, run. View data highlighting View(fake_data) choosing “Run selected line(s)” Run menu.","code":"\nset.seed(4)\nn <- 10\nfake_data <- data.table(\n    treatment = rep(c(\"cn\", \"tr\"), each = n),\n    neutrophil_count_exp1 = rnegbin(n*2, \n                                    mu = rep(c(10, 15), each = n),\n                                    theta = 1),\n    neutrophil_count_exp2 = rnegbin(n*2, \n                                    mu = rep(c(10, 20), each = n),\n                                    theta = 1)\n)\n# View(fake_data)"},{"path":"getting-started-r-projects-and-r-markdown.html","id":"create-a-plot-chunk","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.11.2 Create a “plot” chunk","text":"Create new chunk label “plot”. Insert following R script click chunk’s run buttonEach plot shows mean count group, standard error mean count, p-value t-test. statistical analysis plot typical found experimental biology journals. text teach alterntatives implement better practices.","code":"\ngg_1 <- ggstripchart(data = fake_data,\n                x = \"treatment\",\n                y = \"neutrophil_count_exp1\",\n                color = \"treatment\",\n                palette = \"jco\",\n                add = \"mean_se\",\n                legend = \"none\") +\n    ylab(\"Neutrophil Count (Exp. 1)\") +\n  stat_compare_means(method = \"t.test\",\n                     label.y = 50,\n                     label = \"p.format\") +\n    NULL\n\ngg_2<- ggstripchart(data = fake_data,\n                x = \"treatment\",\n                y = \"neutrophil_count_exp2\",\n                color = \"treatment\",\n                palette = \"jco\",\n                add = \"mean_se\",\n                legend = \"none\") +\n  ylab(\"Neutrophil Count (Exp 2)\") +\n  stat_compare_means(method = \"t.test\",\n                     label.y = 65,\n                     label = \"p.format\") +\nNULL\n\nplot_grid(gg_1, gg_2, labels = \"AUTO\")"},{"path":"getting-started-r-projects-and-r-markdown.html","id":"knit-the-rmd","chapter":"2 Getting Started – R Projects and R Markdown","heading":"2.11.3 Knit the Rmd","text":"Knit html fileKnit word documentIf ’ve installed tinytex (LaTeX distribution), knit pdf file","code":""},{"path":"part-ii-r-fundamentals.html","id":"part-ii-r-fundamentals","chapter":"Part II: R fundamentals","heading":"Part II: R fundamentals","text":"","code":""},{"path":"data.html","id":"data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3 Data – Reading, Wrangling, and Writing","text":"text avoids canned data sets R packages importing wrangling data major obstacle statistics new R learners. Coding import raw data archived text (Excel Google sheets) format best practice increasing replicability.Importing data difficult new R learners two reasonsMany experimental biology researchers archive data format might make sense analysis Excel, doesn’t work analysis statistical software, including R, without significant post-import processing (wrangling) data. includesdata wide format, measures response variable treatment level different columndata transposed format, measures response variable row instead column (measures treatment level separate row)missing values represented different things, sometimes table. things include blank cells, periods, hyphens, word “NA”, numbers -9999.column headers multiple words separated spacessheets variable number empty cells separate values within different groupssheets contain information data using colored blocks cells formatsThese issues solved archiving data (tidy data format)https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html. hallmark tidy data data long format, introduced next section (Section @data-long). Nevertheless, students researchers colleagues share non-tidy data critical skill R users ability tidy data.file hard drive address, file path specific place (specific computer) time (file organization computer). move data file another computer, even another directory within computer, address changes. Modern computer users least vaguely familiar hierarchical organization file storage (students classes seem store everything create desktop) unaware concept file path confused path . use menu-driven system open file, OS constructing file path hood. coding, need give R file path. several imperfect methods coding path data file. one perfect method – function package. discussed Use function construct file path","code":""},{"path":"data.html","id":"data-long","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.1 Long data format – the way data should be","text":"best practices archiving data tables, including archived data journal websites, illustrated format data Table 3.1.1\nTable 3.1: First six rows diet data set.\nbest practices included archived data includeThe data table contains measures individual. archived Excel files download journal websites, researchers separated variable separate tables exclude information tables treatment group. example, researchers typically archive vo2 values Table 3.1 separate table within Excel sheet values split diet group (high fat “HF” low fat “LF”)(Figure 3.1). Don’t .\nFigure 3.1: vo2 values separated rest data split separate diet groups.\ndon’t want separate variable separate tables often want use information measures individual analysis. example, comparison \\(\\texttt{vo2}\\) among treatment levels, want add measure \\(\\texttt{lean_mass}\\) model covariate (often called ANCOVA model). , ’d need know values \\(\\texttt{vo2}\\) values \\(\\texttt{lean_mass}\\) belong individual.Even think analyzed data using best practice, don’t separate variable separate tables. Researchers sometimes fail recognize best practices. re-analysis published data using best practices often require information measures individual. example, many researchers compare ratio \\(\\texttt{vo2}\\) \\(\\texttt{total_mass}\\) among treatment levels. isn’t best practice want check researchers conclusions, ’d need table like Table 3.1 re-analyze data.Even used considered best practice, don’t separate variable separate tables. Sometimes researcher wants use data published data sets parameterize mechanistic model. , use data published data sets explore patterns relationships among variables, example fat mass scales lean mass (total mass). , need table like Table 3.1 re-analyze data.researchers organized table individuals rows. data table measures individual organize data row column belonging one individual. best practice analysis R (statistical software) organize individuals rows (exceptions R packages analyze genomics datasets).researchers organized data long format – , column contains measures single variable. general, organize data wide format, splitting values variable multiple columns, column representing values different group. example wide format Figure 3.1, \\(\\texttt{vo2}\\) subset data split groups columne \\(\\texttt{diet}\\). contrast, \\(\\texttt{vo2}\\) data long format original table (Table 3.1). might useful think long format variable single column values group stacked top .general, R almost statistical software (one exception Graphpad Prism) require data long format.reason long format consistent math underneath statistical modeling data. example, least squares solution linear model \\[\n\\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n\\]\\(\\mathbf{X}\\) model matrix, table numeric values row containing values \\(X\\) variables individual (categorical variables \\(\\texttt{diet}\\) recoded one numeric indicator variables indicating group membership). model matrix long format.\n* second reason organizational simplicity. Statistical models often include multiple variables measured individual – example \\(\\texttt{lean_mass}\\) added covariate example. data wide format, non-awkward way match values \\(\\texttt{lean_mass}\\) values \\(\\texttt{vo2}\\) individual. , models crossed factor variables, example, experiment four groups repesenting four combinations variables \\(\\texttt{genotype}\\) (levels “WT” “KO”) \\(\\texttt{treatment}\\) (levels “Control” “HFD”), non-awkward way (least table single header row) indicate design two crossed factors. wide format, design appears single factor four groups.Don’t confuse data table multiple columns different variables data table wide format. Long wide relevant single variable organized – split multiple columns table wide, stacked single column, table long.One exception long format longitudinal repeated measures data set (Chapter 17), response variable measured multiple times individual. example might pre-post design variable measured treatment applied experiment response measured week multiple weeks. univariate models analyses data, multiple measures individual stacked long format. consequence, now multiple rows data individual (one measure variable). multivariate models analyses data, multiple measures individual treated different variables spread wide format.table includes raw values. Many researchers normalize standarize values variable constructing ratio raw value divided standardizing value. Almost always, best practice leave response variable alone (make ratio) include standardizing value covariate (offset) model. , want check th conclusions particular result researchers used ratio response variable, need raw measures standardizing values.","code":""},{"path":"data.html","id":"data-here","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.2 Use the here function to construct the file path","text":"Importing data R can struggle new R users , unfortunately, online “import” sources give easy superficial methods don’t follow best practices increasing reproducibility allow flexible organization files within project.(TL;DR – use () package)df <- read.table(file=\"clipboard\") imports data copied clipboard, Excel/Sheets file open text file. semi-reproducible, comment specifying filename, worksheet range copied necessary. problematic (catastrophically reproducibility), , researcher know highlighted copied correct range Excel sheet?df <- read.csv(file.choose()) opens familiar “open file” dialog box, lets user navigate file choice. semi-reproducible, comment specifying filename import necessary. catastrophic problem (reproducibility) , researcher know file actually opened R session? researcher might think opened “walker_maine_bee_data_clean_androscoggin.csv” mistakenly opened “walker_maine_bee_data_clean_aroostook.csv”.df <- read.table(file=\"my_data.txt\") df <- read_excel(file=\"my_data.xlsx\") reproducible filename explicitly specified. , method requires “my_data” physically located folder file containing R script (.Rmd file case) violates best practice clean project organization different folders different kinds files (data, R scripts, images, manuscript text, etc.).R Studio elegant import tool environment pane opens custom dialog box allows researcher navigate file, specify part file import, specific sheet range Excel file. reproducibility issues #1 #2 R Studio includes equivalent script, adds relevant information reproducility. One simply copies pastes script code chunk voila! next time script run, data can imported script without using menus dialog boxes. Except ..script seem take account working directory R Markdown file project folder folder containing R Markdown file two-step method fails. personally, ’d prefer run chunk quickly opens data file instead re-navigating file system re-specifying sheet range every time re-start project new R session.least three solutions issues raised , requiring understanding file paths directory structure operating system. file “my_data.xlsx” absolute file path, full address file (filename something like house street number). absolute file path “my_data.xlsx” might “/Users/jwalker/Documents/applied-biostatistics/data/my_data.xlsx”. relative file path file path working directory. R Studio project, working directory project directory, directory containing .Rproj file. working directory console. Importantly, working directory R Markdown code chunk folder containing saved R Markdown file. R Studio Notebook R Markdown file working directory notebook code chunk folder containing saved notebook file. R Markdown file located within rmd folder, located within project folder, relative file path “my_file.xlsx” “../data/my_file.xlsx”. “..” tells file OS move “” parent directory (project folder) “data” tells file OS move “” data folder. put together single address using “/”. beauty relative paths remain – break one’s script – project folder, contents including data folder rmd folder, moved another location hard drive (say new “Research” folder). contrast, absolute file path changes, breaks old script.three solutions areCreate relative path file using something like file_path <- \"../data/my_data.xlsx\". always work fails computers. example, project folder Windows OS (Mac OS) desktop, assigned relative address doesn’t seem look folder containing file.Create setup chunk reroutes working directory project folder using scriptFor work, chunk named “setup”, , text inside curly brackets top chunk “r setup”. , chunk, relative file path file_path <- \"../data/my_data.xlsx\" “my_data.xlsx” immediately inside data folder immediately inside project folder. work machine, work even project folder moved.Use function (). robust solution seems using function () package. function works something like thishere() creates absolute path, one created fly, change (change) correctly project folder moved machine, cloud drive, another machine altogether.","code":"\n# use this in a chuck called \"setup\" to force the working directory to be\n# at the level of the project file.\nknitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())\ndata_folder <- \"data\" # path to data that are imported\nfile_name <- \"my_data.xlsx\"\nfile_path <- here(data_folder, file_name) # paste together parts of the address\nmy_file <- read_excel(file = file_path)"},{"path":"data.html","id":"learning-from-this-chapter","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.3 Learning from this chapter","text":"easiest learn chapter starting clean R Markdown document chapter. Create new R Markdown file save “Rmd” folder project. create setup Rmd, follow steps Create setup R Markdown document (Rmd)Important: import method given work properly Rmd file saved! Get habit creating file, saving immediately, saving often.Important: import method work properly unless working within R studio project. don’t see name project upper right R Studio toolbar, project. Open project moving forward. good habit open R studio double clicking project file icon (R Studio icon Rmd document working ).Notes setup chunk Create setup R Markdown document (Rmd)become better R programmer, kind future loading packages necessary code R Markdown file working . default load everything, future confused something installed. , ’ve disregarded best practice textbook make sure chunks don’t fail missing function.kind future commenting package loaded; usually specific function packagehere <- ::favorite script ever. ? One can read “assign function package object ” (reading script right left). ? turns multiple packages define function called “”. packages loaded package, won’t work – replaced recently loaded package. make sure uses function package, simply reassign package object “” loading packages.","code":""},{"path":"data.html","id":"data-working-in-r","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4 Working in R","text":"","code":""},{"path":"data.html","id":"importing-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1 Importing data","text":"","code":""},{"path":"data.html","id":"importing-excel-files","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.1 Importing Excel files","text":"Source article: Lyu, Yang, et al. “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes modulate aging response nutrient choice.” Elife 10 (2021): e59399.file name: elife-59399-fig1-data1-v2.xlsxLet’s open data Fig. 1H.StepsHighlight copy title article, “Drosophila serotonin 2A receptor signaling coordinates central metabolic processes modulate aging response nutrient choice”Create new folder within “data” folder. Name folder title paper pasting name clipboard. “data ” folder, since contains data publication title folder.Download .xlsx file move folderThe code uses function read_excel() package readxl. amazing power package tidyverse page chapter 11 R Data Science book.book consistently uses protocol importing downloaded Excel files. tidy archived data Excel, user simply copy chunk, paste Rmd, modify stuff inside quotes import data. mindless programming. Don’t . Understand line chunk .","code":"\ndata_from <- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\nfile_name <- \"elife-59399-fig1-data1-v2.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp1h <- read_excel(file_path, \n                    sheet = \"H\")\nexp1h <- clean_names(exp1h) # clean column names\nexp1h <- data.table(exp1h) # convert to data.table\n\n# View(exp1h)"},{"path":"data.html","id":"view-the-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.1.1 View the data","text":"Let’s start last line, commented outThe comment (hashtag) symbol (#) tells R ignore everything line following symbol. Usually comment symbol followed brief comment code. comment mostly future – come back six month hiatus wonder line block code , comment gives information. Sometimes comment used front R code keep R code running. purpose comment symbol .View(exp1h) tells R open window showing spreadsheet view data.table exp1h. R studio, window opened new tab Source editor pane (pane Rmd content). run code, highlight “View(exp1h)” comment symbol click “Run selected line(s)” item Run pop-menu (right side pane’s tool bar). don’t include comment sign highlight tell R ignore everything .can view data exp1h easily holding command key (Mac OS) control key (Windows) clicking mouse “exp1h” lines. Even though use mouse click method, like View(exp1h) end import chunk remind View data make sure everthing imported correctly.","code":"\n# View(exp1h)"},{"path":"data.html","id":"what-does-each-line-in-the-chunk-do","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.1.2 What does each line in the chunk do?","text":"first three lines chunk create directory path file. path includes three variablesdata_folder – assigned “data” setup chunk. “data” folder within project folder contains (contain) datasets text. data come many different published papers, data publication gets folder within “data”.data_from – name “data ” folder within “data” containing data files. text, folder names always name published paper.filename – name file read. may multiple data files within publication’s data folder.put together absolute path using function () package. Take look value file_path confirm.next three lines (starting exp1h <-)read_excel imports data assign data.frame named exp1hclean_names cleans column names exp1hdata.table converts exp1h data.table. data.table data.frame magical properties.steps 2 3, functions take data.frame process way assigned processed data.frame object name (exp1h). script can made slightly “elegant” using “pipe” operator %>%.single line code containing three separate operations piped together. way think isThe read_excel function imports data file located file_path assigns data exp1h. pipe operator sends tothe clean_names function, cleans column names exp1h. pipe operator sends tothe data.table function, convert data.frame data.table.3rd way nested functions.Prior creation pipe operator, nested functions make code compact. However, nested functions often less readable, now mostly avoid use pipe operator instead.Let’s back understand steps, especially clean_names step. first line, line imports file.Look column names (column headers Excel lingo) imported data using names colnames (yes, elebenty million ways anything R) (names general can used return names parts list, colnames specific matrix-like objects). Type console, R Markdown chunk:general, bad practice include spaces, parentheses, special characters /, -, $, ^, column names data frame symbols specific meanings R functions. best practice spaces replace space underscore, example column header ““Weight (mg/fly)” weight_mg_per_fly. coders separate words period (weight.mg.per.fly). Others mash words together single word like weightmgperfly generally avoided result can hard read. Finally, coders use Caps separate words like WeightMgPerFly. easier read simple concatenation underscore easiest read.clean_names janitor package beautiful function clean column names data frame including replacing spaces underscore stripping parentheses. default format snake_case, replaces spaces underscores changes uppercase letter lowercase. Many coders like work lowercase variable names avoid hit shift key. one .Now run line look column names.Finally, run lineYou won’t see difference View data peak column names. ’ve transformed data frame data.table given magical properties.Worst Practices – resist temptation change column names data file, reduces reproducibility. Leave original data files original. Always increase reproducibility!colleague blues – researchers live Excel world save data way efficient computing stuff Excel efficient statistical analysis using R statistical computing software packages (exception Graphpad Prism). Analyzing data much less frustrating data saved format facilitates analysis. Best practices creating data fileshttps://www.youtube.com/watch?time_continue=309&v=Ry2xjTBtNFE – excellent video introduction best practices organizing data spreadsheet subsequently analyzed statistics software.https://www.youtube.com/watch?time_continue=309&v=Ry2xjTBtNFE – excellent video introduction best practices organizing data spreadsheet subsequently analyzed statistics software.Broman, K. W., & Woo, K. H. (2017). Data organization spreadsheets (. e3183v1). https://doi.org/10.7287/peerj.preprints.3183v1 – excelllent review best practices organizing data spreadsheet.Broman, K. W., & Woo, K. H. (2017). Data organization spreadsheets (. e3183v1). https://doi.org/10.7287/peerj.preprints.3183v1 – excelllent review best practices organizing data spreadsheet.","code":"\nexp1h <- read_excel(file_path,\n                    sheet = \"H\") %>%\n  clean_names() %>%\n  data.table()\nexp1h <- data.table(clean_names(read_excel(file_path,\n                    sheet = \"H\"))) # convert to data.table\nexp1h <- read_excel(file_path,\n                    sheet = \"H\")\nnames(exp1h)## [1] \"Diet\"             \"Replicate\"        \"Weight (mg/fly)\"  \"TAG (ug/fly)\"    \n## [5] \"Protein (ug/fly)\" \"TAG/Protein\"\nexp1h <- clean_names(exp1h)\nnames(exp1h)## [1] \"diet\"           \"replicate\"      \"weight_mg_fly\"  \"tag_ug_fly\"    \n## [5] \"protein_ug_fly\" \"tag_protein\"\nexp1h <- data.table(exp1h)"},{"path":"data.html","id":"the-read_excel-function","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.1.3 The read_excel function","text":"read_excel beautifully flexible function Excel. Data can different sheets can different datasets within single sheet. , researchers tend use Excel like blackboard Excel sheet often contains calculations means, standard deviations t-tests. using read_excel important send function enough information read correct data. exp1h data, simply usedwithout specifying sheet, read_excel defaults reading first sheet (“”), wanted.can specify exact range important using range = argumentThis isn’t necessary data “H” sheet contains matrix data extraneous information read_excel function smart enough figure . many data sets wet bench experimental biology, range argument crucial multiple datasets archived single sheet.can also specify arguments. example, following code explicitly tells read_excel use first row data range used column names first row data. TRUE default argument, isn’t necessary exp1h data data want import doesn’t column names (can common archived data sets) need pass argument “col_names = FALSE”.","code":"\nexp1h <- read_excel(file_path) %>%\n  clean_names() %>%\n  data.table()\nexp1h <- read_excel(file_path,\n                    sheet = \"H\",\n                    range = \"A1:F49\") %>%\n  clean_names() %>%\n  data.table()\nexp1h <- read_excel(file_path,\n                    sheet = \"H\",\n                    range = \"A1:F49\",\n                    col_names = TRUE) %>%\n  clean_names() %>%\n  data.table()"},{"path":"data.html","id":"a-quick-plot-of-the-exp1h-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.1.4 A quick plot of the exp1h data","text":"Just fun, let’s use ggboxplot function ggpubr package reproduce something close Fig. 1H. make plot looks exactly like Fig. 1H require wrangling outlined .Notes code make plotthe names columns use x y axes passed inside quotes. R functions, column names passed inside quotes R functions, column names passed -. little rhyme reason .","code":"\nggboxplot(data = exp1h,\n          x = \"diet\",\n          y = \"tag_protein\",\n          ylab = \"TAG/Protein\",\n          xlab = \"\",\n          add = \"jitter\")"},{"path":"data.html","id":"importing-text-files","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.2 Importing text files","text":"Source article: Corrigan, June K., et al. “big-data approach understanding metabolic rate response obesity laboratory mice.” Elife 9 (2020): e53560..file name: mmpc_all_phases.csvLet’s open data Fig. 1.StepsCopy title paper title, “big-data approach understanding metabolic rate response obesity laboratory mice”Create new folder within “data”. Name folder title paper pasting clipboard. “data ” folder, since contains data publication title folder.Click link file , highlight values window clicking inside window typing command-(Mac os) control-(Windows) copy.Paste open, blank text document save “mmpc_all_phases.csv”. saved elsewhere, move file “big-data approach understanding metabolic rate response obesity laboratory mice” folder within “data” folder.don’t dedicated text editor BBEdit MacOS, open new R (Rmd!) document R studio. open new tab Script Editor pane. Paste data R document window. Save “mmpc_all_phases.csv”. R Studio ask sure want extension .csv .R. Yes sure. saved elsewhere, move file “big-data approach understanding metabolic rate response obesity laboratory mice” folder within “data” folder..csv file comma-delimited text file, means entries row separated commas. text file readable text editor software kinds software. Datasets stored text files typically saved either .csv (entries row separated commas) .txt (entries separated tabs). base R way read .csv file using read.csv. read.table function versatile, delimiter can specified. function fread() data.table package fast, smart, flexible. smart sense guesses delimter .sure read section importing Excel files understand code. , import Excel file, first three lines create directory path file. need pipe exp1 data.table function fread automatically opens data data.table. imported Excel file , code sends data object clean_names change column labels snake_case.","code":"\n# construct file path\ndata_from <- \"A big-data approach to understanding metabolic rate and response to obesity in laboratory mice\"\nfile_name <- \"mmpc_all_phases.csv\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# open file and assign to exp1, then pipe to clean_names\n# to change column names to snake_case\nexp1 <- fread(file_path) %>%\n  clean_names()"},{"path":"data.html","id":"a-quick-plot-of-the-exp1-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.2.1 A quick plot of the exp1 data","text":"Let’s use ggscatter function ggpubr package reproduce something close Fig. 1D. make plot looks exactly like Fig. 1D require additional plot arguments.","code":"\nggscatter(data = exp1[diet == \"HF\" &\n                    acclimation == TRUE],\n          x = \"total_mass\",\n          y = \"ee\",\n          color = \"institution\",\n          palette = pal_okabe_ito_4,\n          add = \"reg.line\",\n          ylab = \"energy expenditure (kcal/hr)\",\n          xlab = \"body mass (g)\")"},{"path":"data.html","id":"troubleshooting-file-import","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.3 Troubleshooting file import","text":"get error starts “Error: path exist:” R “seeing” specified file given path ’ve given .Make sure working within project checking project name right side R Studio tool bar. project “None” working within different project, open new project.Make sure loaded package “setup” chunk run setup chunkMake sure assigned data_folder <- \"data\" setup chunk run chunk.Make sure “data” folder one level inside project folder. “one level” means buried deeper inside folders within project folder.Make sure “data ” folder (folder title publication) one level inside “data” folderMake sure data file one level inside correct “data ” folder.Bug alert Make sure name “data …” folder correct script. type name folder. Instead, go finder highlight folder containing data file, copy name, return R markdown script, type folder <- \"\" paste clipboard (name folder) quote marks.Bug alert Make sure file name correct script. folder name, go finder copy file name paste place. Windows use ctrl-instead ctrl-c copy full filename including extension.generally, Humans good understanding misspelld OdDLy capitalized words R language (computer language) literal. R case sensitive (programming languages ). “Prenatal acoustic communication”, “Prenatal Acoustic Communication”, “prenatal acoustic communication” different values. Spelling capitalization perfect, simply close. Spelling includes spaces. frequent bug file name typed “Prenatal acoustic communication” actual name “Prenatal acoustic communication”. Can spot bug? original (need copy) two spaces “acoustic” “communication” incorrect copy one.Spelling bugs avoided simply copying pasting names folders, names files, column names data frames, level names factors, leads general rule R scripting…","code":""},{"path":"data.html","id":"rule-number-one-in-r-scripting-rule1","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.4.1.4 Rule number one in R scripting {# rule1}","text":"Always copy paste text inserted quotesDo try type . warned.","code":""},{"path":"data.html","id":"data-wrangling","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5 Data wrangling","text":"Data archived Excel spreadsheets, least wet-bench experimental biology projects, generally format readily analyzed R, statistical software perhaps Graphpad Prism. Use examples templates import wrangle Excel-archived data project.","code":""},{"path":"data.html","id":"reshaping-data-wide-to-long","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.1 Reshaping data – Wide to long","text":"","code":""},{"path":"data.html","id":"wide-to-long-adipsin-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.1.1 Wide to long – Adipsin data","text":"Source: Adipsin preserves beta cells diabetic mice associates protection type 2 diabetes humansPublic source – Adipsin paper behind paywall. public source paper NIH available.Link source dataFig. 1k Adipsin paper presents bar plot glucose uptake response control (GFP) adipsin treatment. screenshot Excel-archived data shown . data wide format. wide-format, values single variable (, glucose uptake level) given separate columns treatment level (group). values GFP group Column values Adipsin group Column B. Wide format efficient computations spreadsheet, computing means standard deviations columns data, plotting.statistical analyses experimental data R (statistics software), values single variable single column. called long format. ’ve manually rearranged data archived spread sheet long format stacking group’s values single column, shown screen capture . values glucose uptake single column. long format, needs way identify values belong group achieved column “treatment”. adition treatment column.difference wide long also reflects think statistical analysis. t-test compare means glucose uptake GFP Adipsin groups, might think two things: set glucose uptake values GFP group set values Adipsin group. fit linear model, also two things, variable treatment containing treatment level assignment variable glucose_uptake containing glucose uptake values. wide format, nothing suggest treatment variable.many functions tidy data wide long. melt data.table package especially useful. data.table’s version melt reshape2 package.major arguments data.table::melt aremelt(data, id.vars, measure.vars, variable.name, value.name)melt takes data columns listed measure.vars stacks single column named value.name. names columns measure.vars values elements new column named variable.name. elements column id.vars repeated p times, p number columns stacked.Let’s melt three different response variables adipsin data merge single data.table. several ways combine data sets including merge cbind. ’ll compare later.pretty-good-plot using ggpubr package","code":"\nfile_folder <- \"Adipsin preserves beta cells in diabetic mice and associates with protection from type 2 diabetes in humans\"\nfn <- \"41591_2019_610_MOESM3_ESM.xlsx\"\nfile_path <- here(data_folder, file_folder, fn)\n\ntreatment_levels <- c(\"db/db-GFP\", \"db/db-Adipsin\")\n\n# as separate line\nfig_1k_wide <- read_excel(file_path,\n                    sheet = \"Figure 1k\",\n                    range = \"A3:B9\")\nfig_1k_wide <- data.table(fig_1k_wide)\nfig_1k <- melt(fig_1k_wide,\n               measure.vars = treatment_levels,\n               variable.name = \"treatment\",\n               value.name = \"glucose_uptake\")\n\n# or piped -- which do you prefer?\nfig_1k <- read_excel(file_path,\n                    sheet = \"Figure 1k\",\n                    range = \"A3:B9\") %>%\n  data.table() %>%\n  melt(measure.vars = treatment_levels,\n       variable.name = \"treatment\",\n       value.name = \"glucose_uptake\")\n\n# View(fig_1k) # highlight without the comment sign and \"run selected lines()\" to view\n# put warning=FALSE into the chunk header to supress the warning\n\ngg <- ggstripchart(x = \"treatment\",\n                   y = \"glucose_uptake\",\n                   add = \"mean_se\",\n                   data = fig_1k)\n\ngg"},{"path":"data.html","id":"wide-to-long-enteric-nervous-system-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.1.2 Wide to long – Enteric nervous system data","text":"Source: Rolig, . S., Mittge, E. K., Ganz, J., Troll, J. V., Melancon, E., Wiles, T. J., … Guillemin, K. (2017). enteric nervous system promotes intestinal health constraining microbiota composition. PLOS Biology, 15(2), e2000689.Source dataLet’s import reshape data figure 2d. Look excel file data Fig. 2d. single treament four levels, authors organized data level separate columns used column header level name.Let’s melt data wide long stacking four columns single column “neutrophil_count” adding treatment column identifying group.learn (instead just copy modify), ’s best steps run whole chunk. step, look result using View. script includes three extra wrangling steps.Changing column names fig_2d_wide. column names wide format become treatment level names treatment factor reshaping. easier road names shorter “_donor” name redundant. setnames function renames column names.Changing column names fig_2d_wide. column names wide format become treatment level names treatment factor reshaping. easier road names shorter “_donor” name redundant. setnames function renames column names.data, number measures within different treatments differs , consequence, multiple cells NA indicates missing value. View(fig_2d_wide) (can typed console) see . reshaping long format (fig_2d), rows missing values become empty rows – useful information (View ). see , re-run lines chunk line “# omit empty rows”. na.omit function deletes row missing values. , deletes information-less rows. careful na.omit. want delete rows data contain information want.data, number measures within different treatments differs , consequence, multiple cells NA indicates missing value. View(fig_2d_wide) (can typed console) see . reshaping long format (fig_2d), rows missing values become empty rows – useful information (View ). see , re-run lines chunk line “# omit empty rows”. na.omit function deletes row missing values. , deletes information-less rows. careful na.omit. want delete rows data contain information want.analysis plots, want compare values control level, named “wt” fig_2d data. , want “wt” reference level. achieve , levels factor treatment need re-ordered using levels argument. (note, typically add “levels =”, simply pass list levels)analysis plots, want compare values control level, named “wt” fig_2d data. , want “wt” reference level. achieve , levels factor treatment need re-ordered using levels argument. (note, typically add “levels =”, simply pass list levels)","code":"\nfolder <- \"The enteric nervous system promotes intestinal health by constraining microbiota composition\"\nfilename <- \"journal.pbio.2000689.s008.xlsx\"\nfile_path <- here(data_folder, folder, filename)\n\n# figure 2D data\nsheet_i <- \"Figure 2\"\nrange_i <- \"F2:I24\"\nfig_2d_wide <- read_excel(file_path, sheet=sheet_i, range=range_i) %>%\n  clean_names() %>%\n  data.table()\n\n# change column names by replacing without \"_donor\" in each name\n# these new column names will become the levels of the treatment factor\nnew_colnames <- c(\"gf\", \"wt\", \"sox10\", \"iap_mo\")\nsetnames(fig_2d_wide, old=colnames(fig_2d_wide), new=new_colnames)\n\n# wide to long\nfig_2d <- melt(fig_2d_wide, \n              measure.vars=colnames(fig_2d_wide), \n              variable.name=\"treatment\", \n              value.name=\"neutrophil_count\")\n\n# omit empty rows\nfig_2d <- na.omit(fig_2d)\n\n# re-order factors\nfig_2d[, treatment := factor(treatment,\n                         levels = c(\"wt\", \"gf\", \"sox10\", \"iap_mo\"))]\n\n# View(fig_2d)"},{"path":"data.html","id":"wide-to-long-bee-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.1.3 Wide to long – bee data","text":"example pretty easy, columns original data frame melted (stacked). example subset columns stacked. addition, subset remaining columns retained long format data frame. data Panel supplement Fig. 8 (https://journals.plos.org/plosbiology/article/file?type=supplementary&id=info:doi/10.1371/journal.pbio.2003467.s019){target=“_blank”} fromSource: Kešnerová, L., Mars, R.., Ellegaard, K.M., Troilo, M., Sauer, U. Engel, P., 2017. Disentangling metabolic functions bacteria honey bee gut. PLoS biology, 15(12), p.e2003467.Source data","code":"\nfolder <- \"Data from Disentangling metabolic functions of bacteria in the honey bee gut\"\nfilename <- \"journal.pbio.2003467.s001.xlsx\"\n\n# figure 2D data\nsheet_i <- \"S8 Fig\"\nrange_i <- \"A2:H12\"\nfile_path <- here(data_folder, folder, filename)\nfig_s8a_wide <- read_excel(file_path,\n                      sheet=sheet_i,\n                      range=range_i) %>%\n  clean_names() %>%\n  data.table()\n\n# wide to long\nstack_cols <- paste0(\"replicate\", 1:5)\nfig_s8a <- melt(fig_s8a_wide,\n              id.vars = c(\"media\", \"time_h\"),\n              measure.vars = stack_cols, \n              variable.name = \"Replicate\", \n              value.name = \"OD600\") # measure of absorbance at 600nm"},{"path":"data.html","id":"wide-to-long-stacking-multiple-sets-of-columns","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.1.4 Wide to long – stacking multiple sets of columns","text":"Article: GPR109A mediates effects hippuric acid regulating osteoclastogenesis bone resorption micesource dataThe data Fig. 1bTo analyze response, two treatment levels measure need stacked single column. easy using melt data.table package. Add chunk.","code":"\nfolder <- \"GPR109A mediates the effects of hippuric acid on regulating osteoclastogenesis and bone resorption in mice\"\nfile_name <- \"42003_2020_1564_MOESM4_ESM.xlsx\"\nfile_path <- here(data_folder, folder, file_name)\n\nfig1b_wide <- read_excel(file_path,\n                    sheet = \"Fig. 1b\",\n                    range = \"C4:R9\",\n                    col_names = FALSE) %>%\n  data.table()\n\ntreatment_levels <- c(\"Wild type\", \"GPR109A-/-\")\ntreatment_abbrev <- c(\"wt\", \"ko\")\nmeasures <- c(\"BV/TV\", \"Tb.Th\", \"Tb.Sp\", \"Tb.N\", \"BMD\", \"Cs.Th\", \"BV\", \"MA\")\nnew_colnames <- paste(rep(measures, each = 2),\n                              treatment_abbrev,\n                              sep = \"_\")\nsetnames(fig1b_wide,\n         old = colnames(fig1b_wide),\n         new = new_colnames)\nfig1b <- melt(fig1b_wide,\n              measure.vars = list(\n                c(\"BV/TV_wt\", \"BV/TV_ko\"),\n                c(\"Tb.Th_wt\", \"Tb.Th_ko\"),\n                c(\"Tb.Sp_wt\", \"Tb.Sp_ko\"),\n                c(\"Tb.N_wt\", \"Tb.N_ko\"),\n                c(\"BMD_wt\", \"BMD_ko\"),\n                c(\"Cs.Th_wt\", \"Cs.Th_ko\"),\n                c(\"BV_wt\", \"BV_ko\"),\n                c(\"MA_wt\", \"MA_ko\")),\n              variable.name = \"treatment_id\",\n              value.name = measures)\nfig1b[, treatment := ifelse(treatment_id == 1,\n                            treatment_levels[1],\n                            treatment_levels[2])]\nfig1b[, treatment := factor(treatment,\n                            levels = treatment_levels)]"},{"path":"data.html","id":"reshaping-data-transpose-turning-the-columns-into-rows","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.2 Reshaping data – Transpose (turning the columns into rows)","text":"","code":""},{"path":"data.html","id":"transpose-pi3k-inhibitors-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.2.1 Transpose – PI3K inhibitors data","text":"Source: Suppression insulin feedback enhances efficacy PI3K inhibitorsSource dataFigure 3A publication plot blood glucose level taken individual mice four treatment groups six time periods. Data single variable blood glucose, taken individual multiple time points, known longitudial data often mistakenly called repeated measures data. mulitple ways analyze longitudinal data, goood, less good. two reasonable ways archive longitudinal data analysis R. Excel-archived data Figure 3A neither. screen capture two four treatment groups shown .archived data individual mice columns. measure time point rows. treatment group blocks. Typical data analysis R individual mice rows variable columns (exception experimental biology omics data, RNA expression levels. Many packages functions analyze data genes row individual column). Figure 3A data turned side. need transpose data, rotate matrix 90 degrees (make columns rows rows columns) turn data wide format. can create new data.table data long format.NotesRead comments usage keep.names make.names arguments transpose. powerful.pi3k_wide column names times (minutes). presents wrangling problems (column names shouldn’t numbers. useful create long format data.table time column numbers). example, code creates copies column “0” new column “glucose_0” using glucose_0 := get(\"0\"). code glucose_0 := \"0\", values character “0”. code glucose_0 := 0, values number 0. get looks column name whatever inside parentheses.Let’s quick plot examine data","code":"\nfolder <- \"Suppression of insulin feedback enhances the efficacy of PI3K inhibitors\"\nfilename <- \"41586_2018_343_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, folder, filename)\npi3k_side <- read_excel(file_path,\n                    sheet = \"Figure 3A (Blood Glucose)\",\n                    range = \"A2:U7\",\n                    col_names = FALSE) %>%\n  data.table()\n\n# give columns names as the treatment of each mouse\n# verify n=5 per group\ntreatment_levels <- c(\"Chow\", \"Ketogenic\", \"Metformin\", \"SGLT2i\")\ncolnames(pi3k_side) <- c(\"time\",\n                         rep(treatment_levels, each = 5))\n\n# transpose\n# keep colnames in \"side\" as values of treatment col in \"wide\"\n# make values of \"time\" in \"side\" the colnames in \"wide\"\npi3k_wide <- transpose(pi3k_side,\n                       keep.names = \"treatment\", \n                       make.names = \"time\")\n\n# make a baseline column \npi3k_wide[, glucose_0 := get(\"0\")]\n\n# make-up a mouse id for each mouse\npi3k_wide[, id := paste(treatment, 1:.N, sep = \"_\"), by = treatment]\n\n# make treatement a factor with \"chow\" as reference\npi3k_wide[, treatment := factor(treatment, treatment_levels)]\n\n# make a long version\npi3k_long <- melt(pi3k_wide,\n                  id.vars = c(\"treatment\", \"id\", \"glucose_0\"),\n                  variable.name = \"time\",\n                  value.name = \"glucose\")\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long,\n      color = treatment) +\n  geom_line(aes(group = id))"},{"path":"data.html","id":"combining-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.3 Combining data","text":"Source Bak, .M., Vendelbo, M.H., Christensen, B., Viggers, R., Bibby, B.M., Rungby, J., Jørgensen, J.O.L., Møller, N. Jessen, N., 2018. Prolonged fasting-induced metabolic signatures human skeletal muscle lean obese men. PloS one, 13(9), p.e0200817.Source dataThe data randomized crossover design 18 men (9 lean 9 obese) measured multiple metabolic markers two times: 1) post-absorptive state 12 hours overnight fast, 2) prolonged fasting state 72 hours fasting. addition, time point, metabolic markers measured prior insulin infusion. , want reproduce values Table 2, measures mean blood insulin metabolite levels 12 hours 72 hours fasting lean obese groups.difficulty analyst response data “Table 2” sheet variable containing assignment “lean” “obese” group “Table 1” sheet. analyze response, two datasets need combined single data frame. important consideration combining data like matched like. fasting dataset, “like” subject id, data subject id Table 1 data subject ids Table 2. means essentially want glue columns table 2 columns table 1 way insures correct data subject id row. bit complicated data Table 1 contains 18 data rows, one subject id Table 2 contains 36 data rows, 2 subject id, subject data measured 12 hours 72 hours.","code":""},{"path":"data.html","id":"data-subset","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.4 Subsetting data","text":"common see researchers create multiple subsets data processing. practice discouraged variables multiple data frames can hard keep track processing variables different datasets. Instead, subset data level analysis.many ways subset data R. Experienced users tend divide using base R, using tidyverse packages, using data.table. Learn one well. book uses data.table. outlining usage data.table, let’s back bit review different indexing systems.Excel, rows specified (“indexed”) numbers columns letters. Every cell address, example C2 cell 2nd row 3rd column. Notice Excel, column part address comes row part.statistics, extremely common use system \\(x_{ij}\\) value element ith row jth column matrix X. Notice notatin, row index () comes column index (j).programming languages, including R, extremely common use system my_data[, j] value element ith row jth column matrix-like object named “my_data” (data frame R).data.table explicitly refers row index column index j.","code":""},{"path":"data.html","id":"specifying-a-subset-of-rows-observations-or-cases","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.4.1 Specifying a subset of rows (“observations” or “cases”)","text":"subset rows specified using either list row numbers orIn data.table, subset rows specified using either list row numbers combination comparison operators (==, !=, >, <, >=, <=, %%) Boolean logic operators (&, |, ! – “”, “”, “”) .Let’s use pi3k_long data explore . First, plot plasma glucose individuals treatment group across time points.pi3k_long[treatment == \"Chow\",]) subset rows entries column “treatment” take value “Chow” using “equal” (“==”) operatorAnd subset rows entries column “treatment” take value “Chow” using “equal” operator (“!=”).subset rows entries column “treatment” take either value “Chow” value “SGLT2i” combining two “equal” (“==”) operators using (“|”) boolean operatorThe subset rows entries column “time” take either value “30” value “60” using “list” operator (%%). values “time” column look like integers actually treatment levels (act like string character variables).subset rows entries column “time_c” less equal 60 using “less equal ” operator value treatment column list (“Chow”, “SGLT2i”). two comparisons combined (“&”) Boolean operator.result using different operators. describe , subset rows entries column “time_c” less equal 60 using “less equal ” operator value treatment column either “Chow” “SGLT2i”. two comparisons combined (“&”) Boolean operator. order operations determined parentheses, algebra.","code":"\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long,\n      color = treatment) +\n  geom_line(aes(group = id))\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment == \"Chow\",],\n      color = treatment) +\n  geom_line(aes(group = id))\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment != \"Chow\",],\n      color = treatment) +\n  geom_line(aes(group = id))\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[treatment == \"Chow\" | treatment == \"SGLT2i\",],\n      color = treatment) +\n  geom_line(aes(group = id))\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time  %in% c(\"30\", \"60\"),],\n      color = treatment) +\n  geom_line(aes(group = id))\npi3k_long[, time_c := as.numeric(as.character(time))]\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time_c <= 30 & treatment %in% c(\"Chow\", \"SGLT2i\"),],\n      color = treatment) +\n  geom_line(aes(group = id))\npi3k_long[, time_c := as.numeric(as.character(time))]\nqplot(x = time,\n      y = glucose,\n      data = pi3k_long[time_c <= 30 & (treatment == \"Chow\" | treatment == \"SGLT2i\"),],\n      color = treatment) +\n  geom_line(aes(group = id))"},{"path":"data.html","id":"wrangling-columns","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5 Wrangling columns","text":"","code":""},{"path":"data.html","id":"creating-new-columns-that-are-functions-of-values-in-existing-columnes","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5.1 Creating new columns that are functions of values in existing columnes","text":"data.table allows math within index arguments.Example 1: create new column whose value function values columnsIn example, create column contains number marked cells percent total cell count.Notes\\(\\texttt{marked_cells}\\) \\(\\texttt{total_cells}\\) columns data.table figxYou almost certainly want analyze original count column use \\(\\texttt{total_cells}\\) offset model. See section Use GLM offset instead ratio measurement per totalExample 2: create new column whose value function values columnsIn example, create column \\(\\texttt{glucose_auc}\\), area curve glucose measures different time points glucose tolerance test. computation requires auc function (typically, put chunks functions top R Markdown document). computation uses base R apply function.implementation. object time_cols vector containing names columns containing data time point.","code":"\n# count to fraction - but probably want to analyze as a count! See xxx\nfigx[, marked_cells_perc := marked_cells/total_cells * 100]\nauc <- function(x, y, method=\"auc\"){\n  # method = \"auc\", auc computed using trapezoidal calc\n  # method = \"iauc\" is an incremental AUC of Le Floch\n  # method = \"pos.iauc\" is a \"positive\" incremental AUC of Le Floch but not Wolever\n  # method = \"pb.auc\" is AUC of post-time0 values\n  if(method==\"iauc\"){y <- y - y[1]}\n  if(method==\"pos.iauc\"){y[y < 0] <- 0}\n  if(method==\"pb.auc\"){\n    x <- x[-1]\n    y <- y[-1]\n  }\n  n <- length(x)\n  area <- 0\n  for(i in 2:n){\n    area <- area + (x[i] - x[i-1])*(y[i-1] + y[i])\n  }\n  area/2\n}\ntime_cols <- c(\"time_0\", \"time_15\", \"time_30\", \"time_60\", \"time_90\", \"time_120\")\nY <- figx_wide[, .SD, .SDcols = time_cols]\nfigx_wide[, glucose_auc := apply(Y, 1, auc, x = times)]"},{"path":"data.html","id":"change-the-reference-level-of-a-factor","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5.2 Change the reference level of a factor","text":"Factor levels always arranged sensible order model fitting plotting. first level vector reference level, important concept understanding coefficients statistical models fit book.","code":"\ntreatment_levels <- c(\"WT\", \"KO\", \"KO_drug\")\nfigx[, treatment := factor(treatment,\n                           levels = treatment_levels)]"},{"path":"data.html","id":"converting-a-single-column-with-all-combinations-of-a-2-x-2-factorial-experiment-into-two-columns-each-containing-the-two-levels-of-a-factor","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5.3 Converting a single column with all combinations of a 2 x 2 factorial experiment into two columns, each containing the two levels of a factor","text":"","code":""},{"path":"data.html","id":"example-1-tstrspl","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5.3.1 Example 1 (tstrspl)","text":"example data analyzed chapter Linear models two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)Article source: TLR9 beclin 1 crosstalk regulates muscle AMPK activation exercisePublic sourceThe data Figure 2j.Data sourceMelting wide data creates column \\(\\texttt{treatment}\\) containing column names wide data. column type character (often called “string” variable string characters). want split string space, put first part one new column second part second new column.tstrsplit() function data.table package split us. first argument name column contains strings want split. second argument character want split . c(\"genotype\", \"stimulation\") := sets names new columns containing. really elegant solution.replaced original column names initial import several reasons:“Electrical Stimulation” long make coefficient names unwieldy. shortened “Stimulation”, …don’t want factor name include level name. like factor name \\(\\texttt{stimulation}\\), renamed level “Electrical Stimulation” “Active”.splitting column names using space character ” ” two original column names two spaces, 2nd inside single level (“Electrical Stimulation”)read_excel function removed space last column name, read “Tlr9-/-Electrical Stimulation”.peek data prior melting wide longAnd peek first three last three rows melt.","code":"\ndata_from <- \"TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\"\nfile_name <- \"41586_2020_1992_MOESM4_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\ntreatment_levels  <- c(\"WT Resting\",\n                       \"WT Active\",\n                       \"Tlr9-/- Resting\",\n                       \"Tlr9-/- Active\")\nexp2j_wide <- read_excel(file_path,\n                         sheet = \"2j\",\n                         range = \"A5:D13\",\n                         col_names = TRUE) %>%\n  data.table()\n\ncolnames(exp2j_wide) <- treatment_levels\n\nexp2j <- melt(exp2j_wide,\n              measure.vars = treatment_levels,\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\") %>%\n  na.omit() # danger!\n\nexp2j[, c(\"genotype\", \"stimulation\") := tstrsplit(treatment,\n                                                  \" \",\n                                                  fixed = TRUE)]\n\ngenotype_levels <- c(\"WT\", \"Tlr9-/-\")\nstimulation_levels <- c(\"Resting\", \"Active\")\nexp2j[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nexp2j[, stimulation := factor(stimulation,\n                              levels = stimulation_levels)]\n# View(exp2j)"},{"path":"data.html","id":"example-2","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.5.3.2 Example 2","text":"Source: Tauriello, D., Palomo-Ponce, S., Stork, D. et al. TGFβ drives immune evasion genetically reconstituted colon cancer metastasis. Nature 554, 538–543 doi:10.1038/nature25492Source datafilename: “41586_2018_BFnature25492_MOESM10_ESM.xlsx”sheet: “Fig. 4h-tumours”analysis data Fig. 4h specifies single \\(X\\) variable “Treatment” four levels (groups): “Con”, “Gal”, “aPD-L1”, “Gal+aPD-L1”. levels indicate design actually factorial two factors, two levels. first factor levels “Gal” “Gal”. second factor levels “aPD-L1”, “aPD-L1”. single column Treatment “flattens” 2 X 2 factorial design 4 x 1 design. general, want analyze experiment like factorial model, allows us make inferences interaction effect two factors. inferences, need standard error, confidence interval, p-value estimate, can easily get factorial model. order analyze data factorial model, need create two new columns – one column factor variable containing two levels Gal one column factor variable containing two levels aPD-L1.way check results make sure conversion correct compute sample size 2 x 2 combinations, include original treatment column list.looks good.Bug alert break Rule #1, type treatment level “Gal+aPD-L1” “Gal + aPD-L1”, get new columns containing junk.Remember Rule #1. Always copy paste text inserted quotes. easily done typing unique(tumor$treatment) console. function returns unique values column “treatment” data.table “tumor”.unique(tumor$treatment)\n[1] “Con” “Gal” “aPD-L1” “Gal+aPD-L1”Now, copy name level paste code. Repeat done.","code":"\ngal_levels <- c(\"no Gal\", \"Gal\")\ntumor[, gal := ifelse(treatment == \"Gal\" | treatment == \"Gal+aPD-L1\",\n                      gal_levels[2],\n                      gal_levels[1])]\n\napd_levels <- c(\"no aPD-L1\", \"aPD-L1\")\ntumor[, apdl1 := ifelse(treatment == \"aPD-L1\" | treatment == \"Gal+aPD-L1\",\n                      apd_levels[2],\n                      apd_levels[1])]\n\n# re-order factor levels\ntumor[, gal:=factor(gal, gal_levels)]\ntumor[, apdl1:=factor(apdl1, apd_levels)]\ntumor[!is.na(num_positive_per_mm), .(N=.N), by=.(treatment, gal, apdl1)]##     treatment    gal     apdl1   N\n## 1:        Con no Gal no aPD-L1 124\n## 2:        Gal    Gal no aPD-L1  89\n## 3:     aPD-L1 no Gal    aPD-L1 101\n## 4: Gal+aPD-L1    Gal    aPD-L1  58##     treatment    gal     apdl1   N\n## 1:        Con no Gal no aPD-L1 124\n## 2:        Gal    Gal no aPD-L1  89\n## 3:     aPD-L1 no Gal    aPD-L1 101\n## 4: Gal+aPD-L1 no Gal no aPD-L1  58"},{"path":"data.html","id":"missing-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6 Missing data","text":"Source: Deletion Cdkn1b ACI rats leads increased proliferation pregnancy-associated changes mammary gland due perturbed systemic endocrine environmentSource dataSupplement Figure 1F paper shows weight function age class genotype whole body 8 organs. missing weights Excel-archived data. missing data designated minus “-” sign. import data correctly, use na = argument read_excel function.NotesIn R, value “NA” represents missing.default value na = empty (blank) cell (space cell empty).na = accepts list strings, example na = c(\"\", \"-99\", \"--\") read na.","code":"\nfile_folder <- \"Deletion of Cdkn1b in ACI rats leads to increased proliferation and pregnancy-associated changes in the mammary gland due to perturbed systemic endocrine environment\"\nfile_name <- \"journal.pgen.1008002.s008.xlsx\"\nfile_path <- here(data_folder, file_folder, file_name)\n  \nfig_s1f <- read_excel(file_path,\n                    sheet = \"all weights\",\n                    range = \"A2:K57\",\n                    na = \"-\",\n                    col_names = TRUE) %>%\n  clean_names() %>%\n  data.table()\n\nfig_s1f[, genotype := factor(genotype, c(\"+/+\", \"-/-\"))]\nfig_s1f[, age_class := ifelse(age_at_sac_wks <= 6.0, \"4-6\", \"8+\")]\n\n# View(fig_s1f)"},{"path":"data.html","id":"handling-missing-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.1 Handling missing data","text":"","code":""},{"path":"data.html","id":"many-base-r-functions-used-for-summary-measures-require-na-handling","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.1.1 Many base R functions used for summary measures require NA handling","text":"many ways get sample size particular variable. careful using length() counts NA part vector values.","code":"\nmean(fig_s1f[, ovary]) # returns \"NA\"## [1] NA\nmean(fig_s1f[, ovary], na.rm = TRUE) # returns the mean## [1] 0.2489524\nsd(fig_s1f[, ovary]) # returns \"NA\"## [1] NA\nsd(fig_s1f[, ovary], na.rm = TRUE) # returns the mean## [1] 0.151694\nsum(fig_s1f[, ovary]) # returns \"NA\"## [1] NA\nsum(fig_s1f[, ovary], na.rm = TRUE) # returns the mean## [1] 10.456"},{"path":"data.html","id":"the-is.na-function-is-useful","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.1.2 The !is.na function is useful","text":"Notes!.na(ovary) taking subset rows fig_s1f value “ovary” NA (!.na read “.na”)especially useful creating code uses counts. create table means, standard error mean, 95% CIs mean genotype group. first, script generates wrong N group (since missing values), although mean SD correct.compute correct n, necessary computing SE CI, use !.na","code":"\nlength(fig_s1f[, ovary])## [1] 55\nlength(fig_s1f[!is.na(ovary), ovary])## [1] 42\nfig_s1f[, .(mean = mean(spleen, na.rm = TRUE),\n            n = .N,\n            sd = sd(spleen, na.rm = TRUE)),\n        by = genotype]##    genotype      mean  n         sd\n## 1:      -/- 0.5801333 21 0.13680480\n## 2:      +/+ 0.2956667 34 0.04460855\nspleen_summary <- fig_s1f[!is.na(spleen), .(mean = mean(spleen),\n            n = .N,\n            sd = sd(spleen)),\n        by = genotype]\nspleen_summary[, se := sd/sqrt(n)]\nspleen_summary[, lower := mean + se*qt(.025, (n-1))]\nspleen_summary[, upper := mean + se*qt(.975, (n-1))]\nspleen_summary##    genotype      mean  n         sd         se     lower     upper\n## 1:      -/- 0.5801333 15 0.13680480 0.03532285 0.5043734 0.6558933\n## 2:      +/+ 0.2956667 27 0.04460855 0.00858492 0.2780201 0.3133132"},{"path":"data.html","id":"ggplot-functions-automatically-handle-missing-values","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.1.3 ggplot functions automatically handle missing values","text":"useful warning.","code":"\nqplot(x = body_wt_g_sac,\n      y = spleen,\n      color = genotype,\n      data = fig_s1f)"},{"path":"data.html","id":"regression-model-functions-lm-glm-gls-etc.-handle-missing-values-by-default","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.1.4 Regression model functions (lm, glm, gls, etc.) handle missing values by default","text":"Missing data regression model functions lm handled using argument na.action = default “na.omit”, omits rows contain missing value one model variables (includes rows contain missing values columns included model). ’s user took subset data including columns containing model variables deleted row missing values.coefficient table fit model object explictly tell lm function handle missing data.coefficient table fit model object explicitly tell lm handle missing data, using argument na.action = \"na.exclude\". coefficient tables .","code":"\nm1 <- lm(spleen ~ body_wt_g_sac + genotype,\n         data = fig_s1f)\ncoef(summary(m1))##                 Estimate   Std. Error   t value     Pr(>|t|)\n## (Intercept)   0.04238009 0.0242993900  1.744081 8.902319e-02\n## body_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13\n## genotype-/-   0.23760586 0.0147600545 16.097898 8.072069e-19\nm2 <- lm(spleen ~ body_wt_g_sac + genotype,\n         data = fig_s1f,\n         na.action = \"na.exclude\")\ncoef(summary(m2))##                 Estimate   Std. Error   t value     Pr(>|t|)\n## (Intercept)   0.04238009 0.0242993900  1.744081 8.902319e-02\n## body_wt_g_sac 0.00167493 0.0001506493 11.118067 1.170042e-13\n## genotype-/-   0.23760586 0.0147600545 16.097898 8.072069e-19"},{"path":"data.html","id":"butbeware-of-fitted-predicted-or-residual-values-from-regression-model-functions-unless-youve-explictly-told-the-function-how-to-handle-missing-values","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.5.6.2 But…beware of fitted, predicted, or residual values from regression model functions unless you’ve explictly told the function how to handle missing values","text":"Use na.action = \"na.exclude\" want add fitted (predicted) values residuals new columns original data object (fig_sf1). Compare length fitted values vector models m1 (using default “na.omit”) m2 (using “na.exclude”).55 observations (rows data) 42 complete rows missing values. vector fitted values m1 42 fitted values. vector fitted values m2 55 elements, 42 fitted values plus 13 NA elements.important want something like add fitted values (residuals, function ) original data object (fig_sf1). compute spleen weights adjusted mean body weight control (“+/+”) group using residuals m1 m2.computation “spleen_adj_m1” returns warning values residuals(m1) recycled (first 42 elements new column filled 42 residuals last 13 elements new column filled first 13 residuals) – first row missing data, computed adjusted values wrong. Using residuals(m2), adjusted values matched correct row rows missing variables adjusted value (residual compute ).","code":"\nlength(fitted(m1))## [1] 42\nlength(fitted(m2))## [1] 55\nmean_x_control <- mean(fig_s1f[genotype == \"+/+\", body_wt_g_sac])\nb <- coef(m1)\nfig_s1f[, spleen_adj_m1 := b[1] +\n          b[2]*mean_x_control +\n          b[3]*(as.integer(genotype)-1 +\n          residuals(m1))]\nfig_s1f[, spleen_adj_m2 := b[1] +\n          b[2]*mean_x_control +\n          b[3]*(as.integer(genotype)-1 +\n          residuals(m2))]\n# View(fig_s1f)"},{"path":"data.html","id":"saving-data","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.6 Saving data","text":"many projects, uncommon save data. might save simulated data takes long time (tens minutes hours even days) generate simply want work simulated data future regenerate . might save wrangled data takes long time import wrangle want analyze wrangled data future re-import re-wrangle .data used future R projects, data can saved R object using saveRDS()Reading large .Rds file fast compared reading data stored text file. However, data need imported software, spreadsheet, save data text file.Look project directory make sure file ! used write.table() create tab-delimited text file using sep = \"\\t\" specify tabs separate row elements. “ standard character string tab. Check data_from folder (name”Drosophila serotonin 2A receptor signaling coordinates central metabolic processes modulate aging response nutrient choice”) open file text editor.","code":"\ndata_from <- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\noutfile_name <- \"elife-59399-fig1h.Rds\"\nsave_file_path <- here(data_folder, data_from, outfile_name)\nsaveRDS(object = exp1h, file = save_file_path)\n\n# to read this use\nexp1h <- readRDS(save_file_path)\n# save the data to correct data_from folder\ndata_from <- \"Drosophila serotonin 2A receptor signaling coordinates central metabolic processes to modulate aging in response to nutrient choice\"\n\n# tab delimited\noutfile_name <- \"elife-59399-fig1h.txt\"\nsave_file_path <- here(data_folder, data_from, outfile_name)\nwrite.table(exp1h, save_file_path, sep = \"\\t\", quote = FALSE)\n\n# comma delimited\noutfile_name <- \"elife-59399-fig1h.csv\"\nsave_file_path <- here(data_folder, data_from, outfile_name)\nwrite.table(exp1h, save_file_path, sep = \",\", quote = FALSE)"},{"path":"data.html","id":"exercises","chapter":"3 Data – Reading, Wrangling, and Writing","heading":"3.7 Exercises","text":"Import pretty-good-plot data Figure 2i Adipsin paper. need download archive Excel file “Figure 2”. Store within “Adipsin preserves beta cells…” folder.data percent cells staining NKX6.1, transcription factor protein regulates beta cell development pancreas. Beta cells sense glucose levels blood secrete insulin. Disruption insulin signaling system results Diabetes mellitus.data wide format, treatment group separate column. data need melted long format new column called “treatment”.give pretty good plot data (data object named “adipsin_fig2i”)Import quick pretty-good-plot data Figure 3b PI3K paper. need download archive Excel file “Figure 3”. Store within “Suppression insulin feedback enhances…” folder.data c-peptide levels response treatments. C-peptide cleaved pro-insulin polypeptide circulates blood marker much insulin produced beta cells pancreas.data wide format, treatment group separate column. data need melted long format new column called “treatment”.Modify code exercise 1 pretty-good-plot data exercise 1.","code":"\nggstripchart(data = adipsin_fig2i,\n             x = \"treatment\",\n             y = \"nkx6_1\",\n             add = \"mean_se\")"},{"path":"plotting-models.html","id":"plotting-models","chapter":"4 Plotting Models","heading":"4 Plotting Models","text":", along lines Sarah Susanka’s “Big House,” Kolbert asks group, “Pretty Good House look like?” – Michael Maines2Plots focus reader researcher. Instead mindless plotting, researcher ask series questions every plotWhat point element plot?points want communicate?better practices communicating points?points want communicate covered elements?answer questions inform plotted. result pretty good plot. idea pretty good plot borrowed “pretty good house” concept grew collaborative group builders architects Northern New England. “pretty good house” combines best practices building earth friendly, high performance home reasonable cost. pretty good house governing body awards certificates achievement , instead, set metrics collection building practices can achieve .typical pretty good plot contains combination ofModeled effects confidence intervals. “Effects” differences groups response treatment – raison d’etre experiment.Modeled means confidence intervals.Individual data points summary distribution .","code":""},{"path":"plotting-models.html","id":"pretty-good-plots-show-the-model-and-the-data","chapter":"4 Plotting Models","heading":"4.1 Pretty good plots show the model and the data","text":"data introduce best practices plotting come Figure 2d Figure 2e “ASK1 inhibits browning white adipose tissue obesity”, introduced introductor chapter (Analyzing experimental data linear model)","code":""},{"path":"plotting-models.html","id":"pretty-good-plot-component-1-modeled-effects-plot","chapter":"4 Plotting Models","heading":"4.1.1 Pretty good plot component 1: Modeled effects plot","text":"\nFigure 4.1: Effects plot glucose AUC data\nBiologists infer biological consequences treatment interpreting magnitude sign treatment “effects”, differences means among treatment levels. mostly plot treatment level means, effect magnitude sign can inferred indirectly, mentally computing differences means? pretty good plot directly communicates treatment effects uncertainty estimates effects using effects plot.Figure ?? effects plot linear model fit glucose tolerance data. effects plot “flipped”. y-axis categorical variable – contains labels identifying pair groups contrast direction difference. addition pairwise comparisons, include interaction effect y-axis. x-axis continuous variable – contains simple effects, difference means two groups identified y-axis labels. Additionally, y-axis includes estimate \\(diet \\time genotype\\) interaction effect. bars 95% confidence intervals effects (either simple effects interaction effect), range values compatible observed data 95% level.can use effects CIs effects evaluate treatment effects. example, high fat diet (HFD), mean, post-baseline plasma glucose level ASK1\\(\\Delta\\)adipo 3.5 mmol/L less control (ASK1F/F). Differences less 5.3 mmol/L less ASK1F/F levels greater 1.7 mmol/L less ASK1F/F levels compatible data. research community decide 1.7 mmol/L 3.5 mmol/L differences physiologically meaningful effects.","code":""},{"path":"plotting-models.html","id":"pretty-good-plot-component-2-modeled-mean-and-ci-plot","chapter":"4 Plotting Models","heading":"4.1.2 Pretty good plot component 2: Modeled mean and CI plot","text":"\nFigure 4.2: Response plot\nresponse plot Figure 4.2 “shows model” – mean plot shows modeled means, represented large circles, modeled 95% confidence intervals mean, represented error bars, model-adjusted individual response values, represented small colored dots. mean modeled means, modeled error intervals, model-adjusted responses?modeled means error intervals estimated statistical model. Many published plots show sample means sample error intervals, computed within group independently data groups adjusted covariates hierarchical structure data.modeled mean often equal raw mean, always case. , modeled means non-reference groups Figure ?? equal sample means modeled means adjusted baseline measures glucose (Table ??) (specifically, modeled means conditional baseline equal mean baseline reference group).analyses text, modeled error intervals sample error intervals commonly conspicuously different. glucose tolerance data, modeled error intervals calculated pooled estimate \\(\\sigma\\) sample error intervals estimated sample-specific estimates \\(\\sigma\\) (Table ??).Model-adjusted responses responses adjusted covariates model. covariates model, model-adjusted responses raw response. glucose tolerance data, model-adjusted responses modeled, individual response measures individuals baseline glucose (covariate).Modeled means, error intervals, responses commonly plotted values consistent inferences statistical model. many data sets experimental biology plot sample means, error intervals, responses give distorted view inference model.response plot Figure 4.2 also “shows data” plotting response values “jittered” dots. Showing dataallows reader get sense underlying sample size distribution including outliers, can used mentally model check published statistical analysis. Adding box plot, violin plot, dot plot augments communication distributions enough data justify addition.allows reader see overlap individual responses among groups evaluate biological consequences overlap.","code":""},{"path":"plotting-models.html","id":"combining-effects-and-modeled-mean-and-ci-plots-an-effects-and-response-plot.","chapter":"4 Plotting Models","heading":"4.1.3 Combining Effects and Modeled mean and CI plots – an Effects and response plot.","text":"\nFigure 4.3: Effect diet ASK1 deletion post-baseline glucose. Top: effects plot 2 X 2 simple effects (difference means) diet X genotype interaction. Bars 95% confidence intervals effects. Unadjusted p-values linear model given. Bottom: response plot means 95% confidence interval diet X genotype combination.\nCombining effects response plots single plot easy solution issues arise one used. issues?response plot like Figure 4.2 standard biology, fails show effects, uncertainty effects, explicitly. infer effects plot, reader must perform mental math – either compute difference ratio pairs means. mental math easy enough comparisons individual treatment levels much harder comparisons pooled sets treatment levels, example factorial experimental design. mental math excessively difficult reconstruction kind error interval contrasts, example 95% confidence intervals Figure ?? intervals necessary researcher infer range biological consequences compatible experiment’s results. inclusion p-values pairwise comparisons response plot gives significance level contrasts, kinds summary results present (contrasts, error intervals, p-values), p-values least informative.Effects plots uncommon biology outside meta-analysis clinical medicine generally. effects plot alone fails communicate anything sample size conditional distribution data. Equally important, response values often meaningful researchers working field familiar usual unusual values. can useful interpreting biological consequences treatment effects also researchers readers asses credibility data (example, twice, data colleagues data, found mistakes measurement entire data set response variable plotted values weren’t credible).","code":""},{"path":"plotting-models.html","id":"some-comments-on-plot-components","chapter":"4 Plotting Models","heading":"4.1.4 Some comments on plot components","text":"Several recent criticisms bar plots advocated box plots violin plots alternatives. Box plots violin plots useful alternatives jittered dots sufficient data capture distribution wouldn’t advocate replacing plot modeled means confidence intervals box violin plots, communicate different things. importantly, box violin plots communicate treatment effects.Almost plots biology report error bars represent sample standard error. described , sample standard error bars reflect fit model can highly misleading, least interpreting reflect model. Also, sample standard error bars can explicitly include absurd values imply absurd confidence intervals. example, sometimes see standard error bars cross \\(y=0\\) response negative, count. Even standard error bar doesn’t cross zero, common see standard error bars imply (explicitly show) 95% confidence intervals cross zero, responses negative. standard error bar confidence interval crosses zero implies negative means compatible data. absurd implication responses negative values (“bounded ” zero). Explicit implicit error bars cross zero especially common count responses small means. researcher plots confidence intervals, computed using method avoids absurd implications, methods include bootstrap generalized linear models.Significance stars okay, actual p-value better, effects plots best. Many researchers add star symbols plot indicating level significance particular paired comparison. Stars okay sense inferential difference \\(p = 0.015\\) \\(p = 0.045\\). ’s also inferential difference \\(p = 0.0085\\) \\(p = 0.015\\), highlights weakness -chotomizing continuous variable. reason, better, alternative add actual p-value (). serious criticism stars encourages researchers readers focus statistical significance instead effect size uncertainty. valuable alternative, , report effects uncertainty effects plot combined effects--response plot.","code":""},{"path":"plotting-models.html","id":"working-in-r","chapter":"4 Plotting Models","heading":"4.2 Working in R","text":"reasonable goal research project script generate final plots entirely within R environment rely external drawing software add finishing features. section covers basics using R packages create plots. Later chapters cover details specific analyses chapter.ggplot2 one major plotting environments R one seems strongest following, especially among new R users. ggplot2 ability generate extremely personalized finished plots. However, ggplot2 long learning curve one pretty comfortable implementation grammar graphics, creating plot multiple layers (mean points, error intervals, raw data points, p-values, text annotations) modified aesthetics (axis text, point colors) can often require many hours googling.ggpubr extension ggplot2 (calls ggplot2 functions hood) provides many canned functions producing kinds ggplots published biological journals. one line script, researcher can generate publishable plot.ggplot_the_model related functions chapter attempts create simple function creating publication ready plots highlight effect size uncertainty.basics using ggpubr, ggplot2, ggplot_the_model outlined . specific examples chapter.","code":""},{"path":"plotting-models.html","id":"source-data","chapter":"4 Plotting Models","heading":"4.2.1 Source data","text":"Data source: ASK1 inhibits browning white adipose tissue obesityThe source data Figure 2E. response \\(glucose\\_auc\\) “area curve” repeated measures blood glucose 120 minutes glucose tolerance test. Glucose AUC measure glucose tolerance, higher area, higher blood glucose two hours, worse physiological response sudden rise blood glucose. two treatment factor variables: 1) \\(Diet\\), levels “chow” “HFD”, “chow” normal mouse chow “HFD” high fat diet, 2) \\(ask1\\), levels “ASK1F/F” “ASK1Δadipo” “ASK1F/F” control level “ASK1Δadipo” ASK1 adipose-deletion mouse described Chapter 1.","code":""},{"path":"plotting-models.html","id":"import","chapter":"4 Plotting Models","heading":"4.2.1.1 Import","text":"","code":"\ndata_from <- \"ASK1 inhibits browning of white adipose tissue in obesity\"\nfile_name <- \"41467_2020_15483_MOESM4_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n  \n# the data are in \"tranposed\" format -- each row contains the n\n# measures of a treatment level. Read, then transpose\n# to make the treatment levels the columns\nfig2e_wide <- read_excel(file_path,\n                     sheet = \"Source Date_Figure 2\",\n                     range = c(\"A233:O236\"), # lot of NA\n                     col_names = FALSE) %>%\n  data.table %>%\n  transpose(make.names = 1) # turn data onto \n\n# melt the four columns into a single \"glucose_auc\" column\n# and create a new column containing treatment level.\ny_cols <- colnames(fig2e_wide)\n\n# melt\nfig2e <- melt(fig2e_wide,\n               measure.vars = y_cols,\n               value.name = \"glucose_auc\",\n               variable.name = \"treatment\")\n\n# create two new columns that are the split of treatment\nfig2e[, c(\"ask1\", \"diet\") := tstrsplit(treatment,\n                                        \" \",\n                                        fixed=TRUE)]\n\n# since glucose_auc is the only response variable in this\n# data.table, omit all rows with any NA\nfig2e <- na.omit(fig2e)\n# View(fig2e)"},{"path":"plotting-models.html","id":"how-to-plot-the-model","chapter":"4 Plotting Models","heading":"4.2.2 How to plot the model","text":"steps throughout text plotting model fit experimental data arefit statistical modeluse fit model estimate modeled means confidence limits using emmeans emmeans package.use emmeans object estimate contrasts interests using contrast function emmeans.Plot individual points. covariates model, use fit model step 1 plot adjusted values points.Use values step 2 plot modeled means error intervals.including p-value brackets, use values step 3.using ggplot_the_model functions, steps 4-6 done ., fit linear model fig2e data compute use emmeans contrast functions without comment. details functions chapters follow. fit model construction plots simplified .","code":""},{"path":"plotting-models.html","id":"fit-the-model","chapter":"4 Plotting Models","heading":"4.2.2.1 Fit the model","text":"response glucose AUC, area curve data glucose tolerance test. model factorial linear model ask1 genotype diet two factors.","code":"\n# glucose_auc is the AUC of the glucose tolerance curves computed using trapezoidal algorithm\nm1 <- lm(glucose_auc ~ ask1*diet,\n               data = fig2e)"},{"path":"plotting-models.html","id":"compute-the-modeled-means-table-of-estimated-means-and-confidence-intervals","chapter":"4 Plotting Models","heading":"4.2.2.2 Compute the modeled means table of estimated means and confidence intervals","text":"Modeled means computed passing model object (m1) emmeans function specifying columns containing groups using specs = argument`.","code":"\nm1_emm <- emmeans(m1, specs = c(\"ask1\", \"diet\"))\nm1_emm##  ask1       diet emmean   SE df lower.CL upper.CL\n##  ASK1F/F    chow   1691 83.4 41     1523     1859\n##  ASK1Δadipo chow   1637 93.2 41     1449     1826\n##  ASK1F/F    HFD    2257 73.1 41     2110     2405\n##  ASK1Δadipo HFD    1871 70.5 41     1728     2013\n## \n## Confidence level used: 0.95"},{"path":"plotting-models.html","id":"compute-the-contrasts-table-of-estimated-effects-with-confidence-intervals-and-p-values","chapter":"4 Plotting Models","heading":"4.2.2.3 Compute the contrasts table of estimated effects with confidence intervals and p-values","text":"Contrasts among levels, combinations levels, computed passing emmeans object (m1.emm) contrast function. many important variations step. text advocates computing planned comparisons, requires expert knowledge forthought. limit computation four simple effects (effects one factor levels factor).NotesI often use m1_pairs name contrast table. use m1_simple remind ’ve limited comparison four simple effects. compute planned comparisons, might use m1_planned.","code":"\nm1_simple <- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     simple = \"each\",\n                     combine = TRUE,\n                     adjust = \"none\") %>%\n  summary(infer = TRUE)\nm1_simple##  diet ask1       contrast               estimate  SE df lower.CL upper.CL\n##  chow .          ASK1Δadipo - (ASK1F/F)    -53.6 125 41  -306.21      199\n##  HFD  .          ASK1Δadipo - (ASK1F/F)   -386.7 102 41  -591.85     -182\n##  .    ASK1F/F    HFD - chow                566.5 111 41   342.48      790\n##  .    ASK1Δadipo HFD - chow                233.3 117 41    -2.67      469\n##  t.ratio p.value\n##   -0.429  0.6703\n##   -3.808  0.0005\n##    5.107  <.0001\n##    1.997  0.0525\n## \n## Confidence level used: 0.95"},{"path":"plotting-models.html","id":"be-sure-ggplot_the_model-is-in-your-r-folder","chapter":"4 Plotting Models","heading":"4.2.3 Be sure ggplot_the_model is in your R folder","text":"skipped Create R Studio Project textbook, download move file ggplot_the_model.R R folder Project folder.","code":""},{"path":"plotting-models.html","id":"how-to-use-the-plot-the-model-functions","chapter":"4 Plotting Models","heading":"4.2.4 How to use the Plot the Model functions","text":"philosophy underneath functions use model fitted researcher make plots. functions require information three objects:data frame containing modeled datathe modeled means CIs emmeansthe modeled effects, CIs p-values emmeans::contrast.philosophy strikes balance functions statistical modeling hidden researcher sees output manually building ggplots. Actually, strongly encourage researchers learn build plots rely canned functions, outline introducing ggplot_the_model functionsThese functions require fit model object (m1), emmeans object modeled means (m1_emm) contrast object modeled effects (m1_simple).","code":""},{"path":"plotting-models.html","id":"ggplot_the_response","chapter":"4 Plotting Models","heading":"4.2.4.1 ggplot_the_response","text":"response plot , use ggplot_the_response.ggplot_the_response arguments:fit model object lm, lmer, nlme, glmmTMBfit_emm object ‘emmeans’. , data frame looks like object, modeled factor variables columns 1 2 (2nd factor model), column means name “emmean”, columns error intervals named “lower.CL” “upper.CL”fit_pairs object emmeans:contrast. , data frame looks like object.wrap_col = NULL used momentx_label = “none” character variable used X-axis title.y_label = “Response (units)” character variable used Y-axis title. Use expression(paste()) method math.g_label = NULL character variable used grouping variable (2nd factor) title. Use “none” removedots = “sina” controls plotting individual points. sina ggforce package. Alternatives “jitter” “dotplot”dodge_width = 0.8 controls spacing group means models 2nd factor (grouping variable)adjust = 0.5 controls spread dots using dots = \"sina\"contrast_rows = “” controls rows fit_pairs use p-value brackets. Use “none” hide.y_pos = NULL manual control y-coordinates p-value bracketspalette = pal_okabe_ito allows control color palette. default pal_okabe_ito palette color blind palette.legend_position = “top” controls position legend grouping variable (2nd factor two-factor model)flip_horizontal = FALSE controls orientation axes.group_lines = FALSE used plotting lines connecting group means. yet implemented.","code":"\nm1_response_plot <- ggplot_the_response(\n  fit = m1,\n  fit_emm = m1_emm,\n  fit_pairs = m1_simple,\n  palette = pal_okabe_ito_blue,\n  y_label = expression(paste(\"mmol \", l^-1, \" min\")),\n  g_label = \"none\"\n)\n\nm1_response_plot"},{"path":"plotting-models.html","id":"ggplot_the_effects","chapter":"4 Plotting Models","heading":"4.2.4.2 ggplot_the_effects","text":"effects plot , use ggplot_the_effects.ggplot_the_effects argumentsfit model object lm, lmer, nlme, glmmTMBfit_pairs object emmeans:contrast. , data frame looks like object.contrast_rows = “” controls rows fit_pairs include plot.show_p = TRUE controls show/hide p-valueseffect_label = “Effect (units)” character variable title effects axis title.","code":"\nm1_effects_plot <- ggplot_the_effects(\n  fit = m1,\n  fit_pairs = m1_simple,\n  effect_label = expression(paste(\"Effect (mmol \", l^-1, \" min)\"))\n)\n\nm1_effects_plot"},{"path":"plotting-models.html","id":"ggplot_the_model","chapter":"4 Plotting Models","heading":"4.2.4.3 ggplot_the_model","text":"combined response effects plot, use ggplot_the_model.ggplot_the_model argumentsfit ggplot_the_responsefit_emm ggplot_the_responsefit_pairs ggplot_the_responsewrap_col = NULL ggplot_the_responsex_label = “none” ggplot_the_responsey_label = “Response (units)” ggplot_the_responseg_label = NULL ggplot_the_responseeffect_label = “Effect (units)” ggplot_the_effectdots = “sina” ggplot_the_responsedodge_width = 0.8 ggplot_the_responseadjust = 0.5 ggplot_the_responsecontrast_rows = “” ggplot_the_responsey_pos = NULL ggplot_the_responsepalette = pal_okabe_ito ggplot_the_responselegend_position = “bottom” Except default, ggplot_the_responseflip_horizontal = TRUE Except default, ggplot_the_responsegroup_lines = FALSE used plotting lines connecting group means. yet implemented.rel_heights = c(1,1) used control relative heights effects response plot","code":"\nm1_plot <- ggplot_the_model(\n  fit = m1,\n  fit_emm = m1_emm,\n  fit_pairs = m1_simple,\n  palette = pal_okabe_ito_blue,\n  y_label = expression(paste(\"mmol \", l^-1, \" min\")),\n  g_label = \"none\",\n  effect_label = expression(paste(\"Effect (mmol \", l^-1, \" min)\"))\n)\n\nm1_plot"},{"path":"plotting-models.html","id":"ggplot_the_treatments","chapter":"4 Plotting Models","heading":"4.2.4.4 ggplot_the_treatments","text":"prefer plus minus symbols, use minus <- \"\\u2013\" minus sign instead hyphen “-”","code":"\nx_levels <- rbind(\n  ASK1 = c(\"F/F\", \"Δadipo\", \"F/F\", \"Δadipo\"),\n  Diet = c(\"chow\", \"chow\", \"HFD\", \"HFD\")\n)\n\n# this is the same code as above but hiding \n# legend position\nm1_response_plot_base <- ggplot_the_response(\n  fit = m1,\n  fit_emm = m1_emm,\n  fit_pairs = m1_simple,\n  palette = pal_okabe_ito_blue,\n  y_label = expression(paste(\"mmol \", l^-1, \" min\")),\n  g_label = \"none\",\n  legend_position = \"none\"\n)\n\nm1_response_plot2 <- ggplot_the_treatments(\n  m1_response_plot_base,\n  x_levels = x_levels,\n  text_size = 3.5,\n  rel_heights = c(1, 0.1)\n)\n\nm1_response_plot2\nminus <- \"\\u2013\" # good to put this in the setup chunk\n\nx_levels <- rbind(\n  Δadipo = c(minus, \"+\", minus, \"+\"),\n  HFD = c(minus, minus, \"+\", \"+\")\n)\n\nm1_response_plot2 <- ggplot_the_treatments(\n  m1_response_plot_base,\n  x_levels = x_levels,\n  text_size = 3.5,\n  rel_heights = c(1, 0.1)\n)\n\nm1_response_plot2"},{"path":"plotting-models.html","id":"how-to-generate-a-response-plot-using-ggpubr","chapter":"4 Plotting Models","heading":"4.2.5 How to generate a Response Plot using ggpubr","text":"Steps 1-3 completed .","code":""},{"path":"plotting-models.html","id":"plot-individual-points","chapter":"4 Plotting Models","heading":"4.2.5.1 Step 4: Plot the individual points","text":"Using ggplotI’m going show create initial, base plot points using ggplot2 order outline briefly ggplot2 works.ggplot function requires data frame passed data containing data plot aesthetic (aes), passes column names set x y axes. column names must data frame passed data. color = aesthetic sets grouping variable used assign different colors.x-axis discrete numeric. x-axis values 1, 2, 3, 4. instead using numbers labels x-axis values, ggplot uses names groups (four values column “treatment”)Using ggpubr","code":"\nm1_response <- ggplot(\n  data = fig2e,\n  aes(x = treatment, # these 2 lines define the axes\n      y = glucose_auc,\n      color = ask1 # define the grouping variable\n  )) +\n  \n  # surprisingly, the code above doesn't plot anything\n  # this adds the points as a layer\n  geom_jitter(width = 0.2) +\n  \n  # change the title of the y-axis\n  ylab(expression(paste(\"AUC (mmol \", l^-1, \" min)\"))) +\n  \n  # change the theme\n  theme_pubr() +\n\n  # these theme modifications need to be added after re-setting\n  # the theme\n  theme(\n    legend.position = \"none\", # remove legend\n    axis.title.x = element_blank() # remove the x-axis title\n  ) +\n  \n  \n  # change the colors palette for the points\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  \n  NULL\n\n  \nm1_response\nm1_response <- ggstripchart(\n  data = fig2e,\n  x = \"treatment\",\n  y = \"glucose_auc\",\n  color = \"ask1\",\n  xlab = \"\",\n  #ylab = expression(paste(\"AUC (mmol \", l^-1, \" min)\")),\n  ylab = \"AUC (mmol/min)\",\n  palette = pal_okabe_ito_blue,\n  legend = \"none\"\n  )\n\nm1_response"},{"path":"plotting-models.html","id":"step-5-plot-the-modeled-means-and-95-error-intervals","chapter":"4 Plotting Models","heading":"4.2.5.2 Step 5: Plot the modeled means and 95% error intervals","text":"add points error bars m1_response, need tell ggplot x-axis positions (coordinates). positions values “treatment” column fig2e. modeled means 95% CIs m1_emm object “treatment” column, column values. , therefore make column can add modeled means CIs plot.Now add modeled means CIsNotesm1_response generated ggpubr::stripchart ggplot2 object. means modifications plot implemented adding “+” sign.modeled means column “emmean” data frame m1_emm_dt. need tell geom_point find data using data = argument. geom_point() (geoms) assumes points want plot defined x y column names used create plot – don’t exist, need state x y column names aesthetic function aes. Since created “treatment” column m1_emm_dt contains x-axis coordiantes (1, 2, 3, 4), need tell ggplot find x-values. “glucose_auc” column m1_emm_dt need tell geom_point() find y values using aes(y = \"emmean\").Adding modeled error intervals using geom_errorbar follows logic adding modeled means. Importantly, interestingly, y = emmean passed even though information column used plot error bars.Note column names passed ggpubr function must quotes column names passed ggplot2 function quotes","code":"\n# convert m1_emm to a data.table\nm1_emm_dt <- summary(m1_emm) %>%\n  data.table()\n\n# create treatment column\n# make sure it matches values in the two factor columns\nm1_emm_dt[, treatment := c(\"ASK1F/F chow\",\n                           \"ASK1Δadipo chow\",\n                           \"ASK1F/F HFD\",\n                           \"ASK1Δadipo HFD\")]\nm1_response <- m1_response + \n\n  # add layer containing means\n  geom_point(data = m1_emm_dt,\n             aes(y = emmean,\n                 color = ask1), \n             size = 3) +\n  \n  # add layer containing error bars\n  geom_errorbar(data = m1_emm_dt, \n                aes(y = emmean,\n                    ymin = lower.CL, \n                    ymax = upper.CL,\n                    color = ask1), \n                width = 0.05) +\n  \n  NULL\n\nm1_response"},{"path":"plotting-models.html","id":"step-6-adding-p-values","chapter":"4 Plotting Models","heading":"4.2.5.3 Step 6: Adding p-values","text":"p-value brackets added response plot using stat_pvalue_manual ggpubr package. function needs column p-values, pair columns define left right x-axis positions bracket.Now add p-valuesNotes adding p-values plot:y.position argument stat_pvalue_manual() contains position y-axis p-value brackets. typically choose values “eye”. Essentially, look maximum y-value plot choose value just first bracket. may take trial--error position brackets satisfactorily.Use base R indexing specify subset. exampleggpubr::stat_compare_means automates process somewhat function limited statistics anything simplest experiments. don’t advocate ’s use.","code":"\n# convert m1_simple to a data.table\nm1_simple_dt <- data.table(m1_simple)\n\n# create group1 -- column containing x-position\n# of the left side of the bracket\n# need to look at m1_simple_dt to construct this.\n# \"ASK1F/F chow\", \"ASK1Δadipo chow\" ,\"ASK1F/F HFD\", \"ASK1Δadipo HFD\" \n\nm1_simple_dt[, group1 := c(\"ASK1Δadipo chow\",\n                           \"ASK1Δadipo HFD\",\n                           \"ASK1F/F HFD\",\n                           \"ASK1Δadipo HFD\")]\n\n# create group2 -- column containing x-position\n# of the right side of the bracket\n# need to look at m1_simple_dt to construct this.\n# \"ASK1F/F chow\", \"ASK1Δadipo chow\" ,\"ASK1F/F HFD\", \"ASK1Δadipo HFD\" \n\nm1_simple_dt[, group2 := c(\"ASK1F/F chow\",\n                           \"ASK1F/F HFD\",\n                           \"ASK1F/F chow\",\n                           \"ASK1Δadipo chow\")]\n\nm1_simple_dt[, p_rounded := p_round(p.value,\n                                  digits = 2)]\nm1_simple_dt[, p_pretty := p_format(p_rounded,\n                                    digits = 2,\n                                    accuracy = 1e-04,\n                                    add.p = TRUE)]\n# simply assigning this to a new plot with new name\n# because I want to re-use the base plot in the next chunk\nm1_response_p <- m1_response +\n  stat_pvalue_manual(\n    data = m1_simple_dt,\n    label = \"p_pretty\",\n    y.position = c(3300, 3300, 3450, 3600),\n    tip.length = 0.01)\n\nm1_response_p\nm1_response_p <- m1_response +\n  stat_pvalue_manual(\n    data = m1_simple_dt[c(2,4), ], # only rows 2, 4\n    label = \"p_pretty\",\n    y.position = c(3300, 3450),\n    tip.length = 0.01)\n\nm1_response_p"},{"path":"plotting-models.html","id":"a-variation-for-factorial-models","chapter":"4 Plotting Models","heading":"4.2.5.4 A variation for factorial models","text":"experiment Fig2e factorial design analyzed (, original paper) using factorial model. factorial design can represented plot clustering levels.","code":"\ndodge_width = 0.4\njitter_width = 0.2\nm1_simple_dt[, xmin := c(1-dodge_width/4,\n                        2-dodge_width/4,\n                        1-dodge_width/4,\n                        1+dodge_width/4)]\nm1_simple_dt[, xmax := c(1+dodge_width/4,\n                        2+dodge_width/4,\n                        2-dodge_width/4,\n                        2+dodge_width/4)]\n\nm1_response_fac <- ggstripchart(\n  data = fig2e,\n  x = \"diet\",\n  y = \"glucose_auc\",\n  color = \"ask1\",\n  fill = \"ask1\",\n#  ylab = expression(paste(\"AUC (mmol \", l^-1, \" min)\")),\n  ylab = \"AUC (mmol/min)\",\n  palette = pal_okabe_ito_blue,\n#  position = position_dodge(width = dodge_width)\n  position = position_jitterdodge(dodge.width = dodge_width,\n                                  jitter.width = jitter_width)\n) +\n  \n  rremove(\"xlab\") + #ggpubr function\n\n  # add layer containing means\n  geom_point(data = m1_emm_dt,\n             aes(y = emmean,\n                 color = ask1), \n             size = 3,\n             position = position_dodge(width = dodge_width)) +\n  \n  # add layer containing error bars\n  geom_errorbar(data = m1_emm_dt, \n                aes(y = emmean,\n                    ymin = lower.CL, \n                    ymax = upper.CL,\n                    color = ask1), \n                width = 0.05,\n             position = position_dodge(width = dodge_width)) +\n  \n  # add p-value brackets\n  stat_pvalue_manual(\n    data = m1_simple_dt,\n    label = \"p_pretty\",\n    xmin = \"xmin\",\n    xmax = \"xmax\",\n    y.position = c(3300, 3300, 3450, 3600),\n    tip.length = 0.01,\n    size = 3) +\n  \n  NULL\n\nm1_response_fac"},{"path":"plotting-models.html","id":"how-to-add-treatment-combinations-to-a-ggpubr-plot","chapter":"4 Plotting Models","heading":"4.2.5.5 How to add treatment combinations to a ggpubr plot","text":"Many researchers bench biology insert grid treatments x-axis plot. time consuming using external software add . much leaner workflow add treatment grid step generating plot . kludgy way using ggpubr plot. much elegant method described ","code":""},{"path":"plotting-models.html","id":"variant-1-fake-axes","chapter":"4 Plotting Models","heading":"4.2.5.5.1 Variant 1 – Fake axes","text":"Notesggplot_the_treatments work ggplot, including generated ggpubr functions, categorical values x-axis., add treatment combinations manually. table combinations added inside plot area (inside axes). make look nice, weird, x y axes removed new lines inserted create new axis lines. bottom new y-axis line starts treatment table. new x-axis line inserted treatment table.include treatment combination (group) names, add first row x_levels.Note plot grid, grid extend area occupied treatment table using method.","code":"\nuse_this <- FALSE # if false use +/- symbols\nif(use_this == TRUE){\n  x_levels <- rbind(\n    \"ASK1:\" = c(\"F/F\", \"Δadipo\", \"F/F\", \"Δadipo\"),\n    \"Diet:\" = c(\"chow\", \"chow\", \"HFD\", \"HFD\")\n  )\n}else{\n  minus <- \"\\u2013\" # good to put this in the setup chunk\n  x_levels <- rbind(\n    \"Δadipo:\" = c(minus, \"+\", minus, \"+\"),\n    \"HFD: \" = c(minus, minus, \"+\", \"+\")\n  )\n}\n\n\nx_levels_text <- apply(x_levels, 2, paste0, collapse=\"\\n\")\nx_levels_title <- paste(row.names(x_levels), collapse=\"\\n\")\n\nx_axis_min <- 0.4\nx_axis_max <- 4.5\ny_plot_min <- 1200\ny_axis_min <- 1350\ny_axis_max <- 3500\ny_breaks <- seq(1500, 3500, by = 500)\ny_labels <- as.character(y_breaks)\n\nm1_response_final <- m1_response_p +\n  coord_cartesian(ylim = c(y_plot_min, y_axis_max)) +\n  scale_y_continuous(breaks = y_breaks) +\n  theme(\n    axis.line = element_blank(), # remove both x & y axis lines\n    axis.text.x = element_blank(), # remove x-axis labels\n    axis.ticks.x = element_blank() # remove x-axis ticks\n  ) +\n  \n  # add shortened y-axis line that doesn't extend to treatments\n  geom_segment(aes(x = x_axis_min + 0.01,\n                   y = y_axis_min,\n                   xend = x_axis_min + 0.01,\n                   yend = y_axis_max),\n               size = 0.5) +\n  \n  # add x-axis line above treatments\n  geom_segment(aes(x = x_axis_min,\n                   y = y_axis_min,\n                   xend = x_axis_max,\n                   yend = y_axis_min),\n               size = 0.5) +\n  \n  # add treatment combinations\n  annotate(geom = \"text\",\n           x = 1:4,\n           y = y_plot_min,\n           label = x_levels_text) +\n  \n  # add factor names\n  annotate(geom = \"text\",\n           x = x_axis_min,\n           y = y_plot_min,\n           label = x_levels_title,\n           hjust = 0) +\n  NULL\n  \nm1_response_final"},{"path":"plotting-models.html","id":"how-to-generate-a-response-plot-with-a-grid-of-treatments-using-ggplot2","chapter":"4 Plotting Models","heading":"4.2.6 How to generate a Response Plot with a grid of treatments using ggplot2","text":", wrote short script generating base response plot using ggplot. plot, x-axis variable (“treatment”) categorical ggplot uses integers 1, 2, 3, 4 coordinate values position four groups x-axis. Understanding mapping important adding lines plot annotating plot text, requires x,y coordinates position feature. x-axis discrete – add tick x = 1.5 – horizontal dimension plot area continuous can add points lines text x coordinate within plot area.generate plot using continuous x-axis. requires explicitly create numeric column data x-axis position group. easy. advantage now continuous x-axis manipulatable discrete x-axis. use add treatment levels plot","code":""},{"path":"plotting-models.html","id":"first-wrangle-the-data","chapter":"4 Plotting Models","heading":"4.2.6.1 First, wrangle the data","text":"NotesThe first line simply converts categorical variable treatment numeric variable values 1-4 assigned four treatment levels., m1_emm_dt created column x-variable name addedAs , m1_simple_dt created group1 group2 columns x-axis positions two groups contrast created. columns integers group names.","code":"\n# make sure treatment is a factor with the levels in the \n# desired order\n# fig2e[, treatment_i := as.integer(fig2e$treatment)]\n# above line stopped working on 08/28/23 with error:\n# Error in x[[name]] <- value : bad names attribute\n# so here is a non-data.table alternative\nfig2e$treatment_i <- as.integer(fig2e$treatment)\n\n\n# convert m1_emm to a data.table\nm1_emm_dt_i <- summary(m1_emm) %>%\n  data.table()\n\n# create treatment_i column\nm1_emm_dt_i[, treatment_i := 1:4]\n\n# convert m1_simple to a data.table\nm1_simple_dt_i <- data.table(m1_simple)\n\n# create group1 -- column containing x-position\n# of the left side of the bracket\n# need to look at m1_simple_dt to construct this.\n# \"ASK1F/F chow\", \"ASK1Δadipo chow\" ,\"ASK1F/F HFD\", \"ASK1Δadipo HFD\" \nm1_simple_dt_i[, group1 := c(2, 4, 3, 4)]\n\n# create group2 -- column containing x-position\n# of the right side of the bracket\n# need to look at m1_simple_dt to construct this.\n# \"ASK1F/F chow\", \"ASK1Δadipo chow\" ,\"ASK1F/F HFD\", \"ASK1Δadipo HFD\" \nm1_simple_dt_i[, group2 := c(1, 3, 1, 2)]\n\nm1_simple_dt_i[, p_rounded := p_round(p.value,\n                                  digits = 2)]\nm1_simple_dt_i[, p_pretty := p_format(p_rounded,\n                                    digits = 2,\n                                    accuracy = 1e-04,\n                                    add.p = TRUE)]"},{"path":"plotting-models.html","id":"second-generate-the-plot","chapter":"4 Plotting Models","heading":"4.2.6.2 Second, generate the plot","text":"NotesThis code exactly uses x-variable numeric instead factor.","code":"\nm1_response_2 <- ggplot(\n  data = fig2e,\n  aes(x = treatment_i, # these 2 lines define the axes\n      y = glucose_auc,\n      color = ask1 # define the grouping variable\n  )) +\n  \n  # add jittered points\n  geom_jitter(width = 0.2) +\n  \n  # add layer containing means\n  geom_point(data = m1_emm_dt_i,\n             aes(y = emmean,\n                 color = ask1), \n             size = 3) +\n  \n  # add layer containing error bars\n  geom_errorbar(data = m1_emm_dt_i, \n                aes(y = emmean,\n                    ymin = lower.CL, \n                    ymax = upper.CL,\n                    color = ask1), \n                width = 0.05) +\n  \n  # add the p-value brackets\n  stat_pvalue_manual(\n    data = m1_simple_dt_i[c(2,4), ], # only rows 2, 4\n    label = \"p_pretty\",\n    y.position = c(3300, 3450),\n    tip.length = 0.01) +\n  \n  # change the title of the y-axis\n  ylab(expression(paste(\"AUC (mmol \", l^-1, \" min)\"))) +\n  \n  # change the theme\n  theme_pubr() +\n\n  # these theme modifications need to be added after re-setting\n  # the theme\n  theme(\n    legend.position = \"none\", # remove legend\n    axis.title.x = element_blank() # remove the x-axis title\n  ) +\n  \n  # change the colors palette for the points\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  \n  NULL\n\n  \nm1_response_2"},{"path":"plotting-models.html","id":"third-add-the-grid-of-treatments-below-the-x-axis","chapter":"4 Plotting Models","heading":"4.2.6.3 Third, add the grid of treatments below the x-axis","text":"good plot. pretty good plot include effects.","code":"\nminus <- \"\\u2013\" # good to put this in the setup chunk\n\n# I've added x-axis group names in addition to the grid\nx_levels <- rbind(\n  c(\"\", \"Control\", \"Δadipo\", \"HFD\", \"Δadipo+HFD\"),\n  c(\"Δadipo:\", minus, \"+\", minus, \"+\"),\n  c(\"HFD: \", minus, minus, \"+\", \"+\")\n)\n\nx_breaks_text <- apply(x_levels, 2, paste0, collapse=\"\\n\")\nx_breaks <- c(0.5, 1:4)\n\nm1_response_2 <- m1_response_2 +\n  coord_cartesian(xlim = c(0.5, 4.5)) +\n  scale_x_continuous(breaks = x_breaks,\n                     labels = x_breaks_text,\n                     expand = c(0, 0)) +\n  theme(axis.ticks.x = element_blank()) + # remove x-axis ticks\n  \n  NULL\n        \nm1_response_2"},{"path":"plotting-models.html","id":"how-to-generate-an-effects-plot","chapter":"4 Plotting Models","heading":"4.2.7 How to generate an Effects Plot","text":"effects plot built using ggplot2 instead ggpubr plot “flipped” – y-axis categorical variable x-axis continuous variable. base plot estimates (error bars) made using ggpubr (see ) subsequent functions modify plot awkward inconsistencies designation x y-axis (old new?).Notesc(-30,0,0,0) added x coordinate shift first p-value left zero-effect line.done ggpubr?","code":"\n# use the m1_simple_dt object created above\n\n# create nice labels for the contrasts\nm1_simple_dt[, contrast_pretty := c(\"ASK1Δadipo - Control\",\n                                    \"ASK1Δadipo - HFD\",\n                                    \"HFD - Control\",\n                                    \"ASK1Δadipo+HFD - ASK1Δadipo\")]\n# make sure the levels of contrast_pretty are in the order of that in\n# m1_simple_dt\nm1_simple_dt[, contrast_pretty := factor(contrast_pretty,\n                                         levels = contrast_pretty)]\n\nm1_effects <- ggplot(data = m1_simple_dt,\n                     aes(x = estimate,\n                         y = contrast_pretty)) +\n  \n  geom_point() +\n  \n  xlab(\"Effects (mmol/L)\") +\n\n  # add error bars\n  geom_errorbar(aes(x = estimate,\n                    xmin = lower.CL,\n                    xmax = upper.CL),\n                width = 0.02) +\n  \n  # add zero effect line\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = pal_okabe_ito_blue[1]) +\n  \n  # add p-values\n  annotate(geom = \"text\",\n           x = m1_simple_dt$estimate + c(-30,0,0,0),\n           y = 1:4 + 0.2,\n           label = m1_simple_dt$p_pretty,\n           size = 3) +\n  \n  theme_pubr() +\n  rremove(\"ylab\") + #remove y-axis, ggpubr function\n  \n  \n  NULL\n\nm1_effects\n# use the modified m1_simple_dt object created in the previous chunk\n\n# use ggpubr::ggerrorplot with no error\nm1_effects_pubr <- ggerrorplot(data = m1_simple_dt,\n                        y = \"estimate\",\n                        x = \"contrast_pretty\",\n                        ylab = \"Effects (mmol/L)\",\n                        desc_stat = \"mean\", # mean only!\n                        orientation = \"horizontal\") +\n  \n  # remove y-axis title\n  rremove(\"ylab\") + #ggpubr function\n  \n  # add error bars using ggplot function\n  # note using original (not horizontal) axis designation\n  geom_errorbar(aes(y = estimate,\n                    ymin = lower.CL,\n                    ymax = upper.CL),\n                width = 0.02) +\n  \n  # add zero effect line using ggplot function\n  # note using original (not horizontal) axis designation\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\",\n             color = pal_okabe_ito_blue[1]) +\n  \n  # add p-values\n  # note using original (not horizontal) axis designation\n  annotate(geom = \"text\",\n           y = m1_simple_dt$estimate + c(-30,0,0,0),\n           x = 1:4 + 0.3,\n           label = m1_simple_dt$p_pretty,\n           size = 3) +\n  \n  NULL\n\nm1_effects_pubr"},{"path":"plotting-models.html","id":"how-to-combine-the-response-and-effects-plots","chapter":"4 Plotting Models","heading":"4.2.8 How to combine the response and effects plots","text":"","code":""},{"path":"plotting-models.html","id":"using-the-ggpubr-response-plot","chapter":"4 Plotting Models","heading":"4.2.8.1 Using the ggpubr response plot","text":"cowplot::plotgrid() function used generally arrange multiple plots single figure. use combine response effects subplots single plot. response plot m1_response created using ggpubr.response plot flipped code. know want orientation, simply build orientation avoid kinds axis designation ambiguities highlighted ggpubr effects plot.align axis arguments used force plot areas width.rel_heights = c() adjusts relative heights top bottom plot. typically requires fiddling.placement p-values looks different standalone effects plot. improve look combined plot, fiddle placement argument (x y positions) chunk generates effects plot.","code":"\n# modify the response and effects plots for consistency\n\n# change group names for consistency\ngroups_pretty <- c(\"Control\",\n                   \"ASK1Δadipo\",\n                   \"HFD\",\n                   \"ASK1Δadipo+HFD\")\nm1_bottom <- m1_response +\n  scale_x_discrete(labels = groups_pretty) +\n  coord_flip() # rotate to horizontal\n\nm1_top <- m1_effects + # assign to new plot object\n  scale_x_continuous(position=\"top\") # move x-axis to \"top\"\n  \nm1_plot <- plot_grid(m1_top,\n                     m1_bottom,\n                     nrow = 2,\n                     align = \"v\",\n                     axis = \"lr\",\n                     rel_heights = c(1, 1))\n\nm1_plot"},{"path":"plotting-models.html","id":"using-a-response-plot-with-a-treatment-grid","chapter":"4 Plotting Models","heading":"4.2.8.2 Using a response plot with a treatment grid","text":", re-build response plot horizontal orientation","code":"\ntab <- \"\\u0009\"  # good to put this in the setup chunk\nminus <- \"\\u2013\" # good to put this in the setup chunk\nminus_pad <- paste0(minus, \"    \")\nplus_pad <- \"+    \"\n\nx_levels <- rbind(\n  c(minus_pad, plus_pad, minus_pad, plus_pad),\n  c(minus_pad, minus_pad, plus_pad, plus_pad)\n)\nx_breaks_text <- apply(x_levels, 2, paste0, collapse = \"\")\nx_breaks_text <- c(x_breaks_text, \"Δadipo HFD\")\nx_breaks <- c(1:4, 4.5)\n\nm1_response_horiz <- ggplot(\n  data = fig2e,\n  aes(y = treatment_i, # these 2 lines define the axes\n      x = glucose_auc,\n      color = ask1 # define the grouping variable\n  )) +\n  \n  # add jittered points\n  geom_jitter(width = 0.2) +\n  \n  # add layer containing means\n  geom_point(data = m1_emm_dt_i,\n             aes(x = emmean,\n                 color = ask1), \n             size = 3) +\n  \n  # add layer containing error bars\n  geom_errorbar(data = m1_emm_dt_i, \n                aes(x = emmean,\n                    xmin = lower.CL, \n                    xmax = upper.CL,\n                    color = ask1), \n                width = 0.05) +\n  \n  # change the title of the y-axis\n  xlab(expression(paste(\"AUC (mmol \", l^-1, \" min)\"))) +\n  \n  # change the theme\n  theme_pubr() +\n\n  # these theme modifications need to be added after re-setting\n  # the theme\n  theme(\n    legend.position = \"none\", # remove legend\n    axis.title.y = element_blank(), # remove the x-axis title\n    axis.ticks.y = element_blank() # remove y-axis ticks\n  ) +\n  \n  # change the colors palette for the points\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  \n  # add the treatment\n  \n  coord_cartesian(ylim = c(0.5, 4.5)) +\n  scale_y_continuous(breaks = x_breaks,\n                     labels = x_breaks_text,\n                     expand = c(0, 0)) +\n  NULL\n\nm1_response_horiz\nm1_plot <- plot_grid(m1_top,\n                     m1_response_horiz,\n                     nrow = 2,\n                     align = \"v\",\n                     axis = \"lr\",\n                     rel_heights = c(1, 1))\n\nm1_plot"},{"path":"plotting-models.html","id":"how-to-add-the-interaction-effect-to-response-and-effects-plots","chapter":"4 Plotting Models","heading":"4.2.9 How to add the interaction effect to response and effects plots","text":"experiment Figure 2E, good question ask , effect ASK1Δadipo conditional diet? example, scenario ASK1Δadipo lowers AUC amount Chow HFD mice (, effect ASK1Δadipo conditional diet) different underlying biological explanation control browning scenario ASK1Δadipo lowers AUC HFD mice. pursue , need estimate interaction effect. general, experiment factorial design, always want estimates interaction effects","code":""},{"path":"plotting-models.html","id":"adding-interaction-p-value-to-a-response-plot","chapter":"4 Plotting Models","heading":"4.2.9.1 Adding interaction p-value to a response plot","text":"Wrangle","code":"\nm1_coef <- coef(summary(m1))\nixn_estimate <- m1_coef[\"ask1ASK1Δadipo:dietHFD\", \"Estimate\"]\np_ixn <- m1_coef[\"ask1ASK1Δadipo:dietHFD\", \"Pr(>|t|)\"] %>%\n  p_round(digits = 3) %>%\n  p_format(digits = 3, accuracy = 1e-04)\np_ixn_text <- paste0(\"interaction p = \", p_ixn)\nm1_response_ixn <- m1_response_fac +\n  \n  # add line connecting group means\n  # comment out all lines to remove\n  geom_line(data = m1_emm_dt,\n            aes(y = emmean,\n                group = ask1,\n                color = ask1),\n            position = position_dodge(width = dodge_width)) +\n  \n  # add p-value\n  annotate(geom = \"text\",\n           x = 1.5,\n           y = 2300,\n           label = p_ixn_text) +\n  \n  NULL\nm1_response_fac\nm1_response_ixn"},{"path":"plotting-models.html","id":"adding-interaction-effect-to-effects-plot","chapter":"4 Plotting Models","heading":"4.2.9.2 Adding interaction effect to effects plot","text":"","code":""},{"path":"plotting-models.html","id":"add-the-interaction-effect-to-the-contrast-table","chapter":"4 Plotting Models","heading":"4.2.9.2.1 Add the interaction effect to the contrast table","text":"pretty good plot","code":"\n# convert m1_simple to data.table\nm1_simple_dt <- data.table(m1_simple)\n\n# get interaction p-value using emmeans::contrast()\nm1_ixn <- contrast(m1_emm,\n                   interaction = c(\"revpairwise\"),\n                   by = NULL) %>%\n  summary(infer = TRUE) %>%\n  data.table()\n\nsetnames(m1_ixn,\n         old = names(m1_ixn)[1:2], \n         new = names(m1_simple_dt)[1:2])\nm1_ixn[, contrast := \"ask1:diet\"]\n\n# note that column order does not need to be the same\nm1_effects_dt <- rbind(m1_simple_dt,\n                       m1_ixn)\n\nm1_effects_dt[, contrast_pretty := c(\"Δadipo - Control\",\n                                     \"Δadipo+HFD - HFD\",\n                                     \"HFD - Control\",\n                                     \"Δadipo+HFD - Δadipo\",\n                                     \"Interaction\")]\nm1_effects_dt[, p_round := p_round(p.value, digits = 2)]\nm1_effects_dt[, p_pretty := p_format(p_round,\n                                     digits = 2,\n                                    accuracy = 1e-04,\n                                    add.p = TRUE)]\n\n# don't forget this imperative step!\n# otherwise your p-values and factor labels won't match!\nm1_effects_dt[, contrast_pretty := factor(contrast_pretty,\n                                          levels = contrast_pretty)]\nm1_effects <- ggplot(data = m1_effects_dt,\n                     aes(x = estimate,\n                         y = contrast_pretty)) +\n  \n  geom_point() +\n  \n  xlab(\"Effects (mmol/L)\") +\n\n  # add error bars\n  geom_errorbar(aes(x = estimate,\n                    xmin = lower.CL,\n                    xmax = upper.CL),\n                width = 0.02) +\n  \n  # add zero effect line\n  geom_vline(xintercept = 0,\n             linetype = \"dashed\",\n             color = pal_okabe_ito_blue[1]) +\n  \n  # add p-values\n  annotate(geom = \"text\",\n           x = m1_effects_dt$estimate + c(-30,0,0,0,0),\n           y = 1:5 + 0.2,\n           label = m1_effects_dt$p_pretty,\n           size = 3) +\n  \n  theme_pubr() +\n  \n  rremove(\"ylab\") + #remove y-axis, ggpubr function\n\n  NULL\n\nm1_effects\nm1_top <- m1_effects +\n  scale_x_continuous(position=\"top\") # move x-axis to \"top\"\n\nm1_plot <- plot_grid(m1_top,\n                     m1_response_horiz,\n                     nrow = 2,\n                     align = \"v\",\n                     axis = \"lr\",\n                     rel_heights = c(1, 0.9))\n\nm1_plot"},{"path":"part-iii-some-fundamentals-of-statistical-modeling.html","id":"part-iii-some-fundamentals-of-statistical-modeling","chapter":"Part III: Some Fundamentals of Statistical Modeling","heading":"Part III: Some Fundamentals of Statistical Modeling","text":"","code":""},{"path":"uncertainty.html","id":"uncertainty","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","text":"Uncertainty stuff science. mean uncertainty statistics? Uncertainty error estimating parameter, mean sample, difference means two experimental treatments, predicted response given certain change conditions. Uncertainty emerges variability measured variance square root, standard deviation. standard deviation statistic called standard error statistic. Standard errors statistics experimental data atoms matter – everything built .","code":""},{"path":"uncertainty.html","id":"standard-errors-are-used-to-compute-p-values-and-confidence-intervals","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.1 Standard errors are used to compute p-values and confidence intervals","text":"Let’s revisit Table 1.1 experiment presented introductory chapter Analyzing experimental data linear model.\nTable 5.1: Coefficient table linear model fit exp2i data.\nvalues second row give inferential statistics estimate treatment effect. estimate column “Estimate”. guide inference value, use standard error estimate column “Std. Error”. use standard error compute t-value, given column “t value” , , can compute p-value. classic “t-test” experimental biology researchers familiar , although computed using linear model way presented introductory statistics textbooks (two ways give numerically equivalent results). also use standard error compute boundaries 95% confidence interval estimate, given “2.5 %” “97.5 %” columns. importance confidence intervals statistical inference less well appreciated among researchers experimental biology. p-value tool help us infer effect. confidence interval tool help us infer range effects compatible data.chapter uses simulation pump intuition standard errors confidence intervals. wait next chapter focus p-values.","code":""},{"path":"uncertainty.html","id":"background","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2 Background","text":"introductory statistics class, students introduced two measures variability, “standard deviation” “standard error.” terms absolutely fundamental statistics. Yet, many biology researchers confuse terms certainly, introductory students .research biologist uses term “standard deviation,” probably referring sample standard deviation measure variability sample. research biologist uses term “standard error,” probably referring standard error mean, standard error another statistic, difference means regression slope. important point remember understand standard errors standard deviations. make sense soon.","code":""},{"path":"uncertainty.html","id":"sample-standard-deviation","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.1 Sample standard deviation","text":"sample standard deviation measure variability sample. example, look histological section skeletal muscle see diameter fibers (muscle cells) variable. use imaging software measure diameter sample 100 cells get distribution like thisThe mean sample 69.4µm standard deviation 2.8 µm. standard deviation square root variance, computed \\[\\begin{equation}\ns_y = \\sqrt{\\frac{\\sum_{=1}^n{(y_i - \\overline{y})^2}}{n-1}}\n\\tag{5.1}\n\\end{equation}\\]Memorize equation. understand logic measure variability, note \\(y_i - \\overline{y}\\) deviation \\(\\)th value sample mean, numerator sum squared deviations. numerator sum \\(n\\) items denominator \\(n-1\\) variance (almost!) averaged squared deviation. variable samples bigger deviations , therefore, bigger average squared deviations. Since standard deviation square root variance, standard deviation square root average squared deviation. makes similar value averaged deviation (average absolute values deviations since average deviation , definition mean, zero).","code":""},{"path":"uncertainty.html","id":"notes-on-the-variance-and-standard-deviation","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.1.1 Notes on the variance and standard deviation","text":"Variances additive standard deviations . means variance sum two independent (uncorrelated) random variables simply sum variances variables. important many statistical analyses.units variance square original units, awkward interpretation. units standard deviation original variable, much easier interpet.variables approximately normally distributed, can map standard deviation quantiles distribution. example, 68% values within one standard deviation mean, 95% values within two standard deviations, 99% values within three standard deviations.","code":""},{"path":"uncertainty.html","id":"standard-error-of-the-mean","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.2 Standard error of the mean","text":"standard error statistic measure precision statistic. standard error mean measure precision estimate mean. standard error difference means measure precision estimate difference means. smaller standard error, precise estimate.standard error mean (SEM) computed \\[\\begin{equation}\nSEM = \\frac{s_y}{\\sqrt{n}}\n\\tag{5.2}\n\\end{equation}\\]SEM often denoted \\(s_{\\bar{y}}\\) indicate standard deviation mean (\\(\\bar{y}\\)).","code":""},{"path":"uncertainty.html","id":"the-standard-error-of-the-mean-can-be-thought-of-as-a-standard-deviation-of-an-infinitely-long-column-of-re-sampled-means","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.2.1 The standard error of the mean can be thought of as a standard deviation of an infinitely long column of re-sampled means","text":"sense standard error standard deviation? kinda weird. sample 100 cells slide muscle tissue compute mean diameter, can mean standard deviation? one mean!understand SEM standard deviation, imagine sample \\(n\\) values \\(N(\\mu, \\sigma^2)\\) (normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). mean sample estimate \\(\\mu\\) standard deviation sample estimate \\(\\sigma\\)) infinite number times time, write mean new sample. infinitely large sample means sampling distribution mean. standard deviation sampling distribution mean standard error mean. observed SEM estimate true value observed standard deviation estimate \\(\\sigma\\).","code":""},{"path":"uncertainty.html","id":"a-standard-deviation-can-be-computed-for-any-statistic-these-are-all-standard-errors.","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.2.2 A standard deviation can be computed for any statistic – these are all standard errors.","text":"SEM one kind standard error. standard deviation can computed statistic – standard errors. statistics, mean, standard error can computed directly using equation, SEM (equation (5.2)). statistics, equation, need use computer intensive method known bootstrap compute standard error. return bootstrap Section 5.4.","code":""},{"path":"uncertainty.html","id":"notes-on-standard-errors","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.2.2.3 Notes on standard errors","text":"units standard error units measured variable.standard error proportional sample variability (sample standard deviation, \\(s_y\\)) inversely proportional sample size (\\(n\\)). Sample variability function natural variation (really variation diameter among fibers quadriceps muscle) measurement error (imaging software higher resolution can measure diameter less error). Since SEM measure precision estimating mean, means precision increase (SEM decrease) 1) investigator uses methods reduce measurement error 2) investigator computes mean larger sample.last point (SEM decreases sample size) seems obvious looking equation (5.2), since \\(n\\) denominator. course \\(n\\) also denominator equation (5.1) sample standard deviation standard deviation decrease sample size increases. First wouldn’t make sense – variability variability. sample 10,000 cell diameters variable sample 100 cell diameters (think agree ). Second, also obvious equation (5.1). standard deviation square root average averages don’t increase number things summed since numerator (sum) denominator increase \\(n\\).","code":""},{"path":"uncertainty.html","id":"simulations-using-fake-data-as-an-intuition-pump","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3 Simulations – using fake data as an intuition pump","text":"","code":""},{"path":"uncertainty.html","id":"using-google-sheets-to-generate-fake-data-to-explore-the-standard-error","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.1 Using Google Sheets to generate fake data to explore the standard error","text":"statistics interested estimated parameters population using measures sample. goal section use Google Sheets (Microsoft Excel) use fake data discover behavior sampling gain intuition uncertainty using standard errors.","code":""},{"path":"uncertainty.html","id":"steps","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.1.1 Steps","text":"Open Google SheetsIn cell A1 type “mu”. mu greek letter \\(\\mu\\) common notation poplation value (TRUE value!) mean hypothetical measure. cell B1, insert number value \\(\\mu\\). number! can negative positive.cell A2 type “sigma”. sigma greek letter \\(\\sigma\\). \\(\\sigma^2\\) common (universal!) notation population (TRUE) variance measure parameter. Notice true (population) values mean variance greek letters. pretty standard statistics. cell B2, insert positive number (standard deviations positive square roots variance).cell A8 type number 1In cell A9 insert equation “=A8 + 1”. equation ? adding number 1 value cell , resulting value 2.Cell B8, insert equation “=normsinv(rand())*$B$2 + $B$1”. first part equation creates random normal variable mean 0 standard deviation 1. multiplication addition transform random normal variable mean \\(\\mu\\) standard deviation \\(\\sigma\\) (values set cells B1 B2).copy cell B8 paste cell B9. Now Highlight cells A9:B9 copy equations row 107. now 100 random variables sampled infinite population mean \\(\\mu\\) standard deviation \\(\\sigma\\).cell A4 write “mean 10”. cell B4 insert equation “=average(B8:B17)”. resulting value sample mean first 10 random variables created. mean close \\(\\mu\\)?cell A5 write “sd 10”. cell B5 insert equation “stdev(B8:B17)”. result sample standard deviation first 10 random variables. close \\(\\sigma\\)?cell A6 write “mean 100”. cell B6 insert equation “=average(B8:B107)”. resulting value sample mean 100 random variables created. mean closer \\(\\mu\\) mean 10?cell A7 write “sd 100”. cell B7 insert equation “=stdev(B8:B107)”. resulting value sample standard deviation 100 random variables created. SD closer \\(\\sigma\\) sd 10?sample standard deviation measure variability sample. spread sample (value mean), bigger sample standard deviation. sample standard deviation often simply known “” standard deviation, bit misleading since many kinds standard deviations!Remember computed mean standard deviations estimates computed sample. estimates true values \\(\\mu\\) \\(\\sigma\\). Explore behavior sample mean standard deviation re-calculating spreadsheet. Excel, spreadsheet re-calculated simultaneously pressing command equal key. Google, command-R recalculates painfully slow. Instead, using Google Sheets, just type number 1 blank cell, sheet recalculates quickly. . .time re-calculate, new set random numbers generated new means standard deviations computed. Compare mean 10 mean 100 re-calculation. Notice estimates variable. change re-calculation. variable mean 10 compared mean 100? variability estimate mean measure uncertainty estimate. uncertain mean 10 mean 100? variability measured standard deviation. standard deviation mean also called standard error mean. Many researchers loose terms use “” standard error mean standard error mean, even though many kinds standard errors. general, “standard error”” abbreviated “SE.” Sometimes “standard error mean” specifically abbreviated “SEM.”standard error mean measure precision estimating mean. smaller value precise estimate. standard error mean standard deviation mean. kinda weird. sample population one time compute mean, can mean standard deviation? one value! compute value using sample standard deviation: \\(SEM = \\frac{SD}{\\sqrt{N}}\\). understand SEM standard deviation, Imagine recalculating spread sheet infinite number times time, write newly computed mean. standard error mean standard deviation infinitely long column means.","code":""},{"path":"uncertainty.html","id":"using-r-to-generate-fake-data-to-explore-the-standard-error","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2 Using R to generate fake data to explore the standard error","text":"note use “standard deviation” refer sample standard deviation “standard error” refer standard error mean (, can compute standard errors standard deviation kind estimate)","code":""},{"path":"uncertainty.html","id":"part-i","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2.1 part I","text":"exercise , used Google Sheets generate \\(p\\) columns fake data. column \\(n\\) elements, matrix fake data \\(n \\times p\\) (standard fields specify matrix rows columns). much easier R much grows exponentially size matrix grows.start, just generate \\(n \\times p\\) matrix normal random numbers.compacted code (last line code, wrapped looks like two lines) cool thing R. one line ’m creating dataset \\(n\\) rows \\(p\\) columns. column sample standard normal distribution definition mean zero standard deviation 1. , important, sample distribution exactly mean zero standard deviation 1, ’s sample – mean standard deviation small error truth. line two parts (connected magritter pipe operator): first ’m using function rnorm() (random normal) create vector \\(n \\times p\\) random, normal deviates (draws random normal distribution) ’m organizing matrix using function matrix().compute vector means, standard deviations, standard errors column fake_data, use apply() function.apply() workhorse many R scripts often used R scripts place -loop (see ) takes fewer lines code.SEM standard deviation mean, let’s see standard deviation means close true standard error. sampled normal distribution SD=1 true standard isand standard deviation \\(p\\) means isQuestionshow close sd(means) true SE?change p 1000. Now close sd(means) true SE?change p 10,000. Now close sd(means) true SE?","code":"\nset.seed(1) # so everyone gets the same result!\n# R script to gain some intuition about standard deviation (sd) and standard error (se)\n# you will probably need to install ggplot2 using library(ggplot2) \nn <- 6 # sample size\np <- 100 # number of columns of fake data to generate\n\n# this is a step-by-step method for constructing the matrix\nN <- n * p # the total number of fake values to draw from a random distribution\nfake_data <- rnorm(N, mean = 0, sd = 1) # create a vector of N fake values\nfake_data <- matrix(fake_data, nrow = n, ncol = p) # put into a matrix object\n\n# the three step-by-step lines above can be replaced\n# with a more compact code using the pipe operator\nfake_data <- rnorm(n * p, mean = 0, sd = 1) %>%\n  matrix(nrow = n, ncol = p)\nmeans <- apply(fake_data,2,mean) # the apply function is super useful\nsds <- apply(fake_data,2,sd)\nsems <- sds/sqrt(n)\n1/sqrt(n)## [1] 0.4082483\nsd(means)## [1] 0.4150002"},{"path":"uncertainty.html","id":"part-ii---means","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2.2 part II - means","text":"visualization spread, variability, sampled means, sampling distribution means.Compute mean meansQuestionsRemember true mean zero. close, general, sampled means true mean. variable means? quantified?change n 100, replot. means, general, closer true mean? variable means now?mean estimated \\(n=100\\) closer truth, general, mean estimated \\(n=6\\)?Redo \\(n=10000\\)","code":"\nqplot(means, bins = 30)\nmean(means)## [1] -0.05790466"},{"path":"uncertainty.html","id":"part-iii---how-do-sd-and-se-change-as-sample-size-n-increases","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2.3 part III - how do SD and SE change as sample size (n) increases?","text":"Questionswhat mean standard deviations n=6 (set p=1000)mean standard deviations n=100 (set p=1000)n = 1000? (set p=1000)n = 10000? (set p=1000)mean standard deviations change n increases (get smaller? stay size)repeat SEMCongratulations, just done Monte Carlo simulation!","code":"\nmean(sds)## [1] 1.025274\nmean(sems)## [1] 0.4185663"},{"path":"uncertainty.html","id":"part-iv-generating-fake-data-with-for-loops","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2.4 Part IV – Generating fake data with for-loops","text":"-loop used iterate computation.QuestionsWhat sd(means) mean(sems) converge n_sim increased 100 1000 10,000?converge number??correct number?Question number 4 asking E[SEM], “expected standard error mean”. easy formula compute . ?","code":"\nn <- 6 # sample size\nn_sim <- 10^5 # number of iterations of loop (equivalent to p)\nmeans <- numeric(n_sim)\nsds <- numeric(n_sim)\nsems <- numeric(n_sim)\nfor(i in 1:n_sim){\n  y <- rnorm(n) # mean=0 and sd=1 are default so not necessary to specify\n  means[i] <- mean(y)\n  sds[i] <- sd(y)\n  sems[i] <- sd(y)/sqrt(n)\n}\nsd(means)## [1] 0.4077471\nmean(sems)## [1] 0.3887001"},{"path":"uncertainty.html","id":"part-v-the-sampling-distribution-of-means-is-asymptotically-normal-wait-what","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.3.2.5 Part V – the sampling distribution of means is asymptotically Normal (wait, what?)","text":"let’s review standard error. standard error mean standard deviation sampling distribution mean – , standard deviation means taken infinite number samples. fascinating result statistics sampling distribution mean asymptotically Normal regardless distribution sample. mean ? Let’s take positively skewed distribution, say counts marked cells. data generated negative binomial distribution.\\[ counts \\sim NB(\\mu, \\theta) \\]Fig. 5.1 fakesample cell counts 10,000 fake mice.\nFigure 5.1: Positively skewed distribution cell counts.\nLet’s use -loop sample distribution (NB distribution, sample distribution ) 1000 times plot distribution 1000 means. , simulate realistic experiment using \\(n=10\\) sample size.\nFigure 5.2: Distribution 1000 means samples non-normal distribution n = 10.\ndistribution count means Fig. 5.2 still positively skewed less distribution counts Fig. 5.1. Let’s repeat bigger sample size (\\(n = 1000\\))\nFigure 5.3: Distribution 1000 means samples non-normal distribution n = 1000.\ndistribution count means Fig. 5.3 looks Normal-ish. Figures 5.2 5.3 demo , sample size increases, sampling distribution gets closer closer Normal. “approach” Normal rapid go really small samples (\\(n=5\\)) small samples (\\(n=30\\)) slows go big samples (\\(n=100\\)) really big samples (\\(n=1000\\)). , approach asymptotic. rate approach depends sampling distribution sample – non-normal sampling distribution sample, higher sample size needed sampling distribution means nearly-normal.result, sampling distribution means distribution asymptotically normal, Central Limit Theorem consequences huge. Confidence intervals p-values functions sampling distributions means (differences means) say p-value assumes Normality, really Normality sampling distribution means sampling distribution data matters. , sampling distribution means dependent sampling size, means inference assuming Normality less-correct sample size small. sample sizes experimental biology tend small.","code":"\nset.seed(1)\nn <- 10^4\ncounts <- rnegbin(n, mu = 10, theta = 1)\nqplot(counts, bins = 30)\nset.seed(1)\nn_sim <- 1000\nn <- 10\nmeans <- numeric(n_sim)\nfor(i in 1:n_sim){\n  counts <- rnegbin(n, mu = 10, theta = 1)\n  means[i] <- mean(counts)\n}\nqplot(means, bins = 30)\nset.seed(1)\nn_sim <- 1000\nn <- 1000\nmeans <- numeric(n_sim)\nfor(i in 1:n_sim){\n  counts <- rnegbin(n, mu = 10, theta = 1)\n  means[i] <- mean(counts)\n}\nqplot(means, bins = 30)"},{"path":"uncertainty.html","id":"bootstrap","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.4 Bootstrapped standard errors","text":"bootstrap certainly one valuable tools invented modern statistics. , ’s useful tool applied statistics, ’s useful tool understanding statistics. Playing parametric bootstrap almost certainly induce “aha, ’s statisticians mean …” moment.bootstrapped standard error statistic empirical standard deviation statistic finite number samples. basic algorithm bootstrap (“statistic” mean sample)sample \\(n\\) values probability distributioncompute meanrepeat step 1 2 many timesfor bootstrapped standard error, compute standard deviation set means saved iteration steps 1 2.probability distribution can come two sources:parametric bootstrap uses samples parametric probability distribution, Normal distribution poisson distribution (remember, “parametric” distribution completely described set parameters). good question bother? general, one use parametric bootstrap statistic formula standard error, underlying data come parametric probability distribution.non-parametric bootstrap uses resamples data. data resampled replacement. “Resample replacement” means sample \\(n\\) times full set observed values. manually, ) write value original sample piece paper throw pieces hat. ii) pick paper hat, add value sample \\(\\), return paper hat. iii) repeat step ii \\(n\\) times, \\(n\\) original sample size. new sample contains values multiple times (papers picked hat ) missing values (papers picked \\(n\\) picks). good question , bother? non-parametric bootstrap assumes specific parametric probability distribution assume distributio observed sample good approximation true population distribution (case, probability picking certain value good approximation true probability).","code":""},{"path":"uncertainty.html","id":"an-example-of-bootstrapped-standard-errors","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.4.1 An example of bootstrapped standard errors","text":"Data : Fernández, Álvaro F., et al. “Disruption beclin 1–BCL2 autophagy regulatory complex promotes longevity mice.” Nature 558.7708 (2018): 136-140.Experiment: Figure 3bdata file name: 41586_2018_162_MOESM5_ESM.xlsxSee Hidden Code chunk import exp3b dataset.Let’s compute standard error mean \\(\\texttt{positive_nuclei_per_area}\\) WT group using parametric nonparametric bootstrap. implement algorithm using easy--understand code, ’ll first extract set \\(\\texttt{positive_nuclei_per_area}\\) values WT group assign variable.outlined data.table way subset data Subsetting data review line code . exp3b[genotype == \"WT\", ] indexes rows (, returns row numbers) satisfy condtion genotype = “WT”. , put another way, selects subset rows contain value “WT” column “genotype”. exp3b[, positive_nuclei_per_area] indexes column labeled “positive_nuclei_per_area”. Combined, two indices extract values column “positive_nuclei_per_area” subset rows contain value “WT” column “genotype”. resulting vector values assigned variable “wt_nuclei”.","code":"\nwt_nuclei <- exp3b[genotype == \"WT\", positive_nuclei_per_area]"},{"path":"uncertainty.html","id":"parametric-bootstrap","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.4.1.1 parametric bootstrap","text":"","code":"\nset.seed(1) # so we all get the same results!\n# we'll use these as parameters for parametric bootstrap\nn <- length(wt_nuclei)\nmu <- mean(wt_nuclei)\nsigma <- sd(wt_nuclei)\n\nn_boot <- 1000 # number of bootstrap iterations, or p\nmeans <- numeric(n_boot) # we will save the means each iteration to this\n\nfor(iter in 1:n_boot){ # this line sets up the number of iterations, p\n  fake_sample <- rnorm(n, mean = mu, sd = sigma)\n  means[iter] <- mean(fake_sample)\n}\n\nse_para_boot <- sd(means)\nse_para_boot## [1] 0.504529"},{"path":"uncertainty.html","id":"non-parametric-bootstrap","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.4.1.2 non-parametric bootstrap","text":"parametric bootstrapped SEM 0.5. non-parametric bootstrapped SEM 0.51. Run several times get sense much variation bootstrapped estimate SEM given number iterations. Compute parametric standard error using equation (5.2) compare bootstrapped values.","code":"\nset.seed(1) # so we all get the same results!\nn_boot <- 1000 # number of bootstrap iterations, or p\nmeans <- numeric(n_boot) # we will save the means each iteration to this\n\n# inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean\ninc <- 1:n\n\nfor(iter in 1:n_boot){\n  # inc is the set of rows to include in the computation of the mean.\n  means[iter] <- mean(wt_nuclei[inc])\n  # re-sample 1:n to construct inc for the next iteration\n  inc <- sample(1:n, replace = TRUE)\n}\n\nse_np_boot <- sd(means)\nse_np_boot## [1] 0.5102687## [1] 0.5235885"},{"path":"uncertainty.html","id":"confidence-intervals","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.5 Confidence Intervals","text":"","code":""},{"path":"uncertainty.html","id":"just-the-math","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.5.1 Just the math","text":"introduce confidence interval (CI) sample mean concept easily generalized statistic. 95% confidence interval mean 95% probability including true mean, , value true mean lower upper bounds interval. probability long-run frequency sense population sampled many times, 95% CIs constructed sample, 95% CIs contain mean. longer outline interpretation confidence interval next section.SE statistic used construct lower upper boundary confidence interval. CI mean, formula \\[\nCI = \\overline{y} + t^*SE\n\\]\\(t^*\\) (critical value) quantile t-distribution \\(n-1\\) degrees freedom. lower bound 95% CI, want quantile 0.025 (2.5%). upper bound 95% CI, want quantile 0.975 (97.5%). wt_nuclei sample, \\(n = 20\\), lower upper \\(t^*\\) areAnd, lower upper 95% CIs mean wt_nuclei areThe function qt maps probability t-value – opposite t test, maps t-value probability. Sending \\(\\alpha/2\\) \\(1 - \\alpha/2\\) qt returns bounds confidence interval standardized scale. Multiplying bounds standard error o wt_nuclei transforms standardized bounds onto scale wt_nuclei counts.can check manual computation linear model","code":"\nt_lower = qt(0.025, df = (n - 1))\nt_upper = qt(0.975, df = (n - 1))##   t_lower   t_upper \n## -2.093024  2.093024\nse_wt <- sd(wt_nuclei)/sqrt(n)\n# CIs\nlower <- mean(wt_nuclei) + t_lower * se_wt\nupper <- mean(wt_nuclei) + t_upper * se_wt##     2.5%    97.5% \n## 1.106817 3.298584\nconfint(lm(wt_nuclei ~ 1))##                2.5 %   97.5 %\n## (Intercept) 1.106817 3.298584"},{"path":"uncertainty.html","id":"interpretation-of-a-confidence-interval","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.5.2 Interpretation of a confidence interval","text":"Okay, confidence interval? confidence interval mean measure uncertainty estimate mean. 95% confidence interval 95% probability (sense long-run frequency) containing true mean. correct state “95% probability true mean lies within interval”. sound two different probabilities. first (correct interpretation) probability procedure – re-procedure (sample data, compute mean, compute 95% CI), 95% CIs contain true mean. second (incorrect interpretation) probability parameter (\\(\\mu\\), true mean) lies within range. second (incorrect) interepretation CI correct also assume value mean equally probable (Greenland xxx), assumption absurd almost data.Perhaps useful interpretation confidence interval , confidence interval contains range true means compatible data, sense \\(t\\)-test reject null hypothesis difference estimate value within interval (interpretation imply anything true value) (Greenland xxx). “compatibility” interpretation useful implies values outside interval less compatible data.","code":""},{"path":"uncertainty.html","id":"bootstrap-confidence-interval","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.5.3 Bootstrap confidence interval","text":"","code":"\nset.seed(1) # so we all get the same results!\nn_boot <- 1000 # number of bootstrap iterations, or p\nmeans <- numeric(n_boot) # we will save the means each iteration to this\n\n# inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean\ninc <- 1:n\n\nfor(iter in 1:n_boot){\n  # inc is the set of rows to include in the computation of the mean.\n  means[iter] <- mean(wt_nuclei[inc])\n  # re-sample 1:n to construct inc for the next iteration\n  inc <- sample(1:n, replace = TRUE)\n}\n\nci_boot_wt <- quantile(means, c(0.025, 0.975))\nci_boot_wt##     2.5%    97.5% \n## 1.287197 3.306559"},{"path":"uncertainty.html","id":"a-plot-of-a-parametric-ci-vs.-bootstrap-ci-of-the-means","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.5.4 A plot of a “parametric” CI vs. bootstrap CI of the means","text":"dataset exp3b specifically chosen highlight differences bootstrap CI CI computed using Eq. (5.2) standard error mean. ’ll refer latter CI “parametric” CI – ’s CI one gets fit linear model just treatment group.See can follow ’s going .Create function compute 95% parametric CI mean. “parametric” CI, mean CI using Eq. (5.2) SEM.create function anytime recycle code compute something different input, best practice create function compute something.Create function compute 95% non-parametric bootstrap CI meanCreate table parametric bootstrap CIs mean WT KI groupsPlotThe difference parametric bootstrap CI easily seen comparing CIs KI treatment group. parametric CI symmetric mean. true computed. non-parametric bootstrap CI asymmetric mean – longer positive side shorter less positive side. asymmetry reflects positive-skew underlying distribution. nice feature bootstrap CI compared parametric CI computed sample CI bootstrap CI never contain impossible values. parametric CI fit linear model might negative lower bound, imply negative counts consistent data. Negative counts aren’t really consistent anything negative counts impossible.","code":"\n# a simple ci function, that can't handle missing\nci_mean <- function(y,\n                    alpha = 0.05){\n  \n  n <- length(y)\n  ci <- c(\n    \"2.5%\" = mean(y) + qt(alpha/2, df = (n - 1)) * sd(y)/sqrt(n),\n    \"97.5%\" = mean(y) + qt((1 - alpha/2), df = (n - 1)) * sd(y)/sqrt(n)\n  )\n  return(ci)\n}\n# a simple bootstrap function\n\nboot_ci_mean <- function(y,\n                         alpha = 0.05,\n                         n_boot = 1000,\n                         seed = 1){\n  set.seed(seed)\n  n_boot <- 1000 # number of bootstrap iterations\n  means <- numeric(n_boot) # we will save the means each iteration to this\n  \n  # inc indexes the elements to sample. By setting inc to 1:n prior to the loop, the first mean that is computed is the observed mean\n  inc <- 1:n\n  \n  for(iter in 1:n_boot){\n    # inc is the set of rows to include in the computation of the mean.\n    means[iter] <- mean(y[inc])\n    # re-sample 1:n to construct inc for the next iteration\n    inc <- sample(1:n, replace = TRUE)\n  }\n  \n  ci_boot <- quantile(means, c(alpha/2, (1-alpha/2)))\n  return(ci_boot)\n}\nwt_nuclei <- exp3b[genotype == \"WT\", positive_nuclei_per_area]\nki_nuclei <- exp3b[genotype == \"KI\", positive_nuclei_per_area]\n\nexp3b_ci <- exp3b[, .(positive_nuclei_per_area = mean(positive_nuclei_per_area),\n                      ci = ci_mean(positive_nuclei_per_area),\n                      ci_boot = boot_ci_mean(positive_nuclei_per_area)),\n                  by = genotype]\nexp3b_ci[, boundary := rep(c(\"lower\", \"upper\"), 2)]\nexp3b_ci_wide <- dcast(exp3b_ci,\n                       genotype + positive_nuclei_per_area ~ boundary,\n                       value.var = c(\"ci\", \"ci_boot\"))\ngg1 <- ggplot(data = exp3b,\n             aes(x = genotype,\n                 y = positive_nuclei_per_area)) +\n  geom_point(position = position_jitter(\n    width = 0.1,\n    seed = 1),\n    alpha = 0.3) +\n  geom_point(data = exp3b_ci_wide,\n             aes(y = positive_nuclei_per_area),\n             size = 1.5) +\n  geom_errorbar(data = exp3b_ci_wide,\n                aes(ymin = ci_lower,\n                    ymax = ci_upper),\n                width = 0.1) +\n  ylab(\"Positive nuclei per area\") +\n  ggtitle(\"Parametric CI\") +\n  theme_pubr()\n\ngg2 <- ggplot(data = exp3b,\n             aes(x = genotype,\n                 y = positive_nuclei_per_area)) +\n  geom_point(position = position_jitter(\n    width = 0.1,\n    seed = 1),\n    alpha = 0.3) +\n  geom_point(data = exp3b_ci_wide,\n             aes(y = positive_nuclei_per_area),\n             size = 1.5) +\n  geom_errorbar(data = exp3b_ci_wide,\n                aes(ymin = ci_boot_lower,\n                    ymax = ci_boot_upper),\n                width = 0.1) +\n  ylab(\"Positive nuclei per area\") +\n  ggtitle(\"Bootstrap CI\") +\n  theme_pubr()\n\nplot_grid(gg1, gg2, ncol = 2)"},{"path":"uncertainty.html","id":"standard-error-of-a-difference-between-means","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.6 Standard error of a difference between means","text":"Let’s apply concepts standard error confidence interval important statistic experimental biologists – difference means two groups.standard error difference means two groups \\[\ns_{\\bar{y}_2 - \\bar{y}_1} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n\\]standard error difference means square root sum squared standard errors mean variable. understand , recall add two, independent, random variables, variance summed variable sum variances independent variables (okay, variance difference two random variables? different difference just addition negative second variable).","code":""},{"path":"uncertainty.html","id":"the-standard-error-of-a-difference-between-means-is-the-standard-deviation-of-the-sampling-distribution-of-the-difference","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.6.1 The standard error of a difference between means is the standard deviation of the sampling distribution of the difference","text":"Let’s peak sampling distribution difference means simulated control (“Cn”) treated (“Tr”) groups. fake data simulated look like exp2i, introduced introductory chapter Analyzing experimental data linear model.\nFigure 5.4: Sampling distribution difference means fake data modeled look like data exp2i Chapter 1.\nFig. 5.4 sample (N = \\(10^4\\)) sampling distribution difference means fake data modeled look like data exp2i Chapter 1.standard deviation sample isThis pretty close true standard deviation distribution, isThis expected standard error difference means sample \\(n = 6\\) individuals “Cn” population (\\(\\mu=\\) 61.9966034, \\(\\sigma=\\) 9.9963055 \\(n = 6\\) individuals “Tr” population (\\(\\mu=\\) 35.043945, \\(\\sigma=\\) 9.9963055. observed difference standard error first ten fake differences standard errors \\(10^4\\) fake data sets ..","code":"\nset.seed(1) # so we all get the same result!\nn_sim <- 10^4\n\n# set mean, sd and difference to look like exp2i from ch. 1\nm1 <- lm(liver_tg ~ treatment, data = exp2i)\ns_obs <- summary(m1)$sigma\nsigma <- s_obs + rnorm(1, mean = 0, sd = s_obs/sqrt(12))\nb_obs <- coef(m1)\nmu_1 <- b_obs[1] + \n  rnorm(1, mean = 0, sd = sigma/sqrt(12)) # cn mean = b0\nmu_2 <- b_obs[1] + b_obs[2] + \n  rnorm(1, mean = 0, sd = sqrt(2*sigma^2/6)) # tr mean = b0 + b1\ndelta <- mu_2 - mu_1\nn <- nrow(exp2i)/2\n\ndiff <- numeric(n_sim)\nsed <- numeric(n_sim)\ndiff_obs <- b_obs[2]          # observed value\nsed_obs <- sqrt(2*s_obs^2/n)  # observed value\nfor(sim_i in 1:n_sim){\n  Cn <- rnorm(n, mean = mu_1, sd = sigma)\n  Tr <- rnorm(n, mean = mu_2, sd = sigma)\n  diff[sim_i] <- mean(Tr) - mean(Cn)\n  sed[sim_i] <- sqrt(sd(Cn)^2/n + sd(Tr)^2/n)\n}\n\nqplot(diff, bins = 30)\nsd(diff)## [1] 5.761664\nsqrt(sigma^2/n + sigma^2/n)## [1] 5.77137"},{"path":"uncertainty.html","id":"confidence-limits-of-a-difference-between-means","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.7 Confidence limits of a difference between means","text":"Let’s compute 95% confidence interval difference means exp2i data.Now compare values “treatmentASK1Δadipo” row coefficient table linear model fit exp2i data.","code":"\nCn <- exp2i[treatment == \"ASK1F/F\", liver_tg]\nTr <- exp2i[treatment == \"ASK1Δadipo\", liver_tg]\ndiff_1 <- mean(Tr) - mean(Cn)\ns_1 <- sd(Tr)\ns_2 <- sd(Cn)\n\nse <- sqrt(s_1^2/n + s_2^2/n)\n\n# 2n - 2 df because there are 2n values and \n# 2 parameters fit (a df is lost for each fit parameter)\nt_lower = qt(0.025, df = (2*n - 2))\nt_upper = qt(0.975, df = (2*n - 2))\n\nlower <- diff_1 + t_lower * se\nupper <- diff_1 + t_upper * se\n# fit the model\nm1 <- lm(liver_tg ~ treatment, data = exp2i)\n\n# output table\nmodel_table <- cbind(coef(summary(m1)),\n                     confint(m1))\n# print\nmodel_table %>%\n  kable() %>%\n  kable_styling()"},{"path":"uncertainty.html","id":"of-95-cis-of-the-difference-include-the-true-difference","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.7.1 95% of 95% CIs of the difference include the true difference","text":"95% CI difference (estimate true effect) 95% probability “covering true effect”, probability sense long-term frequency – repeated experiment 1000 times, expect 950 measured 95% CIs include true effect. Let’s check frequency 95% CIs cover true effect \\(10^4\\) fake data sets generated .First, compute CIs data set.check, let’s plot first 10 estimates true effect CI estimate fake data simulation.frequency CIs cover true effect isPretty good!","code":"\nlower <- diff + qt(0.025, df = (2*n - 2)) * sed\nupper <- diff + qt(0.975, df = (2*n - 2)) * sed\nci_table <- data.table(\n  sim = factor(1:n_sim),\n  estimate = diff,\n  \"lower\" = lower,\n  \"upper\" = upper\n)\ngg <- ggplot(data = ci_table[1:10,],\n             aes (x = estimate,\n                  y = sim)) +\n  geom_vline(aes(xintercept = delta), linetype = \"dashed\") +\n  geom_point(size = 2) +\n  geom_segment(aes(x = lower,\n                 xend = upper,\n                 y = sim,\n                 yend = sim)) +\n  theme_pubr()\n\ngg\n(sum(delta > lower & delta < upper))/n_sim * 100## [1] 95.1"},{"path":"uncertainty.html","id":"hidden-code","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.8 Hidden code","text":"","code":""},{"path":"uncertainty.html","id":"import-exp3b","chapter":"5 Variability and Uncertainty (Standard Deviations, Standard Errors, and Confidence Intervals)","heading":"5.8.1 Import exp3b","text":"","code":"\ndata_from <- \"Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice\"\nfile_name <- \"41586_2018_162_MOESM5_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp3b <- read_excel(file_path,\n                   sheet = \"Fig. 3b\",\n                   range = \"I4:K50\",\n                   col_names = TRUE) %>%\n  data.table() %>%\n  clean_names()\nsetnames(exp3b, old = \"area_mm2\", new = \"area\")\n\ngenotype_levels <- c(\"WT\", \"KI\")\nexp3b[, genotype := rep(genotype_levels, c(20, 26))]\nexp3b[, genotype := factor(genotype,\n      levels = genotype_levels)]\nexp3b[, positive_nuclei_per_area := positive_nuclei/area]"},{"path":"p-values.html","id":"p-values","chapter":"6 P-values","heading":"6 P-values","text":"general perception “replication crisis” may thus reflect failure recognize statistical tests test hypotheses, countless assumptions entire environment research takes place. uncertain unknown assumptions underpin statistical inferences, treat inferential statistics highly unstable local descriptions relations assumptions data, rather providing generalizable inferences hypotheses models. means treat statistical results much incomplete uncertain currently norm.3A p-value measure compatibility observed data null model. , “compatibility” probability, specifically, probability sampling test-statistic extreme observed test statistic, assumptions used compute p-value true.deconstruct means, implications meaning, let’s use exp2i data Figure 2i study browning white adipose tissue mice introduced introductory chapter # Analyzing experimental data linear model.Data source: ASK1 inhibits browning white adipose tissue obesityThe chunk fits linear model liver_tg response variable treatment single \\(X\\)-varaiable. Figure 6.1 plot modeled means, 95% confidence intervals mean, p-value significance test effect treatment liver triacylglycerol.\nFigure 6.1: UCP1 expression, relative mean level control group. Mean (circle) 95% confidence interval (line) shown. Unadjusted p-values linear model sh-RNA treatment LPS treatment fully crossed.\ncoefficients model, standard error, 95% confidence interval, test-statistic, p-value coefficient shown Table ?? Recall Chapter ?? , model, coefficient intercept term mean liver_tg control group, estimate true mean mice functional ASK1 protein. coefficient treatmentASK1Δadipo term difference means knockout control group, estimate true effect knocking ASK1 adipose tissue. p-value “effect” term 0.012. interpret number?","code":"\nfig_2i_m1 <- lm(liver_tg ~ treatment, data = fig_2i)"},{"path":"p-values.html","id":"a-p-value-is-the-probability-of-sampling-a-value-as-or-more-extreme-than-the-test-statistic-if-sampling-from-a-null-distribution","chapter":"6 P-values","heading":"6.1 A p-value is the probability of sampling a value as or more extreme than the test statistic if sampling from a null distribution","text":"test statistic table t-value. specific model, t-value precisely t-value one get executed classical Student t-test two groups liver TG values. Importantly, generally true. many models text, t-value computed t-value computed classical t-test.t-test, get p-value. probability returned t-test \\(p = \\mathrm{prob}(t \\ge t_{obs} | H_0)\\). Read “p-value probability observing t-value greater equal observed t-value, given null model true.” Probability, text, long run frequency sampling. specific probability associated effect treatment liver TG long-run frequency observing t-value big bigger observed t-value (one actually got data) null true. Let’s parse “long run frequency observing t-value big bigger observed t-value” “null true”.thought experiment: open google sheet insert 12 standard, normal random deviates (true mean zero true variance one) Column , rows 1-12. arbitrarily assign first six values (rows 1-6) treatment second six values (rows 7-12) treatment B. use space immediately data compute mean treatment , mean treatment B, difference means (- B), t-value. Unfortunately, google sheets doesn’t t-value function ’d compute . , since thought experiment. Now “fill right” copy paste functions 999 new columns. now 1000 t-tests. expected value difference means zero (?) actual values form normal distribution zero. close zero (either negative positive direction) zero. expected t-value also zero (?) distribution 1000 t-values look normal tails little fuller. row t-values null distribution, generating data used exact formula values assigned values assigned B. Now think t-value head, say 0.72 (remember t-values largely range -3 +3 although theoretical range \\(-\\infty\\) \\(+\\infty\\). probability observing t 0.72 bigger null true? Look row t-values! Count number \\(t \\ge 0.72\\) divide total number t-values row (1000) probability computed frequency. remember frequentist definition long run frequency, expected frequency limit (’ve generated 1000 even 1,000,000 infinite number columns t-values).asides thought experiment: First, “big bigger” just probability value ? reason probability finding exact t 1/infinity, doesn’t us much good. instead compute probability finding t big, bigger, observed t. Second, t-test probability described “one-tail probability”. difference can positive direction negative direction, usually want count \\(t \\ge 0.72\\) \\(t \\le -0.72\\) add two counts compute frequency extreme extreme values. called “two-tailed probability” find extremes tails distribution. Third, don’t really count \\(t \\ge 0.72\\) take advantage beautiful mathematical properties theoretical t distribution, allows us compute frequentist probability (expected long range frequency) given t-value degrees freedom using t-distribution.Now mean phrase “null true”? people equate “null true” “difference means” phrase entails much . Effectively, phrase short “assumptions used compute” p-value (see Sander Greenland quote start chapter). p-value based modeling real data theoretical sample values randomly sampled distribution assignment individual values treatment random. Random sampling distribution three important consequences. First, random assignment treatment group means expected means group , put differently, expected difference means assigned groups zero. Second, random assignment treatment also means expected variances two groups equal. third, random sampling means values point independent – predict value one point knowing information point. super important : low p-value likely one consequences untrue data. low p-value arise difference true means, arise difference true variances, arise \\(Y\\) values independent . need certain assumptions make p-value meaningful empirical data. assuming independent error homogenous (equal) variances two samples, low p-value evidence unequal means.Let’s summarize: pretty good definition p-value : long-run frequency observing test-statistic large larger observed statistic, null true. succinct way state \\[\\begin{equation}\np = \\mathrm{prob}(t \\ge t_o | H_o)\n\\end{equation}\\]t hypothetically sampled t-value null distribution, \\(t_o\\) observed t-value, \\(H_o\\) null hypothesis. Part null hypothesis expected value parameter estimated usually (always) zero – can called nil null. example, ASK1 deletion effect liver TG levels, expected difference means control knockout mice zero. ,\\[\\begin{equation}\n\\mathrm{E}(\\bar{Y}_{knockout} - \\bar{Y}_{control} | H_o) = 0.0\n\\end{equation}\\]","code":""},{"path":"p-values.html","id":"pump-your-intuition-creating-a-null-distribution","chapter":"6 P-values","heading":"6.2 Pump your intuition – Creating a null distribution","text":"mean liver_tg knockout treatment 21.6 µmol less mean liver_tg control treatment. measured effect, observed differences means. confident effect? Certainly, researchers experiment comparing two control (ASK1F/F) groups, instead control treatment group, measure difference means simply sampling (mice sampled experiment). let’s reframe question: observed differences unusually large compared distribution differences occur effect? , “null true”. answer , compare observed difference null distribution. comparison gives probability (long-run frequency) “sampling” random difference null distribution differences large, larger, observed difference.null distribution? distribution statistic (difference means, better, t-value) null true. , hope pump intuition generating null distribution relevant ASK1 liver TG data. See can understand script reading explanation .\nFigure 6.2: Null distribution difference means two samples , inifinitely large population true mean standard deviation equal observed mean standard deviation ASK1 liver TG data.\ndone ? using rnorm function, ’ve simulated infinitely large population mice distribution liver TG values similar mice assigned control (ASK1F/F) group. true mean (\\(\\mu\\)) standard deviation (\\(\\sigma\\)) simulated TG level equal observed mean standard deviation TG levels control mice.randomly sample 6 values population simulated liver TG values assign \\(\\texttt{sample_1}\\). sample 6 values sample size control experiment.randomly sample 6 values population simulated liver TG values assign \\(\\texttt{sample_1}\\).compute difference means: \\(\\bar{Y}_{sample\\_2} - \\bar{Y}_{sample\\_1}\\).repeat 1-3 100,000 times, time saving difference means.plot distribution 100,000 differences using histogramThe distribution differences null distribution. Notice mode null distribution zero, mean (0.01768) close zero (set \\(n\\) infinity, mean precisely zero). expected difference means two random samples population , course, zero. Don’t gloss statement obvious. tails extend little +20 -20. means uncommon randomly sample value distribution differences extreme observed difference, -21.6. “extreme”, mean value negative -21.6 postive 21.6. uncommon sample value distribution whose absolute value extreme 21.6. uncommon ?100,000 runs, 566 generated data absolute difference large larger 21.6 (“absolute difference” absoute value difference). frequency differences large larger observed difference 0.00566. frequency probability sampling difference extreme observed difference “null”. p-value, p-value coefficient table. p-value coefficient table computed distribution t-values, raw differences. raises question, t-distribution, t-value, generally?","code":"\nseed <- 1\nn_rep <- 10^5 # number of replicate experiments\n\nmu <- mean(fig_2i[treatment == \"ASK1F/F\", liver_tg]) \nsigma <- sd(fig_2i[treatment == \"ASK1F/F\", liver_tg])\n\nn <- nrow(fig_2i[treatment == \"ASK1F/F\",])\n\nd_null <- numeric(n_rep)\nfor(rep in 1:n_rep){\n  sample_1 <- rnorm(n, mean = mu, sd = sigma)\n  sample_2 <- rnorm(n, mean = mu, sd = sigma)\n  d_null[rep] <- mean(sample_2) - mean(sample_1)\n}\n\nqplot(d_null)\ndiff_obs <- fig_2i_m1_coef[\"treatmentASK1Δadipo\", \"Estimate\"]\nnull_diff_extreme <- which(abs(d_null) > abs(diff_obs))\nn_extreme <- length(null_diff_extreme)\n(p_d_null = n_extreme/n_rep)## [1] 0.00566"},{"path":"p-values.html","id":"a-null-distribution-of-t-values-the-t-distribution","chapter":"6 P-values","heading":"6.3 A null distribution of t-values – the t distribution","text":"t-test test differences two values. bethe difference means two samples (“two-sample” t-test)difference mean sample pre-specified value (“one-sample” t-test)difference coefficient linear model zeroA t-test compares observed t-value t-distribution. null distribution introduced distribution mean differences null. distribution mean differences null specific mean standard deviation population modeled sample size experiment. isn’t generally useful, since unique every study (least wasn’t generally useful prior time fast computers. One , statisticians , compute p-values using algorithm ). t-distribution way transforming null distribution mean differences, unique study, distribution function sample size .t-distribution distribution t-values null, t-value difference standardized standard error. two-sample t-test, \\[\\begin{equation}\nt = \\frac{\\bar{y}_2 - \\bar{y}_1}{SE_{\\bar{y}_2 - \\bar{y}_1}}\n\\tag{6.1}\n\\end{equation}\\]numerator effect denominator precision estimate. Like many test statistics, t-value signal--noise ratio – effect signal SE difference noise.t distribution looks like standard, normal distribution, except tails heavy, meaning large-ish values normal. Like standard normal distribution, large t-values unlikely null , therefore, large t low probability – p-value – null.Looking equation two-sample t-test , easy see three features experiment associated large t small p-values: 1) big effect size (numerator equation), 2) small sample standard deviations (results small standard errors difference, denominator equation (6.1), 3) large sample size (results small standard errors difference). quick--dirty generalization, absolute t-values greater 3 uncommon null true.p-value t-test comes comparing observed t null t distribution “counting” values extreme observed t. p-value relative frequency extreme values (relative total number t-values distribution). “counting” quotes nothing really counted – infinite number t-values t-distribution. Instead, t-distribution function integrated compute fraction total area curve t-values extreme observed value. two-tailed test, fraction includes tails (positive t-values positive \\(|t|_{observed}\\) negative t-values negative \\(-|t|_{observed}\\).Let’s repeat simulation null distribution mean differences add computation t-value replicate comparison order generate null distribution t-values. Importantly, ’ve also changed bits code properly think computed t-value . changes :want think first sample assigned “WT” second sample assigned “KO”. , KO sample drawn distribution (hat numbers) WT sample – guarantees difference expected mean.want think observed variances WT KO samples sampled variances fake distribution. Therefore, give variance fake distribution average observed WT KO samples. true (population) standard deviation (\\(\\sigma\\)) simulated data, , square root averaged variance.show script, don’t just cut paste code. Spend time thinking line . Explore copying parts pasting console.\nFigure 6.3: Null distribution t-values. simulation generated 10,000 t-tests true null.\nNow let’s use null distribution t-values compute p-valueHey looks pretty good! Compare p-value coefficient table .p-value can computed counting number simulated t-values, including observed value, equal extreme (either positive negative direction) observed t. Including observed t, 1173 values extreme observed. approximate measure p count divided 100,001 (1 added denominator?), 0.01173. simulation-based p-value (!) close computed observed t-test.","code":"\nseed <- 1\nn_rep <- 10^5 # number of iterations\n\nmu <- mean(fig_2i[treatment == \"ASK1F/F\", liver_tg])\nsd_control <- sd(fig_2i[treatment == \"ASK1F/F\", liver_tg])\nsd_knockout <- sd(fig_2i[treatment == \"ASK1Δadipo\", liver_tg])\nsigma <- sqrt((sd_control^2 + sd_knockout^2)/2)\n\nn <- nrow(fig_2i[treatment == \"ASK1F/F\",])\n\ntreatment <- rep(c(\"WT\", \"KO\"), each = n) %>%\n  factor(levels = c(\"WT\", \"KO\")) # need this for for-loop\nt_null <- numeric(n)\nt_null_manual <- numeric(n)\nfor(rep in 1:n_rep){\n  wt_sample <- rnorm(n, mean = mu, sd = sigma)\n  ko_sample <- rnorm(n, mean = mu, sd = sigma)\n\n  # way no.1 - compute the t-tests using the linear model\n  y <- c(wt_sample, ko_sample)\n  m1 <- lm(y ~ treatment)\n  t_null[rep] <- coef(summary(m1))[\"treatmentKO\", \"t value\"]\n  \n  # way no. 2 - compute the t-tests manually!\n  # check to make sure these are the same as t_null !!!\n  diff <- mean(ko_sample) - mean(wt_sample)\n  se_diff <- sqrt(sd(ko_sample)^2/n + sd(wt_sample)^2/n)\n  t_null_manual[rep] <- diff/se_diff\n\n}\n\n\n# plot the null distribution of t-values\nqplot(t_null)\n# what is the p-value?\n# the p-value is the number of t-values in t_null_2 that are as large\n# or larger than the observed t. Large, negative t-values\n# are as unlikely under the null as large, positive t-values.\n# To account for this, we want to use absolute values in our counts\n# this is a \"two-tail test\"\n\n# first assign the observed t-value\nt_obs <- fig_2i_m1_coef[\"treatmentASK1Δadipo\", \"t value\"]\n\n\n# now count the number of t-values in t_dis as big or bigger than this\n# include the observed value as one of these (so add 1 to the count)\ncount <- sum(abs(t_null) >= abs(t_obs))\n\n# the p-value is the frequency of t_dis >= t_obs, so divide\n# count by the total number of t-values in the distribution.\n# Again add one since the observed value counts as a sample\n(p_ASK1Δadipo <- count/(n_rep))## [1] 0.01173"},{"path":"p-values.html","id":"p-values-from-the-perspective-of-permutation","chapter":"6 P-values","heading":"6.4 P-values from the perspective of permutation","text":"intuitive way think p-values frequency random permutation. permutation re-arrangement items. effect ASK1 deletion liver TG, arrangement values treatment column matters. effect ASK1 deletion liver TG, arrangement values treatment column matter.Think structure liver TG data: two columns, treatment, contains assigned treatment, liver_tg. values treatment column randomly assigned prior start experiment. negative effect ASK1 deletion liver TG, assginment matters – values liver_tg column ASK1Δadipo rows smaller , average, values ASK1F/F rows. , specific value aliver_tg value treatement row. Assignment ASK1F/F ASK1Δadipo changes expected value liver_tg. , true effect, assignment ASK1F/F ASK1Δadipo change expected value liver_tg. expected value every cell liver_tg column regardless treatment column.thought experiment, let’s leave values treatment column , just randomly re-arrange permute values liver_tg column. new expected diference liver TG rows assigned ASK1F/F rows assigned ASK1Δadipo? expected difference Zero. liver_tg values randomly re-arranged, caused treatment assignment.permutation random re-arrangement values column. Consider many thousands permutations values liver_tg column. difference means can computed permuations distribution differences can generated. observed difference extreme relative values distribution? permutation test – compares observed statistic distribution statistic computed many thousands permutations.Let’s create script permutation testFrom distribution distances generated random permuation response, can compute permutation p-value.{rpvalue-d-dist-perm-p } (p_permute <- sum(abs(d_dist_perm) >= abs(d_dist_perm[1]))/n_iter)","code":"\nset.seed(1)\nn_iter <- 5000 # number of random permutations\n\ny <- fig_2i[, liver_tg]\nx <- fig_2i[, treatment]\n\nd_dist_perm <- numeric(n_iter)\n\nfor(iter in 1:n_iter){\n  xbar1 <- mean(y[x == \"ASK1F/F\"])\n  xbar2 <- mean(y[x == \"ASK1Δadipo\"])\n  \n  d_dist_perm[iter] <- xbar2 - xbar1\n  \n  # permute y\n  y <- sample(y, replace=FALSE)\n  # note that, when i=1, the first \"permutation\" is the original arrangement\n}\n\nqplot(d_dist_perm)"},{"path":"p-values.html","id":"parametric-vs.-non-parametric-statistics","chapter":"6 P-values","heading":"6.5 Parametric vs. non-parametric statistics","text":"statistic difference mean liver TG ASK1Δadipo ASK1F/F groups “” p-value. p-value probability observing event given model event generated. p-value coefficient table , event sampling t-value modeled t distribution extreme observed t-value. model generating null distribution t-values includes random sampling distribution defined specific parameters (case, mean variance), parameters define location shape distribution values sampled. p-value computed distribution defined set parameters parametric p-value.p-value computed using permutation test, event probability computing difference means randomly permuted set \\(Y\\) extreme observed difference means. distribution differences permutated \\(Y\\) data sets generated known distributions (normal, Poisson, binomial, etc.) given specific value parameters. Consequently, permutation p-value non-parametric.validity p-values depends set model assumptions, differ model model. permutation p-value fewer assumptions parametric p-value distribution assumed (permutation p-value distribution-free).","code":""},{"path":"p-values.html","id":"frequentist-probability-and-the-interpretation-of-p-values","chapter":"6 P-values","heading":"6.6 frequentist probability and the interpretation of p-values","text":"","code":""},{"path":"p-values.html","id":"background-1","chapter":"6 P-values","heading":"6.6.1 Background","text":"least three different meanings probability.subjective probability probability individual assigns event based prior knowledge kinds information considered reliable evidence. example, asked sample students, probability 30c homeopathic medicine clear Streptococcus infection respiratory system, answers differ variation knowledge basic science, including chemistry physics, knowledge homeopathic medicines , weight different kinds evidence.subjective probability probability individual assigns event based prior knowledge kinds information considered reliable evidence. example, asked sample students, probability 30c homeopathic medicine clear Streptococcus infection respiratory system, answers differ variation knowledge basic science, including chemistry physics, knowledge homeopathic medicines , weight different kinds evidence.classical probability simply one divided number possible unique events. example, six-sided die, six possible unique events. probability rolling 2 \\(\\frac{1}{6}\\) probability rolling odd number \\(\\frac{1}{2}\\).classical probability simply one divided number possible unique events. example, six-sided die, six possible unique events. probability rolling 2 \\(\\frac{1}{6}\\) probability rolling odd number \\(\\frac{1}{2}\\).frequentist probability based concept long run frequency. roll die 10 times, frequency rolling 2 approximately \\(\\frac{1}{6}\\). roll die 100 times, frequency rolling two closer \\(\\frac{1}{6}\\). roll die 1000 times, frequency rolling die even closer \\(\\frac{1}{6}\\). frequentist definition expected frequency given infinite number rolls. events continous outcomes, frequentist probability long run frquency observing outcome equal extreme observed.frequentist probability based concept long run frequency. roll die 10 times, frequency rolling 2 approximately \\(\\frac{1}{6}\\). roll die 100 times, frequency rolling two closer \\(\\frac{1}{6}\\). roll die 1000 times, frequency rolling die even closer \\(\\frac{1}{6}\\). frequentist definition expected frequency given infinite number rolls. events continous outcomes, frequentist probability long run frquency observing outcome equal extreme observed.","code":""},{"path":"p-values.html","id":"this-book-covers-frequentist-approaches-to-statistical-modeling-and-when-a-probability-arises-such-as-the-p-value-of-a-test-statistic-this-will-be-a-frequentist-probability.","chapter":"6 P-values","heading":"6.6.2 This book covers frequentist approaches to statistical modeling and when a probability arises, such as the p-value of a test statistic, this will be a frequentist probability.","text":"t-test, get p-value. several ways think probability. compact way \\(P(data | null)\\), literally read probability data given null (“conditional” null), really short probability data, something extreme data, given null hypothesis true. “probability data” kinda vague. specifically, mean probability statistic data difference means group group B t-value associated difference. , bit formally, probability returned t-test \\(\\mathrm{prob}(t \\ge t_{obs} | H_0)\\). long run frequency observing t-value big bigger observed t-value (one actually got data) null true. Let’s parse “long run frequency observing t-value big bigger observed t-value” “null true”.thought experiment: open google sheet insert 12 standard, normal random deviates (true mean zero true variance one) Column , rows 1-12. arbitrarily assign first six values (rows 1-6) treatment second six values (rows 7-12) treatment B. use space immediately data compute mean treatment , mean treatment B, difference means (- B), t-value. Unfortunately, google sheets doesn’t t-value function ’d compute . , since thought experiment. Now ``fill right’’ copy paste functions 999 new columns. now 1000 t tests. expected value difference means zero (?) actual values form normal distribution zero. close zero (either negative positive direction) zero. expected t-value also zero (?) distribution 1000 t-values look normal tails little fuller. row t-values null distribution, generating data used exact formula values assigned values assigned B. Now think t-value head, say 0.72 (remember t-values largely range -3 +3 although theoretical range \\(-\\infty\\) \\(+\\infty\\). probability observing t 0.72 bigger null true? Look row t-values! Count number \\(t \\ge 0.72\\) divide total number t-values row (1000) probability computed frequency. remember frequentist definition long run frequency, expected frequency limit (’ve generated 1000 even 1,000,000 infinite number columns t-values).asides thought experiment: First, “big bigger” just probability value ? reason probability finding exact t 1/infinity, doesn’t us much good. instead compute probability finding t big, bigger, observed t. Second, t-test probability described “one-tail probability”. difference can positive direction negative direction, usually want count \\(t \\ge 0.72\\) \\(t \\le -0.72\\) add two counts compute frequency extreme extreme values. called “two-tailed probability” find extremes tails distribution. Third, don’t really count \\(t \\ge 0.72\\) take advantage beautiful mathematical properties theoretical t distribution, allows us compute frequentist probability (expected long range frequency) given t-value degrees freedom using t-distribution.Now mean phrase “null true”? people equate “null true” ``difference means’’ phrase entails much . Effectively, phrase means p-value based modeling real data theoretical sample points randomly sampled distribution assignment individual points treatment random. model means theoretical sample three properties: First, random assignment treatment sampling distribution means expected means , put differently, expected difference means assigned groups zero. Second, random assignment treatment sampling distribution also means expected variances two groups equal. third, random sampling means values point independent – predict value one point knowing information point. super important : get really low p-value, one consequences may untrue data, example true means two treatment groups really different, mean variances differ two groups, mean data (technically, errors) independent . need certain assumptions make p-value meaningful empirical data. assuming independent error homogenous (equal) variances two samples, low p-value evidence unequal means.","code":""},{"path":"p-values.html","id":"two-interpretations-of-the-p-value","chapter":"6 P-values","heading":"6.6.3 Two interpretations of the p-value","text":"Since want working scientists want use p-values tool, need know interpret (use) p-value make reasonable inferences avoid mis-interpreting p-value making unreasonable even incorrect inferences. Two different interpretations p-value arose development frequentist statistics. Ronald Fisher (developed bulk framework frequentist statistics) thought p-value quantitative measure evidence null hypothesis. Jerzy Neyman Egon Pearson (Neyman-Pearson) thought p-value qualitative, threshold metric used decision making – act effect. Modern researchers biology typically use interpretation odd hybrid two, often leads illogical inference. Regardless, understanding distinction Fisher Neyman-Pearson inform write results manuscript.Fisher working context agricultural experiments, goal discover better agricultural practices – yields five varieties crop differ agricultural practice? Fisher thought p evidence null; smaller p, stronger evidence mean two sampling distributions differ given model assumptions true. Fisher never thought single experiment definitive. decision following experiment partly informed p-value Fisher offered formal rule p-value lies threshold decision.Neyman-Pearson thought p necessary sufficient information make decision accepting null (least rejecting null) rejecting null accepting alternative hypothesis. decision balances two sorts errors: Type (false positives), called \\(\\alpha\\), Type II (false negatives), called \\(\\beta\\). false positive occurs null rejected (\\(p < \\alpha\\)) effect treatment (null true). false negative occurs test fails reject null (\\(p > \\alpha\\)) actually effect (null false). \\(\\alpha\\) set experimenter long-term frequency (“rate”) false positives null true experimenters willing accept.setting \\(\\alpha\\), experimenter designs experiment achieve acceptable rate \\(\\beta\\). Since \\(\\beta\\) false negative rate, \\(1-\\beta\\) rate making false negative error. , stated without double negative, \\(1-\\beta\\) rate rejecting null (“finding effect”) really effect. called power experiment. experiment high power low probability Type II error. experiment low power high probability Type II error. Power partly determined sample size, bigger sample smaller p-value, things equal (think context formula t-value). Power function error variance, natural variance component added measurement error (think context formula t-value). Power also function \\(\\alpha\\). set low \\(\\alpha\\) (say, \\(\\alpha=0.01\\)), test conservative. likely fail reject null even null false. researcher can increase power increasing sample size, using clever strategies reduce measurement error, increasing alpha.experimenter sets \\(\\alpha\\), computes sample size needed achieve certain level power (\\(1-\\beta\\)), experiment. thoughtful researcher set \\(\\alpha\\) considering weighing pros cons different levels \\(\\alpha\\). false positives costly consequences (expense, time, deleterious side-effects), set \\(\\alpha\\) low value, 0.01 0.001. example, initial screen identified previously unknown candidate potentially functions focal system researcher, researcher might decide set low \\(\\alpha\\) (0.001) initial tests candidate avoid devoting time, personnel, expense chasing phantom (false-positive candidate). false positives trivial consequences, set \\(\\alpha\\) high value, 0.05, 0.1, even 0.2. example, initial tests candidate functional system cheap fast construct, researcher might choose set high \\(\\alpha\\) screen identifies candidates. False positive candidates don’t cost lab much effort identify false, missing positive candidates small \\(\\alpha\\) (results low power) screen stage costs researcher discovery potentially exciting component functional system.Fisher’s interpretation, \\(\\alpha\\), \\(\\beta\\), alternative hypothesis, sharp decision rule. Instead, Fisher, p continuous measure evidence null value interpreted subjectively informed knowledgeable expert using additional information make decisions. Neyman-Pearson rejected Fisher’s conception p evidence null arguing single experimental p-value noisy without embedding formal system decision making maintains long-term type error rates \\(\\alpha\\), given certain power. Neyman-Pearson, p compared threshold, \\(\\alpha\\) alone makes decision. Neyman-Pearson, p treated continuous information. \\(p=0.00000001\\) evidence use reject null \\(p=0.049\\).","code":""},{"path":"p-values.html","id":"nhst","chapter":"6 P-values","heading":"6.6.4 NHST","text":"biology researchers today interpret p using combination Fisher Neyman-Pearson concepts become known Null Hypothesis Significance Testing (NHST).Nearly papers biology either explicitly state something like “P values < 0.05 considered statistically significant” implicitly use 0.05 “level significance” (\\(\\alpha\\)). Comparing p-value pre-defined \\(\\alpha\\) Neyman-Pearson.Unlike Neyman-Pearson, little evidence researchers thoughtfully considering level \\(\\alpha\\) experiment. Instead, researchers mindlessly choose \\(\\alpha=0.05\\) everyone else uses.Unlike Neyman-Pearson, somewhat spirit Fisher, researchers, journals, textbooks, advocate polychotomizing statistically significant p “significance bins” – three asterisks \\(p < 0.001\\), two asterisks \\(0.001 < p < 0.01\\), one asterisk \\(0.01 < p < 0.05\\)). Neyman-Pearson. , Neyman-Pearson developed system control long-run frequency Type error, controlled strict use \\(\\alpha\\). observed p-value *** bin * bin meaningless system using Neyman-Pearson. “accept” (\\(p \\ge \\alpha\\)) “reject” (\\(p < \\alpha\\)).Many researchers report exact p-values \\(p < 0.05\\) “n.s.” (significant) \\(p > 0.05\\). Reporting exact p-values Fisher. Reporting n.s. Neyman-Pearson.Many researchers polychomotomize p-value space just 0.05 using language “marginally significant”.","code":""},{"path":"p-values.html","id":"some-major-misconceptions-of-the-p-value","chapter":"6 P-values","heading":"6.7 Some major misconceptions of the p-value","text":"Setting type error rate \\(\\alpha\\) 0.05 pervasive ’m going simply use “0.05” instead “alpha” discussing misconceptions.","code":""},{"path":"p-values.html","id":"misconception-p-0.05-means-there-is-no-effect-of-treatment","chapter":"6 P-values","heading":"6.7.1 Misconception: \\(p > 0.05\\) means there is no effect of treatment","text":"Many researchers believe \\(p > 0.05\\) “effect.” frequentist hypothesis test show effect doesn’t exist, null low probablity producing test statistic extreme extreme observed effect. Even true effect treatment, high p-value can occur ofa low signal:noise ratio, signal true effect size (magnitude true difference response) noise combination intrinsic (biological) extrinsic (experimental) error.small sample size, small relative sample size necessary high power.statement “effect knockout glucose tolerance” valid conclusion frequentist hypothesis test. similar statement “found effect knockout glucose tolerance” misleading frequentist hypothesis test can neither find effect find effect.","code":""},{"path":"p-values.html","id":"misconception-a-p-value-is-repeatable","chapter":"6 P-values","heading":"6.7.2 Misconception: a p-value is repeatable","text":"Many researchers believe p-value precise measure – experiment replicated, similar p result. belief requires least two misconceptions. First, null true, p-value equally likely. \\(p=0.00137\\) just likely \\(p=0.492\\). words, null true, p-value replicable ! Second, p-value highly dependent sample, can highly variable among replications, true p-value, can estimate standard error. Let’s explore .","code":""},{"path":"p-values.html","id":"the-incredible-inconsistency-of-the-p-value","chapter":"6 P-values","heading":"6.7.2.1 The incredible inconsistency of the p-value","text":"replicable conclusion experiment p-value t-test 0.03? conclusion based \\(p < 0.05\\), conclusion replicable. simulation shows results 15 replicates experiment true power 40%. five “significant” results (one less expected) several replicates high p-values.\nFigure 6.4: Variability p-values power 0.4\n","code":""},{"path":"p-values.html","id":"what-is-the-distribution-of-p-values-under-the-null","chapter":"6 P-values","heading":"6.7.2.2 What is the distribution of p-values under the null?","text":"often ask students, “true effect (difference means), repeat experiment thousands times, likely p-value?”. common answer (although answers uncommon) \\(p = 0.5\\). Sometimes rephrase question, true effect (difference means), repeat experiment thousands times, think distribution p-values look like?” typical answer distribtion look like normal curve peak 0.5, (presumably tails abruptly stop 0 1).","code":""},{"path":"p-values.html","id":"misconception-0.05-is-the-lifetime-rate-of-false-discoveries","chapter":"6 P-values","heading":"6.7.3 Misconception: 0.05 is the lifetime rate of false discoveries","text":"important widespread misconception researcher consistently uses \\(\\alpha=0.05\\), frequency incorrectly concluding effect exists, “discovering” effect, lifetime researcher, 5%. incorrect. \\(\\alpha\\) rate false positives subset tests null hypothesis true. \\(\\alpha\\) Type error rate.mental conception “lifetime rate false discoveries” False Discovery Rate, frequency false positives divided frequency positives (sum false true positives).pump intution differences Type error rate False Discovery Rate, imagine test1000 null hypotheses lifetime60% true nulls, means 600 true nulls 400 true effectsalpha 5%. means expect find \\(p \\le 0.05\\) 30 times (\\(0.05 \\times 600\\)) null truepower 25%. means expect find \\(p \\le 0.05\\) 100 times (\\(0.25 \\times 400\\)) null falseWe made \\(30 + 100=130\\) “discoveries” (experiments \\(p \\le 0.05\\)), but30 130, 23%, “false discoveries”. false discovery rate.Think . null never true, false discovery – every \\(p \\le 0.05\\) true discovery (false discovery rate 0%). null always true, every \\(p < 0.05\\) false discovery (false discovery rate 100%).","code":""},{"path":"p-values.html","id":"misconception-a-low-p-value-indicates-an-important-effect","chapter":"6 P-values","heading":"6.7.4 Misconception: a low p-value indicates an important effect","text":"Many researchers write results believe small p-value means effect big important. may misconception may arise ubiquitous use “significant” indicate small p-value “” “extremely” “wicked” significant indicate really small p-value. Regardless, misconception. small p-value usually result high power (can occur even power low) power function effect size, variability (standard deviation), sample size. small p result large effect size can also result small effect size sample size big enough.easy simulate (see script ). Let’s model effect genotype gene height","code":"\nset.seed(1)\nrho <- 0.5\nn <- 10^4\ngenotype <- c(\"+/+\", \"+/-\", \"-/-\")\nSigma <- diag(2)\nSigma[1,2] <- Sigma[2,1] <- rho\nX <- rmvnorm(n, mean=c(0,0), sigma=Sigma)\ncolnames(X) <- c(\"X1\", \"X2\")\nbeta <- c(0.05, 0.05)\ny <- X%*%beta + rnorm(n)\nfit <- lm(y ~ X)\ncoefficients(summary(fit))##                Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept) 0.007472959 0.01007946 0.7414046 4.584656e-01\n## XX1         0.044304824 0.01154709 3.8368830 1.253725e-04\n## XX2         0.048228101 0.01170855 4.1190490 3.835033e-05"},{"path":"p-values.html","id":"misconception-a-low-p-value-indicates-high-model-fit-or-high-predictive-capacity","chapter":"6 P-values","heading":"6.7.5 Misconception: a low p-value indicates high model fit or high predictive capacity","text":"page 606, Lock et al “Statistics: Unlocking Power Data”, authors state item D “p-value ANOVA table 0.000 model whole effective predicting grade point averages.” incorrect. p-value measure predictive capability model p-value function signal, noise (unmodeled error), sample size predictive ability function just signal:noise ratio. signal:noise ratio tiny, predictive ability small p-value can tiny sample size large. easy simulate (see script ). whole-model p-value exceptionally small (0.00001002) relative predictive ability, measured \\(R^2\\), near zero (0.002).","code":"\nset.seed(1)\nrho <- 0.5\nn <- 10^4\nSigma <- diag(2)\nSigma[1,2] <- Sigma[2,1] <- rho\nX <- rmvnorm(n, mean=c(0,0), sigma=Sigma)\ncolnames(X) <- c(\"X1\", \"X2\")\nbeta <- c(0.05, -0.05)\ny <- X%*%beta + rnorm(n)\nfit <- lm(y ~ X)\nsummary(fit)## \n## Call:\n## lm(formula = y ~ X)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6449 -0.6857  0.0148  0.6756  3.6510 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.007473   0.010079   0.741 0.458466    \n## XX1          0.044305   0.011547   3.837 0.000125 ***\n## XX2         -0.051772   0.011709  -4.422  9.9e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.008 on 9997 degrees of freedom\n## Multiple R-squared:  0.0023, Adjusted R-squared:  0.002101 \n## F-statistic: 11.52 on 2 and 9997 DF,  p-value: 1.002e-05"},{"path":"p-values.html","id":"what-the-p-value-does-not-mean","chapter":"6 P-values","heading":"6.8 What the p-value does not mean","text":"p probability null true. formally, probability \\(Prob(null | data)\\) p-value \\(P(data | null)\\). . \\(P(null | data)\\) probability null true given data. \\(P(data | null)\\) probability data, something extreme data, conditional true null.\\(1-p\\) probability alternativep measure effect size.p one experiment level evidence null another experimentp great indicator likely, H0 H1.one treatment level \\(p < 0.05\\) another treatment level \\(p > 0.05\\), evidence treatment levels different effects outcome.","code":""},{"path":"p-values.html","id":"on-using-p-values-in-experimental-biology","chapter":"6 P-values","heading":"6.9 On using p-values in experimental biology","text":"flow experiments article GDF15 mediates effects metformin body weight energy balance typical many experimental biology studies.(Figure 1) cited observational studying showing association metformin increased blood levels peptide hormone GDF15 humans led experiments confirm association humans mice. positive results inferred \\(p < 0.05\\) led follow-experiments Figures 2, 3, 4.(Figure 2) Experiments GDF15 knockout GFRAL (GDF15 receptor) knockout probe GDF15 necessary metformin-associated weight loss, food intake reduction, energy expenditure increase. positive results inferred \\(p < 0.05\\) led conclusion GDF15 signaling mediator metformin-induced weight loss via GDF15 signaling food intake energy expenditure.(Figure 3) Experiments GDF15 knockout GFRAL (GDF15 receptor) knockout probe GDF15 necessary metformin-associated regulation glucose homeostasis. negative results inferred \\(p > 0.05\\) led conclusion GDF15 signaling mediator metformin-induced decreases fasting glucose insulin.(Figure 4) Experiments probe tissues respond metformin upregulating GDF15 expression. positive results inferred \\(p < 0.05\\) led follup , confirmatory experiments using alternative methods tissue sources. positive results inferred \\(p < 0.05\\) original follow experiments led conclusion metformin upregulates GDF15 expression small intestine, colon, rectum, kidney.researchers using p-values tool just draw conclusions effects lack effects , importantly identify follow-experiments. positive results (\\(p < 0.05\\)) publication motivate research group others execute follow-experiments future studies. , effectively experimental biology papers, p-values used Neyman-Pearson framework despite ubiquitous statement “consider \\(p < 0.05\\) significant” methods. Instead, smaller p-values seem give researchers greater confidence conclusion. , decision strategy seems , p-value least close 0.05 expect treatment effect given available information, p value close less 0.05 good enough act effect.abandon p-values emphasize effect sizes uncertainty. Problem biological consequences effect size unknown assays use units effect sizes meaningless.abandon p-values favor model selection model averagingabandon frequentist statistics favor Bayesian methods. going happen. Decisions still made.abandon decisions based p-values decisions based Bayes factorsabandon “signficance”. Worry researchers claim effects p = 0.1 whatev. cares?cost probably optimistic p-values due pseudoreplication controlling FDR sufficient replication, continue better statistics.","code":""},{"path":"p-values.html","id":"better-reproducibility","chapter":"6 P-values","heading":"6.10 Better reproducibility","text":"true randomization selection, blind scoringknowledge statistics helps experimental designuse best practice statistics especially accounting pseudoreplication blockingcombine experiments properlyexperiments used make decisions move forward 1) replicate experiments 2) use conservative p-values (ideally results shouldn’t need formal test)xxx finish section","code":""},{"path":"p-values.html","id":"multiple-testing-and-controlling-for-false-positives","chapter":"6 P-values","heading":"6.11 Multiple testing and controlling for false positives","text":"Bench biologists compute bunch p-values every paper. example, 2 x 2 factorial experiment (e.g. WT-Chow, WT-HFD, KO-Chow, KO-HFD), six differences means (pairwise comparisons) plus interaction effect. Researchers typically compute p-values multiple measured outcomes per experiment. 5 outcomes 2 x 2 factorial experiment, researchers might report 35 p-values. number potential p-values, can rise quickly one variables measured multiple time points, weekly body weight measurements 12 weeks, multiple glucose measurements 120 minute duration glucose tolerance test. just Figure 1!p-values used claim effect (discovery) used pursue line follow-experiments, multiple false positives. , ’ll define false positive either False positives lead non-reproducible research. Researchers want limit false positives costly time money pursue lines research based mistaken model something works. Exciting, positive results one study spawn follow-experiments just original research team also research groups. , researchers want researchers limit false positives. course, funding agencies want researchers limit false positives.","code":""},{"path":"p-values.html","id":"controlling-the-family-wise-error-rate-when-all-tests-are-testing-the-same-hypothesis","chapter":"6 P-values","heading":"6.11.1 Controlling the family-wise error rate when all tests are testing the same hypothesis","text":"","code":""},{"path":"p-values.html","id":"recommendations","chapter":"6 P-values","heading":"6.12 Recommendations","text":"Simply report exact p-value, along CI estimate.P-values noisy, little reason report two significant digits (report “\\(p = 0.011\\)” “\\(p = 0.0108\\)”) although journals recommend two significant digits.high p-values, report “\\(p = 0.23\\)” “\\(p = n.s.\\)”.small p-values, little reason report one significant digit (report “\\(p = 0.0002\\)” “\\(p = 0.00018\\)”).really small p-values, little reason report exact p-value (report “\\(p < 0.0001\\)” “\\(p = 2.365E-11\\)”). Recognize “really small” entirely arbitrary. Rafael Irizarry suggested p-values less something like probability killed lightning strike reported “\\(p < m\\)”, \\(m\\) probability killed lightning strike4. According Google University, 0.00000142 one year 0.00033 one lifetime. text use \\(p < ls\\)” p-values less 0.0001 – lifetime probability killed lightning strike someone spends much time indoors analyzing data.\\(p < 0.05\\) (\\(\\alpha\\)) report “significant” – fact, avoid word “significant”. english language, “significant” implies big important. Small p-values can result even trivially small effects \\(n\\) big sample variation small. phrase “ASK1 knockout significant effect reducing liver TG (\\(p = 0.011\\))” ispotentially misleading, interpret “significant” mean “large effect regulation liver TG”,wrong, interpret “significant” mean “ASK1 knockout effect”.\nlow p-value evidence effect ASK1 knockout zero, wager knocking gene expressed white adipose cells effect (however small) liver TG.decision needs made (“devote time, expense, personel pursue ?”), p-value useful tool. p smaller say 0.001, pretty good evidence data fluke sampling, long justifiably confident assumptions went computing p-value. replicate experiment small p-value better evidence. p closer 0.01 0.05, weak evidence fluke sampling variability p. replicate experiment small p-value much better evidence.","code":""},{"path":"p-values.html","id":"primary-sources-for-recommendations","chapter":"6 P-values","heading":"6.12.1 Primary sources for recommendations","text":"Statistical tests, P values, confidence intervals, power: guide misinterpretations.“Q: many colleges grad schools teach p = 0.05? : ’s still scientific community journal editors use. Q: many people still use p = 0.05? : ’s taught college grad school.” – ASA Statement Statistical Significance P-Values“discuss proposal, abandon statistical significance. recommend dropping NHST paradigm—p-value thresholds intrinsic —default statistical paradigm research, publication, discovery biomedical social sciences.” – Abandon Statistical Significance“conclude, based review articles special issue broader literature, time stop using term “statistically significant” entirely. variants “significantly different,” “\\(p<0.05\\),” “nonsignificant” survive, whether expressed words, asterisks table, way.” – Moving World Beyond “p < 0.05”“agree, call entire concept statistical significance abandoned.”–Scientists rise statistical significance","code":""},{"path":"p-values.html","id":"problems","chapter":"6 P-values","heading":"6.13 Problems","text":"Problem 1 – simulate distribution p null. many ways straightforard approach toCreate \\(2n \\times m\\) matrix random normal deviates mean 0 sd 1Do t-test column, first \\(n\\) values assigned one group remaining \\(n\\) values assigned second group. Save p-value .Plot histogram p-values.distribution? likely value p?Problem 2 – simulate power. , many ways following Problem 1.\n1. Create \\(2n \\times m\\) matrix random normal deviates mean 0 sd 1\n2. Add effect first \\(n\\) values column. Things think \n. good effect size add? effect/sd ratio, known Cohen’s d, relative (standardized) measure effect size. Cohen suggest 0.2, 0.5, 0.8 small, medium, large standardized effects.\nb. effect added individual? Yes! random component captures individual variation response.\n3. t-test column matrix, using first \\(n\\) values group 1 remaining \\(n\\) values group 2. Save p-values .\n4. Compute power, relative frequency \\(p \\le 0.05\\).\n5. Repeat different values \\(n\\), effect size, sd, vary one time. power vary three parameters?","code":""},{"path":"errors-in-inference.html","id":"errors-in-inference","chapter":"7 Errors in inference","heading":"7 Errors in inference","text":"","code":""},{"path":"errors-in-inference.html","id":"classical-nhst-concepts-of-wrong","chapter":"7 Errors in inference","heading":"7.1 Classical NHST concepts of wrong","text":"described chapter (p-values), two types error occur classical Neyman-Pearson hypothesis testing, NHST version dominates modern practice. Type error occurs null hypothesis true p-value test less \\(\\alpha\\). false positive, positive test rejects null. Type II error occurs null hypothesis false p-value test greater \\(\\alpha\\). false negative, negative test accepts (fails reject) null. Power error frequency true, positive tests (frequency avoiding Type II error). \\(\\alpha\\) error rate Type error researcher willing accept. Ideally, researcher sets \\(\\alpha\\) based evaluation pros cons Type Type II error specific experiment. practice, researchers follow completely arbitary practice setting \\(\\alpha = 0.05\\).researcher care \\(\\alpha\\) power? Typically, researchers don’t give \\(\\alpha\\) much thought. power considered context calculating sample size experiment grant proposal. researchers care rates Type error power (similar concepts) can help guide decisions model fit specific dataset.","code":""},{"path":"errors-in-inference.html","id":"type-i-error","chapter":"7 Errors in inference","heading":"7.1.1 Type I error","text":"classical Neyman-Pearson hypothesis testing, important property hypothesis test size test, may include entire procedure culminates hypothesis test. “Size” weird name probability rejecting null null true. Size \\(\\alpha\\). \\(\\alpha\\) nominal value – size actual value specific parameterization model.probably come surprise researchers learn size common tests used data look like researcher’s data 0.05. “used data look like researcher’s data” important – t-test doesn’t one size. data conform assumptions (independence, homogeneity, normality), size t-test \\(\\alpha\\). violation, especially sample size differs groups, size t-test can move away \\(\\alpha\\). test size less \\(\\alpha\\) “conservative” (fewer nulls rejected think, status quo often maintained). test size greater \\(\\alpha\\) “anti-conservative”, “liberal” (nulls rejected think, status quo less often maintained). conservative tests reduce power. liberal tests artificially increase power increase rate false rejection, can mean “false discovery” p-values used arbiter discovery.","code":""},{"path":"errors-in-inference.html","id":"size-example-1-the-size-of-a-t-test-vs.-a-permutation-test-when-the-data-meet-the-assumptions","chapter":"7 Errors in inference","heading":"7.1.1.1 Size example 1: the size of a t-test vs. a permutation test, when the data meet the assumptions","text":"","code":"\nset.seed(1)\nn <- 10\nn_iter <- 10000\np_t <- numeric(n_iter)\np_perm <- numeric(n_iter)\n\ntreatment <- rep(c(\"cn\", \"tr\"), each = n)\nfor(iter in 1:n_iter){\n  sample_1 <- rnorm(n, mean = 10, sd = 1)\n  sample_2 <- rnorm(n, mean = 10, sd = 1)\n  y <- c(sample_1, sample_2)\n  m1 <- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] <- coef(summary(m1))[\"treatmenttr\", \"Pr(>|t|)\"]\n  \n  m2 <- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] <- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\nsize_t <- sum(p_t < 0.05)/n_iter\nsize_perm <- sum(p_perm < 0.05)/n_iter\nsize_table <- data.table(Method = c(\"lm\", \"perm\"),\n                         Size = c(size_t, size_perm))\nknitr::kable(size_table, digits = 4)"},{"path":"errors-in-inference.html","id":"size-example-2-the-size-of-a-t-test-vs.-a-permutation-test-when-the-data-have-a-right-skewed-distribution","chapter":"7 Errors in inference","heading":"7.1.1.2 Size example 2: the size of a t-test vs. a permutation test, when the data have a right skewed distribution","text":"","code":"\nset.seed(1)\nn <- 10\nn_iter <- 10000\np_t <- numeric(n_iter)\np_perm <- numeric(n_iter)\n\ntreatment <- rep(c(\"cn\", \"tr\"), each = n)\nfor(iter in 1:n_iter){\n  #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n  sample_1 <- rnegbin(n, mu = 100, theta = 1)\n  sample_2 <- rnegbin(n, mu = 100, theta = 1)\n  y <- c(sample_1, sample_2)\n  # qplot(x=treatment, y = y)\n  m1 <- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] <- coef(summary(m1))[\"treatmenttr\", \"Pr(>|t|)\"]\n  \n  m2 <- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] <- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\nsize_t <- sum(p_t < 0.05)/n_iter\nsize_perm <- sum(p_perm < 0.05)/n_iter\nsize_table <- data.table(Method = c(\"lm\", \"perm\"),\n                         Size = c(size_t, size_perm))\nknitr::kable(size_table, digits = 4)"},{"path":"errors-in-inference.html","id":"size-example-3-the-size-of-a-t-test-vs.-a-permutation-test-when-the-data-have-heterogenous-variance-and-the-sample-size-is-unequal","chapter":"7 Errors in inference","heading":"7.1.1.3 Size example 3: the size of a t-test vs. a permutation test, when the data have heterogenous variance and the sample size is unequal","text":"","code":"\nset.seed(1)\nn1 <- 10\nn2 <- n1/2\nn_iter <- 10000\np_t <- numeric(n_iter)\np_perm <- numeric(n_iter)\n\ntreatment <- rep(c(\"cn\", \"tr\"), times = c(n1, n2))\nfor(iter in 1:n_iter){\n  #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n  sample_1 <- rnorm(n1, mean = 10, sd = 0.5)\n  sample_2 <- rnorm(n2, mean = 10, sd = 1)\n  y <- c(sample_1, sample_2)\n  # qplot(x=treatment, y = y)\n  m1 <- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] <- coef(summary(m1))[\"treatmenttr\", \"Pr(>|t|)\"]\n  \n  m2 <- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] <- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\nsize_t <- sum(p_t < 0.05)/n_iter\nsize_perm <- sum(p_perm < 0.05)/n_iter\nsize_table <- data.table(Method = c(\"lm\", \"perm\"),\n                         Size = c(size_t, size_perm))\nknitr::kable(size_table, digits = 4)"},{"path":"errors-in-inference.html","id":"power","chapter":"7 Errors in inference","heading":"7.1.2 Power","text":"classical Neyman-Pearson hypothesis testing, important property hypothesis test power test. “Power” probability rejecting null null false. common way think power , power test’s ability “detect” effect exists. makes sense using Neyman-Pearson Fisher (Using Fisher, p-value detector effect – reasoning brain ). Using Fisher, say power sensitivity test (takes less sample provide signal).","code":""},{"path":"errors-in-inference.html","id":"power-example-1-the-power-of-a-t-test-vs.-a-permutation-test-when-the-data-meet-the-assumptions","chapter":"7 Errors in inference","heading":"7.1.2.1 Power example 1: the power of a t-test vs. a permutation test, when the data meet the assumptions","text":"","code":"\nset.seed(1)\nn <- 10\nn_iter <- 10000\np_t <- numeric(n_iter)\np_perm <- numeric(n_iter)\n\ntreatment <- rep(c(\"cn\", \"tr\"), each = n)\nfor(iter in 1:n_iter){\n  sample_1 <- rnorm(n, mean = 10, sd = 1)\n  sample_2 <- rnorm(n, mean = 11, sd = 1)\n  y <- c(sample_1, sample_2)\n  m1 <- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] <- coef(summary(m1))[\"treatmenttr\", \"Pr(>|t|)\"]\n  \n  m2 <- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] <- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\npower_t <- sum(p_t < 0.05)/n_iter\npower_perm <- sum(p_perm < 0.05)/n_iter\npower_table_normal <- data.table(Method = c(\"lm\", \"perm\"),\n                         Power = c(power_t, power_perm))\nknitr::kable(power_table_normal, digits = 3)"},{"path":"errors-in-inference.html","id":"power-example-2-the-power-of-a-t-test-vs.-a-permutation-test-when-the-data-look-like-typical-count-data","chapter":"7 Errors in inference","heading":"7.1.2.2 Power example 2: the power of a t-test vs. a permutation test, when the data look like typical count data","text":"","code":"\nset.seed(1)\nn <- 10\nn_iter <- 10000\np_t <- numeric(n_iter)\np_perm <- numeric(n_iter)\n\ntreatment <- rep(c(\"cn\", \"tr\"), each = n)\n\nfor(iter in 1:n_iter){\n  #  qplot(rnegbin(n = 10^4, mu = 100, theta = 1))\n  sample_1 <- rnegbin(n, mu = 100, theta = 1)\n  sample_2 <- rnegbin(n, mu = 300, theta = 1)\n  y <- c(sample_1, sample_2)\n  # qplot(x=treatment, y = y)\n  m1 <- lm(y ~ treatment) # no data statement necessary because both variables in workspace\n  p_t[iter] <- coef(summary(m1))[\"treatmenttr\", \"Pr(>|t|)\"]\n  \n  m2 <- lmp(y ~ treatment,\n            perm = \"Prob\",\n            settings = FALSE)\n  p_perm[iter] <- coef(summary(m2))[\"treatment1\", \"Pr(Prob)\"]\n}\npower_t <- sum(p_t < 0.05)/n_iter\npower_perm <- sum(p_perm < 0.05)/n_iter\npower_table_count <- data.table(Method = c(\"lm\", \"perm\"),\n                         Power = c(power_t, power_perm))\nknitr::kable(power_table_count, digits = 3)"},{"path":"errors-in-inference.html","id":"a-non-neyman-pearson-concept-of-power","chapter":"7 Errors in inference","heading":"7.2 A non-Neyman-Pearson concept of power","text":"Size power concepts specific Neyman-Pearson hypothesis testing framework. Size power also limited () use research program null hypothesis never (rarely) strictly true. said, concept size power useful. example, framed power distribution p-values instead frequency p-values less \\(\\alpha\\).Table ?? shows p-value 10th, 25th, 50th, 75th, 90th percentile set p-values computed Power Example 2 (count data). nth percentile value ordered set numbers n % less value 100 - n% greater value. 50th percentile median. table shows percentiles except 90th, permutation p-value smaller t-test p-value. , importantly, value 75% ~ 0.12. means experiments generate data something like fake data generated Power Example 2, permutation test sensistive incompatibility null model data t-test, except random samples methods fail.","code":"\nquantile_list <- c(0.1, 0.25, 0.5, 0.75, 0.9)\npercentiles_t <- quantile(p_t, quantile_list)\npercentiles_perm <- quantile(p_perm, quantile_list)\n\nalt_power_table <- data.table(method = c(\"t-test\", \"permutation\"),\n                              (rbind(percentiles_t,\n                                     percentiles_perm)))\nknitr::kable(alt_power_table, digits = c(1, 4, 3, 3, 2, 2))"},{"path":"errors-in-inference.html","id":"estimation-error","chapter":"7 Errors in inference","heading":"7.2.1 Estimation error","text":"","code":""},{"path":"errors-in-inference.html","id":"coverage","chapter":"7 Errors in inference","heading":"7.2.2 Coverage","text":"text advocates reporting confidence interval reported effect size. important property estimator coverage probability, often shortened “coverage”.","code":""},{"path":"errors-in-inference.html","id":"type-s-error","chapter":"7 Errors in inference","heading":"7.2.3 Type S error","text":"Instead framing “size” concept rate Type error, framed rate estimate correct direction (meaning, sign effect true value). ,","code":""},{"path":"errors-in-inference.html","id":"type-m-error","chapter":"7 Errors in inference","heading":"7.2.4 Type M error","text":"","code":""},{"path":"part-iv-introduction-to-linear-models.html","id":"part-iv-introduction-to-linear-models","chapter":"Part IV: Introduction to Linear Models","heading":"Part IV: Introduction to Linear Models","text":"","code":""},{"path":"intro-linear-models.html","id":"intro-linear-models","chapter":"8 An introduction to linear models","heading":"8 An introduction to linear models","text":"students familiar idea linear model learning equation line, \\[\\begin{equation}\nY = mX + b\n\\tag{8.1}\n\\end{equation}\\]\\(m\\) slope line \\(b\\) \\(Y\\)-intercept. useful think equation (8.1) function maps values \\(X\\) values \\(Y\\). Using function, input value \\(X\\), always get value Y output.linear model function, like equation (8.1), fit set data, often model process generated data something like data. line Figure 8.1A just , line, line Figure 8.1B linear model fit data Figure 8.1B.\nFigure 8.1: line vs. linear model. () line \\(y=-3.48X + 105.7\\) drawn. (B) linear model fit data. model coefficients numerically equal slope intercept line .\n","code":""},{"path":"intro-linear-models.html","id":"lm-specifications","chapter":"8 An introduction to linear models","heading":"8.1 Two specifications of a linear model","text":"","code":""},{"path":"intro-linear-models.html","id":"the-error-draw-specification","chapter":"8 An introduction to linear models","heading":"8.1.1 The “error draw” specification","text":"introductory textbooks, linear model typically specified using error-draw scheme.\\[\\begin{align}\nY &= \\beta_0 + \\beta_1 X + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\tag{8.2}\n\\end{align}\\]first line specification two components: linear predictor \\(Y = \\beta_0 + \\beta_1 X\\) error \\(\\varepsilon\\). linear predictor component looks like equation line except 1) \\(\\beta_0\\) used intercept \\(\\beta_1\\) slope 2) intercept term precedes slope term. re-labeling re-arrangement make notation linear model flexible complicated linear models. example \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon\\) model \\(Y\\) function two \\(X\\) variables.linear predictor deterministic systematic part specification. equation line, linear predictor component linear model function maps specific value \\(X\\) unique value \\(Y\\). mapped value expected value, expectation, given specific input value \\(X\\). expectation often written \\(\\mathrm{E}[Y|X]\\), read “expected value \\(Y\\) given \\(X\\)”, “given X” means specific value X. text often use word conditional place “given”. example, read \\(\\mathrm{E}[Y|X]\\) “expected value \\(Y\\) conditional \\(X\\)”. important recognize \\(\\mathrm{E}[Y|X]\\) conditional mean – mean value \\(Y\\) observe \\(X\\) specific value \\(x\\) (\\(X = x\\)).second line specification (8.2) read “epsilon distributed Normal mean zero variance sigma squared”. line explicitly specifies distribution error component line 1. error component linear model random “draw” normal distribution mean zero variance \\(\\sigma^2\\). second line shows error component first line stochastic. Using error-model specification, can think measurement \\(Y\\) expected value plus random value sampled normal distribution specified variance. stochastic part specification draws “error” population, refer error-draw specification linear model.","code":""},{"path":"intro-linear-models.html","id":"the-conditional-draw-specification","chapter":"8 An introduction to linear models","heading":"8.1.2 The “conditional draw” specification","text":"second way specifying linear model using conditional-draw scheme.\\[\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma^2)\\\\\n\\mathrm{E}(Y|X) &= \\mu\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\n\\tag{8.3}\n\\end{align}\\]first line states response variable \\(Y\\) random variable independently drawn normal distribution mean \\(\\mu\\) variance \\(\\sigma^2\\). first line stochastic part statistical model. second line simply states \\(\\mu\\) (greek letter “mu”) first line conditional mean (expectation). third line liner predictor, states \\(\\mu_i\\) generated given \\(X=x_i\\). , linear predictor systematic (deterministic) part statistical model. systematic value \\(x_i\\) always generate \\(\\mu_i\\). conditional-draw specification, can think measurement (\\(y_i\\)) random draw specified distribution. \\(Y\\) “error” drawn specified distribution, refer conditional-draw specification linear model.","code":""},{"path":"intro-linear-models.html","id":"comparing-the-error-draw-and-conditional-draw-ways-of-specifying-the-linear-model","chapter":"8 An introduction to linear models","heading":"8.1.3 Comparing the error-draw and conditional-draw ways of specifying the linear model","text":"two ways specifying model encourage slightly different ways thinking data (response varible \\(Y\\)) generated. error-draw specification “generates” data 1) constructing \\(y_i\\) “” given \\(x_i\\) (conditional expection), 2) adding error \\(e_i\\) drawn normal distribution mean zero specified variance. conditional-draw specification “generates” data 1) constructing \\(y_i\\) “” given \\(x_i\\), 2) drawing random variable specified distribution whose mean expectation. random draw “error” measured value \\(y_i\\). error draw generation, need one hat random numbers, conditional draw generation, need hat value \\(x_i\\).short script generates data implementing error-draw conditional-draw specifications. See can follow logic code match meaning two ways specifying linear model.’s code? rnorm() pseudorandom number generator simulates random draws normal distribution specified mean variance. algorithm generate numbers entirely deterministic – numbers truly random “pseudorandom”. list numbers returned closely approximates set true, random numbers. sequence numbers returned determined “seed”, can set set.seed() function (R use internal seed set user).error-draw specification useful thinking data generation data analyzed generalized linear models, models allow one specify distribution families Normal (binomial, Poisson, Gamma families). fact, thinking model predictor plus error can lead misconception , generalized linear model, error (residuals fit) distribution non-Normal distribution modeled. true distributions modeled using generalized linear models (Normal) negative values (residuals must negative values since mean residuals zero). Introductory biostatistics textbooks typically introduce error-draw specification introductory textbooks recommend data transformation non-parametric tests data approximately normal. unfortunate generalized linear models extremely useful real biological data.Although linear model (statistical model generally) model data-generating process, linear models typically used actually generate data. Instead, use linear model understand something real dataset, think data one realization process generates data like . linear model model process. said, incredibly useful use linear models create fake datasets least two reasons: probe understanding statistical modeling generally , specifically, check model actually creates data like real dataset analyzing.","code":"\nn <- 5\nb_0 <- 10.0\nb_1 <- 1.2\nsigma <- 0.4\nx <- 1:n\ny_expected <- b_0 + b_1*x\n\n# error-draw. Note that the n draws are all from the same distribution\nset.seed(1)\ny_error_draw <- y_expected + rnorm(n, mean = 0, sd = sigma)\n\n# conditional-draw. Note that the n draws are each from a different\n# distribution because each has a different mean.\nset.seed(1)\ny_conditional_draw <- rnorm(n, mean = y_expected, sd = sigma)\n\ndata.table(X = x,\n           \"Y (error draw)\" = y_error_draw,\n           \"Y (conditional draw)\" = y_conditional_draw)##    X Y (error draw) Y (conditional draw)\n## 1: 1       10.94942             10.94942\n## 2: 2       12.47346             12.47346\n## 3: 3       13.26575             13.26575\n## 4: 4       15.43811             15.43811\n## 5: 5       16.13180             16.13180"},{"path":"intro-linear-models.html","id":"anova-notation-of-a-linear-model","chapter":"8 An introduction to linear models","heading":"8.1.4 ANOVA notation of a linear model","text":"Many textbooks treat ANOVA differently regression express linear model ANOVA model (generally use phrase “linear model”). ANOVA models variations \\[\\begin{equation}\ny_{ij} = \\mu + \\tau_{} + \\varepsilon_{ij}\n\\tag{8.4}\n\\end{equation}\\]Unlike error conditional draw specifications , ANOVA model doesn’t linear predictor form regression equation (equation line) – , neither \\(X\\) variables coefficients (\\(\\beta\\)). Instead, ANOVA model made linear combination means deviations means. (8.4), \\(\\mu\\) grand mean (mean means groups), \\(\\tau_i\\) deviation mean group \\(\\) grand mean (effects), \\(\\varepsilon_{ij}\\) deviation (error) individual \\(j\\) mean group \\(\\). Traditional ANOVA computes effects statistics inference computing means deviations means. Modern linear models compute effects statistics inference solving coefficients regression model.","code":""},{"path":"intro-linear-models.html","id":"a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables","chapter":"8 An introduction to linear models","heading":"8.2 A linear model can be fit to data with continuous, discrete, or categorical \\(X\\) variables","text":"linear model fit data Figure 8.1B, \\(X\\) variable continuous, can take real number minimum \\(X\\) maximum \\(X\\) data. biological data, variables continuous positive, real numbers (zero physically possible recorded data true value less minimum measurable amount). One exception composition (fraction total), can zero. Negative values can occur variables negative represent direction (work, electrical potential) rate. Discrete variables numeric limited certain real numbers. biological variables discrete counts, can zero, negative. Categorical variables non-numeric descriptions measure. Many categorical variables text experimentally controlled treatment variable interest (variable \\(treatment\\) containing values “wild type” “knockout”) measured covariates (variable \\(sex\\) containing values “female” “male”).","code":""},{"path":"intro-linear-models.html","id":"fitting-linear-models-to-experimental-data-in-which-the-x-variable-is-continuous-or-discrete","chapter":"8 An introduction to linear models","heading":"8.2.1 Fitting linear models to experimental data in which the \\(X\\) variable is continuous or discrete","text":"linear model fit data numeric (continous discrete) \\(X\\) classical regression result typically communicated regression line. experiment introduced Chapter ?? [Linear models single, continuous X] good example. experiment, researchers designed experiment measure effect warming timing photosynthetic activity. Temperature experimentally controlled one five settings (0, 2.25, 4.5, 6.75, 9 °C ambient temperature) within twelve, large enclosures. response variable illustrated example Autumn “green-”, day year (DOY) transition loss photosynthesis. intercept slope parameters regression line (Figure 8.2) coefficients linear model. slope (4.98 days per 1 °C added warmth) estimates effect warming green-DOY. often appreciated introductory biostatistics level slope difference conditional means. point regression line expected value \\(Y\\) specified value \\(X\\), , conditional mean \\(\\mathrm{E}(Y|X)\\). slope difference expected values pair points differ \\(X\\) one unit.\\[\\begin{equation}\nb_1 = \\mathrm{E}(Y|X=x+1) - \\mathrm{E}(Y|X=x+1)\n\\end{equation}\\]show Figure 8.2 using points regression line \\(x = 5\\) \\(x = 6\\). Thinking regression coefficient difference conditional means especially useful understanding coefficients categorical \\(X\\) variable, described .\nFigure 8.2: Illustration slope linear model numeric X. slope (coefficient X) difference expected value two X one unit apart. illustrated points line x = 5 x = 6.\n","code":""},{"path":"intro-linear-models.html","id":"slope","chapter":"8 An introduction to linear models","heading":"8.2.2 Fitting linear models to experimental data in which the \\(X\\) variable is categorical","text":"Linear models can fit experimental data \\(X\\) variable categorical – focus text! model fit data Figure 8.1B, coefficient \\(X\\) slope line. Perhaps surprisingly, 1) can fit model like equation (8.2) data \\(X\\) variable categorical 2) coefficient \\(X\\) slope. possible? slope line \\(\\frac{y_2 - y_1}{x_2 - x_1}\\) \\((x_1, y_1)\\) \\((x_2, y_2)\\) graph coordinates two points line. denominator slope function \\((x_2 - x_1)\\) \\(X\\) categorical?solution using linear model categorical \\(X\\) recode factor levels numbers. example outlined Chapter ?? (Analyzing experimental data linear model). value \\(X\\) individual mouse number indicates treatment assignment – value 0 given mice functional ASK1 gene value 1 given mice knocked gene. regression line goes two group means (Figure 8.3). (0, 1) coding, \\(\\overline{x}_{ASK1Δadipo} - \\overline{x}_{ASK1F/F} = 1\\), denominator slope equal one slope simply equal numerator \\(\\overline{y}_{ASK1Δadipo} - \\overline{y}_{ASK1F/F}\\). coefficient (slope!) difference conditional means.\nFigure 8.3: Illustration slope linear model categorical X. slope (coefficient X) difference conditional means.\n","code":""},{"path":"intro-linear-models.html","id":"statistical-models-are-used-for-prediction-explanation-and-description","chapter":"8 An introduction to linear models","heading":"8.3 Statistical models are used for prediction, explanation, and description","text":"Researchers typically use statistical models understand relationships one \\(Y\\) variables one \\(X\\) variables. relationships includeDescriptive modeling. Sometimes researcher merely wants describe relationship \\(Y\\) set \\(X\\) variables, perhaps discover patterns. example, arrival spring migrant bird (\\(Y\\)) function sex (\\(X_1\\)) age (\\(X_2\\)) might show males younger individuals arrive earlier. Importantly, another \\(X\\) variable added model (one dropped), coefficients, therefore, precise description, change. , interpretation coefficient descriptor conditional covariates (\\(X\\) variables) model. descriptive model, implication causal effects goal prediction. Nevertheless, hard humans discuss descriptive model without using causal language, probably means hard us think models mere description. Like natural history, descriptive models useful patterns want explanation, using explicit causal models including experiments.Descriptive modeling. Sometimes researcher merely wants describe relationship \\(Y\\) set \\(X\\) variables, perhaps discover patterns. example, arrival spring migrant bird (\\(Y\\)) function sex (\\(X_1\\)) age (\\(X_2\\)) might show males younger individuals arrive earlier. Importantly, another \\(X\\) variable added model (one dropped), coefficients, therefore, precise description, change. , interpretation coefficient descriptor conditional covariates (\\(X\\) variables) model. descriptive model, implication causal effects goal prediction. Nevertheless, hard humans discuss descriptive model without using causal language, probably means hard us think models mere description. Like natural history, descriptive models useful patterns want explanation, using explicit causal models including experiments.Predictive modeling. Predictive modeling common applied research. example, fisheries researchers might model relationship population density habitat variables predict subset ponds region suitable brook trout (Salvelinus fontinalis) reintroduction. goal build model minimal prediction error, error predicted actual values future sample. predictive modeling, \\(X\\) (“predictor”) variables largely instrumental – related \\(Y\\) goal modeling, although sometimes investigator may interested relative importance among \\(X\\) predicting \\(Y\\) (example, collecting data may time consuming, expensive, enviromentally destructive, know subset \\(X\\) important predicting \\(Y\\) useful strategy).Predictive modeling. Predictive modeling common applied research. example, fisheries researchers might model relationship population density habitat variables predict subset ponds region suitable brook trout (Salvelinus fontinalis) reintroduction. goal build model minimal prediction error, error predicted actual values future sample. predictive modeling, \\(X\\) (“predictor”) variables largely instrumental – related \\(Y\\) goal modeling, although sometimes investigator may interested relative importance among \\(X\\) predicting \\(Y\\) (example, collecting data may time consuming, expensive, enviromentally destructive, know subset \\(X\\) important predicting \\(Y\\) useful strategy).Explanatory (causal) modeling. often, researchers explicitly interested \\(X\\) variables causally related \\(Y\\). fisheries researchers want reintroduce trout may want develop manage set ponds maintain healthy trout populations. active management requires intervention change habitat traits direction, magnitude, cause desired response. model predictive – specific change \\(X\\) predicts specific response \\(Y\\) – coefficients model provide knowledge system functions – changes inputs cause change output. Causal interpretation model coefficients requires set strong assumptions \\(X\\) variables model. assumptions typically met experimental designs observational designs.Explanatory (causal) modeling. often, researchers explicitly interested \\(X\\) variables causally related \\(Y\\). fisheries researchers want reintroduce trout may want develop manage set ponds maintain healthy trout populations. active management requires intervention change habitat traits direction, magnitude, cause desired response. model predictive – specific change \\(X\\) predicts specific response \\(Y\\) – coefficients model provide knowledge system functions – changes inputs cause change output. Causal interpretation model coefficients requires set strong assumptions \\(X\\) variables model. assumptions typically met experimental designs observational designs.observational designs, biologists often explicit goal modeling use combination descriptive, predictive, causal language describe discuss results. Many papers read researchers intend explanatory inference norms within biology community, mask intention “predictive” language. , advocate embracing explicit, explanatory modeling transparent model’s goal assumptions.","code":""},{"path":"intro-linear-models.html","id":"what-is-the-interpretation-of-a-regression-coefficient","chapter":"8 An introduction to linear models","heading":"8.4 What is the interpretation of a regression coefficient?","text":"regression coefficient difference \\(Y\\) expect see see one unit difference X, see difference covariate (X).","code":""},{"path":"intro-linear-models.html","id":"what-do-we-call-the-x-and-y-variables","chapter":"8 An introduction to linear models","heading":"8.5 What do we call the \\(X\\) and \\(Y\\) variables?","text":"inputs linear model (\\(X\\) variables) many names. text, \\(X\\) variables typicallytreatment variables – term makes sense categorical variables often used variables factor containing treatment assignment (example “control” “knockout”)factor variables (simply, factors) – , term makes sense categorical variablescovariates – term usually used non-focal \\(X\\) variables statistical model.linear model regression model regression modeling, \\(X\\) variables typically calledindependent variables (often shortened IV) – “independent” sense statistical model least, \\(X\\) function \\(Y\\).predictor variables (simply, “predictors”) – makes sense prediction models.explanatory variables – term usually applied observational designs best used explicit goal causal modeling.text, output linear model (\\(Y\\) variable variables model multivariate) often calle either ofresponse variable (simply, “response”)outcome variable (simply, “outcome”)terms causal connotation everyday english. terms often used regression modeling observational data, even model explicitly causal. term, common introductory textbooks, isdependent variable – “dependent” sense statistical model least, \\(Y\\) function \\(X\\).","code":""},{"path":"intro-linear-models.html","id":"modeling-strategy","chapter":"8 An introduction to linear models","heading":"8.6 Modeling strategy","text":"“best practice” sequence steps used throughout text analyze experimental data isexamine data using exploratory plots toexamine individual points identify outliers likely due data transcription errors measurement blundersexamine outlier points biologically plausible, raise ref flags undue influe fit models. information used inform researcher strategy handle outliers statistical analysis, including algorithms excluding data implementation robust methods.provide useful information initial model filtering (narrowing list potential models relevant question data). Statistical modeling includes diverse array models, yet almost methods used researchers biology, models book, generalizations linear model specified (8.3). experiments, may multiple models relevant question data. Model checking (step 3) can help decide model ultimately use.fit model, order estimate model parameters uncertainty estimates.check model, means use series diagnostic plots computations model output check fit model reasonably approximates data. diagnostic plots suggest poor approximation, choose different model go back step 2.inference model, means use fit parameters learn, uncertainty, system, predict future observations, uncertainty.plot model, means plot data, may adjusted, estimated parameters (results dervived estimates) uncertainty.Note step 1 (exploratory plots) data mining, exploring data patterns test.","code":""},{"path":"intro-linear-models.html","id":"predictions-from-the-model","chapter":"8 An introduction to linear models","heading":"8.7 Predictions from the model","text":"linear model specified Model (8.2), fit model \\[\\begin{equation}\ny_i = b_0 + b_1 x_i + e_i\n\\tag{8.5}\n\\end{equation}\\]\\(b_0\\) \\(b_1\\) coefficients fit model \\(e_i\\) residuals fit model. can use coefficients residuals recover \\(y_i\\), although rarely done. commonly, use coefficients calculate conditional means (mean conditional specified value \\(X\\)).\\[\\begin{equation}\n\\hat{y}_i = b_0 + b_1 x_i\n\\tag{8.6}\n\\end{equation}\\]conditional means typically called fitted values, \\(X\\) \\(X\\) used fit model, predicted values, \\(X\\) new. “Predicted values” often shortened “prediction”.","code":""},{"path":"intro-linear-models.html","id":"inference-from-the-model","chapter":"8 An introduction to linear models","heading":"8.8 Inference from the model","text":"goal inference, want use fit parameters learn, uncertainty, system. Using equation (8.5), coefficients \\(b_0\\) \\(b_1\\) point estimates true, generating parameters \\(\\beta_0\\) \\(\\beta_1\\), \\(e_i\\) estimates \\(\\varepsilon_i\\) (true, biological “noise”), \\(\\frac{\\sum{e_i^2}}{N-2}\\) estimate true, population variance \\(\\sigma^2\\) (covered chapter xxx may recognize \\(\\frac{\\sum{e_i^2}}{N-2}\\) formula variance). , using equation (8.6), \\(\\hat{y}_i\\) point estimate parameter \\(\\mu_i\\) (true mean conditional \\(X=x_i\\)). Throughout text, Greek letters refer theoretical parameter Roman letters refer point estimates.uncertainty estimates parameters due sampling standard error estimate. routine report standard errors means coefficients model. standard error estimate \\(\\sigma\\) available, effectively never reported, least experimental biology literature, presumably variance thought nuisance parameter (noise) something worthy study. pity. Certainly treatments can effect variance addition mean.Parametric inference assumes response drawn probability distribution (Normal, Poisson, Bernouli, etc.). Throughout text, emphasize reporting interpreting point estimates interval estimates point estimate. confidence interval type interval estimate. confidence interval parameter measure uncertainty estimate. 95% confidence interval 95% probability (sense long-run frequency) containing parameter. probability property population intervals computed using sampling measuring procedure. correct, without assumptions, state 95% probability parameter lies within interval. Perhaps useful interpretation interval compatability interval contains range estimates compatible data, sense \\(t\\)-test reject null hypothesis difference estimate value within interval (interpretation imply anything true value).Another kind inference significance test, computation probability “seeing data” something extreme data, given specified null hypothesis. probability p-value, can reported point estimate confidence interval. reasonable arguments made influential statisticians p-values useful lead researchers quagmire misconceptions impede good science. Nevertheless, current methodology fields Biology developed way become completely dependent p-values. think point, p-value can useful, imperfect tool inference, show compute p-values throughout text.Somewhat related significance test hypothesis test, Null-Hypothesis Signficance Test (NHST), \\(p\\)-value significance test compared pre-specified error rate called \\(\\alpha\\). Hypothesis testing developed formal means decision making rarely use NHST experimental biology. almost applications p-values see literature read ecology, evolution, physiology, wet-bench biology, comparing \\(p\\)-value \\(\\alpha\\) adds value communication results.","code":""},{"path":"intro-linear-models.html","id":"assumptions-for-inference-with-a-statistical-model","chapter":"8 An introduction to linear models","heading":"8.8.1 Assumptions for inference with a statistical model","text":"data generated process “linear parameters”, means different components model added together. additive part model containing parameters linear predictor specifications (8.2) (8.3) . example, cubic polynomial model\\[\\begin{equation}\n\\mathrm{E}(Y|X) = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3\n\\end{equation}\\]linear model, even though function non-linear, different components added. linear predictor additive, can compactly defined using matrix algebra\\[\\begin{equation}\n\\mathrm{E}(Y|X) = \\mathbf{X}\\boldsymbol{\\beta}\n\\end{equation}\\]\\(mathbf{X}\\) model matrix \\(\\boldsymbol{\\beta}\\) vector parameters. discuss chapter xxx.Generalized Linear Model (GLM) form \\(g(\\mu_i) = \\eta_i\\) \\(\\eta\\) (Greek letter “eta”) linear predictor\\[\\begin{equation}\n\\eta = \\mathbf{X}\\boldsymbol{\\beta}\n\\end{equation}\\]GLMs extensions linear models. non-linear models linear parameters, , predictor simple dot product model matrix vector parameters. example, Michaelis-Menten model non-linear model\\[\\begin{equation}\n\\mathrm{E}(Y|X)  = \\frac{\\beta_1 X}{\\beta_2 + X}\n\\end{equation}\\]non-linear parameters parts added together. text covers linear models generalized linear models, non-linear models also non-linear parameters.draws probability distribution independent. Independence implies uncorrelated \\(Y\\) conditional \\(X\\), , two \\(Y\\) value \\(X\\), predict value one given value . example, ASK1 data , “uncorrelated” implies predict glucose level one mouse within specific treatment combination given glucose level another mouse combination. linear models, assumption often stated “independent errors” (\\(\\varepsilon\\) model (8.2)) instead independent observations.lots reasons conditional responses might correlated. mouse example, correlation within treatment group arise subsets mice treatment group siblings housed cage. generally, measures within among experimental units (field sites humans rats) ’d expect measures within unit err model direction. Multiple measures within experimental units (site individual) creates “clustered” observations. Lack independence clustered observations can modeled using models random effects. models go many names including linear mixed models (common Ecology), hierarchical models, multilevel models, random effects models. linear mixed model variation model (8.2). text introduces linear mixed models chapter xxx.Measures taken sites closer together measures taken closer time measures closely related biological species tend similar values measures taken sites apart times apart species less closely related. Space time phylogeny create spatial temporal phylogenetic autocorrelation. Correlated error due space time phylogeny can modeled Generalized Least Squares (GLS) models. GLS model variation model (8.2).","code":""},{"path":"intro-linear-models.html","id":"specific-assumptions-for-inference-with-a-linear-model","chapter":"8 An introduction to linear models","heading":"8.8.2 Specific assumptions for inference with a linear model","text":"Constant variance homoskedasticity. common way thinking error term \\(\\varepsilon\\) constant variance, short way saying random draws \\(\\varepsilon\\) model (8.2) (identical) distribution. explicitly stated second line model specification (8.2). think using model specification (8.3), homoskedasticity means \\(\\sigma\\) \\(N(\\mu, \\sigma)\\) constant observations (conditional probability distributions identical, conditional mean adjusted \\(\\mu\\))Many biological processes generate data error function mean. example, measures biological variables grow, lengths body parts population size, variances “grow” mean. , measures counts, number cells damaged toxin, number eggs nest, number mRNA transcripts per cell variances function mean. Heteroskedastic error can modeled Generalized Least Squares, generalization linear model, Generalized Linear Models (GLM), “extensions” classical linear model.Normal Gaussian probability distribution. , common way thinking error term \\(\\varepsilon\\) Normal. Using model specification (8.3), ’d say conditional probablity distribution response normal. normal probability distribution implies 1) response continuous 2) conditional probability symmetric around \\(mu_i\\). conditional probability distribution long left right tail skewed left right. Counts (number cells, number eggs, number mRNA transcripts) binary responses (sucessful escape sucessful infestation host) continuous often often asymmetric probablity distributions skewed right sometimes can reasonably modeled using linear model often modeled using generalized linear models, , , extension linear model equation (8.3). classical linear model specific case GLM.common misconception inference linear model assumes raw response variable normally distributed. error-draw conditional-draw specifications linear model show precisely conception wrong. Model (??) states explicitly error normal distribution – distribution \\(Y\\) mix distribution \\(X\\) error. Model (8.3) states conditional outcome normal distribution, , distribution adjusting variation \\(X\\).","code":""},{"path":"intro-linear-models.html","id":"linear-modelregression-model-or-statistical-model","chapter":"8 An introduction to linear models","heading":"8.9 “linear model,”regression model”, or “statistical model”?","text":"Statistical modeling terminology can confusing. \\(X\\) variables statistical model may quantitative (continuous integers) categorical (names qualitative amounts) mix two. Linear models quantitative independent variables often called “regression models.” Linear models categorical independent variables often called “ANOVA models.” Linear models mix quantitative categorical variables often called “ANCOVA models” focus one categorical \\(X\\) “regression models” tend many independent variables.confusion partly results history development regression analysis observational data ANOVA analysis experimental data. math underneath classical regression (without categorical variables) linear model. math underneath classical ANOVA computation sums squared deviations group mean, “sums squares”. basic output regression table coefficients standard errors. basic ouput ANOVA ANOVA table, containing sums squares along mean-squares, F-ratios, p-values. historical differences usage, underlying math, output, many textbooks biostatistics organized around regression “vs.” ANOVA, presenting regression “” observational studies ANOVA “” experiments.recognized many decades experiments can analyzed using technique classical regression categorical variables coded numbers (, explained later) regression ANOVA variations general, linear model. Despite , “regression vs. ANOVA” way--thinking dominates teaching biostatistics.avoid misconceptions arise thinking statistical analysis “regression vs. ANOVA”, use term “linear model” general, umbrella term cover everything book. linear model, mean model linear parameters, including classical regression models, marginal models, linear mixed models, generalized linear models. avoid repetition, ’ll also use “statistical model”.","code":""},{"path":"regression.html","id":"regression","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9 Linear models with a single, continuous X (“regression”)","text":"","code":""},{"path":"regression.html","id":"a-linear-model-with-a-single-continuous-x-is-classical-regression","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1 A linear model with a single, continuous X is classical “regression”","text":"","code":""},{"path":"regression.html","id":"analysis-of-green-down-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.1 Analysis of “green-down” data","text":"introduce principles modeling single continuous \\(X\\) variable, ’ll use dataset fromRichardson, .D., Hufkens, K., Milliman, T. et al. Ecosystem warming extends vegetation activity heightens vulnerability cold temperatures. Nature 560, 368–371 (2018).Source dataThe data long-term experiment effects warming CO2 high-carbon northern temperate peatland focal dataset study. experiment involves 10 large, temperature CO2 controlled enclosures. CO2 set 400 ppm five enclosures 900 ppm five enclosures. Temperature five enclosures within CO2 level set 0, 2.25, 4.5, 6.75, 9 °C ambient temperature. multiple temperature levels regression design, allows researcher measure non-linear effects. Read experimental design beautiful implementation.question pursued study , causal effect warming timing (phenology) transition photosynthetic activity (“green-”) spring transition photosynthetic activity (“green-”) fall? researchers measured transition dates, Day Year (DOY), using foliage color. , focus transition photosynthesis “green-” DOY.Import dataExamine dataNo plot shows obvious outlier might due measurement blunders curation error. linear regression left-plot clearly shows linear response sufficient capture effect temperature day green-.choose model. \\(X\\) variable (\\(temperature\\)) experimentally set five levels, data reasonably modeled using either linear model categorical \\(X\\) linear model continuous \\(X\\). advantage modeling \\(temperature\\) continuous variable one effect, slope regression line. modeled categorical factor five levels, , minimum, four interesting effects (difference means non-reference level reference (temperature = 0) level). Also, inference, modeling \\(temperature\\) continuous variable increases power hypothesis tests.choose model. \\(X\\) variable (\\(temperature\\)) experimentally set five levels, data reasonably modeled using either linear model categorical \\(X\\) linear model continuous \\(X\\). advantage modeling \\(temperature\\) continuous variable one effect, slope regression line. modeled categorical factor five levels, , minimum, four interesting effects (difference means non-reference level reference (temperature = 0) level). Also, inference, modeling \\(temperature\\) continuous variable increases power hypothesis tests.fit modelfit modelcheck modelThe Q-Q plot indicates distribution residuals well within expected normal sample cause concern inference.spread-location plot shows conspicuous trend spread changes conditonal mean. cause concern inference.inference modelThe effect added temperature day green-4.98 d per 1 °C (95% CI: 3.7, 6.3; p < 0.001).plot model\nFigure 9.1: Modification published Figure 2c showing experimental effect warming date autumn green-(transition fall foliage color) mixed shrub community. bottom panel scatterplot. regression line shows expected value Y (transition day year) given value X (added temperature). slope regression line estimate effect. estimate 95% confidence interval estimate given top panel.\nReport resultsThe modeled effect added temperature Slope: 4.98 (3.7, 6.26) d per 1 °C (9.1).","code":"\ngg1 <- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth(method = \"lm\")\ngg2 <- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2))\ngg3 <- qplot(x = temperature,\n      y = transition_date,\n      data = fig2c) +\n  geom_smooth()\nplot_grid(gg1, gg2, gg3, ncol=3)\n# Step 1: fit the model\nm1 <- lm(transition_date ~ temperature, data = fig2c)\n# check normality assumption\nset.seed(1)\nqqPlot(m1, id=FALSE)\n# check homogeneity assumption\nspreadLevelPlot(m1, id=FALSE)## \n## Suggested power transformation:  0.6721303\nm1_coeff <- summary(m1) %>%\n  coef()\nm1_confint <- confint(m1)\nm1_coeff <- cbind(m1_coeff, m1_confint)\nm1_coeff##               Estimate Std. Error   t value     Pr(>|t|)      2.5 %     97.5 %\n## (Intercept) 289.458750  3.0593949 94.613071 1.738650e-13 282.403773 296.513728\n## temperature   4.982745  0.5541962  8.990941 1.866888e-05   3.704767   6.260724"},{"path":"regression.html","id":"learning-from-the-green-down-example","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.2 Learning from the green-down example","text":"Figure 9.1 scatterplot green-DOY mixed-shrub community \\(Y\\) axis added temperature \\(X\\) axis. line data regression line, expected value Y (green-DOY) given specific value X (added temperature). slope line effect added temperature timing green-. intercept regression line value response (day green-) \\(X\\) equal zero. often, value interest although value reported allow predictions model. Also often, value intercept meaningful value \\(X = 0\\) far outside range measured \\(X\\), value absurd impossible (example, investigating effect body weight metabolic rate, value \\(weight = 0\\) impossible).intercept slope coefficients model fit data, \\[\\begin{equation}\nday_i = b_0 + b_1 temperature_i + e_i\n\\tag{9.1}\n\\end{equation}\\]day day green-, temperature added temperature, refers (“indexes”) ith enclosure. model completely reconstructs day green-ten enclosures. example, day green-enclosure 8 \\[\\begin{equation}\n332 = 289.458750 + 4.982745 \\times 6.73 + 9.00737\n\\end{equation}\\]coefficients model estimates parameters generating model fit data\\[\\begin{align}\nday &= \\beta_0 + \\beta_1 temperature + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\tag{9.2}\n\\end{align}\\]generating model data used make inference, example, measure uncertainty prediction timing green-future warming, measure uncertainty effect temperature green-.","code":""},{"path":"regression.html","id":"using-a-regression-model-for-explanation-causal-models","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3 Using a regression model for “explanation” – causal models","text":"text, “explanatory” means “causal” goal explanatory modeling estimate causal effects using causal interpretation linear model (=regression) coefficients. “? learned stats 101 course interpret regression coefficients causally”.Statisticians (statistics textbooks) quite rigid regression coefficient descriptive (“observational”, see ) interpretation causal interpretation. time, statisticians (statistics textbooks) seem issue interpeting modeled effects experiment causally, since interpreted. modeled effects simply coefficients linear (= regression) model, historical practice muddled.Part muddled history arises use “regression” models fit observational data one continuous \\(X\\) variables use “ANOVA” models fit experimental data one categorical \\(X\\). separation seems blinded statisticians working formal probabilistic statements underlying causal interpretations effect estimates ANOVA synthesis statements probabilistic statements regression modeling. Two major approaches developing formal, probabilistic statements causal modeling statistics Rubin causal model -operator Pearl. Despite gigantic progress approaches, little none found way biostatistics textbooks.","code":""},{"path":"regression.html","id":"what-a-regression-coefficient-means","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3.1 What a regression coefficient means","text":"Description: saw one unit larger value X, saw, average, difference b Y.Prediction: see one unit larger value X, expect see difference b Y.Explanation (causation): set X one unit larger value, expect Y change b.linear (“regression”) model coefficient, coefficient temperature, \\(\\beta_1\\), three interpretations, observational interpretation, predictive interpretation, causal interpretation. text causal interpretations, explain meant , need clarify differences causal observational interpretations.clarify differences, ’s useful remember expected value regression model conditional mean\\[\\begin{equation}\n\\textrm{E}[day|temperature] = \\beta_0 + \\beta_1 temperature\n\\tag{9.3}\n\\end{equation}\\]words “expected value day conditional temperature beta-knot plus beta-one times temperature”. expected value long run average – infinite number enclosures \\(temperature=x\\) (\\(x\\) specific value added temperature, say 2.5 °C), average \\(day\\) enclosures \\(\\beta_0 + \\beta_1 x\\).parameter \\(\\beta_1\\) difference conditional means.\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|temperature = x+1] - \\textrm{E}[day|temperature = x]\n\\tag{9.4}\n\\end{equation}\\]words, “beta-one expected value day green-temperature equals x + 1 minus expected value day green-temperature equals x.” short way state “beta-one difference conditional means”.tl;dr. Note “+ 1” definition mere convenience. Since slope line \\(\\frac{y_2 - y_1}{x_2 - x_1}\\), (\\(x_1\\), \\(y_1\\)) (\\(x_2\\), \\(y_2\\)) coordinates two points line, convenient choose two points differ \\(X\\) one unit, makes fraction equal numerator . numerator difference conditional means. also units regression coefficient “per unit \\(X\\) even defined difference two \\(Y\\) values.difference observational causal interpretations \\(\\beta_1\\) depends “event” conditioned \\(\\textrm{E}[day|temperature]\\). Let’s start causal interpretation, think regression coefficients green-experiment.","code":""},{"path":"regression.html","id":"causal-interpretation-conditional-on-doing-x-x","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3.2 Causal interpretation – conditional on “doing” X = x","text":"causal interpretation regression, \\(\\textrm{E}[day|temperature]\\) conditioned “” real hypothetical intervention system set value \\(temperature\\) specific value \\(x\\) (“\\(temperature=x\\)), keeping everything else system . can stated explicitly \\(\\textrm{E}[day|\\;\\;temperature = x]\\). Using -operator, can interpret \\(\\beta_1\\) effect coefficient.\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|\\;\\;temperature = x+1] - \\textrm{E}[day|\\;\\;temperature = x]\n\\end{equation}\\]words, “beta-one expected value day green-set temperature x + 1, minus expected value day green-set temperature x.”tl;dr. green-experiment, researchers didn’t set temperature intervened enclosures ambient temperature + 1 ambient + 2.25, ambient + 4.5, ambient + 6.75, ambient + 9.0. (see tl;dr ), + 1 mere convenience definition.","code":""},{"path":"regression.html","id":"observational-interpretation-conditional-on-seeing-x-x.","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3.3 Observational interpretation – conditional on “seeing” X = x.","text":"observational interpretation regression, \\(\\textrm{E}[day|temperature]\\) conditioned sampling data “seeing” value \\(temperature\\). can state explicitly \\(\\textrm{E}[day|\\;see\\;temperature]\\). , can interpret \\(\\beta_1\\) observational coefficient\\[\\begin{equation}\n\\beta_1 = \\textrm{E}[day|\\;see\\;temperature = x+1] - \\textrm{E}[day|\\;see \\;temperature = x]\n\\end{equation}\\]words, “beta-one expected value day green-see temperature equals x + 1 minus expected value day green-see temperature equals x.” understand mean “observational”, let’s imagine green-data come experiment researchers intervened set added temperature specifc value ten sites naturally vary mean annual temperature. single site 10 years data, years warmer years colder. Data kind study observational – researcher didn’t intervene set \\(X\\) values merely observed \\(X\\) values.sample (“see”) site mean annual temperature 2.5 °C reference value, expected day green-\\(\\textrm{E}[day|temperature = 2.5 °C]\\). , values near \\(\\textrm{E}[day|temperature = 2.5 °C]\\) probable values away \\(\\textrm{E}[day|temperature = 2.5 °C]\\). , information site mean annual temperature 2.5 °C reference, best prediction day green-\\(\\textrm{E}[day|temperature = 2.5 °C]\\). claim 4.98 day delay green-caused warmer temperature, expected delay relative reference seen data.seeing interpretation regression coefficient descriptive– description mathematical relationship. interpretation, coefficient causal sense expected response \\(Y\\) intervene system change \\(X\\) \\(x\\) \\(x+1\\).","code":""},{"path":"regression.html","id":"omitted-variable-bias","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3.4 Omitted variable bias","text":"consequence interpreting regression coefficient causally instead observationally?\nFigure 9.2: Directed Acyclic (causal) Graph hypothetical world day green-caused two, correlated environmental variables, temperature moisture, noise factor (U) represents unspecified set additional variables correlated either temperature moisture.\nLet’s expand thought experiment observational data set green dates. thought experiment, two variables systematically affect green-DOY. first temperature plants experience; effect \\(temperature\\) \\(\\beta_1\\). second soil moisture plants experience; effect \\(moisture\\) \\(\\beta_2\\). \\(temperature\\) \\(moisture\\) correlated value \\(\\rho\\). causal model graphically represented causal graph .Lets fit two linear models.\\[\\begin{align}\n(\\mathcal{M}_1)\\; day &= b_0 + b_1 \\; temperature + b_2 \\; moisture + \\varepsilon\\\\\n(\\mathcal{M}_2)\\; day &= b_0 + b_1 \\; temperature + \\varepsilon\n\\end{align}\\]\\(\\mathcal{M}_1\\) linear model including \\(temperature\\) \\(moisture\\) \\(X\\) variables \\(\\mathcal{M}_2\\) linear model including \\(temperature\\) \\(X\\) variable. Given true causal model , \\(b_1\\) unbiased estimate true causal effect temperature \\(\\beta_1\\) \\(\\mathcal{M}_1\\) expected value \\(b_1\\) \\(\\beta_1\\). contrast, \\(b_1\\) biased estimate true causal effect temperature \\(\\beta_1\\) \\(\\mathcal{M}_2\\). expected value \\(b_1\\) \\(\\mathcal{M}_2\\) true, causal effect plus bias term.\\[\\begin{equation}\n\\mathrm{E}(b_1|\\mathcal{M}_1) = \\beta + \\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}}\n\\end{equation}\\]bias (\\(\\rho \\alpha \\frac{\\sigma_{moisture}}{\\sigma_{temperature}}\\)) omitted variable bias. omitted variable \\(moisture\\) unmeasured, confounding variable. variable \\(X_2\\) confounder variable \\(X_1\\) \\(X_2\\) correlation \\(X_1\\) path response \\(Y\\) \\(X_1\\). ommitted variable bias, sample data, estimate effect doesn’t converge truth truth plus bias.","code":""},{"path":"regression.html","id":"causal-modeling-with-experimental-versus-observational-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.3.5 Causal modeling with experimental versus observational data","text":"Causal interpretation requires conditioning “X=x”. green-data, “X=x” achieved random treatment assignment enclosures. random treatment assignment achieve ? Look Figure 9.2. values \\(treatment\\) randomly assigned, , definition, expected value correlation \\(treatment\\) \\(moisture\\) zero. fact, expected value correlation \\(treatment\\) measurable variable study site zero. Given, , expected value regression coefficient \\(temperature\\) (\\(b_1\\)) \\(\\beta\\) \\(\\rho=0\\). , estimate true effect unbiased. doesn’t matter \\(moisture\\), variable, excluded model. (said, may want include moisture variables model increase precision causal effect. addressed chapter “Models Covariates”). experiment experiments used answer causal questions.Can use observational data causal modeling? Yes, methods outside scope text. mathematical foundation known path analysis, developed geneticist evolutionary biologist Sewell Wright (include work inspired much think biology academic grandfather great-grandfather). Causal analysis observational data biology rare outside ecology epidemiology. Good starting points modern development causal analysis Hernán MA Robins JM (2020) Burgess et al. (2018). gentle introduction Pearl Mackenzie (2018).","code":""},{"path":"regression.html","id":"using-a-regression-model-for-prediction-prediction-models","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4 Using a regression model for prediction – prediction models","text":"researchers green-study also presented estimates effect temperature green-using two observational datasets. Let’s use one Extended Data Figure 3d explore using regression model prediction. data taken measurements day green-86 year period single site. response variable \\(green\\_down\\_anomaly\\) (difference day green year mean days 86 years). predictor variable \\(autumn\\_temp\\_anomaly\\) (difference mean temperature year mean means).plot figure ?? shows regression line linear model fit data. Two statistics given plot.p-value slope (coefficient \\(b_1\\) \\(autumn\\_temp\\_anomaly\\))\\(R^2\\) model fit.addition, two intervals shown.95% confidence interval expected values blue. width band measure precision estimates expected values.95% prediction interval predictions gray. width band measure well model predicts.important researchers knows statistics bands , compute ignore , say communicating results.","code":""},{"path":"regression.html","id":"ci-and-p-value","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.1 95% CI and p-value","text":"95% confidence interval expected values p-value function standard error slope coefficient \\(b_1\\) complimentary statistics. p-value probability sampling null results \\(t\\)-value extreme observed t coefficient \\(b_1\\). null includes hypothesis \\(\\beta_1 = 0\\). 95% confidence interval expected values band containing expected values (mean \\(green\\_down\\_anomaly\\) conditional \\(autumn\\_temp\\_anomaly\\)) compatible data. couple useful ways thinking band.band captures infinite number regression lines compatible data. steep predict smaller expected values low end \\(autumn\\_temp\\_anomaly\\) higher expected values high end \\(autumn\\_temp\\_anomaly\\). Others less steep predict higher expected values low end \\(autumn\\_temp\\_anomaly\\) lower expected values high end \\(autumn\\_temp\\_anomaly\\).band captures 95% CI conditional mean every value \\(X\\). Consider 95% CI conditional mean mean value \\(X\\). move away mean value (lower higher \\(X\\)), 95% CI conditional mean increases.95% CI p-value useful statistics report purpose causal modeling, example using experimental green-data (95% CI presented confidence band expected values CI estimate effect added temperature). 95% CI p-value also useful statistics report purpose descriptive modeling, simply wanting know conditional mean response related \\(X\\) variable.","code":""},{"path":"regression.html","id":"prediction-interval-and-r2","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.2 95% prediction interval and \\(R^2\\)","text":"\\(R^2\\) 95% prediction interval function population variability \\(green\\_down\\_anomaly\\) conditional \\(autumn\\_temp\\_anomaly\\) (spread points along vertical axis regression line) complimentary statistics. Briefly,\\(R^2\\) measure fraction variance response accounted model (sources say “explained model” odd use “explain”). number 0 1 measure “predictability” goal prediction modeling.95% prediction interval contain new observation 95% time. provides bounds prediction – given new observation, 95% probability interval \\(x_{new}\\) contain \\(y_{new}\\).understand \\(R^2\\) 95% prediction interval bit better, let’s back .\\[\\begin{equation}\ngreen\\_down\\_anomaly_i = b_0 + b_1 autumn\\_temp\\_anomaly_i + e_i\n(\\#eq:doy_fit)\n\\end{equation}\\]Model @ref(eq:eq:doy_fit) recovers measured value \\(green\\_down\\_anomaly\\) year, given \\(autumn\\_temp\\_anomaly\\) year. equation includes linear predictor (\\(b_0 + b_1 autumn\\_temp\\_anomaly_i\\)) residual predictor (\\(e_i\\)). predictor part @ref(eq:doy_fit) used compute \\(\\hat{y}\\) (“y hat”).\\[\\begin{equation}\n\\hat{y}_i = b_0 + b_1 autumn\\_temp\\_anomaly_i\n(\\#eq:doy_hat)\n\\end{equation}\\]\\(\\hat{y}\\) fitted values, values computed data used fit, predicted values (simply prediction), values computed values predictor variables outside set used fit model. purpose plotting, generally, models categorical factors \\(X\\), prefer either modeled values conditional means fitted values.","code":""},{"path":"regression.html","id":"r2","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.3 \\(R^2\\)","text":"good model accounts sizable fraction total variance response. fraction \\(R^2\\) value, given summary(m1) accessed directly \\(R^2\\) fraction total variance \\(Y\\) generated linear predictor.\\[\\begin{equation}\nR^2 = \\frac{\\mathrm{VAR}(\\hat{y})}{\\mathrm{VAR}(y)}\n\\end{equation}\\]\\(R^2\\) vary zero (model accounts nothing) one (model accounts everything). \\(R^2\\) often described fraction total variation explained model” usage “explain” observational causal. ambiguous usage “explain” statistics, prefer avoid word.can useful sometimes think \\(R^2\\) terms residual error, variance residuals model. larger residual error, smaller \\(R^2\\), \\[\\begin{equation}\nR^2 = 1 - \\frac{\\mathrm{VAR}(e)}{\\mathrm{VAR}(y)}\n\\end{equation}\\]smaller residuals, higher \\(R^2\\) closer predicted values measured values. sum model variance residual variance equals total variance , consequently, \\(R^2\\) signal signal + noise ratio. noise \\(R^2\\) sampling variance individual measures. noise t-value sampling variance parameter (m1, sampling variance \\(b_1\\)). important distinction means t \\(R^2\\) related 1:1. simple model single \\(X\\), one might expect \\(R^2\\) big p-value slope tiny, isn’t necessarily true different meaning noise . study large sample size \\(n\\) can tiny p-value small \\(R^2\\). p-value good indicator predictability. \\(R^2\\) . explored .","code":"\nsummary(m1)$r.squared## [1] 0.1305754\nyhat <- fitted(m1)\ny <- efig_3d[, green_down_anomaly]\nvar(yhat)/var(y)## [1] 0.1305754\ne <- residuals(m1)\ny <- efig_3d[, green_down_anomaly]\n1 - var(e)/var(y)## [1] 0.1305754"},{"path":"regression.html","id":"prediction-interval","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.4 Prediction interval","text":"good prediction model high predictability, meaning range predicted values narrow. 95% CI reasonable range communicate. decision making using prediction, better look numbers band plot.Given data fit model, see 2 °C increase mean fall temperature, expect autumn green-extend 3.2 days. expectation average. ’d expect 95% actual values range -6.4 days 12.8 days. lot variability. prediction reasonable probability week early late. may seem surprising given p-value fit, 0.0006. p-value reliable indicator predictability. statistic related blue band, gray band.understand prediction interval bit , p-value good indicator predictability, ’s useful understand makes prediction interval “wide”. width prediction interval function two kinds variabilityThe variance expected value specific value \\(X\\). square standard error \\(b_1\\). blue band communicating CI based variance. p-value related wideth band.variance \\(Y\\) specific value \\(X\\). variance \\(\\sigma^2\\). useful learning think equation estimate variance.\\[\\begin{equation}\n\\hat\\sigma^2 = \\frac{\\sum{e_i^2}}{N-2}\n\\end{equation}\\], \\(e_i\\) residual ith case. denominator (\\(N-2\\)) degrees freedom model. Computing \\(\\hat\\sigma^2\\) manually helps insure understand going .Remember assumption linear models working point , variance constant values \\(X\\), single \\(\\sigma\\). Later, cover linear models model heterogeneity variance. \\(\\sigma\\) function variability population – population standard deviation conditional \\(X\\).Importantly, predictability function components variability. consequence, \\(R^2\\), p-value, indicator predictability. observational green-data, even thousands years data, still pretty low \\(R^2\\) population variability \\(green\\_down\\_anomaly\\) given \\(autumn\\_temp\\_anomaly\\).","code":"\nsummary(m1)$sigma # R's calculation of sigma hat## [1] 4.736643\ndf <- summary(m1)$df[2] # R's calculation of df. Check that this is n-2!\nsqrt(sum(e^2)/df)## [1] 4.736643"},{"path":"regression.html","id":"median-absolute-error-and-root-mean-square-error-are-absolute-measures-of-predictability","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.5 Median Absolute Error and Root Mean Square Error are absolute measures of predictability","text":"p-value good guide predictability. \\(R^2\\) proportional predictability really useful absolute sense. want predict effect warming day green-, like measure predictability units green-, days. prediction interval gives value \\(X\\). overall measure predictability?Three overall measures predictability \\(\\hat{\\sigma}\\), estimate \\(\\sigma\\). standard deviation sample conditional \\(X\\).\\(RMSE\\), root mean squared error. \\[\\begin{align}\nSSE &= \\sum{(y_i - \\hat{y}_i)^2}\\\\\nMSE &= \\frac{SSE}{N}\\\\\nRMSE &= \\sqrt{MSE}\n\\end{align}\\]\\(SSE\\) (“sum squared error”) sum squared residuals (\\(y_i - \\hat{y}_i\\)) model. \\(MSE\\) (“mean squared error”) mean squared errors. \\(RMSE\\) square root mean squared error. \\(RMSE\\) almost equal \\(\\hat{\\sigma}\\). difference denominator, \\(N\\) computation \\(RMSE\\) \\(df\\) (degrees freedom model, \\(N\\) minus number fit parameters) computation \\(\\hat{\\sigma}\\).\\(MAE\\), mean absolute error. \\[\\begin{equation}\nMAE = \\frac{1}{N}\\sum{|y_i - \\hat{y}_i|}\n\\end{equation}\\]goal analysis prediction, one statistics reported. model fit observational green-data Extended Figure 3d, three statistics given Table ?? (two decimal places simply show numerical difference \\(\\hat{\\sigma}\\) \\(RMSE\\)). measures “average” prediction error units response. average error either 4.7 3.6 days, depending statistic report. difference? \\(\\hat{\\sigma}\\) \\(RMSE\\) functions squared error big differences predicted actual value emphasized. error 6 days twice bad error 3 days, report \\(RMSE\\). \\(RMSE\\) \\(\\hat{\\sigma}\\)? Simply researchers using prediction models familiar \\(RMSE\\). error 6 days twice bad error 3 days, report \\(MAE\\).","code":""},{"path":"regression.html","id":"prediction-modeling-is-more-sophisticated-than-presented-here","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.4.6 Prediction modeling is more sophisticated than presented here","text":"data response non-linear function predictor, data many predictor variables, researchers often build model using model selection method. Stepwise regression classical model selection method commonly taught intro biostatistics commonly used researchers. Stepwise regression method model selection many well-known problems avoided.excellent books useful building models model selection areThe Elements Statistical LearningRegression StoriesRegression Modeling Strategies","code":""},{"path":"regression.html","id":"using-a-regression-model-for-creating-a-new-response-variable-comparing-slopes-of-longitudinal-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.5 Using a regression model for creating a new response variable – comparing slopes of longitudinal data","text":"study example, researchers compared growth rate tumors mice fed normal chow versus mice methionine restricted diet. Growth rate wasn’t actually compared. Instead, researchers used t-test compare size tumor measured six different days. problem multiple t-tests dataset errors (residuals model) one day correlated errors another day repeated measures mouse. correlation inflate Type error rate.Instead six t-tests, better strategy data use regression calculate tumor growth rate mouse. sixteen mice sixteen fit models. use “loop” fit model data single mouse use slope (\\(b_1\\)) estimate tumor growth rate mouse.Use -loop estimate growth rate mouse. pass loopthe subset fig1f (data long format) belonging mouse createdthe linear model volume ~ day fit subsetthe coefficient day (slope, \\(b_1\\)) inserted mouse ’s row column “growth” data.table “fig1f_wide”.end loop, data.table fig1f_wide one row mouse, column treatment factor (diet) column called “growth” containing mouse’s growth rate. also columns tumor volume mouse day ignored.Step 3. fit modelStep 5. inference modelStep 6. plot model","code":"\nN <- nrow(fig1f_wide)\nid_list <- fig1f_wide[, id]\nfor(i in 1:N){\n  mouse_i_data <- fig1f[id == id_list[i]] # subset\n  fit <- lm(volume ~ day, data = mouse_i_data)\n  fig1f_wide[id == id_list[i], growth := coef(fit)[2]]\n}\n# View(fig1f_wide)\n# qplot(x = treatment, y = growth, data = fig1f_wide)\n# qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_smooth(aes(group = id), method = \"lm\", se = FALSE)\nm1 <- lm(growth ~ treatment, data = fig1f_wide)\nm1_coef <- summary(m1) %>%\n  coef\nm1_ci <- confint(m1)\n(m1_coef_table <- cbind(m1_coef, m1_ci))##              Estimate Std. Error   t value     Pr(>|t|)     2.5 %    97.5 %\n## (Intercept)  52.63406   3.102096 16.967258 9.865155e-11  45.98072  59.28739\n## treatmentMR -23.57616   4.387026 -5.374065 9.809457e-05 -32.98540 -14.16693"},{"path":"regression.html","id":"using-a-regression-model-for-for-calibration","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.1.6 Using a regression model for for calibration","text":"","code":""},{"path":"regression.html","id":"working-in-r-1","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2 Working in R","text":"","code":""},{"path":"regression.html","id":"fit-the-model","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.1 Fitting the linear model","text":"linear model fit lm function, flexible workhorse much text.m1 lm model object contains many different kinds information, model coefficients.’ll return others, first, let’s explore flexibility lm function. Two arguments sent functionthe model formula transition_date ~ temperature form Y ~ X, Y X names columns data.model formula can assigned variable, useful building functions. exampleBoth Y X can also column names embedded within function, exampleorThe data frame (remember data.table data frame) containing columns variable names model formula. data argument necessary usually better way (exception researcher wants create matrix Y variables construct model matrix)type ?lm console see arguments lm function.","code":"\nm1 <- lm(transition_date ~ temperature, data = fig2c)\ncoef(m1)## (Intercept) temperature \n##  289.458750    4.982745\ncoef_table <- function(x, y, data){\n  m1_form <- formula(paste(y, \"~\", x))\n  m1 <- lm(m1_form, data = data)\n  return(coef(summary(m1)))\n}\n\ncoef_table(x = \"temperature\",\n           y = \"transition_date\",\n           data = fig2c)##               Estimate Std. Error   t value     Pr(>|t|)\n## (Intercept) 289.458750  3.0593949 94.613071 1.738650e-13\n## temperature   4.982745  0.5541962  8.990941 1.866888e-05\nm2 <- lm(log(transition_date) ~ temperature, data = fig2c)\ncoef(m2)## (Intercept) temperature \n##   5.6690276   0.0160509\nm3 <- lm(scale(transition_date) ~ scale(temperature), data = fig2c)\ncoef(m3)##        (Intercept) scale(temperature) \n##       4.171921e-16       9.539117e-01\nx <- fig2[, temperature]\ny <- fig2[, transition_date]\nm4 <- lm(y ~ x)\ncoef(m4)## (Intercept)           x \n## 204.8866185   0.4324755"},{"path":"regression.html","id":"getting-to-know-the-linear-model-the-summary-function","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.2 Getting to know the linear model: the summary function","text":"lm function returns lm object, ’ve assigned name m1. m1 contains lots information fit linear model data. information want purposes can retrieved summary function, general-purpose R command works many R objects.:Call. model fitResiduals. summary distribution residuals. one can get sense distribution (inference, model assumes normal distribution mean zero). useful ways examine distribution introduced later chapter.Coefficients table. contains linear model coefficients standard error associated \\(t\\) \\(p\\) values.column values “Estimate” coefficients fitted model (equation (9.1)). , 289.4587503 intercept (\\(b_0\\)) 4.9827453 effect \\(temperature\\) (\\(b_1\\)).column values “Std. Error” standard errors coefficients.column values “t value” t-statistics coefficient. t-value signal noise ratio. coefficient \\(b_1\\) “signal” SE noise. Get used thinking ratio. t-value greater 3 indicates “pretty good” signal relative noise, one much 2 .column values “Pr(>|t|)” p-value, t-test estimate. p-value test ? p-value tests hypothesis “probable data, extreme data, true parameter zero?”. Formally \\(p = \\mathrm{freq(t' \\ge t|H_o)}\\), \\(t'\\) hypothetical t-value, t observed \\(t\\)-value, \\(H_o\\) null hypothesis. return p-values Chapter xxx.Signif. codes. Significance codes extremely common wet bench experimental biology literature much recommend. ’ll return p-values chapter.Beneath Signif. codes model statistics usefulResidual standard error \\(\\sqrt{\\sum{e_i^2}/(n-2)}\\), \\(e_i\\) residuals fitted model. “degrees freedom” number \\(e_i\\) “allowed vary” fitting parameters, total sample size (\\(n\\)) minus number parameters model. fit model two fit parameters (\\(b_0\\) \\(b_1\\) df \\(n-2\\). Note denominator residual standard error equation.Multiple R-squared. important imperfect summary measure whole model effectively measures much total variance response variable “explained ” model. value lies zero 1. ’s good measure report manuscript, especially observational data.F-statistic p-value. statistics whole model (individual coefficients) just don’t find useful.","code":"\nsummary(m1)## \n## Call:\n## lm(formula = transition_date ~ temperature, data = fig2c)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.5062 -3.8536  0.6645  2.7537  9.0074 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 289.4588     3.0594  94.613 1.74e-13 ***\n## temperature   4.9827     0.5542   8.991 1.87e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.443 on 8 degrees of freedom\n## Multiple R-squared:  0.9099, Adjusted R-squared:  0.8987 \n## F-statistic: 80.84 on 1 and 8 DF,  p-value: 1.867e-05"},{"path":"regression.html","id":"inference-the-coefficient-table","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.3 Inference – the coefficient table","text":"different ways ofNote p-value coefficient temperature small conclude data compatible model temperature effect day green-. need formal hypothesis test ? haven’t learned much learned slope “likely exactly zero” (Temperature effects everything biology). far important question relationship temperature day green-, sign magnitude effect uncertainty estimate effect. , want coefficient SE confidence interval, table. Remember, working definition confidence interval:confidence interval contains range parameter values compatible data, sense \\(t\\)-test reject null hypothesis difference estimate value within intervalA textbook way defining CI : 95% CI parameter 95% probability including true value parameter. mean 95% probability true value lies interval. subtle important difference. way thinking proper meaning textbook definition: don’t know true value \\(\\beta_1\\) can 1) repeat experiment sampling, 2) re-estimate \\(\\beta_1\\), 3) re-compute 95% CI. 1-3 many times, 95% CIs include \\(\\beta_1\\) within interval.Confidence intervals often interpreted like \\(p\\)-values. , researcher looks see CI overlaps zero , concludes “effect”. First, conclusion correct – inability find sufficient evidence effect mean effect, simply means insufficient evidence conclude effect!Second, want use CI guide us big small effect might reasonably , given data. , CI measure parameter values “compatible” data. biological interpretations small-end big-end interval’s range radically differ, don’t enough precision analysis reach unambiguous conclusion.","code":"\n# step by step\n\nm1_summary <- summary(m1) # get summary\nm1_coef_p1 <- coef(m1_summary) # extract coefficient table\nm1_coef_p2 <- confint(m1) # get CIs\nm1_coef <- cbind(m1_coef_p1, m1_coef_p2) # column bind the two\n\n# this can be shortened (but still readable) using\nm1_coef <- cbind(coef(summary(m1)),\n                 confint(m1))\nm1_coef##               Estimate Std. Error   t value     Pr(>|t|)      2.5 %     97.5 %\n## (Intercept) 289.458750  3.0593949 94.613071 1.738650e-13 282.403773 296.513728\n## temperature   4.982745  0.5541962  8.990941 1.866888e-05   3.704767   6.260724"},{"path":"regression.html","id":"how-good-is-our-model-model-checking","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.4 How good is our model? – Model checking","text":"two, quite different senses meant good model.good model predicting? , much variance response (stuff “explained”) accounted model? described .well data look like sample modeled distribution? well, consider alternative models. model checking.inference, good model generates data look like real data. true, fit model well-behaved residuals. several aspects “well-behaved” checked diagnostic plot. model checking covered detail chapter xxx.Inference model (9.2) assumes data sampled normal distribution. check , use quantile-quantile Q-Q plot. qqPlot function car package generates useful plot Base R.Approximately normal residuals track solid line stay largely within boundaries marked dashed lines. residuals m1 fit green-data track solid line remain within dashed lines, indicating adequate model fit.Note formal test normality often recommended. Formal tests add value diagnostic check. Robustness inference (example, p-value) function type degree “non-normalness”, p-value. small sample, much power test normality, samples non-normal distributions pass test (\\(p > 0.05\\)) deemed “normal”. large samples, samples distributions deviate slightly normal fail test (\\(p < 0.05\\)) deemed “normal”. Inference many non-normal samples large \\(n\\) robust (meaning infernece likely fool randomness).Inference model (9.2) assumes homogeneity response conditional \\(X\\). continuous \\(X\\), means residuals approximately equal variance low, mid, high values \\(X\\) (everywhere ). One can visually inspect spread points \\(Y\\) direction across groups categorical \\(X\\) along X-axis continuous \\(X\\). useful method checking residual variance changes (usually increases) conditional mean \\(Y\\) spread-location plot. spreadLevelPlot(m1) function car package useful.dashed blue line shows linear trends magenta line shows non-linear trends. green-data, dashed line flat magenta-line shows looks like random fluctations. Taken together, two lines indicate adequate model fit.","code":"\nset.seed(1)\nqqPlot(m1, id=FALSE)\nspreadLevelPlot(m1)## \n## Suggested power transformation:  0.6721303"},{"path":"regression.html","id":"plotting-models-with-continuous-x","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.5 Plotting models with continuous X","text":"","code":""},{"path":"regression.html","id":"quick-plot","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.5.1 Quick plot","text":"qplot ggplot package useful initial examinations dataSome variations quick plot explore includeuse geom_smooth(method = \"lm\")","code":"\nqplot(x = temperature, y = transition_date, data = fig2c) + geom_smooth()"},{"path":"regression.html","id":"ggpubr-plots","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.5.2 ggpubr plots","text":"ggpubr package functions automates construction publication-ready ggplots. ggscatter can generate publishable plot little effort.take little effort add useful modifications.NotesThe interval shown 95% confidence interval expected values, want communicate experimental green-data. want use prediction model, want 95% prediction interval. ’m sure ggpubr can plot prediction interval. see plotted prediction interval Figure ?? , see Hidden Code section .arguments ggscatter function typed explicitly (x = \"temperarture\" just \"temperature\"). argument starts new line increase readability.+ ggscatter function adds additional layers plot. additional component started new line increase readability.first line stat_cor function (everything within aes()) plots \\(R^2\\) instead correlation coefficient \\(r\\). Copy pasting whole line just works.Comment line ggplot script starting stat_cor re-run (comment inserting # front line. consistent way triple-click line highlight line type command-shift-c Mac OS). script runs without error NULL added final plot component. Adding “NULL” useful trick.","code":"\nggscatter(data = fig2c,\n          x = \"temperature\",\n          y = \"transition_date\",\n          add = \"reg.line\",\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\")\nggscatter(data = fig2c,\n          x = \"temperature\",\n          y = \"transition_date\",\n          color = \"black\",\n          fill = \"red\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          conf.int = TRUE,\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\") +\n  \n  stat_regline_equation(size = 4,\n                        label.y = 340) +\n  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = \"~`,`~\")),\n           size = 4,\n           label.y = 335) +\n\n  NULL\nggscatter(data = efig_3d,\n          x = \"autumn_temp_anomaly\",\n          y = \"green_down_anomaly\",\n          color = \"black\",\n          fill = \"red\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          conf.int = TRUE,\n          xlab = \"Added Temperature (°C)\",\n          ylab = \"Day of Green-down (DOY)\")"},{"path":"regression.html","id":"ggplots","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.5.3 ggplots","text":"Use ggplot ggplot2 package full control plots. See Hidden Code generated plots Figure 9.1 .","code":""},{"path":"regression.html","id":"creating-a-table-of-predicted-values-and-95-prediction-intervals","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.2.6 Creating a table of predicted values and 95% prediction intervals","text":"efig_3d data.table created importing data Extended Data Figure 3d . fit model isThe predict(efig3d_m1) function used compute either fitted predicted values, either confidence prediction interval values.Unless specified newdata, default x-values used generate y predict(efig3d_m1) x-values data used fit model. returned values expected value \\(x_i\\) data. argument newdata passes data.frame (remember data.table data.frame!) new x-values. Since x-values data used fit model, returned \\(y_{hat}\\) predicted values.range x-values data isLet’s get predicted values value \\(autumn\\_temp\\_anomaly = 2\\).predicted values across range measured valuesAdd 95% confidence intervals (used create band plotting)Change 95% prediction intervalsPut together pretty table. ’ve used knitr’s kable function table packages R allow extensive control output.","code":"\nefig3d_m1 <- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\nrange(efig_3d[,autumn_temp_anomaly])## [1] -2.673793  2.568045\npredict(efig3d_m1, newdata = data.table(autumn_temp_anomaly = 2))##        1 \n## 3.192646\nnew_dt <- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2))\npredict(efig3d_m1, newdata = new_dt)##         1         2         3         4 \n## -3.192646 -1.596323  1.596323  3.192646\npredict(efig3d_m1, \n        newdata = new_dt,\n        interval = \"confidence\",\n        se.fit = TRUE)$fit##         fit        lwr        upr\n## 1 -3.192646 -5.2485713 -1.1367215\n## 2 -1.596323 -2.9492686 -0.2433778\n## 3  1.596323  0.2433778  2.9492686\n## 4  3.192646  1.1367215  5.2485713\npredict(efig3d_m1, \n        newdata = new_dt,\n        interval = \"prediction\",\n        se.fit = TRUE)$fit##         fit        lwr       upr\n## 1 -3.192646 -12.833739  6.448446\n## 2 -1.596323 -11.112325  7.919679\n## 3  1.596323  -7.919679 11.112325\n## 4  3.192646  -6.448446 12.833739\nefig3d_m1 <- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\nnew_dt <- data.table(autumn_temp_anomaly = c(-2, -1, 1, 2))\nprediction_table <- predict(efig3d_m1, \n                            newdata = new_dt,\n                            interval = \"prediction\",\n                            se.fit = TRUE)$fit\nprediction_table <- data.table(new_dt, prediction_table)\npretty_names <- c(\"Autumn Temp Anomaly\", \"Estimate\", \"2.5%\", \"97.5%\")\nsetnames(prediction_table, old = colnames(prediction_table), new = pretty_names)\nknitr::kable(prediction_table, digits = c(1, 1, 1, 1))"},{"path":"regression.html","id":"hidden-code-1","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.3 Hidden code","text":"","code":""},{"path":"regression.html","id":"import-and-plot-of-fig2c-ecosystem-warming-experimental-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.3.1 Import and plot of fig2c (ecosystem warming experimental) data","text":"ImportCreating response plot (bottom component)Creating effects plot (top component)Combining response effects plots single plot","code":"\ndata_from <- \"Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures\"\nfile_name <- \"41586_2018_399_MOESM3_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\nfig2 <- read_excel(file_path) %>% # import\n  clean_names() %>% # clean the column names\n  data.table() # convert to data.table\n# View(fig2)\n\nfig2c <- fig2[panel == \"2c\",]\n# View(fig2c)\nm1_b <- coef(m1)\nm1_ci <- confint(m1)\nm1_b0_text <- paste0(\"Intercept: \",\n                  round(m1_b[1],1),\n                  \" (\",\n                  round(m1_ci[1,1],1),\n                  \", \",\n                  round(m1_ci[1,2],1),\n                  \") d\")\nm1_b1_text <- paste0(\"Slope: \",\n                  round(m1_b[2],2),\n                  \" (\",\n                  round(m1_ci[2,1],2),\n                  \", \",\n                  round(m1_ci[2,2],2),\n                  \") d per 1 °C\")\n\n# regression line\nm1_x <- min(fig2c[, temperature])\nm1_xend <- max(fig2c[, temperature])\nm1_y <- m1_b[1] + m1_b[2]*m1_x\nm1_yend <- m1_b[1] + m1_b[2]*m1_xend\n\nfig2c_gg_response <- ggplot(data = fig2c,\n             aes(x = temperature,\n                y = transition_date)) +\n  \n  # regression line first, to not block point\n  geom_segment(x = m1_x,\n               y = m1_y,\n               xend = m1_xend,\n               yend = m1_yend) +\n  # create black edge to points\n  # geom_point(size = 4,\n  #            color = \"black\") +\n  geom_point(size = 3,\n             color = pal_okabe_ito[1]) +\n   scale_x_continuous(breaks = c(0, 2.25, 4.5, 6.75, 9)) +\n  xlab(\"Plot temperature (ΔT, °C)\") +\n  ylab(\"Autumn green-down (DOY)\") +\n  theme_pubr() +\n  \n  NULL\n\n# fig2c_gg_response\nm1_coeff_dt <- data.table(term = row.names(m1_coeff),\n                          data.table(m1_coeff))[2,] %>%\n  clean_names()\nm1_coeff_dt[ , p_pretty := pvalString(pr_t)]\n\nmin_bound <- min(m1_coeff_dt[, x2_5_percent])\nmax_bound <- min(m1_coeff_dt[, x97_5_percent])\n\ny_lo <- min(min_bound+min_bound*0.2,\n            -max_bound)\ny_hi <- max(max_bound + max_bound*0.2,\n            -min_bound)\ny_lims <- c(y_lo, y_hi)\n\nfig2c_gg_effect <- ggplot(data=m1_coeff_dt, \n                          aes(x = term,\n                              y = estimate)) +\n  # confidence level of effect\n  geom_errorbar(aes(ymin=x2_5_percent, \n                    ymax=x97_5_percent),\n                width=0, \n                color=\"black\") +\n  # estimate of effect\n  geom_point(size = 3) +\n  \n  # zero effect\n  geom_hline(yintercept=0, linetype = 2) +\n  \n  # p-value\n  annotate(geom = \"text\",\n           label = m1_coeff_dt$p_pretty,\n           x = 1,\n           y = 7.5) +\n  \n  # aesthetics\n  scale_y_continuous(position=\"right\") +\n  scale_x_discrete(labels = \"Temperature\\neffect\") +\n  ylab(\"Effects (day/°C)\") +\n  coord_flip(ylim = y_lims) + \n  theme_pubr() +\n  theme(axis.title.y = element_blank()) +\n  \n  NULL\n\n#fig2c_gg_effect\nfig3d_fig <- plot_grid(fig2c_gg_effect,\n                       fig2c_gg_response,\n                       nrow=2,\n                       align = \"v\",\n                       axis = \"lr\",\n                       rel_heights = c(0.4,1))\nfig3d_fig"},{"path":"regression.html","id":"import-and-plot-efig_3d-ecosysem-warming-observational-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.3.2 Import and plot efig_3d (Ecosysem warming observational) data","text":"ImportPlot","code":"\ndata_from <- \"Ecosystem warming extends vegetation activity but heightens vulnerability to cold temperatures\"\nfile_name <- \"41586_2018_399_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\nefig_3d <- read_excel(file_path,\n                   range = \"J2:K87\",\n                   col_names = FALSE) %>% # header causing issues\n  data.table() # convert to data.table\nsetnames(efig_3d, old = colnames(efig_3d), new = c(\"autumn_temp_anomaly\", \"green_down_anomaly\"))\n\n# View(efig_3d)\nm1 <- lm(green_down_anomaly ~ autumn_temp_anomaly, data = efig_3d)\n\n# get x for drawing slope\nminx <- min(efig_3d[,autumn_temp_anomaly])\nmaxx <- max(efig_3d[,autumn_temp_anomaly])\nnew_x <- seq(minx, maxx, length.out = 20)\nnew_data <- data.table(autumn_temp_anomaly = new_x)\nnew_data[, yhat := predict(m1, newdata = new_data)]\nnew_data[, conf_lwr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"confidence\",\n                           newdata = new_data)$fit[, \"lwr\"]]\nnew_data[, conf_upr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"confidence\",\n                           newdata = new_data)$fit[, \"upr\"]]\nnew_data[, pred_lwr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"prediction\",\n                           newdata = new_data)$fit[, \"lwr\"]]\nnew_data[, pred_upr := predict(m1, \n                           se.fit = TRUE,\n                           interval = \"prediction\",\n                           newdata = new_data)$fit[, \"upr\"]]\n\ngg <- ggscatter(data = efig_3d,\n          x = \"autumn_temp_anomaly\",\n          y = \"green_down_anomaly\",\n          color = \"black\",\n          shape = 21,\n          size = 3,\n          add = \"reg.line\",\n          add.params = list(color = \"steelblue\",\n                            fill = \"lightgray\"),\n          xlab = \"Temperature Anomaly (°C)\",\n          ylab = \"Day of Green-down Anomaly (DOY)\") +\n\n  geom_ribbon(data = new_data,\n              aes(ymin = pred_lwr,\n                  ymax = pred_upr,\n                  y = yhat,\n                  fill = \"band\"),\n              fill = \"gray\",\n              alpha = 0.3) +\n  \n  geom_ribbon(data = new_data,\n              aes(ymin = conf_lwr,\n                  ymax = conf_upr,\n                  y = yhat,\n                  fill = \"band\"), alpha = 0.3) +\n  \n  stat_cor(aes(label = paste(..rr.label.., ..p.label.., sep = \"~`,`~\")),\n           size = 4,\n           label.y = 10) +\n  \n  scale_fill_manual(values = pal_okabe_ito) +\n\n  theme(legend.position=\"none\") +\n\n  NULL  \n\ngg"},{"path":"regression.html","id":"import-and-plot-of-fig1f-methionine-restriction-data","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.3.3 Import and plot of fig1f (methionine restriction) data","text":"ImportCreating response plot (bottom component)Creating effects plot (top component)Combining response effects plots single plot","code":"\ndata_from <- \"Dietary methionine influences therapy in mouse cancer models and alters human metabolism\"\nfile_name <- \"41586_2019_1437_MOESM2_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nfig1f_wide <- read_excel(file_path,\n                    sheet = \"f\",\n                    range = \"B7:R12\",\n                    col_names = FALSE) %>%\n  data.table() # convert to data.table\n\nsetnames(fig1f_wide,\n         old = colnames(fig1f_wide),\n         new = c(\"day\",\n                 paste0(\"Cn_\", 1:8),\n                 paste0(\"MR_\", 1:8)))\n\nfig1f_wide <- transpose(fig1f_wide, make.names = 1, keep.names = \"id\")\nfig1f_wide[, treatment := factor(substr(id, 1, 2))]\n\ndays <- c(21, 25, 28, 30, 34, 39)\nfig1f <- melt(fig1f_wide,\n              id.vars <- c(\"treatment\", \"id\"),\n              measure.vars <- as.character(days),\n              variable.name = \"day\",\n              value.name = \"volume\")\nfig1f[, day := as.numeric(as.character(day))]\n# View(fig1f)\n# qplot(x = day, y = volume, color = treatment, data = fig1f) + geom_line(aes(group = id))\nfig1f_gg_response <- ggplot(data = fig1f,\n                      aes(x = day, y = volume, color = treatment)) +\n  geom_point() +\n  geom_smooth(aes(group = id), method = \"lm\", se = FALSE) +\n  xlab(\"Day\") +\n  ylab(expression(Tumor~Volume~(mm^3))) +\n  scale_color_manual(values = pal_okabe_ito) +\n  theme_pubr() +\n  theme(\n    legend.position = c(.15, .98),\n    legend.justification = c(\"right\", \"top\"),\n    legend.box.just = \"right\",\n    legend.margin = margin(6, 6, 6, 6),\n    legend.title = element_blank()\n    ) +\n\n  NULL\n\n# fig1f_gg_response\nm1_coeff_dt <- data.table(term = row.names(m1_coef_table),\n                          data.table(m1_coef_table))[2,] %>%\n  clean_names()\nm1_coeff_dt[ , p_pretty := pvalString(pr_t)]\n\nmin_bound <- min(m1_coeff_dt[, x2_5_percent])\nmax_bound <- min(m1_coeff_dt[, x97_5_percent])\n\ny_lo <- min(min_bound+min_bound*0.2,\n            -max_bound)\ny_hi <- max(max_bound + max_bound*0.2,\n            -min_bound)\ny_lims <- c(y_lo, y_hi)\n\nfig1f_gg_effect <- ggplot(data=m1_coeff_dt, \n                          aes(x = term,\n                              y = estimate)) +\n  # confidence level of effect\n  geom_errorbar(aes(ymin=x2_5_percent, \n                    ymax=x97_5_percent),\n                width=0, \n                color=\"black\") +\n  # estimate of effect\n  geom_point(size = 3) +\n  \n  # zero effect\n  geom_hline(yintercept=0, linetype = 2) +\n  \n  # p-value\n  annotate(geom = \"text\",\n           label = m1_coeff_dt$p_pretty,\n           x = 1,\n           y = 7.5) +\n  \n  # aesthetics\n  scale_y_continuous(position=\"right\") +\n  scale_x_discrete(labels = \"MR\\neffect\") +\n  ylab(expression(Growth~(mm^3/day))) +\n  coord_flip(ylim = y_lims) + \n  theme_pubr() +\n  theme(axis.title.y = element_blank()) +\n  \n  NULL\n\n#fig1f_gg_effect\nfig1f_gg <- plot_grid(fig1f_gg_effect,\n                      fig1f_gg_response,\n                      nrow=2,\n                      align = \"v\",\n                      axis = \"lr\",\n                      rel_heights = c(0.4,1))\nfig1f_gg"},{"path":"regression.html","id":"try-it","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.4 Try it","text":"","code":""},{"path":"regression.html","id":"a-prediction-model-from-the-literature","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.4.1 A prediction model from the literature","text":"data come top, middle plot Figure 1e ofParker, B.L., Calkin, .C., Seldin, M.M., Keating, M.F., Tarling, E.J., Yang, P., Moody, S.C., Liu, Y., Zerenturk, E.J., Needham, E.J. Miller, M.L., 2019. integrative systems genetic analysis mammalian lipid metabolism. Nature, 567(7747), pp.187-193.Public sourceSource dataThe researchers built prediction models hybrid mouse diversity panel (HMDP) predict liver lipid levels measured plasma lipid levels mice humans. value predictor (\\(X\\)) variable individual measured value single plasma lipid predicted value, score, prediction model based entire panel lipid measurements individual. \\(Y\\) variable individual total measured level across family lipids (ceramides, triacylglycerols, diacylglycerols) liver individual. question , well prediction score predict actual liver level?Use data TG (triacylglycerols) (top, middle plot Figure 1e). column D “score” prediction model using plasma lipid levels. \\(X\\) variable (column header “fitted.total.Cer”). column E total measured liver TG, \\(Y\\) variable.Fit linear model y ~ x.Create publication-quality plot liver TG (Y-axis) score (X-axis) – researchers labeled “fitted.total.Cer”. Include \\(R^2\\) plot.Advanced – add 95% prediction interval plot (template code Hidden Code section efig3d)Create table expected liver TG 95% prediction interval liver TG score values (13.5, 14, 14.5, 15, 15.5, 16).Comment predictability liver TG using plasma scores.`","code":""},{"path":"regression.html","id":"intuition-pumps","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.5 Intuition pumps","text":"","code":""},{"path":"regression.html","id":"correlation-and-r2","chapter":"9 Linear models with a single, continuous X (“regression”)","heading":"9.5.1 Correlation and $R^2","text":"code belowchange value n explore effect variability estimate. Look variability, magnitude SE, magnitude p-value.change value beta_1 explore effect different slopes correlations look like. Notice variance fixed (simulation) expected slope expected correlation equal. (’ve fixed variance simple simulation, code fail abs(beta_1) >= 1).","code":"\nn <- 100 # choose between 3 and 10^5\nbeta_1 <- 0.6 # choose a value between -0.99 and 0.99\nx <- rnorm(n)\ny <- beta_1*x + sqrt(1-beta_1^2)*rnorm(n)\nm1 <- lm(y ~ x)\nslope <- paste(\"b_1: \", round(coef(m1)[2], 3))\nse <- paste(\"SE: \", round(coef(summary(m1))[2,2], 3))\nr <- paste(\"r: \", round(cor(x,y), 3))\nr2 <- paste(\"R^2: \", round(summary(m1)$r.squared, 3))\nggscatter(data = data.table(x=x, y=y), x = \"x\", y = \"y\",\n          add = \"reg.line\") +\n  ggtitle(label = paste(slope,se,r,r2,sep=\";  \"))"},{"path":"oneway.html","id":"oneway","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","text":"traditional hypothesis-testing strategy, researchers use t-test factor variable two groups, ANOVA followed post-hoc tests factor variable two groups. linear-modeling strategy, fit model, regardless number groups.","code":""},{"path":"oneway.html","id":"oneway-data","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1 A linear model with a single factor (categorical Xvariable) estimates the effects of the levels of factor on the response","text":"introduce linear model single factor (categorical \\(X\\) variable), ’ll use data set experiments designed measure effect lipid 12,13-diHOME brown adipose tissue (BAT) thermoregulation mechanism effect.Lynes, M.D., Leiria, L.O., Lundh, M., Bartelt, ., Shamsi, F., Huang, T.L., Takahashi, H., Hirshman, M.F., Schlein, C., Lee, . Baer, L.., 2017. cold-induced lipokine 12, 13-diHOME promotes fatty acid transport brown adipose tissue. Nature medicine, 23(5), pp.631-637.Public sourceData sourceDownload source data files move new folder named “cold-induced lipokine 12,13-diHOME promotes fatty acid transport brown adipose tissue”.Cold temperature neurotransmitter/hormone norepinephrine known stimulate increased thermogenesis BAT cells. project, researchers probed question “pathway mediates effect cold-exposure BAT thermogenesis?”. “discovery” component project, researchers measured plasma levels 88 lipids known signaling properties humans exposed one hour normal (20 °C) cold temperature (14 °C) temperature. 88 lipids, 12,13-diHOME largest response cold treatment. researchers followed experiments mice.","code":""},{"path":"oneway.html","id":"oneway-example1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1 Example 1 (fig3d) – two treatment levels (“groups”)","text":"Let’s start experiment Figure 3d, designed measure effect 12,13-diHOME plasma triglyceride level. 12,13-diHOME stimulates BAT activity, levels 12,13-diHOME mice less levels control mice.","code":""},{"path":"oneway.html","id":"step-1-understand-the-experiment-design-and-the-focal-comparisons","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.1 Step 1 – Understand the experiment design and the focal comparisons","text":"Design: single, categorical XResponse variable: \\(\\texttt{serum_tg}\\), measure serum triglycerides (mg/dl). \\(\\texttt{serum_tg}\\) continuous variable.Factor variable: \\(\\texttt{treatment}\\), levels:“Vehicle” – injected saline; negative control giving expected response given handling injection, 12,13-diHOME“12,13-diHOME”Contrasts interest12,13-diHOME - Vehicle. Estimates effect 12,13-diHOME treatment. focal contrast (contrast).","code":""},{"path":"oneway.html","id":"step-2-import","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.2 Step 2 – import","text":"Open data , necessary, wrangle analyzable format. script import data section Hidden code .","code":""},{"path":"oneway.html","id":"step-3-inspect-the-data","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.3 Step 3 – inspect the data","text":"second step examine data toget sense sample size balancecheck biologically implausible outliers suggest measurement failure, transcription error (notebook, cell)assess outliers outlier strategy robust analysisassess reasonable distributions models analysis.obviously implausible data points. normal distribution model good, reasonable start. can checked thoroughly fitting model.","code":"\nggstripchart(data = fig3d,\n          x = \"treatment\",\n          y = \"serum_tg\",\n          add = c(\"mean_sd\")\n       )"},{"path":"oneway.html","id":"step-4-fit-the-model","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.4 Step 4 – fit the model","text":"","code":"\nfig3d_m1 <- lm(serum_tg ~ treatment, data = fig3d)"},{"path":"oneway.html","id":"step-5-check-the-model","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.5 Step 5 – check the model","text":"Q-Q plot indicates distribution residuals well within expected normal sample cause concern inference. spread-location plot shows conspicuous trend spread changes conditonal mean. cause concern inference.Write something like .Rmd file following model check code chunk:“residuals well within range expected sampling Normal distribution. heterogeneity residuals well within range expected sampling single distribution.”","code":"\nset.seed(1)\n# qqPlot(fig3d_m1, id=FALSE)\n# spreadLevelPlot(fig3d_m1, id=FALSE)\nggcheck_the_model(fig3d_m1)"},{"path":"oneway.html","id":"step-6-inference","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.6 Step 6 – inference","text":"","code":""},{"path":"oneway.html","id":"coefficient-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.6.1 coefficient table","text":"","code":"\nfig3d_m1_coef <- cbind(coef(summary(fig3d_m1)),\n                        confint(fig3d_m1))\nkable(fig3d_m1_coef, digits = c(1,2,1,4,1,1)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"emmeans-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.6.2 emmeans table","text":"","code":"\nfig3d_m1_emm <- emmeans(fig3d_m1, specs = \"treatment\")\nkable(fig3d_m1_emm, digits = c(1,1,2,0,1,1)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"contrasts-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.6.3 contrasts table","text":"","code":"\nfig3d_m1_pairs <- contrast(fig3d_m1_emm,\n                            method = \"revpairwise\") %>%\n  summary(infer = TRUE)\n\nkable(fig3d_m1_pairs, digits = c(1,1,1,0,1,1,2,4)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"step-6-plot-the-model","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.7 Step 6 – plot the model","text":"norm bench biology research response plot.researchers want explitly communicate treatment effect, “plot model”.","code":"\ngg1 <- ggplot_the_response(\n  fig3d_m1,\n  fig3d_m1_emm,\n  fig3d_m1_pairs,\n  legend_position = \"none\",\n  y_label = \"Serum TG (µg/dL)\",\n  palette = pal_okabe_ito_blue\n)\ngg1\ngg2 <- ggplot_the_model(\n  fig3d_m1,\n  fig3d_m1_emm,\n  fig3d_m1_pairs,\n  legend_position = \"none\",\n  y_label = \"Serum TG (µg/dL)\",\n  effect_label = \"Effects (µg/dL)\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.5,1)\n)\ngg2"},{"path":"oneway.html","id":"step-7-report-the-model-results","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.1.8 Step 7 – report the model results","text":"Different ways reporting results increasing order making claims evidenced statistical analysis,“12,13-diHOME reduced serum TG”“12,13-diHOME reduced serum TG (Estimate = -7.17 µg/dL; 95% CI: -12.4, -1.9; p = 0.012)”“estimated effect 12,13-diHOME serum TG -7.17 µg/dL (95% CI: -12.4, -1.9, \\(p = 0.012\\)).”Don’t “12,13-diHOME significantly reduced serum TG (\\(p = 0.012\\))”problematic: Significance applies p-value effect. English usage, “significant” means “large” “important” p-value good evidence either size effect importance effect (see p-value chapter). interpret size effect estimated effect size CI importance effect knowledge physiological consequences TG reduction range CI.","code":""},{"path":"oneway.html","id":"understanding-the-analysis-with-two-treatment-levels","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2 Understanding the analysis with two treatment levels","text":"variable \\(\\texttt{treatment}\\) Figure 3d mouse experiment, single, categorical \\(X\\) variable. linear model, categorical variables called factors. \\(\\texttt{treatment}\\) can take two different values, “Vehicle” “12,13-diHOME”. different values factor factor levels (just “levels”). “Levels” strange usage word; less formal name levels “groups”. Nominal categorical factor, levels units unordered, even variable based numeric measurement. example, might design experiment mice randomly assigned one three treatments: one hour 14 °C, one hour 18 °C, one hour 26 °C. model treatment nominal categorical factor, simply three levels. certainly choose arrange levels meaningful way plot, analysis , levels units order. Ordinal categorical factors levels ordered information relative distance. treatment 18 °C similar 14 °C 26 °C. Nominal categorical factors default R factors analyzed text.","code":""},{"path":"oneway.html","id":"linear-models-are-regression-models","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.1 Linear models are regression models","text":"linear model fit serum TG data \\[\\begin{align}\nserum\\_tg &= treatment + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\tag{10.1}\n\\end{align}\\]notation potentially confusing variable \\(\\texttt{treatment}\\) factor containing words “Vehicle” “12,13-diHOME” numbers. linear model (10.1) can specified using notation regression model using\\[\\begin{align}\nserum\\_tg &= \\beta_0 + \\beta_1 treatment_{12,13-diHOME} + \\varepsilon\\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\tag{10.2}\n\\end{align}\\]Model (10.2) regression model \\(treatment_{12,13-diHOME}\\) variable \\(\\texttt{treatment}\\), containing words “Vehicle” “12,13-diHOME” numeric variable indicates membership group “12,13-diHOME”. variable contains number 1 mouse belongs “12,13-diHOME” number 0 mouse doesn’t belong “12,13-diHOME”. \\(treatment_{12,13-diHOME}\\) known indicator variable indicates group membership. several ways coding indicator variables way described called dummy treatment coding. Dummy-coded indicator variables sometimes called dummy variables.lm function creates indicator variables table, something called model matrix.columns model matrix names model terms fit model. R names dummy variables combining names factor name level within factor. \\(X\\) variable R creates model matrix fit linear model model (10.2) \\(treatment12,13-diHOME\\). can see names terms coefficient table fit model.alternatives dummy coding creating indicator variables. Dummy coding default R makes sense thinking experimental data obvious control level. also like interpretation “interaction effect” using Dummy coding. classical coding ANOVA deviation effect coding, creates coefficients deviations grand mean. contrast R, Deviation coding default many statistical software packages including SAS, SPSS, JMP. method coding can make difference ANOVA table. Watch – ’ve found several published papers researchers used default dummy coding interpreted ANOVA table used deviation coding. getting ahead somewhat moot, don’t advocate reporting ANOVA tables.Recall stats 101 slope \\(X\\) model \\(Y = b_0 + b_1 X\\) \\(b_1 = \\frac{\\textrm{COV}(X,Y)}{\\textrm{VAR}(X)}\\). can generalized using equation\\[\\begin{equation}\n\\mathbf{b} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\end{equation}\\]\\(\\mathbf{X}\\) model matrix containing column intercept, columns indicator variables, columns numeric covariates. \\(\\mathbf{b}\\) vector containing model coefficients, including intercept first element. first part RHS (\\(\\mathbf{X}^\\top \\mathbf{X}\\)) matrix “sums squares cross-products” columns \\(\\mathbf{X}\\). Dividing element matrix \\(N-1\\) gives us covariance matrix \\(\\mathbf{X}\\), contains variances \\(X\\) columns along diagonal, component role denominator stats 101 equation. Matrix algebra doesn’t division, inverse matrix multiplied second part. second part RHS (\\(\\mathbf{X}^\\top \\mathbf{y}\\)) vector containing cross-products column \\(\\mathbf{X}\\) \\(\\mathbf{y}\\). Dividing element vector \\(N-1\\) gives us covariances \\(X\\) \\(y\\), component role numerator stats 101 equation.Self-learning. lm fits model y ~ X X model matrix. Fit model using standard formula model using model matrix. coefficient table .m1 <- lm(serum_tg ~ treatment, data = fig3d)X <- model.matrix(~ treatment, data = fig3d)m2 <- lm(serum_tg ~ X, data = fig3d)coef(summary(m1))coef(summary(m2))","code":"\nX <- model.matrix(~ treatment, data = fig3d)\nN <- nrow(X)\nX[1:N,]##    (Intercept) treatment12,13-diHOME\n## 1            1                     0\n## 2            1                     0\n## 3            1                     0\n## 4            1                     0\n## 5            1                     0\n## 6            1                     0\n## 7            1                     1\n## 8            1                     1\n## 9            1                     1\n## 10           1                     1\n## 11           1                     1\n## 12           1                     1"},{"path":"oneway.html","id":"the-estimates-in-the-coefficient-table-are-estimates-of-the-parameters-of-the-linear-model-fit-to-the-data.","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.2 The Estimates in the coefficient table are estimates of the parameters of the linear model fit to the data.","text":"row names coefficient table column names model matrix. model terms. two terms (two rows) two parameters regression model (10.2). values column \\(\\texttt{Estimate}\\) coefficient table estimates regression parameters \\(\\beta_0\\) \\(\\beta_1\\). estimates coefficients fit model, \\(b_0\\) \\(b_1\\).","code":""},{"path":"oneway.html","id":"oneway-what-coefs-are","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.3 The coefficients of a linear model using dummy coding have a useful interpretation","text":"\nTable 10.1: Understanding model coefficients linear model single treatment variable two groups. means interpretation column conditional means.\nimportant understand interpretation coefficients fit linear model (10.1) (Table 10.1).coefficient \\(b_0\\) conditional mean response reference level, “Vehicle”. Remember conditional mean mean group value one \\(X\\) variables.coefficient \\(b_1\\) difference conditional means “12,13-diHOME” level reference (“Vehicle”) level:\\[\\begin{equation}\n\\mathrm{E}[serum\\_tg|treatment = \\texttt{\"12,13-diHOME\"}] - \\mathrm{E}[serum\\_tg|treatment = \\texttt{\"Vehicle\"}]\n\\end{equation}\\]additional covariates model, difference equal difference sample means \\(\\bar{Y}_{12,13-diHOME} - \\bar{Y}_{Vehicle}\\). direction difference important – non-reference level minus reference level.estimate \\(b_1\\) effect interested . Specifically, measured effect 12,13-diHOME serum TG. inject 12,13-diHOME, find mean serum TG decreased -7.2 µg/dL relative mean serum TG mice injected saline. Importantly, reference level property experiment set whomever analyzing data. Since non-reference estimates differences means, often makes sense set “control” treatment level reference level.Many beginners mistakenly memorize coefficient \\(b_1\\) equal mean non-reference group (“12,13-diHOME”). Don’t . regression model, \\(b_0\\) mean. coefficient \\(b_1\\) model (10.2) difference means.\nFigure 10.1: coefficients linear model single categorical X mean. means two treatment levels serum TG data shown large, filled circles dashed lines. intercept (\\(b_0\\)) mean reference treatment level (“Vehicle”). coefficient \\(b_1\\) difference treatment level’s mean reference mean. linear model continuous \\(X\\), coefficient \\(b_1\\) effect.\ngeometric interpretation coefficients illustrated Figure 10.1. \\(b_0\\) conditional mean reference level (“Vehicle”) estimate \\(\\beta_0\\), true, conditional mean population. \\(b_1\\) difference conditional means first non-reference level (“12,13-diHOME”) reference level (“Vehicle”) estimate \\(\\beta_1\\), true difference conditional means population without treatment 12,13-diHOME.tl;dr. population? experimental biology examples text, might consider population idealized, infinitely large set mice, fish, fruit flies, communities sample reasonably representative subset. experiments 12,13-diHOME study, population might conceived hypothetical, infinitely large set 12-week-old, male, C57BL/6J mice, raised mouse facility Joslin Diabetes Center. even abstract way way think population infinitely large set values generated linear model.","code":""},{"path":"oneway.html","id":"better-know-the-coefficient-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.4 Better know the coefficient table","text":"\\(\\texttt{(Intercept)}\\) row contains statistics \\(b_0\\) (estimate \\(\\beta_0\\)). Remember \\(b_0\\) conditional mean reference treatment (“Vehicle”).\\(\\texttt{treatment12,13-diHOME}\\) row contains statistics \\(b_1\\) (estimate \\(\\beta_1\\)). Remember \\(b_1\\) difference conditional means groups “12,13-diHOME” “Vehicle”.column \\(\\texttt{Estimate}\\) contains model coefficients, estimates parameters.column \\(\\texttt{Std. Error}\\) contains model SEs coefficients. SE \\(\\texttt{(Intercept)}\\) standard error mean (SEM). SE \\(\\texttt{treatment12,13-diHOME}\\) standard error difference (SED).column \\(\\texttt{t value}\\) contains test statistic coefficients. value ratio \\(\\frac{Estimate}{SE}\\). model, interested test statistic \\(b_1\\). Effectively, never interested test statistic \\(b_0\\) mean group never zero.column \\(\\texttt{Pr(>|t|)}\\) contains p-values test statistic coefficients. model, models text, interested p-value non-intercept coefficients.columns \\(\\texttt{2.5 %}\\) \\(\\texttt{97.5 %}\\) contain lower upper limits 95% confidence interval estimate.","code":"\nfig3d_m1_coef %>%\n  kable(digits = c(1,2,1,4,1,1)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"the-emmeans-table-is-a-table-of-modeled-means-and-inferential-statistics","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.5 The emmeans table is a table of modeled means and inferential statistics","text":"\n(#tab:fig3d_m1_emm_kable)Estimated marginal means table model fig3d_m1.\ntable marginal means model fit fig3d serum TG data (Model (10.1)) given Table @ref(tab:fig3d_m1_emm_kable). table marginal means gives **modeled* mean, standard error confidence interval specified groups. test-statistic p-value significance test. text, ’ll refer table “emmeans table”, since output emmeans function (“em” abbreviation “estimated marginal”). ’ll use “modeled means” refer means estimate means fit linear model.marginal mean mean set conditional means. example, treatment factor three levels, conditional means means level marginal mean mean three means. , conditional means expected values given continous covariate, marginal mean expected value mean covariate. specified emmeans table fig3d data exciting simply contains conditional means – values marginalized \\(X\\). emmeans table contains different sorts means (conditional, marginal, adjusted), text generally refer means table “modeled means”.\nTable 10.2: emmeans table contains modeled means, SE, CIs.\n\nTable 10.3: summary table contains sampled means, SE, CIs.\nmeans emmeans table modeled means. , many linear models, equal sampled means. case models one continuous covariates, marginal means explicitly specified.Unlike modeled means, modeled standard errors confidence intervals , effectively, never equal sample standard errors confidence intervals. many models, plots using sample statistics can lead deceiving inference differences groups. text advocates plotting model – using modeled means confidence intervals plots.exceptionally important understand difference means, SEs, CIs emmeans table statistics name summary table data.statistics summary table sampled means, SEs, CIs – statistics computed group using data group.understand modeled SEs CIs, recall standard error sample mean \\(\\frac{s}{\\sqrt{n}}\\), \\(s\\) sample standard deviation \\(n\\) sample size group. computation SE emmeans table uses equation, except numerator sample standard deviation group model standard deviation, estimate true standard deviation \\(\\sigma\\). sample SE, denominator modeled SE sample size \\(n\\) group. Since numerator modeled SE groups, modeled SE groups sample size, seen marginal means table model fit Figure 3d data. true sampled SEs, since sampled standard deviations always differ.may seem odd use common standard deviation computation modeled SEs. . Remember assumption linear model homogeneity variances – residuals \\(e_i\\) drawn distribution (\\(N(0, \\sigma^2)\\)) (“single hat”) regardless group. model standard deviation \\(\\hat{\\sigma}\\) estimate square root variance distribution. Given interpretation, useful think sample standard deviation estimate \\(\\sigma\\) (linear model assumes differences among sample standard deviations due entirely sampling). model standard deviation precise estimate \\(\\sigma\\) since computed larger sample (\\(N\\) residuals).model standard deviation called “pooled” standard deviation ANOVA literature computed sample-size weighted average sample standard deviations.modeled standard error mean uses estimate \\(\\sigma\\) fit model. estimate \\[\\begin{equation}\n\\hat{\\sigma} = \\sqrt{\\frac{\\sum{(y_i - \\hat{y}_i)^2}}{df}}\n\\end{equation}\\]Create code chunk computes . Recall \\((y_i - \\hat{y}_i)\\) set residuals model, can extracted using residuals(fit) “fit” fit model object. \\(df\\) model degrees freedom, \\(N-k\\), \\(N\\) total sample size \\(k\\) number parameters fit. makes sense – sample variance one parameter fit, mean group. model fig3d_m1, two parameters fit, intercept coefficient treatment12,13-diHOME.","code":""},{"path":"oneway.html","id":"estimates-of-the-effects-are-in-the-contrasts-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.6 Estimates of the effects are in the contrasts table","text":"table important reporting treatment effects CIs plotting model. contrast difference means. two treatment levels, table contrasts doesn’t give information coefficient table – single contrast coefficient \\(b_1\\) coefficient table. Nevertheless, advocate computing table stay consistent script (function) plot model uses table coefficient table.value column \\(\\texttt{Estimate}\\) mean non-reference group (“12,13-diHOME”) minus mean reference group (“Vehicle”).value “SE” column standard error difference (SED), specifically difference estimate column. SE computed using model standard deviation \\(\\sigma\\).values “lower.CL” “upper.CL” columns bounds 95% confidence interval estimate. Remember (Chapter ??) think interval containing potential values true parameter (true difference means two groups) reasonably compatible data. Don’t think interval 95% probability containing true effect. Remember confidence interval applies procedure parameter – 95% CIs hypothetical, replicate experiments meet assumptions used compute CI include true effect.columns “t.ratio” “p.value” contains t p values significance (hypothesis!) test estimate. t-statistic ratio estimate SE estimate (use console confirm given values table). signal (estimate) noise (SE estimate) ratio. p-value probability sampling normal distribution observed standard deviation, randomly assigning sampled values either “Vehicle” “12,13-diHOME”, fitting linear model, observing t-value extreme observed t. small p-value consistent experiment “sampling distributions mean” – meaning adding treatment affects mean distribution. logic used infer treatment effect. Unfortunately, also consistent experiment approximating conditions model, including non-random assignment, non-independence, non-normal conditional responses, variance heterogeneity. rigorous researcher sure model conditions approximated “good enough” use p-value infer treatment effect mean.","code":""},{"path":"oneway.html","id":"t-and-p-from-the-contrasts-table-when-there-are-only-two-levels-in-x-are-the-same-as-t-and-p-from-a-t-test","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.2.7 t and p from the contrasts table – when there are only two levels in \\(X\\) – are the same as t and p from a t-test","text":"Comparecoefficient table:contrast table:t-test:NotesThe default t.test R Welch t-test heterogenous variance. compute Student t-test, use var.equal = TRUE.“statistic” t-test output contains t-value t-test. precisely t-statistic coefficient table contrast table.p-values three tables precisely .t p values t-test linear model, t-test specific case linear model. Reasons abandon classic t-tests learn linear modeling strategy includeA linear modeling strategy encourages researchers think effect uncertainty effect just p-value.linear model nearly infinitely flexible expandible t-test limited variations.rarely reason ever use t.test() function. Throw function away. Ignore web pages teach use . t-test easy learn, encourages overuse. tool t-test, every problem looks like comparison two-means.","code":"\nm1 <- lm(serum_tg ~ treatment, data = fig3d)\ncoef(summary(m1)) %>%\n  kable() %>%\n  kable_styling()\nemmeans(m1, specs = \"treatment\") %>%\n  contrast(method = \"revpairwise\") %>%\n  kable() %>%\n  kable_styling()\nm2 <- t.test(fig3d[treatment == \"12,13-diHOME\", serum_tg],\n             fig3d[treatment == \"Vehicle\", serum_tg],\n             var.equal = TRUE)\nglance(m2) %>% # glance is from the broom package\n  kable() %>%\n  kable_styling()"},{"path":"oneway.html","id":"oneway-example2","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3 Example 2 – three treatment levels (“groups”)","text":"","code":""},{"path":"oneway.html","id":"understand-the-experiment-design","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.1 Understand the experiment design","text":"data come experiment reported Figure 2a 12,13-diHOME article described . experiment designed probe hypothesis 12,13-diHOME mediator known stimulators increased BAT activity (exposure cold temperature sympathetic nervous system activation). Mice assigned control (30 °C), one-hour exposure 4 °C, 30 minute norepinephrine (NE) treatment level (NE neurotransmitter sympathetic neurons targeting peripheral tissues).design: single, categorical X three levels.response variable: \\(\\texttt{diHOME}\\), serum concentration 12,13-diHOME. continuous variable.factor variable: \\(\\texttt{treatment}\\), levels:“Control” – negative control. expect diHOME low relative two treated conditions.“Cold” – focal treatment 1. Given working model 12,13-diHome mediator stimulation BAT, response relatively high compared Control. archived data, group “1 hour cold”.“NE” – focal treatment 2. Given working model 12,13-diHome mediator stimulation BAT, response relatively high compared Control. archived data, group “30 min NE”.planned contrastsCold - Control – diHOME mediator cold, difference positive.NE - Control – diHOME mediator NE, difference positive.contrast Cold - NE interest.","code":""},{"path":"oneway.html","id":"fig2a-m1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.2 fit the model","text":"","code":"\nfig2a_m1 <- lm(diHOME ~ treatment, data = fig2a)"},{"path":"oneway.html","id":"check-the-model","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.3 check the model","text":"Q-Q plot indicates distribution residuals within expected normal sample. spread-location plot shows conspicuous trend spread changes conditonal mean. little cause concern inference linear model.Write something like .Rmd file following model check code chunk:“residuals within range expected sampling Normal distribution. heterogeneity residuals well within range expected sampling single distribution.”","code":"\nset.seed(1)\nggcheck_the_model(fig2a_m1)"},{"path":"oneway.html","id":"inference-from-the-model-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.4 Inference from the model","text":"","code":""},{"path":"oneway.html","id":"coefficient-table-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.4.1 coefficient table","text":"","code":"\nfig2a_m1_coef <- cbind(coef(summary(fig2a_m1)),\n                        confint(fig2a_m1))\nfig2a_m1_coef %>%\n  kable(digits = c(1,2,1,4,1,1)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"emmeans-table-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.4.2 emmeans table","text":"","code":"\nfig2a_m1_emm <- emmeans(fig2a_m1, specs = \"treatment\")\n\nfig2a_m1_emm %>%\n  kable(digits = c(1,1,2,0,1,1)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"contrasts-table-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.4.3 contrasts table","text":"","code":"\nfig2a_m1_planned <- contrast(fig2a_m1_emm,\n                           method = \"trt.vs.ctrl\",\n                           adjust = \"none\",\n                           level = 0.95) %>%\n  summary(infer = TRUE)\n\nfig2a_m1_planned %>%\n  kable(digits = c(1,1,1,0,1,1,2,4)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"plot-the-model","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.5 plot the model","text":"","code":"\nggplot_the_model(\n  fig2a_m1,\n  fig2a_m1_emm,\n  fig2a_m1_planned,\n  legend_position = \"none\",\n  y_label = \"12,13-diHOME (pmol/mL)\",\n  effect_label = \"Effects (pmol/mL)\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.5,1)\n)"},{"path":"oneway.html","id":"report-the-model-results","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.3.6 Report the model results","text":"Different ways reporting results increasing order making claims evidenced statistical analysis,“NE exposure increased 12,13-diHOME effect cold exposure 12,13-diHOME clear”.“estimated effect NE exposure consistent NE-induced increase 12,13-diHOME estimated effect cold exposure less clear”.“estimated effect NE exposure consistent NE-induced increase 12,13-diHOME (Estimate = 14.8 pmol/mL; 95% CI: 5.4, 24.1; \\(p = 0.004\\)) estimated effect cold exposure less clear (Estimate = 7.1 pmol/mL; 95% CI: -2.7, 16.9; \\(p = 0.14\\))”.first statement makes definitive claim NE causes increase unreasonable probability magnitude effect occurring random sampling. addition experiment infected experiment implementation decisions make p-value unreliable. second statement makes tentative claim. third statement adds statistics provide evidence tentative claim. statistics moved combination figure, figure caption, supplement.Don’t thisNE exposures significantly increased 12,13-diHOME (\\(p = 0.004\\))”effect cold exposure 12,13-diHOME (\\(p = 0.14\\))”first statement problematic: Significance applies p-value effect. English usage, “significant” means “large” “important” p-value good evidence either size effect importance effect (see p-value chapter). interpret size effect estimated effect size CI importance effect knowledge physiological consequences TG reduction range CI.second statement problematic: p > 0.05, high p-value generally, evidence effect p-value (1 - p) give probability treatment effect zero. One use equivalence test give [probability effect less physiologically meaningful magnitude][https://journals.sagepub.com/doi/abs/10.1177/1948550617697177]","code":""},{"path":"oneway.html","id":"understanding-the-analysis-with-three-or-more-treatment-levels","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4 Understanding the analysis with three (or more) treatment levels","text":"","code":""},{"path":"oneway.html","id":"better-know-the-coefficient-table-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4.1 Better know the coefficient table","text":"fit regression model data Figure 2a \\[\\begin{equation}\ndiHOME_i = b_0 + b_1 treatment_{Cold,} + b_2 treatment_{NE,} + e_i\n\\tag{10.3}\n\\end{equation}\\]coefficients model \\(\\texttt{Estimate}\\) column coefficient table.\\(\\texttt{(Intercept)}\\) row contains statistics \\(b_0\\) (estimate \\(\\beta_0\\)). , \\(b_0\\) mean reference group, “Control”.\\(\\texttt{treatmentCold}\\) row contains statistics \\(b_1\\) (estimate \\(\\beta_1\\)). , \\(b_1\\) difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{\"Cold\"}] - \\mathrm{E}[diHOME|treatment = \\texttt{\"Control\"}]\\). difference conditional means equal difference sample means two groups model additional covariates model.\\(\\texttt{treatmentNE}\\) row contains statistics \\(b_2\\) (estimate \\(\\beta_2\\)). , \\(b_2\\) difference \\(\\mathrm{E}[diHOME|treatment = \\texttt{\"NE\"}] - \\mathrm{E}[diHOME|treatment = \\texttt{\"Control\"}]\\). make mistake thinking value \\(\\texttt{Estimate}\\) mean “NE” group.number non-intercept coefficients generalizes number levels factor variable. \\(k\\) levels factor, \\(k-1\\) indicator variables, coefficient (\\(b_1\\) \\(b_{k-1}\\)) estimating effect treatment level relative control (using dummy coding).– make mistake thinking values \\(\\texttt{Estimate}\\) \\(\\texttt{treatmentCold}\\) \\(\\texttt{treatmentNE}\\) rows means “Cold” “NE” groups. coefficients differences means. , emphasize understanding coefficients, \\(b_1\\) \\(b_2\\) “slopes”. Don’t visualize single line control mean non-control means. Slopes plural – two regression lines. \\(b_1\\) slope line control mean “Cold” mean \\(b_2\\) slope line control mean “NE” mean. numerator slope difference group’s mean control mean. denominator slope 1 (value 1 row assigned group).Two understand names model terms, ’s useful recall order factor levels \\(\\texttt{treatment}\\), isGiven ordering, lm function creates regression model intercept column “Control” group (first group list reference level), indicator variable “Cold” group called treatmentCold, indicator variable “NE” group called treatmentNE. can see model names peeking model matrix fit modelThe column \\(\\texttt{treatmentCold}\\) dummy-coded indicator variable containing number 1, individual “Cold” group, number 0, otherwise. column \\(\\texttt{treatmentNE}\\) dummy-coded indicator variable containing number 1, individual “NE” group, number 0, otherwise.model coefficients, parameters, model term, interpretation summarized following table.\nTable 10.4: Understanding model coefficients linear model single treatment variable three groups. means interpreation column conditional means.\n","code":"\nlevels(fig2a$treatment) ## [1] \"Control\" \"Cold\"    \"NE\"\nfig2a_m1_X <- model.matrix(fig2a_m1)\n\nhead(fig2a_m1_X)##   (Intercept) treatmentCold treatmentNE\n## 1           1             0           0\n## 2           1             1           0\n## 3           1             0           1\n## 4           1             0           0\n## 5           1             1           0\n## 6           1             0           1"},{"path":"oneway.html","id":"the-emmeans-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4.2 The emmeans table","text":"table important reporting means CIs plotting model. example 1, modeled means column “emmean” sample means group (compute simply computed mean group). , true model, generally true., example 1, SE mean sample SE modeled SE – numerator estimate \\(\\sigma\\) fit model, includes residuals groups combined. SEs report SEs used compute p-value CI report, , tell “story”. SE “Cold” group bit higher sample size \\(n\\) group smaller 1.","code":""},{"path":"oneway.html","id":"the-contrasts-table","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4.3 The contrasts table","text":"table important reporting treatment effects CIs plotting model. contrast difference means.contrast table information coefficient table, generally true models treatment factors two groups. “Working R” , show compute contrast table pairwise comparisons (contrasts possible pairings groups)column \\(\\texttt{Contrast}\\) contains names contrasts. Note name gives direction difference.values column \\(\\texttt{estimate}\\) contrasts. differences conditional means groups identified \\(\\texttt{Contrast}\\) column. effects interested .value “SE” column standard error difference (SED) contrast. SE computed using model standard deviation \\(\\sigma\\).values “lower.CL” “upper.CL” columns bounds 95% confidence interval estimate. Remember (Chapter ??) think interval containing potential values true parameter (true difference means two groups) reasonably compatible data. Don’t think interval 95% probability containing true effect. Remember confidence interval applies procedure parameter – 95% CIs hypothetical, replicate experiments meet assumptions used compute CI include true effect.columns “t.ratio” “p.value” contains t p values significance (hypothesis!) test estimate. t-statistic ratio estimate SE estimate (use console confirm given values table). signal (estimate) noise (SE estimate) ratio. p-value probability sampling normal distribution observed standard deviation, randomly assigning sampled values three groups (using original sample sizes ), fitting linear model, observing t-value extreme observed t. small p-value consistent experiment “sampling distributions mean” – meaning adding treatment affects mean distribution. logic used infer treatment effect. Unfortunately, also consistent experiment approximating conditions model, including non-random assignment, non-independence, non-normal conditional responses, variance heterogeneity. rigorous researcher sure model conditions approximated “good enough” use p-value infer treatment effect mean.","code":""},{"path":"oneway.html","id":"oneway-ttest","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4.4 t and p from the contrasts table – when there are more than two levels in \\(X\\) – are not the same as those from pairwise t-tests among pairs of groups","text":"chunk computes contrast table includes comparisons pairs groups factor \\(\\texttt{treatment}\\) (adds 3rd comparison contrast table planned comparisons ). t-tests contrasts derived single fit linear model.contrast analysis chunk , researchers commonly fit separate t-tests pair treatment levels.NotesAgain, default t.test R Welch t-test heterogenous variance. compute Student t-test, use var.equal = TRUETo see full t.test output, type “test1” console.Compare t p values three independent tests t p-values single linear model.t p-values computed three separate tests differ t p-values computed single linear model shown contrasts table . values differ SE denominators used compute \\(t\\)-values differ. linear model uses value \\(\\sigma\\) compute SED (denominator t) three t-tests contrast table. separate t-test uses different value \\(\\sigma\\) compute SED. correct? Neither – simply make different assumptions data generating model.importantly, never methods, look p-values, convince method p-values match hypothesis correct method. Human brains , good . called p-hacking. p-hack, interpretation p-value effectively meaningless. P-hacking leads irreproducible science.general, using linear model better practice separate t-tests. reason homogeneity variance assumption. assume homogeneity variances, can think sample standard deviation three groups estimate \\(\\sigma\\). linear model, use three groups estimate \\(\\sigma\\) separate t-test, use two groups. Consequently, estimate \\(\\sigma\\) linear model precise t-tests. difference can large individual data set (’s pretty big fig2a data), long-run advantage using linear model instead separate t-tests pretty small, especially three groups (precision increases groups).can drop homogeneity variance assumption either linear model three, separate t-tests. outlined “Heterogeneity variance”. case, t p-values three comparisons . Still, linear model (models heterogenity) better practice separate t-tests linear model much flexible expandable.","code":"\nfig2a_m1_pairs <- contrast(fig2a_m1_emm,\n                           method = \"revpairwise\",\n                           adjust = \"none\") %>%\n  summary(infer = TRUE)\n# classic t-test\ntest1 <- t.test(fig2a[treatment == \"Cold\", diHOME],\n                fig2a[treatment == \"Control\", diHOME],\n                var.equal = TRUE)\n\ntest2 <- t.test(fig2a[treatment == \"NE\", diHOME],\n                fig2a[treatment == \"Control\", diHOME],\n                var.equal = TRUE)\n\ntest3 <- t.test(fig2a[treatment == \"NE\", diHOME],\n                fig2a[treatment == \"Cold\", diHOME],\n                var.equal = TRUE)"},{"path":"oneway.html","id":"the-contrasts-table-when-there-are-more-than-two-levels-in-x-has-multiple-p-values.-how-to-handle-this-multiple-testing-is-highly-controversial","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.1.4.5 The contrasts table – when there are more than two levels in \\(X\\) – has multiple p-values. How to handle this “multiple testing” is highly controversial","text":"Multiple testing practice adjusting p-values confidence intervals account expected increase frequency Type error batch, family, tests. Multiple testing concept exists Neyman-Pearson hypothesis testing strategy. multiple tests used answer question , family. Issues surrounding multiple testing fleshed detail Chapter xxx “Best Practices”. Computing adjusted values covered “Working R” section.","code":""},{"path":"oneway.html","id":"working-in-r-2","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2 Working in R","text":"","code":""},{"path":"oneway.html","id":"fit-the-model-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.1 Fit the model","text":"described Fitting linear model, diHOME ~ treatment model formula. functions used fit models text use model formula. Many base R package functions meant either “easy” follow ANOVA strategy use lists dependent independent variables instead model formula. won’t use consistent linear modeling way thinking data analysis.\\(\\texttt{treatment}\\) specifically coded factor variable import wrangle chunk R automatically create correct indicator variables. categorical variables RHS formula converted factors user, R treat character variables factors create indicator variables. categorical variable numeric (example, variable time might levels 1, 2, 3 instead “1 hour”, “2 hours”, “three hours”), R treat variable numeric create indicator variables. user use formula y ~ factor(time) force R create indicator variable. prefer explicitly create version variable recoded factor using something like dt[, time_fac := factor(time, levels = c(\"1\", \"2\", \"3\"))].","code":"\nm1 <- lm(diHOME ~ treatment, data = fig2a)"},{"path":"oneway.html","id":"controlling-the-output-in-tables-using-the-coefficient-table-as-an-example","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.2 Controlling the output in tables using the coefficient table as an example","text":"standard print m1_coef object long display often wraps printed display, can make hard read. use knitr::kable print table fewer decimal places kableExtra::kable_styling make little prettier.","code":"\nm1_coef <- cbind(coef(summary(m1)),\n                 confint(m1))\nm1_coef##                Estimate Std. Error  t value    Pr(>|t|)     2.5 %   97.5 %\n## (Intercept)   12.023075   3.081337 3.901902 0.001595771  5.414264 18.63189\n## treatmentCold  7.140386   4.570362 1.562324 0.140527829 -2.662066 16.94284\n## treatmentNE   14.794354   4.357669 3.395015 0.004355868  5.448083 24.14063\n# the row names are not part of the m1_coef object\n# so there is no digit designation for this column\n\nm1_coef %>% # pipe the m1_coef object to kable\n  kable(digits = c(2,3,3,5,2,2)) %>% \n  kable_styling()\n# explore other styles in the kableExtra package"},{"path":"oneway.html","id":"using-the-emmeans-function","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.3 Using the emmeans function","text":"NotesNote printing emmeans object displays useful information. , information includes confidence level used. object printed using kable (“Inference” “Understanding” sections ), table printed additional information lost.emmeans computes modeled means combinations levels factor variables specified specs.two factor variables model, passed specs, modeled means combinations levels two variables computed. one factor variable passed, marginal means (averaged levels missing factor) computed. become clear chapter “Models two () categorical X variables”.continuous covariates model, modeled means computed average values covariates. covariates need passed specs argument.can pass numeric integer covariates specs control value covariates used compute modeled means. outlined Adding covariates linear model","code":"\nm1_emm <- emmeans(m1, specs = \"treatment\")\nm1_emm##  treatment emmean   SE df lower.CL upper.CL\n##  Control     12.0 3.08 14     5.41     18.6\n##  Cold        19.2 3.38 14    11.92     26.4\n##  NE          26.8 3.08 14    20.21     33.4\n## \n## Confidence level used: 0.95"},{"path":"oneway.html","id":"using-the-contrast-function","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.4 Using the contrast function","text":"NotesNote printing contrast object displays useful information, including confidence level used method adjustment multiple tests. object printed using kable() %>% kable_styling() (“Inference” “Understanding” sections ), table printed additional information lost.method argument used control set contrasts computed. See .adjust argument controls adjust multiple tests. method default adjustment method. See .level argument controls percentile boundaries confidence interval. default 0.95. Including argument value makes level transparent.","code":"\nm1_planned <- contrast(m1_emm,\n                       method = \"trt.vs.ctrl\",\n                       adjust = \"none\",\n                       level = 0.95) %>%\n  summary(infer = TRUE)\n\nm1_planned##  contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n##  Cold - Control     7.14 4.57 14    -2.66     16.9   1.562  0.1405\n##  NE - Control      14.79 4.36 14     5.45     24.1   3.395  0.0044\n## \n## Confidence level used: 0.95"},{"path":"oneway.html","id":"method-argument","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.4.1 method argument","text":"method = argument used control set contrasts computed. Type help(\"contrast-methods\") console see list available methods. Also, read comparisons contrasts vignette emmeans::contrast()method = \"trt.vs.ctrl\" computes non-reference minus reference contrasts. method used “Inference” section gives two contrasts planned comparisons identified “understand experimental design” step.default adjustment multiple tests “dunnettx”, Dunnett’s test. “none” specified inference section comparisons planned.method = \"pairwise\" method = \"revpairwise\" compute pairwise comparisons. prefer “revpairwise” contrasts include reference direction non-reference minus reference.default adjustment multiple tests “tukey”, Tukey’s HSD method.","code":"\nm1_tukey <- contrast(m1_emm,\n                       method = \"revpairwise\",\n                       adjust = \"tukey\",\n                       level = 0.95) %>%\n  summary(infer = TRUE)\nm1_tukey##  contrast       estimate   SE df lower.CL upper.CL t.ratio p.value\n##  Cold - Control     7.14 4.57 14    -4.82     19.1   1.562  0.2936\n##  NE - Control      14.79 4.36 14     3.39     26.2   3.395  0.0114\n##  NE - Cold          7.65 4.57 14    -4.31     19.6   1.675  0.2490\n## \n## Confidence level used: 0.95 \n## Conf-level adjustment: tukey method for comparing a family of 3 estimates \n## P value adjustment: tukey method for comparing a family of 3 estimates"},{"path":"oneway.html","id":"adjustment-for-multiple-tests","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.4.2 Adjustment for multiple tests","text":"See Section 15.8.3 chapter “Linear models two categorical \\(X\\) – Factorial linear models” thorough explanation p-value adjustment multiple tests arise experiment two treatment groups. , just outline choices available emmeans::contrast function.“none” – adjustment“dunnettx” – Dunnett’s test method used comparing treatments single control.“tukey” – Tukey’s HSD method method used compare pairwise comparisons.“bonferroni” – Bonferroni general purpose method compare set multiple tests. test conservative. better method “holm”“holm” – Holm-Bonferroni general purpose method like Bonferroni powerful.“fdr” – controls false discovery rate Type error rate family tests. One might use exploratory experiment.“mvt” – based multivariate t distribution using covariance structure variables.","code":""},{"path":"oneway.html","id":"planned-comparisons-using-custom-contrasts","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.4.3 Planned comparisons using custom contrasts","text":"purely exploratory experiment, certain contrasts test focal hypotheses. care little none contrasts. can limit contrasts computed using emmeans::contrast() passing custom contrasts method.create set vectors many elements rows emmeans table. Name vector using () group name row emmeans table. vector created row j, set value jth element 1 set values zero.create list contrasts, set list difference two named vectors step 1.","code":"\n# m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\ncn <- c(1,0,0) # control is in row 1\ncold <- c(0,1,0) # cold in in row 2\nne <- c(0,0,1) # ne is in row 3\n\n# contrasts are the difference in the vectors created above\n# the focal contrasts are in the understand the experimental\n# design section\n# 1. (cold - cn) \n# 2. (ne - cn)\n\nplanned_contrasts <- list(\n  \"Cold - Cn\" = c(cold - cn),\n  \"NE - Cn\" = c(ne - cn)\n)\nm1_planned <- contrast(m1_emm,\n                       method = planned_contrasts,\n                       adjust = \"none\"\n) %>%\n  summary(infer = TRUE)\n\nm1_planned %>%\n  kable(digits = c(1,1,2,0,1,1,1,4)) %>%\n  kable_styling()"},{"path":"oneway.html","id":"how-to-generate-anova-tables","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.5 How to generate ANOVA tables","text":"ANOVA Analysis Variance. Researchers frequently use term “ANOVA” name analysis experiment single-factor two groups. However, ANOVA general method inference complex experimental designs. ANOVA models regression models different ways expressing underlying linear model.\nTable 10.5: ANOVA table Figure 2a (example 2) data.\nNotesDo confuse statistics ANOVA table coefficient table.ANOVA table 10.5 two rows. first row contains statistics factor \\(\\texttt{treatment}\\). statistics address null hypothesis “effect treatment – groups population mean”. p value factor whole individual comparisons different groups within factor.second row contains statistics error – residuals model.\\(\\texttt{F value}\\) test statistic. ratio variances, analysis called Analysis Variance. numerator variance \\(\\texttt{Mean Sq}\\) \\(\\texttt{treatment}\\). numerator variance value $ \\(\\texttt{treatment}\\). denominator variance value $ \\(\\texttt{Residuals}\\).\\(\\texttt{Mean Sq}\\) contains mean square term (row) table. mean square variance. Remember numerator variance sum squared differences observed mean values. numerator \\(\\texttt{Mean Sq}\\) value \\(\\texttt{Sum Sq}\\) row. , remember denominator variance degree freedom. denominator \\(\\texttt{Mean Sq}\\) value \\(\\texttt{DF}\\) row.ANOVA table single factor two groups single p-value treatment term. single p-value probability sampling value F large larger observed F null (true effects either treatment specifications generating model true). much can number - want estimate effect sizes uncertainty don’t get ANOVA table. Many textbooks, websites, colleagues suggest 1) fit ANOVA, 2) check F, , \\(F < 0.05\\), 3) “tests ANOVA”. tests ANOVA planned comparisons post-hoc tests described using linear model. classical ANOVA, initial computation cell means (means treatment combinations) sums squares logical first step decomposition sums squares compute contrasts. modern linear models using regression, ANOVA first step unnecessary recommended.PI, manager, thesis committee, journal editor insists ANOVA, convince otherwise, generate ANOVA table R. Note even though generating table, computation contrast table inference part ANOVA.","code":""},{"path":"oneway.html","id":"the-afex-aov_4-function","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.5.1 The afex aov_4 function","text":"package afex developed make much easier researchers generate ANOVA tables look like statistics packages including SAS, SPSS, JMP, Graphpad Prism.NotesThe afex package three function names generating ANOVA table statistics – ’m using aov_4 functions uses linear model formula argument (specifically, used lme4 package), consistent rest text.formula includes addition random factor ((1|id)) even though really random factor model. See Section 12.1 brief explanation random factor. random factor (factor variable “id” created line fit model line) identifies individual mouse response variable measured. response measured individual mouse, “id” really random factor addition model formula necessary aov_4 function work.easy get ANOVA table don’t want R. want ANOVA table matches one Graphpad Prism JMP similar software, best practice using ANOVA functions afex package.mean “ANOVA table don’t want”? factorial ANOVA unbalanced data, three ways compute sums squares different terms ANOVA table. SAS termed Type , II, III sums squares names stuck. Following SAS, almost statistics packages use Type III default () method computing ANOVA tables. R uses Type default. good arguments using Type II. distinction moot single factor ANOVA multi-factor ANOVA balanced designs moot unbalanced multi-factor ANOVA ANOVA covariates. want ANOVA table R match generated Graphpad Prism JMP (Type III), afex package best practice.","code":"\n# .I is a data.table function that returns the row number\nfig2a[, fake_id := paste(\"mouse\", .I)]\n\nm1_aov4 <- aov_4(diHOME ~ treatment + (1|fake_id),\n                 data = fig2a)\n\nanova(m1_aov4)## Anova Table (Type 3 tests)\n## \n## Response: diHOME\n##           num Df den Df    MSE      F     ges  Pr(>F)  \n## treatment      2     14 56.968 5.7651 0.45163 0.01491 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"oneway.html","id":"the-car-anova-function","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.2.5.2 The car Anova function","text":"car package extremely useful Anova function although using bit like brain surgery watched youtube video.Notescar::Anova arguments reporting Type III sum squares. , relevant single factor ANOVA covariates avoid catastrophic code future, good know best practices now, ’m pre-peating written Section 15.11.4.2.Background: default model matrix lm function uses dummy (treatment) coding. Type 3 SS ANOVA (kind matches Graphpad Prism JMP), need tell lm use sum (deviation) coding.best practice method changing contrasts model matrix using contrasts argument within lm function, code fit m1_type3. safest practice sets contrasts specific fit.coefficients m1_type3 different m1. intercept grand mean coefficients non-reference levels (effects) deviations grand mean. don’t find definition “effects” useful experiments biology.contrasts (differences means among pairs groups) contrast table , regardless contrast coding.Danger!. Many online sites suggest bit code Type III ANOVA using car::Anova:options(contrasts = c(\"contr.sum\", \"contr.poly\")’re reading book, almost certainly don’t want code resets R computes coefficients linear models SS ANOVA tables. effect future analysis contrasts set something else new R session started.base R aov anovaNotesMany introduction statistics textbooks websites use base R aov function. don’t find function useful given afex package functions.base R anova useful know .","code":"\ntype3 <- list(treatment = contr.sum)\nm1_type3 <- lm(diHOME ~ treatment,\n                data = fig2a,\n                contrasts = type3)\nAnova(m1_type3, type=\"3\")## Anova Table (Type III tests)\n## \n## Response: diHOME\n##             Sum Sq Df  F value   Pr(>F)    \n## (Intercept) 6308.4  1 110.7355 4.95e-08 ***\n## treatment    656.9  2   5.7651  0.01491 *  \n## Residuals    797.5 14                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1_aov <- aov(diHOME ~ treatment, data = fig2a)\nsummary(m1_aov)##             Df Sum Sq Mean Sq F value Pr(>F)  \n## treatment    2  656.9   328.4   5.765 0.0149 *\n## Residuals   14  797.5    57.0                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## 1 observation deleted due to missingness\n# same as m1 in the Example 2 section\nm1 <- lm(diHOME ~ treatment, data = fig2a)\nanova(m1)## Analysis of Variance Table\n## \n## Response: diHOME\n##           Df Sum Sq Mean Sq F value  Pr(>F)  \n## treatment  2 656.85  328.43  5.7651 0.01491 *\n## Residuals 14 797.55   56.97                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"oneway.html","id":"hidden-code-2","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.3 Hidden Code","text":"","code":""},{"path":"oneway.html","id":"importing-and-wrangling-the-fig3d-data-for-example-1","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.3.1 Importing and wrangling the fig3d data for example 1","text":"","code":"\ndata_folder <- \"data\"\ndata_from <- \"The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue\"\n# need data_folder and data_from from earlier chunk\nfile_name <- \"41591_2017_BFnm4297_MOESM3_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# ignore the column with animal ID. Based on methods, I am inferring\n# that the six mice in vehicle group *are different* from the\n# six mice in the 1213 group.\ncol_names_3d <- c(\"Vehicle\", \"1213\")\ntreatment_levels <- c(\"Vehicle\", \"12,13-diHOME\")\nfig3d <- read_excel(file_path,\n                     sheet = \"Figure 3d\",\n                     range = \"B3:C9\",\n                     col_names = TRUE) %>%\n  data.table() %>%\n  melt(measure.vars = col_names_3d,\n       variable.name = \"treatment\",\n       value.name = \"serum_tg\")\n\n# change group name of \"1213\"\nfig3d[treatment == \"1213\", treatment := \"12,13-diHOME\"]\n\n# make treatment a factor with the order in \"treatment_levels\"\nfig3d[, treatment := factor(treatment, treatment_levels)]\n\n#View(fig3d)"},{"path":"oneway.html","id":"importing-and-wrangling-the-fig2a-data-for-example-2","chapter":"10 Linear models with a single, categorical X (“t-tests” and “ANOVA”)","heading":"10.3.2 Importing and wrangling the fig2a data for example 2","text":"","code":"\n# need data_folder and data_from from earlier chunk\nfile_name <- \"41591_2017_BFnm4297_MOESM2_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# assuming mice are independent and not same mouse used for all three treatment\nmelt_col_names <- paste(\"Animal\", 1:6)\nfig2a <- read_excel(file_path,\n                     sheet = \"Fig 2a\",\n                     range = \"A3:G6\",\n                     col_names = TRUE) %>%\n  data.table() %>%\n  melt(measure.vars = melt_col_names,\n       variable.name = \"id\",\n       value.name = \"diHOME\") # cannot start a variable with number\nsetnames(fig2a, old = colnames(fig2a)[1], new = \"treatment\")\n\ntreatment_order <- c(\"Control\", \"Cold\", \"NE\")\nfig2a[, treatment := factor(treatment, treatment_order)] # order levels\n\n#View(fig2a)"},{"path":"model-checking.html","id":"model-checking","chapter":"11 Model Checking","heading":"11 Model Checking","text":"","code":""},{"path":"model-checking.html","id":"check-check","chapter":"11 Model Checking","heading":"11.1 All statistical analyses should be followed by model checking","text":"us linear model infer effects predict future outcomes. inference uncertain. Given model assumptions, can quantify uncertainty standard errors, standard errors can compute confidence intervals p-values. good practice use series diagnostic plots, diagnostic statistics, simulation check well data approximate fit model model assumptions. Model checking used check subjective confidence modeled estimates uncertainty provide empirical evidence subjective decision making analysis workflow.NHST blues – Researchers often encouraged textbooks, colleagues, literature test assumptions t-test ANOVA formal hypothesis tests distributions Shapiro-Wilks test normality Levine test homogeneity. strategy, alternative t-test/ANOVA used distribution test’s p-value less cut-(0.05). Common alternatives include 1) transformations response either make normal variances homogenous, 2) implementation alternative tests Mann-Whitney-Wilcoxon (MWW) test non-normal data Welch t-test/ANOVA heterogenous variances. logic test normality homogeneity t-test/ANOVA isn’t consistent frequentist thinking failure reject null hypothesis mean null hypothesis true. shouldn’t conclude sample “normal” variances “homogenous” distributional test’s p-value > 0.05. , maybe distributional pre-test “objective” model check? logic objective decision rule suffers several issues. First, subsequent p-value ttest/ANOVA test valid p-value long-run frequency test-statistic large larger observed statistic conditional null – conditional subset nulls \\(p > 0.05\\) distribution test. Second, real data approximately normal; small \\(n\\), hard reject null normal distribution low power, , \\(n\\) increses, normality test reject real dataset. Third, importantly, analysis follow logic goals. goal estimation effects, get meaningful estimates non-parametric test (exceptions) transformed response, methods entirely computing “correct” p-value. Good alternatives classic non-parametric tests transformations bootstrap estimates confidence limits, permutation tests, generalized linear models.","code":""},{"path":"model-checking.html","id":"linear-model-assumptions","chapter":"11 Model Checking","heading":"11.2 Linear model assumptions","text":"facilitate explanation assumptions linear model extensions linear model, use error-draw conditional-draw specifications linear model single \\(X\\) variable.error draw:\n\\[\\begin{align}\ny &= \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\\\\n\\varepsilon_i &\\sim N(0, \\sigma^2)\n\\tag{11.1}\n\\end{align}\\]conditional draw:\n\\[\\begin{align}\ny_i &\\sim N(\\mu_i, \\sigma^2)\\\\\n\\mathrm{E}(Y|X=x_i) &= \\mu_i\\\\\n\\mu_i &= \\beta_0 + \\beta_1 x_i\n\\tag{11.2}\n\\end{align}\\]model generates random data using set rules specified model equations. quantify uncertainty estimated parameters, including standard errors, confidence intervals, p-values, make assumption data experiment random sample generated using rules.two rules specified model (Model (??)) areThe systematic component data generation \\(\\beta_0 + \\beta_1 X\\).generally, linear models text specify systematic components linear parameters. Perhaps better name “additive parameters”. Additive (linear) simply means can add products parameter \\(X\\) variable get conditional expectation \\(\\mathrm{E}(Y|X)\\).observational inference, rule \\(\\mathrm{E}(Y| see\\;X=x_i) = \\mu_i\\) sufficient. causal inference data experiment, observational data well defined causal diagram, need modify \\(\\mathrm{E}(Y|\\; X=x_i) = \\mu_i\\).stochastic component data generation “IID Normal”, IID independent identically distributed Normal refers Normal (Gaussian) distribution. IID assumption common linear models. , purpose text, define “linear model” broadly model linear parameters. includes many extensions classical linear model including generalized least squares linear models, linear mixed models, generalized additive models, generalized linear models. Parametric inference form linear models requires specification distribution family sample. Families use book include Normal, gamma, binomial, poisson, negative binomial. text also cover distribution free methods quantifying uncertainty using bootstrap permutation, specify sampling distribution family.","code":""},{"path":"model-checking.html","id":"a-bit-about-iid","chapter":"11 Model Checking","heading":"11.2.1 A bit about IID","text":"Independent means random draw one case predicted random draw case. lack independence creates correlated error. lots reasons errors might correlated. individuals measured within among cages, tanks, plots, field sites, ’d expect measures within unit (cage, tank, plot, site) err model direction environmental features shared individuals within unit individuals units. Multiple measures within experimental units create “clusters” error. Lack independence clustered error can modeled using generalized least squares (GLS) models directly model structure error random effects models. Random effects models go many names including linear mixed models (common Ecology), hierarchical models, multilevel models. GLS random effects models variations linear models.tl;dr – Measures taken within individual time (repeated measures) correlated common areas biology. ecology evolutionary studies, measures taken sites closer together measures taken closer time measures closely related biological species tend similar error measures taken sites apart times apart species less closely related. Space time phylogeny create spatial, temporal, phylogenetic autocorrelation.Identical means random draws given value \\(X\\) distribution. Using error-draw specification model , can restated , error-draw (\\(\\varepsilon_i\\)) every \\(\\) distribution \\(N(0, \\sigma^2\\)). Using conditional-draw specification, can restated , random-draw \\(y_i\\) every \\(\\) expected value \\(\\mu = \\mu_i\\) distribution \\(N(\\mu_i, \\sigma^2\\)). Understand importance . Parametric inference using model assumes sampling variance \\(\\mu\\) single value \\(X\\) values \\(X\\). \\(X\\) continuous, means spread points around regression line values \\(X\\) data. \\(X\\) categorical, means spread points around mean group groups. consequence “identical”, , classical linear models, assumption homogeneity (homoskedasticity) variance. sampling variance differs among \\(X\\), variances heterogenous heteroskedastic. Experimental treatments can affect variance response addition mean response. Heterogenous variance can modeled using Generalized Least Squares (GLS) linear models. Many natural biological processes generate data error function mean. example, measures biological variables grow, size body parts, variances “grow” mean. , measures counts, number cells damaged toxin, number eggs nest, number mRNA transcripts per cell variances function mean. growth count measures can sometimes reasonably modeled using linear model often, better modeled using Generalized Linear Model (GLM).","code":""},{"path":"model-checking.html","id":"diagnostic-plots-use-the-residuals-from-the-model-fit","chapter":"11 Model Checking","heading":"11.3 Diagnostic plots use the residuals from the model fit","text":"","code":""},{"path":"model-checking.html","id":"residuals","chapter":"11 Model Checking","heading":"11.3.1 Residuals","text":"residual statistical model \\(y_i - \\hat{y}_i\\). Remember \\(\\hat{y}_i\\) predicted value \\(Y\\) \\(X\\) value \\(x_i\\) (compactly written \\(X=x_i\\)). remember \\(\\hat{y}_i\\) estimate \\(\\mu_i\\). linear models (generalized linear models), residuals fit model estimates \\(\\varepsilon\\) equation (11.1). true generalized linear models GLMs specified using (11.1).Alert common misconception inference linear model assumes response (measured \\(Y\\)) IID Normal. wrong. Either specification linear model shows precisely conception wrong. Model (11.1) explicitly shows error normal distribution – distribution \\(Y\\) mix distribution \\(X\\) error. general way thinking assumed distribution uses specification model (11.2), shows conditional response assumed IID normal. Remember, conditional response (\\(y_i\\)) random draw infinite set responses given value \\(X\\).Let’s look distribution residuals versus distribution responses hypothetical experiment single, categorical \\(X\\) variable (experimental factor) two levels (“Cn” control “Tr” treatment). true parameters \\(\\beta_0 = 10\\) (true mean control group, \\(\\mu_{0}\\)), \\(\\beta_1=4\\) (difference true mean treatment minus true mean control, \\(\\mu_1 - \\mu_0\\)), \\(\\sigma = 2\\) (error standard deviation).\n(#fig:model-check-histogram, model-check-residuals1)Histogram () response, showing modes near true means group (B) residuals, mode groups zero.\nplot shows histogram response () residuals (B). plot response, mode (highest bar, bin cases) includes true mean group. , expected given \\(\\beta_1=4\\), modes two groups 4 units apart. easy see plot response normal distribution. Instead, distincly bimodal. distribution response within level looks like drawn normal distribution – . plot residuals, values groups shifted mean group zero. consequence shift combined set residuals look like drawn Normal distribution.two plots suggest two different approaches model checking. First, examine responses within level experimental factor. , second, examine residuals fit model, ignoring residuals come multiple groups. first inefficient requires many checks levels factor. second requires single check.Alert textbooks recommend formal hypothesis tests normality recommend inefficient, multiple testing group separately. isn’t wrong, ’s just work needs also suffers “multiple testing”.","code":""},{"path":"model-checking.html","id":"normal-qq","chapter":"11 Model Checking","heading":"11.3.2 A Normal Q-Q plot is used to check for characteristic departures from Normality","text":"Normal Q-Q plot scatterplot ofsample quantiles y axis. sample quantiles vector \\(N\\) residuals rank order, smallest (negative) largest (positive). Sometimes vector standardized dividing residual standard deviation residuals (makes difference interpretation Q-Q plot).standard normal quantiles x axis. vector standard, Normal quantiles given \\(N\\) elements vector. “Standard Normal” means normal distribution mean zero standard deviation (\\(\\sigma\\)) one. Normal quantile expected deviation given probability. example, probability 0.025, Normal quantile -1.959964. Check understanding: 2.5% values Normal distribution mean 0 standard deviation one negative -1.959964. Normal quantiles Normal Q-Q plot computed set \\(N\\) values evenly split probability span 0 1. \\(N=20\\), beA Normal Q-Q plot test data Normal. Instead, Normal Q-Q plot used check characteristic departures Normality signatures certain well-known distribution families. researcher can look QQ-plot reason departure small choose fit classic linear model using Normal distribution. , researcher can look QQ-plot reason departure large enough fit generalized linear model specific distribution family.Stats 101 quantile value distribution greater \\(p\\) percent values distribution. 2.5% quantile uniform distribution 0 1 0.025. 2.5% quantile standard normal distribution -1.96 (remember 95% values standard normal distribution -1.96 1.96). 50% quantile uniform distribution 0.5 50% quantile standard normal distribution 0.0 (median distribtion – 50% values smaller 50% values larger).Stats 201 Q-Q plot generally scatter plot two vectors quantiles either can come sample theoretical distribution. GLM chapter, text introduce Q-Q plots residual quantiles transformed expected uniform distribution. plotted theoretical uniform quantiles 0 1.Intuition Pump – Let’s construct Normal Q-Q plot. quantile (percentile) vector numbers value point specified percentage rank. median 50% quantile. 95% confidence intervals 2.5% 97.5% quantiles. Normal Q-Q plot, want plot quantiles residuals set theoretical quantiles.get observed quantiles, rank residuals fit linear model negative positive – quantiles! example, \\(n=145\\) residuals, 73rd point 50% quantile.theoretical quantile normal distribution can constructed using qnorm function returns normal quantiles specified vector percents. Alternatively, one randomly sample \\(n\\) points using rnorm. course sampled quantiles approximate expected theoretical quantiles, add use method .Now simply plot observed theoretical quantiles. Often, standardized quantiles plotted. standardized variable mean zero standard deviation one computed 1) centering vector zero subtracting mean every value, 2) dividing value standard deviation vector. Recognize standard deviation function deviations mean, doesn’t matter operations done first. standardized theoretical quantile specified qnorm(p, mean = 0, sd = 1), default.Code look something like ","code":"\np <- data.frame(quantile = qnorm(ppoints(1:20)))\nrow.names(p) <- ppoints(1:20)\np##          quantile\n## 0.025 -1.95996398\n## 0.075 -1.43953147\n## 0.125 -1.15034938\n## 0.175 -0.93458929\n## 0.225 -0.75541503\n## 0.275 -0.59776013\n## 0.325 -0.45376219\n## 0.375 -0.31863936\n## 0.425 -0.18911843\n## 0.475 -0.06270678\n## 0.525  0.06270678\n## 0.575  0.18911843\n## 0.625  0.31863936\n## 0.675  0.45376219\n## 0.725  0.59776013\n## 0.775  0.75541503\n## 0.825  0.93458929\n## 0.875  1.15034938\n## 0.925  1.43953147\n## 0.975  1.95996398"},{"path":"model-checking.html","id":"normal-qq-plot-of-the-fake-data-generated-above","chapter":"11 Model Checking","heading":"11.3.2.1 Normal QQ-plot of the fake data generated above","text":"sampled distribution approximates sample normal distribution, scatter fall along line bottom, left top, right plot. interpretation normal Q-Q plot enhanced line “expected values” sample quantiles sample residuals drawn normal distribution. closer sample quantiles line, closely residuals approximate expectation normal distribution. sampling, sampled values always deviate line, especially ends. shaded gray area Q-Q plot Figure ?? 95% confidence bands quantiles. pattern observed quantiles individual points outside boundaries indicates sample unusual sampled Normal distribution.Biological datasets frequently departures Normal Q-Q plot characteristic specific distribution families, including lognormal, binomial, poisson, negative binomial, gamma, beta. useful learn read Normal Q-Q plot help guide model data.intepretation Q-Q plot Figure ??? small end distribution (bottom-left), sample values bit negative expected, means left tail bit extended. large end (upper-right), sample values , bit less positive expected, means right tail bit shortened. departure direction left skewed distribution. fit different model given deviations? guide us, compare quantiles 95% confidence band quantiles. Clearly observed quantiles within range quantiles ’d expect sampling Normal distribution.","code":""},{"path":"model-checking.html","id":"mapping-qq-plot-departures-from-normality","chapter":"11 Model Checking","heading":"11.3.3 Mapping QQ-plot departures from Normality","text":"Let’s look simulated samples drawn non-normal distributions identify characteristic deviations. set plots shows(left panel) histogram 10,000 random draws non-Normal distribution (blue). histogram superimposed 10,000 random draws Normal distribution (orange) mean variance non-normal distribution.(middle panel) Box plots strip chart random subset (\\(N=1000\\)) data left panel.(right panel) Normal Q-Q plot non_Normal data .Skewed-Right Q-QThe Normal Q-Q plot sample right-skewed distribution characterized sample quantiles high (right) end positive expected Normal quantiles. Often, quantiles low (left) end also less negative expected normal quantiles. consequence concave pattern.histograms left panel explain pattern. right tail skewed-right distribution extends right tail Normal. easy see , rank values distribution small large (quantiles), upper quantiles skewed-right distribution larger matching quantile Normal distribution. example, 99,990th quantile skewed-right distribution much positive 99,990th quantile Normal distribution. opposite occurs left tail, extends negative direction Normal skewed-right distribution.middle panel compares boxplot stripchart samples two distributions show researchers look publication-ready plots well published plots colleagues. skewed-right plot exhibits several hallmarks skewed-right distribution including 1) median line (horizontal line within box) closer 25th percentile line (lower end box) 75th percentile line (upper end box), 2) longer upper lower whisker (vertical lines extending box), 3) outliers upper whisker lower whisker, 4) lengthened, upward smear scatter points high end values, relative compact smear low end values.Skewed-Left Q-QThe Normal Q-Q plot sample left-skewed distribution characterized sample quantiles low (left) end negative expected Normal quantiles. Often, quantiles high (right) end also less positive expected normal quantiles. consequence concave pattern.histograms left panel explain pattern. left tail skewed-left distribution extends left tail Normal. easy see , rank values distribution small large (quantiles), lower quantiles skewed-left distribution negative matching quantile Normal distribution. example, 10th quantile skewed-left distribution much negative 10th quantile Normal distribution. opposite occurs right tail, extends positive direction Normal skewed-left distribution.skewed-left plot middle panel highlights several hallmarks skewed-left distribution including 1) median line (horizontal line within box) closer 75th percentile line (lower end box) 25th percentile line (upper end box), 2) longer lower upper whisker (vertical lines extending box), 3) outliers lower whisker upper whisker, 4) lengthened, downward smear scatter points low end values, relative compact smear upper end values.Heavy Tail Q-QThe Normal Q-Q plot sample heavy-tail distribution characterized sample quantiles low (left) end negative expected Normal quantiles quantiles high (right) end positive expected normal quantiles.histograms left panel explain pattern. tail, heavy-tail distribution density – values far mean – compared Normal distribution. origin “heavy tail”. easy see , rank values distribution small large (quantiles), lower quantiles heavy tail distribution negative matching quantile Normal distribution. example, 10th quantile heavy-tail distribution much negative 10th quantile Normal distribution. Likewise, upper quantiles heavy tail distribution positive matching quantile Normal distribution. example, 99,990th quantile heavy-tail distribution much positive 99,990th quantile Normal distribution.heavy-tail plot middle panel shows boxplot outliers Normal plot. hard recognize plot real data.","code":""},{"path":"model-checking.html","id":"mapping-characteristic-departures-on-a-q-q-plot-to-specific-distributions","chapter":"11 Model Checking","heading":"11.3.3.1 Mapping characteristic departures on a Q-Q plot to specific distributions","text":"Continuous response variables length, area, weight, duration often look like samples continous probability distribution right-skewed, lognormal gamma distributions.Count response variables frequently look like samples discrete probability distribution right-skewed, poisson, quasi-poisson, negative binomial distributions.Proportion (fraction whole) response variables frequently look like samples continuous probability distribution bounded 0 1, beta distribution. Samples beta distribution can left skewed, mean near 1, right-skewed, mean near zero, symmetrical, mean near 0.5.","code":""},{"path":"model-checking.html","id":"pump-your-intuition-confidence-bands-of-a-q-q-plot","chapter":"11 Model Checking","heading":"11.3.3.2 Pump your intuition – confidence bands of a Q-Q plot","text":"introducing confidence bands Q-Q plot , stated “pattern observed quantiles individual points outside boundaries indicates sample unusual sampled Normal distribution.” Let’s use parametric bootstrap explore .Sample \\(n\\) values Normal distributionCompute sample quantiles re-ordering residuals sampled values sampled mean, negative positive.Plot quantiles Normal quantiles \\(n\\) points.Repeat steps 1-3 \\(n\\_iter\\) times, superimposing new sample quantiles previous sample quantiles. creates band sample quantiles \\(n\\_iter\\) iterations sampling \\(n\\) values Normal distribution.value Normal quantile, compute 95 percentile range sampled quantiles. Draw ribbon inside boundaries.","code":"\nn_iter <- 1000\nn <- 20\nnormal_qq <- ppoints(n) %>%\n  qnorm()\nsample_qq <- numeric(n_iter*n)\ninc <- 1:n\nfor(iter in 1:n_iter){\n  y <- rnorm(n)\n  y_res <- y - mean(y)\n  sample_qq[inc] <- y_res[order(y_res)]\n  inc <- inc + n\n}\n\nqq_data <- data.table(normal_qq = normal_qq,\n                      sample_qq = sample_qq)\n\nqq_ci <- qq_data[, .(median = median(sample_qq),\n                     lower = quantile(sample_qq, 0.025),\n                     upper = quantile(sample_qq, 0.975)),\n                 by = normal_qq]\n\nggplot(data = qq_data,\n       aes(x = normal_qq,\n           y = sample_qq)) +\n  geom_point(alpha = 0.2) +\n  geom_ribbon(data = qq_ci,\n              aes(ymin = lower,\n                  ymax = upper,\n                  y = median,\n                  fill = \"band\"),\n              fill = pal_okabe_ito[1],\n              alpha = 0.3) +\n  xlab(\"Normal Quantile\") +\n  ylab(\"Sample Quantile\") +\n  theme_grid() +\n  \n  NULL"},{"path":"model-checking.html","id":"model-checking-spread-level","chapter":"11 Model Checking","heading":"11.3.4 Model checking homoskedasticity","text":"","code":""},{"path":"model-checking.html","id":"using-r","chapter":"11 Model Checking","heading":"11.4 Using R","text":"","code":""},{"path":"model-checking.html","id":"hidden-code-3","chapter":"11 Model Checking","heading":"11.5 Hidden Code","text":"Source: Wellenstein, M.D., Coffelt, S.B., Duits, D.E., van Miltenburg, M.H., Slagter, M., de Rink, ., Henneman, L., Kas, S.M., Prekovic, S., Hau, C.S. Vrijland, K., 2019. Loss p53 triggers WNT-dependent systemic inflammation drive breast cancer metastasis. Nature, 572(7770), pp.538-542.Public sourceData sourceFit linear model","code":"\ndata_from <- \"Loss of p53 triggers WNT-dependent systemic inflammation to drive breast cancer metastasis\"\nfile_name <- \"41586_2019_1450_MOESM3_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n  \ntreatment_levels <- c(\"Trp53+/+\", \"Trp53-/-\")\nfig1f <- read_excel(file_path,\n                     sheet = \"Fig. 1f\",\n                     range = \"A2:B23\") %>%\n  data.table() %>%\n  melt(measure.vars = treatment_levels,\n       variable.name = \"treatment\",\n       value.name = \"il1beta\")\nfig1f[, treatment := factor(treatment, treatment_levels)]\n\n# be careful of the missing data. This can create mismatch between id and residual unless specified in lm\n\n# head(fig1f)\nm1 <- lm(il1beta ~ treatment,\n         data = fig1f)"},{"path":"model-checking.html","id":"normal-q-q-plots","chapter":"11 Model Checking","heading":"11.5.1 Normal Q-Q plots","text":"car::qqPlot several important arguments control type Q-Q plot. function uses base graphics instead ggplot2. Typically, plots published possibly supplement. Q-Q Plots Worm Plots Scratch good source arguments qqPlot. Three important arguments :simulate. passing lm object, default confidence band generated parametric bootstrap (simulate = TRUE). band differ somewhat time replot unless set seed set.seed. Setting argument simulate = FALSE returns parametric band.line. passing lm object, default line fit robust regression (line = \"robust\"). Setting argument line = \"quartiles\" fits line throught 25th 75th percentile (“quartiles”) quantiles.id. default identifies index two points extreme quartiles. Set FALSE hide.robust line sensitive departures Normality quartiles line.ggpubr::ggqqplot generates pretty, ggplot2 based Normal Q-Q plot, using standard method computing line confidence band.","code":"\n# defaults: robust line with bootstrap CI\nset.seed(1)\nqqPlot(m1, id = FALSE)\n# classic: standard line with parametric CI\nqqPlot(m1,\n       line = \"quartiles\",\n       simulate = FALSE,\n       id = FALSE)\nm1_residuals <- data.table(m1_residuals = residuals(m1))\nm1_residuals[, studentized := m1_residuals/sd(m1_residuals)]\nggqqplot(data = m1_residuals,\n                          x = \"studentized\")"},{"path":"violations.html","id":"violations","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12 Violations of independence, homogeneity, or Normality","text":"","code":""},{"path":"violations.html","id":"oneway-paired-t","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1 Lack of independence","text":"","code":""},{"path":"violations.html","id":"violations-paired-t","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.1 Example 1 (exp1b) – a paired t-test is a special case of a linear mixed model","text":"Lynes, M.D., Leiria, L.O., Lundh, M., Bartelt, ., Shamsi, F., Huang, T.L., Takahashi, H., Hirshman, M.F., Schlein, C., Lee, . Baer, L.., 2017. cold-induced lipokine 12, 13-diHOME promotes fatty acid transport brown adipose tissue. Nature medicine, 23(5), pp.631-637.Public sourceData sourceThe data experiment Figure 1b 12,13-diHOME article previous chapter.Response variable – \\(\\texttt{diHOME}\\): concentration (pmol per ml) putative lipokine 12,13 di-HOME. continuous variable.Factor variable – \\(\\texttt{treatment}\\), levels: “saline” “cold”. Coded factor.Blocking variable – id: identification code individual human.Design – Blocked. Plasma concentrations 12,13-diHOME measured two times individual, injection saline 1 hour cold exposure.consequence two measures human correlated error. blocked designs like , correlated error arises expect measures within block (individual exp1b experiment) similar measures among blocks. blocked design powerful experimental method reduce uncertainty estimates differences among means – , consequently, power statistical test. paired t-test exploits design classic Student’s t-test ignores .Multiple response measures per individual violate independent sampling assumption inference. lack independence two groups, “test?” strategy points paired t-test place Student’s t-test.paired t-test special case linear mixed model. Linear mixed models discussed chapter Models random factors – Blocking pseudoreplication.good way think model generating data \\[\n\\begin{equation}\n\\texttt{diHome} = (\\beta_0 + \\gamma_{0_j}) + \\beta_1 (\\texttt{treatment_cold}) + \\varepsilon\n\\end{equation}\n\\]\\(\\beta_0\\) expected value \\(\\texttt{diHome}\\) humans given saline treatment.\\(gamma_{0_j}\\) effect human j expected value \\(\\texttt{diHome}\\). \\(\\gamma_{0_j}\\) kind random effect.\\(\\beta_0 + \\gamma_{0_j}\\) expected value \\(\\texttt{diHome}\\) human j given saline treatment. random intercept human \\(j\\).\\(\\beta_0 + \\gamma_{0_j} + \\beta_1\\) expected value \\(\\texttt{diHome}\\) human j given cold treatment.Importantly, random effect (\\(\\gamma_{0_j}\\)) isn’t modeled, variation component picked model error \\(e\\). Consequently, error component correlated subsets \\(e\\) individual expected similar values individuals. Correlated error discussed chapter Models random factors – Blocking pseudoreplication. Importantly, random effect isn’t modeled, increased variance \\(e\\) decreases precision estimate. Consequently, CIs wider significance tests less power.","code":""},{"path":"violations.html","id":"fit-the-model-2","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.1.1 Fit the model","text":"function lmer() lme4 package allows addition random factors model formula. Otherwise, function works like lm function.added random factor \\(\\texttt{id}\\), contains id human. \\(\\texttt{id}\\) didn’t appear nowhere, column data.table exp1b.Random factors added embedded within parentheses. random intercept added using formula (1|factor) \\(\\texttt{factor}\\) names column data containing random factor.\\(\\texttt{id}\\) random factor. \\(\\texttt{treatment}\\) fixed factor. Models fixed random factors go many names. text, use “linear mixed model”.","code":"\n# fit the model\nexp1b_m1 <- lmer(diHOME ~ treatment + (1|id), data = exp1b)"},{"path":"violations.html","id":"inference","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.1.2 Inference","text":"Inference using linear model added random factor:Inference using paired t-test:NotesThe t p values linear model t-tests paired t-test special case linear model.","code":"\n# estimated marginal means table\nexp1b_m1_emm <- emmeans(exp1b_m1, specs = \"treatment\")\n\n# contrasts table\nexp1b_m1_pairs <- contrast(exp1b_m1_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\n\nexp1b_m1_pairs %>%\n  kable(digits = c(1,1,2,1,1,1,5,5)) %>%\n  kable_styling()\nexp1b_ttest <- t.test(\n  x = exp1b[treatment == \"cold\", diHOME],\n  y = exp1b[treatment == \"saline\", diHOME],\n  paired = TRUE\n)\n\nexp1b_ttest## \n##  Paired t-test\n## \n## data:  exp1b[treatment == \"cold\", diHOME] and exp1b[treatment == \"saline\", diHOME]\n## t = 4.2722, df = 8, p-value = 0.002716\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  0.1078778 0.3609173\n## sample estimates:\n## mean difference \n##       0.2343975"},{"path":"violations.html","id":"violations-rmanova","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.2 Example 2 (diHOME exp2a) – A repeated measures ANOVA is a special case of a linear mixed model","text":"structure Figure 2A experiment chapter Linear models single, categorical X ambiguous. clear archived data measures 12,13 diHome separate mice within three treatments (“Control”, “1 hour cold”, “30 min NE”) mice used set. mouse measured three times, assume researchers stated , didn’t. , outline modeling correlated data two groups, let’s say mouse Figure 2A experiment subjected three treatments. three measures per mouse response measure violates independence assumption.linear model (regression notation) \\[\n\\begin{equation}\n\\texttt{diHome} = (\\beta_0 + \\gamma_{0_j}) + \\beta_1 \\texttt{treatment}_\\texttt{cold} + \\beta_2 \\texttt{treatment}_\\texttt{NE} + \\varepsilon\n\\end{equation}\n\\]","code":""},{"path":"violations.html","id":"fit-the-model-3","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.2.1 Fit the model","text":"NotesIf treatment factor two groups, nothing special done relative model treatment factor two groups.","code":"\n# fit the model\nexp2a_m1 <- lmer(diHOME ~ treatment + (1|id), data = exp2a)"},{"path":"violations.html","id":"inference-1","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.2.2 Inference","text":"Inference using linear model added random factor:","code":"\n# estimated marginal means table\nexp2a_m1_emm <- emmeans(exp2a_m1, specs = \"treatment\")\n\n# contrasts table\nexp2a_m1_planned <- contrast(exp2a_m1_emm,\n                     method = \"trt.vs.ctrl\",\n                     adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nexp2a_m1_planned %>%\n  kable(digits = c(1,1,2,1,1,1,5,5)) %>%\n  kable_styling()"},{"path":"violations.html","id":"repeated-measures-anova","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.2.3 repeated measures ANOVA","text":"fields, analysis called repeated measures ANOVA others might called two-way Model II (mixed effect) ANOVA one fixed one random factor.NotesThe measure \\(\\texttt{diHOME}\\) animal_6 1 hour cold treatment missing. use exp2a data repeated measures ANOVA, “Control” “30 min NE” values animal_6 excluded. exclusion isn’t necessary linear mixed model, one advantages linear mixed model repeated measures ANOVA.contrasts model areLets remove animal 6 rerun linear mixed model exp2a_m1Notes -Inference .","code":"\nexp2a_m2 <- aov_4(diHOME ~ treatment + (treatment|id),\n                           data = exp2a)## Warning: Missing values for following ID(s):\n## Animal 6\n## Removing those cases from the analysis.\nexp2a_m1_planned <- emmeans(exp2a_m2, specs = \"treatment\") %>% \n  contrast(method = \"trt.vs.ctrl\",\n           adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nexp2a_m1_planned %>%\n  kable(digits = c(1,1,2,1,1,1,5,5)) %>%\n  kable_styling()\nexp2a_complete <- exp2a[id != \"Animal 6\"]\n\nexp2a_complete_m1 <- lmer(diHOME ~ treatment + (1|id), data = exp2a_complete)\n\nexp2a_m1_complete_planned <- emmeans(exp2a_complete_m1, specs = \"treatment\") %>% \n  contrast(method = \"trt.vs.ctrl\",\n           adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nexp2a_m1_complete_planned %>%\n  kable(digits = c(1,1,2,1,1,1,5,5)) %>%\n  kable_styling()"},{"path":"violations.html","id":"inferences-from-the-linear-mixed-model-and-paired-t-tests-are-not-the-same-when-there-are-more-than-two-groups","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.1.2.4 Inferences from the linear mixed model and paired t-tests are not the same when there are more than two groups","text":"Inference using two paired t-tests:Notesimportant – t-test comparison “1 hour cold” “Control” excluded mouse 6 missing “1 hour cold” measure. linear mixed model fit data, including “Control” “30 min NE” measures. consequence precision linear mixed model.paired t-test t p values equal linear mixed model. partly missing data (note 1). Even missing data, wouldn’t equal linear mixed model uses estimate \\(\\sigma\\) computed single model three groups compute standard errors. See comparison t-tests contrast table linear model Section ??.results separate, paired t-tests linear mixed model differ way might affect inference system. correct? Neither – simply make different assumptions data generating model.linear model strategy power precision advantage small. best reason use linear models instead separate t-tests learning use linear models, extensions, gives phenomenal cosmic power.compute separate paired t-tests linear models convince assumption method p-value matches hypothesis correct. See p-hacking discussion .","code":"\nexp2a_ttest_cold <- t.test(\n  x = exp2a[treatment == \"1 hour cold\", diHOME],\n  y = exp2a[treatment == \"Control\", diHOME],\n  paired = TRUE\n)\nexp2a_ttest_cold## \n##  Paired t-test\n## \n## data:  exp2a[treatment == \"1 hour cold\", diHOME] and exp2a[treatment == \"Control\", diHOME]\n## t = 2.1973, df = 4, p-value = 0.09293\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##  -1.925189 16.533005\n## sample estimates:\n## mean difference \n##        7.303908\nexp2a_ttest_ne <- t.test(\n  x = exp2a[treatment == \"30 min NE\", diHOME],\n  y = exp2a[treatment == \"Control\", diHOME],\n  paired = TRUE\n)\nexp2a_ttest_ne## \n##  Paired t-test\n## \n## data:  exp2a[treatment == \"30 min NE\", diHOME] and exp2a[treatment == \"Control\", diHOME]\n## t = 3.7564, df = 5, p-value = 0.01321\n## alternative hypothesis: true mean difference is not equal to 0\n## 95 percent confidence interval:\n##   4.670273 24.918436\n## sample estimates:\n## mean difference \n##        14.79435"},{"path":"violations.html","id":"oneway-welch","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2 Heterogeneity of variances","text":"Heterogeneity variance among treatment groups problem inference, especially sample size unequal among groups (statisticians tend agree heterogeneity much problematic non-normal response).","code":""},{"path":"violations.html","id":"a-welch-t-test-is-a-special-case-of-a-linear-model-for-heterogeneity","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.0.1 A Welch t-test is a special case of a linear model for heterogeneity","text":"“test?” strategy points Welch’s t-test place Student’s t-test heterogeneity variances treatment groups. Welch t-test infrequent experimental biology literature, perhaps becauseit poorly known doesn’t occur researchers use test models heterogeneity variances.heterogeneity often arises right-skewed data, often analyzed non-parametric test like Mann-Whitney U test.Welch t-test special case linear model explicitly models within-group variance using generalized least squares (GLS). 95% CI mean differences p-values fit gls linear model Welch’s t-test . Advantages using linear modeling strategy researcher uses model estimate effects (difference means) measures uncertainty effects (standard errors confidence intervals difference). Advantages specifically using GLS linear model easily expanded analyze complex designs including 1) one factor, 2) added covariates, 3) correlated residuals due non-independence.statisticians argue researchers always use Welch t-test instead Student’s t-test. Given logic, researchers consider using GLS linear models complex experimental designs (added covariates, factorial) place classical ANCOVA two-way ANOVA.Modeling variance heterogeneity focus Chapter 19 account brief. Heterogeneity can modeled using generalized least squares linear model gls function. weights argument used model variances using group’s sample variance. example, use data Figure 1b experiment, can compared analysis data Example 2 .","code":""},{"path":"violations.html","id":"fit-the-model-4","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.0.2 Fit the model","text":"model exp2a_m3 uses variance computed group separately estimate \\(\\sigma\\) group. coefficient table GLS model ","code":"\n# gls fails with missing data\nsubdata <- exp2a[is.na(diHOME) == FALSE,] # omit rows with missing data\nexp2a_m3 <- gls(diHOME ~ treatment,\n                data = subdata,\n                weights = varIdent(form = ~ 1 | treatment))"},{"path":"violations.html","id":"inference-from-the-linear-model","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.0.3 Inference from the linear model","text":"NotesImportant reporting CIs p-values. Unlike linear model modeling homogenous variance, CIs p-values coefficients \\(\\texttt{treatment1 hour cold}\\) \\(\\texttt{treatment30 min NE}\\) p-values equivalent contrasts contrasts table (see ). reason , computation CI p-values two tables use two different degrees freedom. Report CI p-values contrast table using Satterthwaite df.modeled means contrasts computed lm objectNotesThe SE means table modeled SEs equal sample SE means, specified GLS model.NotesCompare statistics “1 hour cold - Control” “30 min NE - Control” coefficients \\(\\texttt{treatment1 hour cold}\\) \\(\\texttt{treatment30 min NE}\\) coefficient table. estimates, SE, t CIs p values differ. contrast function using different method (“satterthwaite”) computing degrees freedom results different value t-distribution tail area used compute CI p value.","code":"\nexp2a_m3_coef <- cbind(coef(summary(exp2a_m3)),\n                       confint(exp2a_m3))\n\nexp2a_m3_coef %>%\n  kable(digits = c(4,4,4,6,4,4)) %>%\n  kable_styling()\nexp2a_m3_emm <- emmeans(exp2a_m3, specs=\"treatment\")\nexp2a_m3_emm##  treatment   emmean   SE df lower.CL upper.CL\n##  Control       12.0 1.20  5     8.94     15.1\n##  1 hour cold   19.2 2.93  4    11.04     27.3\n##  30 min NE     26.8 4.41  5    15.49     38.1\n## \n## Degrees-of-freedom method: satterthwaite \n## Confidence level used: 0.95\nexp2a_m3_pairs <-  contrast(exp2a_m3_emm,\n                            method = \"revpairwise\",\n                            adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nexp2a_m3_pairs %>%\n  kable(digits = c(1,4,4,1,4,4,4,5)) %>%\n  kable_styling()"},{"path":"violations.html","id":"inference-using-welch-t-test","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.0.4 Inference using Welch t-test","text":"Notesthe default t.test Welch t-test. However, ’ve included var.equal = FALSE method transparent.","code":"\ntest1 <- t.test(exp2a[treatment == \"1 hour cold\", diHOME],\n       exp2a[treatment == \"Control\", diHOME],\n       var.equal = FALSE)\n\ntest2 <- t.test(exp2a[treatment == \"30 min NE\", diHOME],\n       exp2a[treatment == \"Control\", diHOME],\n       var.equal = FALSE)\n\ntest3 <- t.test(exp2a[treatment == \"30 min NE\", diHOME],\n       exp2a[treatment == \"1 hour cold\", diHOME],\n       var.equal = FALSE)"},{"path":"violations.html","id":"compare-inference-from-the-linear-model-and-welch-t-tests","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.0.5 Compare inference from the linear model and Welch t-tests","text":"Compare contrast p values three Welch t-tests pairs treatment levels exp2a experiment.t p-values computed GLS linear model three, pairwise Welch t-tests (6th decimal place). estimating \\(\\sigma^2\\) separately group pooled (among two groups t-test three groups linear model) estimate use degrees freedom compute p-value.Let’s summarize comparisonsInference linear model using homogenous variance (lm function) Student’s t-test two levels treatment variable.Inference linear model using homogenous variance (lm function) series pairwise, Student’s t-tests differ two levels treatment variable.Inference GLS linear model using heterogenous variance (gls function) Welch t-test regardless number levels treatment variable.Even though linear model models heterogeneity Welch t-test produce results, researchers use linear model becauseA linear modeling strategy encourages researchers think effect uncertainty effect just p-value.linear model nearly infinitely flexible expandible t-test extremely limited flexibility (Welch t-test one way expand classical, Student’s t-test).","code":""},{"path":"violations.html","id":"when-groups-of-the-focal-test-have-variance","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.2.1 When groups of the focal test have >> variance","text":"","code":""},{"path":"violations.html","id":"the-conditional-response-isnt-normal","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3 The conditional response isn’t Normal","text":"common hear read statistitican issues inference classical linear model (including t-tests ANOVA), non-normal “data” least worrisome – much concerned lack independence heterogeneity.theoretical basis sentiment central limit theorem. short, repeatedly 1) sample non-normal distribution 2) compute sample mean, distribution resampled means approximately normal (distribution converges normal sample size increases). inferential statistics using classical linear model assume normal distribution means, inference classical linear models increasingly robust non-normal distributions sample size increases. main issue applied statistics experimental biology small sample sizes.Empirical results simulations support theory. Actual type error using classical linear models non-normal data tends close nominal (say, 0.05) level.said, generalized linear models explicitly model distribution power classical linear models pretty good reason use GLMs.Confidence intervals computed using normal distribution non-normal data can awkward, example, CIs might include impossible values negative counts percent cells expressing certain marker 100 %.normality assumption classical linear model apply distribution response (Y) variable certainly distribution treatment (X) variable residuals fit model (actually, applies distribution means, sampling distribution normal means normal). equivalent way think normality assumption applies distribution response values \\(X\\) sample – known conditional response. Contrary common practice experimental biologists, advice textbooks, good reason use p-value test normality decision tool use either t-test non-parametric test like Mann-Whitney-Wilcoxon. test normality small samples frequently result \\(p > 0.05\\) (“normal”), even fake data sampled nonnormal distribution, test large sample size frequently result \\(p < 0.05\\) (“-normal”) even histogram residuals appears normal.Normal Q-Q plot (Section 11.3.2) useful tool decision use alternative inference using Normal assumption. Q-Q plot indicates residuals far expected normal distribution, researcher several alternatives, best choice partially depends goals analysis. alternative linear model assuming normal distribution include:Count data – count data tend right skewed groups higher mean counts tend larger variance.Generalized linear models using either negative binomial quasi-poisson distribution family good alternative consider.Bootstrap confidence intervals permutation p-values resampling techniques. Bootstrap confidence intervals good alternative sample size much larger (say, > 40) typical experimental biology. Permutation p-values good alternative regardless sample size.GLS linear model account heterogeneity variance typically accompanies non-normal data. doesn’t address non-normal distribution , , heterogeneity typically much larger problem non-normality.Log transformation count response controversial best. log transformations can pretty good making response look like sampled normal distribution (can make variances similar). …counts include zero, researchers add kludge factor (typically equal 1) counts prior transformation. value kludge factor arbitrary matters (adding .1 gives different results adding 1). log transformation (transformation) raises interpretation problems. log transformation, effect difference means log-transformed counts. don’t know magnitude effect log scale means biologically. backtransform , effect ratio geometric means two groups. ratio effect easy enough interpret really want model geometric arithmetic means? Finally, log transformation specifically address either shape distribution heterogeneity often comes non-normal shape. generalized linear model can specifically model shape variance.Classic non-parametric tests Mann-Whitney-Wilcoxon invented computers fast enough perform permutation tests development Generalized Linear Models (modern methods). Since least 1990, better alternatives non-parametric tests.Fraction (proportion) data – example, number cells expressing certain marker relative counted cells sample. Fraction data hard bounds 0 1, 0 100 converted percent. Fraction data tend right skewed mean closer zero left skewed mean closer upper bound.Generalized linear model using binomial distribution family logit link (“logistic regression”) good alternative consider. model used Bernouili (success/fail) responses (example subject “lived” “died”), success assigned value 1 fail assigned value 0. Using model proportion data equivalent assigning 0 (“express marker”) 1 (“expresses marker”) cells count.Bootstrap confidence intervals permutation p-values resampling techniques. Bootstrap confidence intervals good alternative sample size much larger (say, > 40) typical experimental biology. Permutation p-values good alternative regardless sample size.GLS linear model account heterogeneity variance typically accompanies non-normal data. doesn’t address non-normal distribution , , heterogeneity typically much larger problem non-normality.arcsin transformation. See [arcsine asinine: analysis proportions ecology] (https://esajournals.onlinelibrary.wiley.com/doi/full/10.1890/10-0340.1)Classic non-parametric tests Mann-Whitney-Wilcoxon invented computers fast enough perform permutation tests development Generalized Linear Models (modern methods). Since least 1990, better alternatives non-parametric tests.Generalized linear models explicitly model distribution response covered detail chapter Linear models count data – Generalized Linear Models .","code":""},{"path":"violations.html","id":"violations-6f","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3.1 Example 1 (fig6f) – Linear models for non-normal count data","text":"Source article: Exercise reduces inflammatory cell production cardiovascular inflammation via instruction hematopoietic progenitor cellsData sourcePublic source","code":""},{"path":"violations.html","id":"fit-a-linear-model-inference-assuming-normal-conditional-response","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3.1.1 Fit a Linear Model – Inference assuming Normal conditional response","text":"fit modelcheck modelThe left plot shows classic right skew. right plot shows evidence positive relationship mean variance. suggest fit negative binomial quasi-poisson model.inference","code":"\nm1 <- lm(neutrophil_count ~ treatment, data = fig6f)\nggcheck_the_model(m1)\nm1_emm <- emmeans(m1, specs = \"treatment\")\nm1_pairs <- contrast(m1_emm,\n                     method = \"trt.vs.ctrl\") %>%\n  summary(infer = TRUE)"},{"path":"violations.html","id":"fit-a-generalized-linear-model-inference-assuming-negative-binomial-conditional-response","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3.1.2 Fit a Generalized Linear Model – inference assuming negative binomial conditional response","text":"Generalized linear models count data covered thoroughly chapter Linear models count data – Generalized Linear Models .fit modelNotesThe function glmmTMB allows fitting multiple distribution families works like lm function tweaks, including argument distribution family.inferenceNotesThe coef function glmmTMB object list several elements. element \\(\\texttt{\\$cond}\\)” returns coefficient table.\\(b_0\\) (\\(\\texttt{(intercept)}\\)) natural log mean reference (“sedentary”) group.\\(b_1\\) (\\(\\texttt{(intercept)}\\)) difference natural log mean exercise group mean natural log mean sedentary group mean.coefficient values link scale, scale transformation group means GLM fit. link transformation fit log (natural log transformation). general, values response scale interpretable. \\(exp(b_0)\\) modeled mean reference group response scale (model covariates, sample mean reference group). \\(exp(b_1)\\) ratio exercise sedentary means. Remember: \\(exp(log() - log(b)) = /b\\)NotesThe argument type = \"response\" tells emmeans contrast return estimates response (link) scale. See notes coefficient table brief explanation link response scale. detailed explanation, see Chapter Linear models count data – Generalized Linear Models ","code":"\nm2 <- glmmTMB(neutrophil_count ~ treatment,\n              data = fig6f,\n              family = nbinom2(link = \"log\"))\nm2_coef <- coef(summary(m2))$cond\n\nm2_coef %>%\n  kable(digits = c(2,3,1,5)) %>%\n  kable_styling()\nm2_emm <- emmeans(m2,\n                  specs = \"treatment\",\n                  type = \"response\")\nm2_pairs <- contrast(m2_emm,\n                     method = \"trt.vs.ctrl\",\n                     type = \"response\") %>%\n  summary(infer = TRUE)"},{"path":"violations.html","id":"compare-inference-from-the-lm-and-glm-models","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3.1.3 Compare inference from the LM and GLM models","text":"NotesGeneralized linear models covered thoroughly Chapter 18.effect estimated GLM difference ratio exercise group mean sedentary group mean. differences ratios indicate much bigger (smaller) mean one group compared another linear model “much bigger” difference glm “much bigger” multiple (“times”) – mean exercise group .59X mean sedentary group.CIs means effect asymmetric GLM. CIs ","code":"\ngg1 <- ggplot_the_model(\n  m1,\n  m1_emm,\n  m1_pairs,\n  legend_position = \"none\",\n  y_label = \"Neutrophil Count (/mL)\",\n  effect_label = \"Effect (/mL)\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.75,1)\n)\n\ngg2 <- ggplot_the_model(\n  m2,\n  m2_emm,\n  m2_pairs,\n  legend_position = \"none\",\n  y_label = \"Neutrophil Count (/mL)\",\n  effect_label = \"Relative Effect\",\n  palette = pal_okabe_ito_blue,\n  rel_heights = c(0.75,1),\n  effect_x_lim = c(0.35, 1.25)\n)\n\nplot_grid(gg1, gg2, nrow=2, labels = \"AUTO\")"},{"path":"violations.html","id":"my-data-arent-normal-what-is-the-best-practice","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.3.2 My data aren’t normal, what is the best practice?","text":"major advantage using GLM count data instead classical linear model increased power significance test increased precision estimates. , CIs reflect asymmetry uncertainty, unlike CIs classical linear model. However, researchers gaurd slight increase Type error GLM.\nFigure 12.1: () histogram distribution used simulation non-normal count response (sampled negative binomial distribution mu theta equal observed values Fig. 6f). (B) example sampled fake data set.\n\nTable 12.1: Table Type error Power computed simulations non-normal count data. model columns linear model, generalized least squares, GLM negative binomial, GLM quasipoisson, permutation, Mann-Whitney-Wilcoxon. values model columns frequency simulated experiments p\nTable 12.1 summarizes results simulation compare performance alternatives t-test. simulated data modeled look like Figure 6f (Figure 12.1). methods classical linear model (lm) (equivalent t-test), GLS linear model (gls), negative binomial GLM (glm-nb), quasi-poisson GLM (glm-qp), permutation test (lmp), Mann-Whitney-Wilcoxon test. Performance based p-values test. frequency \\(p < 0.05\\) given column method. simulation run treatment effect, frequency simulated Type error, given nominal error \\(\\alpha = 0.05\\). Values less 0.05 conservative values greater 0.05 liberal. simulation run treatment effect, frequency simulated power test.limited parameter space non-normal distribution (shape count distribution including relationship mean variance, sample size, effect size), negative binomial GLM substantially higher power classical linear model, although comes cost inflated Type error. Even Normal simulated distribution, negative binomial GLM higher power classical linear model, , cost Type error.simulations negative binomial distributed response, quasipoisson GLM consistently higher power classical linear model excellent Type error control. performance permutation test similar quasipoisson GLM. simulations negative binomial distributed response, GLS linear model power classical linear model. Perhaps surprisingly, Mann-Whitney-Wilcoxon fails outperform classical linear model even underperforms classical linear parts parameter space.","code":""},{"path":"violations.html","id":"hidden-code-4","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.4 Hidden Code","text":"","code":""},{"path":"violations.html","id":"importing-and-wrangling-the-exp1b-data","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.4.1 Importing and wrangling the exp1b data","text":"","code":"\ndata_from <- \"The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue\"\nfile_name <- \"41591_2017_BFnm4297_MOESM1_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp1b <- read_excel(file_path,\n                     sheet = \"Fig 1 b thur c\",\n                     range = \"A1:C20\",\n                     col_names = TRUE) %>%\n  data.table() %>%\n  clean_names() %>%\n  na.omit() # get rid of blank row\n\nsetnames(exp1b,\n         old = names(exp1b),\n         new = c(\"sample\", \"diHOME\", \"bat_activity\"))\n\nexp1b[, id := substr(sample, 1, 6)]\nexp1b[, treatment := ifelse(substr(sample, 8,8) ==\n                              \"C\", \"cold\", \"saline\")]\nexp1b[, treatment := factor(treatment, levels = c(\"saline\", \"cold\"))]\n\n#View(exp1b)"},{"path":"violations.html","id":"importing-and-wrangling-the-exp2a-data","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.4.2 Importing and wrangling the exp2a data","text":"","code":"\ndata_from <- \"The cold-induced lipokine 12,13-diHOME promotes fatty acid transport into brown adipose tissue\"\nfile_name <- \"41591_2017_BFnm4297_MOESM2_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# assuming mice are independent and not same mouse used for all three treatment\nmelt_col_names <- paste(\"Animal\", 1:6)\nexp2a <- read_excel(file_path,\n                     sheet = \"Fig 2a\",\n                     range = \"A3:G6\",\n                     col_names = TRUE) %>%\n  data.table() %>%\n  melt(measure.vars = melt_col_names,\n       variable.name = \"id\",\n       value.name = \"diHOME\") # cannot start a variable with number\nsetnames(exp2a, old = colnames(exp2a)[1], new = \"treatment\")\n\ntreatment_order <- c(\"Control\", \"1 hour cold\", \"30 min NE\")\nexp2a[, treatment := factor(treatment, treatment_order)] # order levels\n\n#View(exp2a)"},{"path":"violations.html","id":"importing-and-wrangling-the-fig6f-data","chapter":"12 Violations of independence, homogeneity, or Normality","heading":"12.4.3 Importing and wrangling the fig6f data","text":"","code":"\n# need data_folder from earlier chunk\ndata_from <- \"Exercise reduces inflammatory cell production and cardiovascular inflammation via instruction of hematopoietic progenitor cells\"\nfile_name <- \"41591_2019_633_MOESM8_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# assuming mice are independent and not same mouse used for all three treatment\nmelt_col_names <- c(\"Sedentary\", \"Exercise\")\nfig6f <- read_excel(file_path,\n                     sheet = \"Figure 6f\",\n                     range = \"A7:B29\",\n                     col_names = TRUE) %>%\n  data.table() %>%\n  melt(measure.vars = melt_col_names,\n       variable.name = \"treatment\",\n       value.name = \"neutrophils\") %>%\n  na.omit() # danger!\n\ntreatment_levels <- melt_col_names\nfig6f[, treatment := factor(treatment,\n                            levels = treatment_levels)]\n\n# neutrophils is count/10^6\nfig6f[, neutrophil_count := round(neutrophils*10^6, 0)]\n\nfig6f[1, neutrophil_count]## [1] 4936007\n#View(fig6f)"},{"path":"issues.html","id":"issues","chapter":"13 Issues in inference","heading":"13 Issues in inference","text":"","code":""},{"path":"issues.html","id":"replicated-experiments-include-textttexperiment-as-a-random-factor-better-than-one-way-anova-of-means","chapter":"13 Issues in inference","heading":"13.1 Replicated experiments – include \\(\\texttt{Experiment}\\) as a random factor (better than one-way ANOVA of means)","text":"Replicated experiments norm bench biology. Researchers commonly 1) analyze mean response replicate, 2) pool data, ignoring come multiple, independent experiments, 3) analyze “representive” experiment.best practice analyzing data multiple, independent experiments, one almost never used outside certain subfields pipelines alternative exist, linear mixed model experiment id added random factor. mixed-effect repeated measures ANOVA equivalent one version kind linear model. analysis data pooled independent experiments linear mixed model best practice increases precision power relative t-test/ANOVA experiment means avoids pseudoreplication.’s important understand assumptions different models analyzing replicated experiments. study multiple, replicated experiments, replications “independent” – solutions re-made, instruments drifted re-calibrated, grad student work lived another day. experiment unique set factors contribute error variance response variable. measures within experiment share component error variance unique experiment , consequence, error (residuals) within experiment similar residuals experiments. violation independent sampling assumption linear model lack independence creates correlated error. consequences three analyses :Linear model/t-test/ANOVA pooled data ignoring experiment. model assumes perfect replication experiments, essentially, world exactly . assumption true, replicate within experiment independent data point. study n = 5 mice per treatment replicated five times n = 25. two treatment levels, t-test 48 df. , measures within experiment share common source contributing variance response within experiment (temperature room 1°C cooler causing machine give slightly different reading), 25 independent measures five measures within experiment independendent. true df less 48, much less depends magnitude correlated error resulting non-independence. Pooling data across experiments correlated error creates incorrectly small standard error mean, resulting incorrectly narrow confidence intervals incorrectly small p-values. Pooling data inflates false discovery.Linear model/t-test/ANOVA experiment means. Analysis means violates independence assumption means treatment levels within independent experiment expected share common sources measure variability experiment. Instead inflating false discovery, correlated error generally works discovery. correlated error among means can modeled increase precision gain power.Linear model experiment means \\(\\texttt{experiment_id}\\) added random factor account correlated error due unique sources variance within experiment. model equivalent mixed effect ANOVA full data set.Reproducibility blues. Researchers improving best practices archiving data public servers. Nevertheless, extremely rare find full data many experiments. analyses means multiple experiments, researchers almost always archive means – data means pooled – full set measurements experiment. , full data archived experiment id included – data complete pooled. way truly replicate result means archived (means computed correctly?). , way re-analyze data alternative models require full data experiment identified.","code":""},{"path":"issues.html","id":"issues-exp4d","chapter":"13 Issues in inference","heading":"13.1.1 Multiple experiments Example 1 (wound healing Exp4d)","text":"Article source: Distinct inflammatory wound healing responses complex caudal fin injuries larval zebrafishdata sourceThe experiment designed estimate effect STAT3(https://en.wikipedia.org/wiki/STAT3){target=“_blank”} knockout number cells expressing cytoskeletal protein vimentin wounded tissue.Following Fisher, can confident effect consistently get small p-values replicate experiments.\nTable 13.1: P-values three independent experients exp4d data.\nmethods combining p-values need raw data.","code":"\nids <- levels(exp4d[, experiment_id])\nn_exp <- length(ids)\np <- list()\nfor(exp_i in ids){\n  exp4d_m1 <- lm(vimentin_cells ~ genotype,\n           data = exp4d[experiment_id == exp_i,]) # subset\n  exp4d_m1_test <- coef(summary(exp4d_m1))\n  p[[exp_i]] <- exp4d_m1_test[\"genotypeSTAT3_ko\", \"Pr(>|t|)\"]\n}\n\ndata.table(\n  experiment = ids,\n  p = unlist(p)\n) %>%\n  kable(digits = 3,\n        caption = \"P-values from the three independent experients of the exp4d data.\") %>%\n  kable_styling()"},{"path":"issues.html","id":"issues-combined-experiments","chapter":"13 Issues in inference","heading":"13.1.2 Models for combining replicated experiments","text":"linear mixed model combining replicated experiments easily fit using aov_4 function afex package. aov_4 function fits mixed-effects ANOVA model, factorial ANOVA includes fixed random factors. Experiment 4d data, variable \\(\\texttt{genotype}\\) fixed factor variable \\(\\texttt{experiment_id}\\) random factor. information fixed random factors given Linear Mixed Model chapter.mixed-effect ANOVA models fit aov_4 linear mixed models inference equivalent balanced designs – give detail . aov_4 function uses linear mixed model formula interface useful substitute actually fitting equivalent linear mixed model fitting linear mixed model can sensitive data sometimes fails, even mixed-effect ANOVA model “works”.fit modelinference modelplot model","code":"\nexp4d_m1 <- aov_4(vimentin_cells ~ genotype + (genotype | experiment_id),\n           data = exp4d)\nexp4d_m1_emm <- emmeans(exp4d_m1, specs = \"genotype\")\n\nexp4d_m1_pairs <- contrast(exp4d_m1_emm,\n                           method = \"revpairwise\") %>%\n  summary(infer = TRUE)\nexp4d_m1_pairs %>%\n  kable(digits = c(1,2,3,1,2,2,1,5)) %>%\n  kable_styling()\n# wrangling to get this to work\n# ggplot_the_response doesn't like the fit object from afex. That's okay,\n# we use it only to get the individual points\nexp4d_m0 <- lm(vimentin_cells ~ genotype,\n           data = exp4d)\nexp4d_m0 <- lm(vimentin_cells ~ genotype * experiment_id,\n           data = exp4d)\nexp4d_m0_emm <- emmeans(exp4d_m0, specs = c(\"genotype\", \"experiment_id\"))\n\nggplot_the_response(fit = exp4d_m0,\n                    fit_emm = exp4d_m0_emm,\n                    fit_pairs = exp4d_m1_pairs,\n                    y_label = \"vimGFP+ cell count\",\n                    palette = pal_okabe_ito_blue)"},{"path":"issues.html","id":"understanding-model-exp4d_m1","chapter":"13 Issues in inference","heading":"13.1.3 Understanding Model exp4d_m1","text":"univariate multivariate models fit AOV_4 gives researchers easy tool R replicating ANOVA results software, Graphpad Prism JMP. really contrasts want, ANOVA table, can get contrasts passing aov_4 object emmeans.formula Model exp4d_m1 misleading. formula specifies linear mixed model random intercept random slope (see Models random factors – Blocking pseudoreplication model fit.aov_4 function actually fits two models.linear model aggregated data adjusted degrees freedom (“aov” univariate model). “Aggregated” data means treatment level within experiment (means-pooled data).multivariate linear model (multivariate model) aggregated data. multivariate model multiple Y variables. exp4d data, experiment (containing means treatments) seperate \\(Y\\) variable.default output univariate model researcher can choose multivariate model.\nTable 13.2: Contrasts univariate multivarate models fit exp4d data.\nunivariate multivariate models single factor two treatment levels, data. Otherwise two differ. See Section 16.7.10 Linear Mixed Models chapter.univariate model equivalent linear mixed model exp4d_m2 (see next section) aggregated data missing data.number replicates within treatment experiment combinations, univariate model equivalent linear mixed model exp4d_m3 full (aggregated) data.","code":"\nexp4d_m1_pairs_multi <- emmeans(exp4d_m1,\n                                specs = \"genotype\",\n                                model = \"multivariate\") %>%\n  contrast(method = \"revpairwise\") %>%\n  summary(infer = TRUE)"},{"path":"issues.html","id":"the-univariate-model-is-equivalent-to-a-linear-mixed-model-of-the-aggregated-data-model-exp4d_m2","chapter":"13 Issues in inference","heading":"13.1.4 The univariate model is equivalent to a linear mixed model of the aggregated data (Model exp4d_m2)","text":"aggregate datafit model\nTable 13.3: Contrasts univariate model fit AOV_4 (exp4d_m1) linear mixed model aggregated data (exp4d_m2)\nNotesThe univariate model equivalent Model exp4d_m2 missing data (example, one treatment data experiment 2).\n2.factor \\(\\texttt{experiment_id}\\) added random intercept using formula notation (1 | experiment_id). random intercept models common variation shared within experiments.Random intercepts introduced Example 1 (exp1b) – paired t-test special case linear mixed model Violations chapter described detail Example 1 – random intercepts slopes explainer (demo1) Linear Mixed Model chapter.","code":"\n# compute the experiment means\nexp4d_means <- exp4d[, .(vimentin_cells = mean(vimentin_cells)),\n                     by = .(genotype, experiment_id)]\nexp4d_m2 <- lmer(vimentin_cells ~ genotype + (1 | experiment_id),\n                 data = exp4d_means)"},{"path":"issues.html","id":"a-linear-mixed-model-of-the-full-data","chapter":"13 Issues in inference","heading":"13.1.5 A linear mixed model of the full data","text":"NotesModel exp4d_m3 equivalent univariate mixed-effect ANOVA (exp4d_m1) linear mixed model aggregated data (exp4d_m2) number subsamples every genotype experiment combination.Model exp4d_m3 linear mixed model random factor added two random intercepts.(1|experiment_id) intercept models common variation shared within experiments.(1|experiment_id:genotype) interaction intercept models common variation shared within experiment genotype combinations.function lmer returns “boundary (singular) fit” message, warning researchers use caution using model inference. message rare number experiments small (2-4).Note 5 raises question, bother linear mixed model full data aov_4 function always “works” design balanced? short answer , just use aov_4 model default (univariate model) outputs. long answer , read chapter Models random factors – linear mixed models.","code":"\nexp4d_m3 <- lmer(vimentin_cells ~ genotype +\n                         (1|experiment_id) +\n                         (1|experiment_id:genotype),\n           data = exp4d)"},{"path":"issues.html","id":"analysis-of-the-experiment-means-has-less-precision-and-power","chapter":"13 Issues in inference","heading":"13.1.6 Analysis of the experiment means has less precision and power","text":"Compare inference mixed-ANOVA exp4d_m1 linear mixed model exp4d_m2 () linear model experiment means (equivalent t-test/ANOVA)\nTable 13.4: Contrasts univariate model fit AOV_4 (exp4d_m1) linear model experiment means (exp4d_m4)\nNotesModel exp4d_m4 violates independence assumption residuals within experiment expected similar. Analyzing experiment means fix . aggregated data, correlated error masks true effect.SE estimate \\(\\texttt{genotype}\\) effect much smaller mixed ANOVA/random intercept model fixed effect model experiment means.degrees freedom fixed effect model slighly larger mixed model exp4d_m1. Degrees freedom measure independent variation – fixed effect model assumes every measure independent. aren’t. inflated degrees freedom pseudoreplication.Despite (slightly) inflated degrees freedom fixed effects model, mixed model much narrow CIs smaller p-value. item 2.increased precision power mixed model relative fixed effect model experiment means general result one specific example.","code":"\nexp4d_m4 <- lm(vimentin_cells ~ genotype, data = exp4d_means)"},{"path":"issues.html","id":"dont-do-this-a-t-testfixed-effect-anova-of-the-full-data","chapter":"13 Issues in inference","heading":"13.1.7 Don’t do this – a t-test/fixed-effect ANOVA of the full data","text":"exp4d_m5 fixed effect model (added random factors).\n(#tab:issues-exp4d_m5-show)Contrasts univariate model fit AOV_4 (exp4d_m1) linear model full data (exp4d_m5)\nNotesModel exp4d_m4 violates independence assumption residuals within experiment expected similar.SE estimate \\(\\texttt{genotype}\\) effect much smaller mixed ANOVA/random intercept model fixed effect model degrees freedom fixed effect model much greater mixed model exp4d_m1. Degrees freedom measure independent variation – fixed effect model assumes every measure independent. aren’t. inflated degrees freedom pseudoreplication.consequence pseudoreplication -small p-value. Pseudoreplication leads false discovery.","code":"\nexp4d_m5 <- lm(vimentin_cells ~ genotype, data = exp4d)"},{"path":"issues.html","id":"issues-pre-post","chapter":"13 Issues in inference","heading":"13.2 Comparing change from baseline (pre-post)","text":"\nFigure 13.1: Serum DPP4 levels baseline 3 months two groups. () actual experiment: two treatment levels randomly allocated treatment baseline. interested effect DMAB 3 months use baseline measure increase precision. (B) thought experiment: two treatment levels different groups baseline. give DMAB groups baseline estimate difference response.\nlongitudinal experiment, response variable measured individuals (baseline) (post-baseline) condition applied. Experiments one post-baseline measure taken known pre-post experiments. simplicity, call baseline measure \\(\\texttt{pre}\\) post-baseline measure \\(\\texttt{post}\\).Researchers often analyze pre-post data comparing change score (\\(\\texttt{post} - \\texttt{pre}\\)) groups using \\(t\\)-test one-way ANOVA (non-parametric equivalent Mann-Whitney-Wilcoxon). linear model form t-test \\[\n\\texttt{post} - \\texttt{pre} = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\varepsilon\n\\]change score model. similar analysis t-test ANOVA using percent change baseline response:\\[\n\\frac{\\texttt{post} - \\texttt{pre}}{\\texttt{pre}} \\times 100 = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\varepsilon\n\\]best practice estimate treatment effect depends much treatment applied relative baseline (\\(\\texttt{pre}\\)) measure. Consider two experiments Figure 13.1In 13.1A, individuals randomized “Placebo” “Denosumab” groups. Plasma DPP4 measured baseline, treatment applied, , three months later, plasma DPP4 re-measured. treatment “Denosumab” want compare “Placebo”. key feature design individuals initial group treated baseline measure. Consequently, expected difference means measure baseline zero.13.1A, individuals randomized “Placebo” “Denosumab” groups. Plasma DPP4 measured baseline, treatment applied, , three months later, plasma DPP4 re-measured. treatment “Denosumab” want compare “Placebo”. key feature design individuals initial group treated baseline measure. Consequently, expected difference means measure baseline zero.13.1B, treatment interest (knockout) applied prior baseline, DPP4 measured baseline without Denosumab groups given Denosumab measured three months later. treatment Denosumab “knockout” want compare “wild type” two different conditions. key individuals randomly sampled two different groups (wild type knockout). Consequently, expect expected difference means measure baseline zero.13.1B, treatment interest (knockout) applied prior baseline, DPP4 measured baseline without Denosumab groups given Denosumab measured three months later. treatment Denosumab “knockout” want compare “wild type” two different conditions. key individuals randomly sampled two different groups (wild type knockout). Consequently, expect expected difference means measure baseline zero.best practice Design 2 change score model given (equivalents discussed ). best practice Design 1 change score model linear model baseline measure added covariate.\\[\n\\texttt{post} = \\beta_{0} + \\beta_{1}(\\texttt{treatment}_{\\texttt{tr}}) + \\beta_{2}(\\texttt{pre}) + \\varepsilon\n\\]model commonly known ANCOVA model (Analysis Covariance) even ANOVA table generated. explanation best practice given , section regression mean. ANCOVA linear model common clinical medicine pharmacology, researchers frequently warned regression mean statisticians. contrast, ANCOVA linear model rare basic science experimental biology. analysis linear models added covariates focus chapter Linear models added covariates.Alert! individuals sampled population treatment randomized baseline, test difference means response variable baseline use ANCOVA linear model \\(p > 0.05\\) change score model \\(p < 0.05\\). best practice function design, leads expectation difference means baseline.individuals sampled population treatment randomized baseline, expected difference baseline zero.Use ANCOVA linear model.can go wrong use change score model? Regression mean.individuals sampled two populations treatment applied prior baseline, expect difference baseline zero (even treatment magic).Use change score model.can go wrong use ANCOVA linear model? Two things. First, ANCOVA linear model computes biased estimate true effect, meaning sample size increases estimate converge true value true value plus bias. Second, change score power ANCOVA linear model assumption sampling different populations baseline.Alert! Researchers also use two-way ANOVA repeated measures ANOVA analyze data like . Two-way ANOVA invalid multiple measures individual violates independence assumption. Repeated measures ANOVA give equivalent results pre-post experiment better practice methods available one post-baseline measure.better practice methods linear models correlated error, including GLS linear mixed models. pre-post design, models give equivalent results change score model also allows estimate additional effects interest (see ). detail analysis pre-post experiments, see Linear models longitudinal experiments – . pre-post designs chapter.","code":""},{"path":"issues.html","id":"pre-post-example-1-dpp4-fig4c","chapter":"13 Issues in inference","heading":"13.2.1 Pre-post example 1 (DPP4 fig4c)","text":"source: Identification osteoclast-osteoblast coupling factors humans reveals links bone energy metabolismThe data left panel Figure 13.1 experiment estimate effect Denosumab plasma levels enzyme DPP4 humans. Denosumab monocolonal antibody inhibits osteoclast maturation survival. Osteoclasts secrete enzyme DPP4.","code":""},{"path":"issues.html","id":"for-the-ancova-linear-model-the-data-need-to-be-in-wide-format","chapter":"13 Issues in inference","heading":"13.2.1.1 For the ANCOVA linear model, the data need to be in wide format","text":"ANCOVA linear model, baseline measure added covariate thought separate variable “response”. makes sense – “response” treatment treatment hasn’t applied? means baseline post-baseline measures DPP4 separate columns data (Table 13.5.\nTable 13.5: First six rows DPP4 data wide format showing baseline post-baseline measures DPP4 separate variables.\n","code":""},{"path":"issues.html","id":"fit-the-ancova-model","chapter":"13 Issues in inference","heading":"13.2.1.2 Fit the ANCOVA model","text":"","code":"\nm1 <- lm(DPP4_post ~ treatment + DPP4_baseline,\n         data = fig4c)"},{"path":"issues.html","id":"inference-2","chapter":"13 Issues in inference","heading":"13.2.1.3 Inference","text":"coefficient tableNotesHere, care \\(\\texttt{treatmentDenosumab}\\) row. estimated effect Denosumab serum DPP4 adjusted baseline (use word “control” baseline values controlled manipulative sense).Chapter Linear models added covariates explains interpretation coefficients detail.emmeans tableNotesThe means conditional treatment value \\(\\texttt{DPP4_baseline}\\). emmeans uses grand mean \\(\\texttt{DPP4_baseline}\\) compute conditional means. conditional means adjusted baseline DPP4. Note means equal sample means.contrast tableNotesAs coefficient table, contrast estimated effect Denosumab serum DPP4. difference means adjusted (controlled!) baseline values.\nFigure 13.2: Estimated effect Denosumab serum DPP4 relative placebo.\nNotesThe treatment means Figure 13.2 conditional means adjusted baseline measure , therefore, equal sample means. estimated effect difference conditional means sample means inferential statistics (CI, p-value) based difference conditional sample means.","code":"\nm1_coef <- cbind(coef(summary(m1)),\n                 confint(m1))\n\nm1_coef %>%\n  kable(digits = c(1,2,2,4,1,1)) %>%\n  kable_styling()\nm1_emm <- emmeans(m1, specs = \"treatment\")\n\nm1_emm %>%\n  summary() %>%\n  kable(digits = c(1,2,2,0,1,1)) %>%\n  kable_styling()\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\n\nm1_pairs %>%\n  kable(digits = c(1,3,2,0,1,1,1,3)) %>%\n  kable_styling()\nggplot_the_model(m1,\n                 m1_emm,\n                 m1_pairs,\n                 y_label = \"Serum DPP4 (ng/mL)\",\n                 effect_label = \"Effect (ng/mL)\",\n                 palette = pal_okabe_ito_blue,\n                 legend_position = \"none\")"},{"path":"issues.html","id":"what-if-the-data-in-example-1-were-from-from-an-experiment-where-the-treatment-was-applied-prior-to-the-baseline-measure","chapter":"13 Issues in inference","heading":"13.2.2 What if the data in example 1 were from from an experiment where the treatment was applied prior to the baseline measure?","text":"","code":""},{"path":"issues.html","id":"fit-the-change-score-model","chapter":"13 Issues in inference","heading":"13.2.2.1 Fit the change score model","text":"NotesThe change score created LHS model formula. Alternatively, change score created variable fig4c_fake data.table using fig4c_fake[, change := DPP4_post - DPP4_baseline]. Making change model formula shows flexibility model formula method fitting linear models R.","code":"\nm1 <- lm(DPP4_post - DPP4_baseline ~ genotype,\n         data = fig4c_fake)\nm1_emm <- emmeans(m1, specs = \"genotype\")\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\")\nm1_pairs %>%\n  kable() %>%\n  kable_styling()"},{"path":"issues.html","id":"issues-interaction","chapter":"13 Issues in inference","heading":"13.2.2.2 Rethinking a change score as an interaction","text":"treatment randomized baseline, researcher focus effect treatment (difference post-basline measures), adjusted baseline measures. addition baseline variable covariate increases precision treatment effects power significance test.treatment generated prior baseline, researcher focus difference change baseline post-baseline, \\[\neffect = (post_{ko} - pre_{ko}) - (post_{wt} - pre_{wt})\n\\]difference change scores, difference differences. difference differences interaction effect genotype (“wt” “ko”) time period measurement DPP4 (baseline post-baseline). usual way estimate interaction effects linear model two crossed factors, covered detail chapter # Linear models two categorical X – Factorial designs (“two-way ANOVA”). interaction effect (equal difference among mean change scores) can estimated model\\[\n\\texttt{dpp4} = \\beta_{0} + \\beta_{1}(\\texttt{genotype}_{\\texttt{ko}}) + \\beta_{2}(\\texttt{time}_{\\texttt{post}}) + \\beta_{3}(\\texttt{genotype}_{\\texttt{ko}} \\times \\texttt{time}_{\\texttt{post}}) + \\epsilon\n\\]R script looks like thisThis model two factors (\\(\\texttt{genotype}\\) \\(\\texttt{time}\\)), two levels. two levels \\(\\texttt{time}\\) “pre” “post”. \\(genotype_{ko}\\) indicator variable “ko” \\(time_{post}\\) indicator variable “post”Don’t fit model – data violate independence assumption! violation arises \\(\\texttt{dpp4}\\) measured twice individual measures components response (\\(\\texttt{dpp4}\\) measures stacked single column). violation doesn’t arise ANCOVA linear model baseline measures covariate response (remember independence assumption applies response variable).Alert! pretty common see model fit pre-post longitudinal data. consequence violation invalid (large) degrees freedom computing standard errors p-values. kind pseudoreplication.model correlated error due two measures per individual, use linear mixed model using \\(\\texttt{id}\\) added random factor. Linear mixed models introduced Violations independence, homogeneity, Normality chapter covered detail Models random factors – Blocking pseudoreplication chapter.NotesThis result change score score model.interaction effect one coefficients model get CIs change score model, need use Satterthwaite’s formula degrees freedom. pass emmeans function using lmer.df = \"Satterthwaite\"interaction contrast computed using contrast function using interaction = \"revpairwise\" instead method = \"revpairwise\".","code":"\nm2_fixed <- lm(dpp4 ~ genotype*time,\n               data = fig4c_fake_long)\nm2 <- lmer(dpp4 ~ genotype*time + (1|id),\n           data = fig4c_fake_long)\nm2_emm <- emmeans(m2,\n                  specs = c(\"genotype\", \"time\"),\n                  lmer.df = \"Satterthwaite\")\nm2_ixn <- contrast(m2_emm,\n                   interaction = \"revpairwise\")\nm2_ixn %>%\n  kable() %>%\n  kable_styling()"},{"path":"issues.html","id":"the-linear-mixed-model-estimates-additional-effects-that-we-might-want","chapter":"13 Issues in inference","heading":"13.2.2.3 The linear mixed model estimates additional effects that we might want","text":"linear mixed model change score model give result effect treatment response different conditions, linear mixed model estimates additional effects may interest. use real example (Example 2) demonstrate .","code":""},{"path":"issues.html","id":"pre-post-example-2-xx-males-fig1c","chapter":"13 Issues in inference","heading":"13.2.3 Pre-post example 2 (XX males fig1c)","text":"Source: AlSiraj, Y., Chen, X., Thatcher, S.E., Temel, R.E., Cai, L., Blalock, E., Katz, W., Ali, H.M., Petriello, M., Deng, P. Morris, .J., 2019. XX sex chromosome complement promotes atherosclerosis mice. Nature communications, 10(1), pp.1-13.Response variable – \\(\\texttt{fat_mass}\\). Fat mass measured mouse baseline (exposed standard chow diet) one week western diet.Fixed factor – design two crossed factors (sex chromosome complement gonad type) two levels collapse four treatment combinations single factor \\(\\texttt{treatment}\\) four levels: “female_xx”, “female_xy”, “male_xx”, “male_xy”. Male female typical sex merely observed constructed presence absence SRY autosome using Four Core Genotype mouse model. SRY determines gonad develops (ovary testis). Females autosome SRY. Males . Similarly, chromosome complement observed manipulated. “xx”, neither chromosome SRY natural condition two X chromosomes. “xy”, SRY removed Y chromosome.Random factor \\(\\texttt{id}\\). identification individual mouse.Planned comparisons“female_xy” - “female_xx” baseline (chow diet)“male_xx” - “male_xy” baseline (chow diet)“female_xy” - “female_xx” one week (western diet)“male_xx” - “male_xy” one week (western diet)interaction contrast (3 - 1) addresses, effect chromosome complement females conditional dietthe interaction contrast (4 - 2) addresses, effect chromosome complement males conditional diet","code":""},{"path":"issues.html","id":"fit-the-change-score-model-1","chapter":"13 Issues in inference","heading":"13.2.3.1 Fit the change score model","text":"change score model estimates planned comparisons 5 6.Inference change score model","code":"\nm1 <- lm(week_1 - baseline ~ treatment,\n         data = fig1c_wide)\nm1_emm <- emmeans(m1, specs = \"treatment\")\n\nm1_planned <- contrast(m1_emm,\n                       method = \"revpairwise\",\n                       adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nm1_planned[c(1, 6),] %>%\n  kable(digits = c(1,2,2,1,2,2,2,3)) %>%\n  kable_styling()"},{"path":"issues.html","id":"using-the-linear-mixed-model-to-compute-all-six-planned-comparisons","chapter":"13 Issues in inference","heading":"13.2.3.2 Using the linear mixed model to compute all six planned comparisons","text":"Notesimportant add lmer.df = \"Satterthwaite\" argumentinteraction contrastsThe interaction contrasts estimate planned comparisons 5 6.NotesThese results using change scores.Simple effectsThe simple effects estimate planned comparisons 1-4.can combine six planned comparisons single table.","code":"\nm2 <- lmer(fat_mass ~ treatment*time + (1|id), data = fig1c)\nm2_emm <- emmeans(m2,\n                  specs = c(\"treatment\", \"time\"),\n                  lmer.df = \"Satterthwaite\")\nm2_ixn <- contrast(m2_emm,\n         interaction = c(\"revpairwise\"),\n         by = NULL,\n         adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nm2_planned_ixn <- m2_ixn[c(1,6), ] %>%\n  data.table()\n\nm2_planned_ixn %>%\n  kable(digits = c(1,1,2,2,1,2,2,2,3)) %>%\n  kable_styling()\n# get simple effects from model\n\nm2_pairs <- contrast(m2_emm,\n         method = c(\"revpairwise\"),\n         simple = \"each\",\n         combine = TRUE,\n         adjust = \"none\") %>%\n  summary(infer = TRUE)\n\n# reduce to planned contrasts\nm2_planned_simple <- m2_pairs[c(1,6,7,12),] %>%\n  data.table()\n\n# clarify contrast\nm2_planned_simple[, contrast := paste(time, contrast, sep = \": \")]\n\n# dump first two cols\nkeep_cols <- names(m2_planned_simple)[-(1:2)]\nm2_planned_simple <- m2_planned_simple[, .SD, .SDcols = keep_cols]\n\nm2_planned_simple %>%\n  kable(digits = c(1,3,3,1,2,2,2,5)) %>%\n  kable_styling()\n# create contrast table for ixns\nm2_planned_ixn[, contrast := paste0(\"ixn: \", treatment_revpairwise)]\n\n# dump first two cols\nkeep_cols <- names(m2_planned_ixn)[-(1:2)]\nm2_planned_ixn <- m2_planned_ixn[, .SD, .SDcols = keep_cols]\n\n# row bind -- smart enought to recognize column order\nm2_planned <- rbind(m2_planned_simple,\n                    m2_planned_ixn)\n\nm2_planned %>%\n  kable(digits = c(1,2,2,1,2,2,2,5)) %>%\n  kable_styling()"},{"path":"issues.html","id":"issues-regression-to-mean","chapter":"13 Issues in inference","heading":"13.2.4 Regression to the mean","text":"Regression mean phenomenon extreme value sampled, next sample likely less extreme – closer mean. makes sense. randomly sample single human male individual 6’10” (four standard deviations mean), height next human male randomly sample almost certainly closer mean (5’10” united states). phenomenon also applies sampling mean. randomly sample five human males mean height group 5’6” (3 SEM mean), mean height next sample five human males measure almost certainly closer mean. , phenomenon applies sampling difference means. randomly sample two groups five human males difference mean heights 5.7” (3 SED), difference mean height next sample two groups human males measure almost certainly closer zero.regression mean apply analysis change scores pre-post experiment? Consider experiment response body weight mice. pre-post experiment, mice randomized treatment group baseline. Weight measured baseline post-baseline. expect difference means baseline zero. treatment effect, expect difference means post-baseline zero. sampling error difference weight baseline. expect mean difference post-baseline? . Even taken 1 hour, post-baseline weight mouse equal baseline weight variation water intake loss, food intake, fecal weight, variables affect body weight (within-mouse variance). correlation pre post measures – individual mice weigh mean baseline generally weigh mean post-baseline. , bigger within-mouse variance (, longer time difference baseline post-baseline), smaller correlation. consequence uncontrolled variables contribute sampling variance means difference means baseline post-baseline. difference unusually large baseline uncontrolled factors contribution within mouse variance, expect difference post-baseline less large – regression mean (Figure 13.3A). regression mean baseline post-baseline measures emerge treatment time interaction, , equivalently, difference mean change score treatments (Figure 13.3B).\nFigure 13.3: Regression mean. individual values () fake data sampled normal distribution true mean equal 30 baseline (gray line) 35 post-baseline(gray line). unusually large different means baseline extreme event. Consequently, difference post-baseline much smaller. regression mean easily visualized lines converge post-baseline value. (B) results linear model (t-test) fit data () using change scores response variable. apparent treatment effect due regression mean.\n","code":""},{"path":"issues.html","id":"longitudinal-designs-with-more-than-one-post-baseline-measure","chapter":"13 Issues in inference","heading":"13.3 Longitudinal designs with more than one-post baseline measure","text":"rigorous analysis longitudinal data typically requires sophisticated statistical models many purposes, longitudinal data can analyzed simple statistical models using summary statistics longitud data. Summary statistics include slope line response fairly linear Area Curve (AUC) humped responses.","code":""},{"path":"issues.html","id":"issues-auc","chapter":"13 Issues in inference","heading":"13.3.1 Area under the curve (AUC)","text":"AUC (Area curve) common simple summary statistic analyzing data glucose tolerance test many longitudinal experiments. use AUC glucose tolerance tests (GTT) example.","code":""},{"path":"issues.html","id":"auc-and-iauc","chapter":"13 Issues in inference","heading":"13.3.1.1 AUC and iAUC","text":"Let’s generate plot fake GTT data single individual order clarify AUC measurements define new ones.\nFigure 13.4: Glucose tolerance curve measures fake GTT data. . Glucose values individual, B. glucose values shifted baseline value zero. post-baseline values change scores (glucose - baseline), change baseline. filled area AUC () iAUC (B). dashed line mean glucose test period () mean change baseline test period (B).\nAUC – glucose tolerance curve individual connected set straight lines serves proxy continuous change glucose period (Figure 13.4A). AUC area set straight lines (Figure 13.4A) conveniently computed sum areas connected trapezoids created connected lines.iAUC – incremental AUC (iAUC) baseline-zeroed AUC. can visualized area connected lines rigidly shifted baseline value zero (Figure 13.4B). iAUC computed using trapezoid rule first subtracting individual’s baseline value individual’s values (including baseline).","code":"\nfake_auc <- data.table(\n  time = c(0, 15, 30, 60, 120),\n  glucose = c(116, 268, 242, 155, 121)\n)\nfake_auc[, glucose_change := glucose - glucose[1]]"},{"path":"issues.html","id":"issues-iauc-change-score","chapter":"13 Issues in inference","heading":"13.3.1.2 Rethinking the iAUC as a change-score","text":"baseline-zeroed values glucose used compute iAUC change scores baseline measure. makes iAUC change score – AUC minus area baseline.","code":""},{"path":"issues.html","id":"rethinking-a-t-test-of-the-auc-as-a-t-test-of-the-glucuose-concentration-averaged-over-the-test-period","chapter":"13 Issues in inference","heading":"13.3.1.3 Rethinking a t-test of the AUC as a t-test of the glucuose concentration averaged over the test period","text":"glucose concentration averaged post-baseline period individual \\(glucose_{gtt-post} = \\frac{AUC}{Period}\\). Importantly, t-test \\(glucose_{gtt-post}\\) equivalent t-test \\(AUC\\) mean glucose values simply AUC values times constant (test period) individuals.","code":""},{"path":"issues.html","id":"rethinking-a-t-test-of-the-iauc-as-a-t-test-of-the-change-from-baseline-change-score-averaged-over-the-test-period","chapter":"13 Issues in inference","heading":"13.3.1.4 Rethinking a t-test of the iAUC as a t-test of the change from baseline (change score) averaged over the test period","text":"change baseline (change score) averaged test period individual \\(glucose_{gtt-change} = \\frac{iAUC}{Period}\\). , t-test \\(glucose_{gtt-change}\\) equivalent t-test \\(iAUC\\) \\(glucose_{gtt-change}\\) values simply iAUC values times constant (test period) individuals.","code":""},{"path":"issues.html","id":"rethinking-auc-as-a-pre-post-design.","chapter":"13 Issues in inference","heading":"13.3.1.5 Rethinking AUC as a pre-post design.","text":"can now rethink data used construct AUC pre-post design using baseline value (\\(glucose_0\\)) measure \\(pre\\), \\(glucose_{gtt-post}\\) measure \\(post\\) \\(glucose_{gtt-change}\\) measure \\(post - pre\\) (note \\(glucose_{gtt-change} = glucose_{gtt-post} - glucose_0\\)). , can use principles outlined Comparing change baseline (pre-post) determine best practices.","code":""},{"path":"issues.html","id":"issues-auc-best","chapter":"13 Issues in inference","heading":"13.3.1.6 Best practice strategies for analyzing AUC","text":"treatment applied prior baseline measure, use change score model (use linear mixed model want estimate effect baseline). common kind design experimental biology literature\nglucose_gtt_post - glucose_0 ~ treatment\niauc ~ treatment. t p values equivalent 1a\nglucose ~ treatment*time + (1|id). linear mixed model allows computation effect treatment baseline effect treatment change response condition (interaction effect). LMM glucose values time periods pre-post LMM values \\(\\texttt{glucose_0}\\) \\(\\texttt{glucose_gtt_post}\\) stacked data column \\(\\texttt{glucose}\\). t p values interaction effect equivalent 1a 1b.\nglucose_gtt_post - glucose_0 ~ treatmentiauc ~ treatment. t p values equivalent 1aglucose ~ treatment*time + (1|id). linear mixed model allows computation effect treatment baseline effect treatment change response condition (interaction effect). LMM glucose values time periods pre-post LMM values \\(\\texttt{glucose_0}\\) \\(\\texttt{glucose_gtt_post}\\) stacked data column \\(\\texttt{glucose}\\). t p values interaction effect equivalent 1a 1b.treatment randomized baseline, use ANCOVA linear model.\nglucose_post ~ treatment + glucose_0.\nauc ~ treatment + glucose_0. t p values equivalent 2a units response effect.\nglucose_post ~ treatment + glucose_0.auc ~ treatment + glucose_0. t p values equivalent 2a units response effect.","code":""},{"path":"issues.html","id":"the-difference-in-iauc-between-groups-is-an-interaction-effect.-this-is-an-important-recognition.","chapter":"13 Issues in inference","heading":"13.3.1.7 The difference in iAUC between groups is an interaction effect. This is an important recognition.","text":"treatment randomized baseline, potential effects glucose tolerance test :effect treatment baseline, measure going independent added glucose. baseline effect.effect glucose infusion, measure physiological response absorptive state. effect treatment level change scores. Researchers typically interested effect (know glucose levels rise fall).difference change baseline new condition (glucose infusion), equivalent difference change scores. interaction effect. interaction effect evidence difference treatment control post-baseline (absorptive state) period difference occurring baseline (fasted state) something different (Figure 13.5).\nFigure 13.5: Effects glucose tolerance data experiment treatment randomized baseline. “something different interaction effect”. difference change baseline (difference change scores).\niAUC change score (see Rethinking iAUC change-score ). Recall (re-read) section 13.2.2.2 difference mean change-score two groups interaction effect linear model two factors $} (“cn” “tr”) $} (“pre” “post”) interaction.\\[\n\\texttt{glucose} = \\beta_0 + \\beta_1 (\\texttt{treatment}_\\texttt{tr}) + \\beta_2 (\\texttt{time}_\\texttt{post}) + \\beta_3 (\\texttt{treatment}_\\texttt{tr} \\times \\texttt{time}_\\texttt{post}) + \\varepsilon\n\\]means t-test iAUC equivalent thet-test interaction effect treatment time.recognition adds important perspective controversy using iAUC analysis glucose tolerance curves. iAUC often used place AUC “adjust” baseline variation glucose belief adjustment makes AUC measure independent (uncorrelated ) baseline glucose. Allison et al. note, change score doesn’t us. correct way adjust baseline variation adding baseline measure covariate linear model (Section 13.2 ).\\[\n\\texttt{glucose_mean_post} = \\beta_0 + \\beta_1 (\\texttt{treatment}_\\texttt{tr}) + \\beta_2 (\\texttt{glucose_0}) + \\varepsilon\n\\]Nevertheless, pre-post design, treatment applied prior baseline measure, interaction effect want measure treatment effect difference post-baseline means conditional (adjusted ) baseline. want change score model ANCOVA linear model.","code":""},{"path":"issues.html","id":"issues-in-the-analysis-of-designs-where-the-treatment-is-applied-prior-to-the-baseline-measure","chapter":"13 Issues in inference","heading":"13.3.1.8 Issues in the analysis of designs where the treatment is applied prior to the baseline measure","text":"Almost glucose tolerance tests experimental biology literature designs treatment (genotype, diet, exercise) applied prior baseline measure expect difference means zero baseline. , common, far standard, researchers analyze data using t tests post-hoc tests AUC adjusted baseline response. equivalent recommended linear model 1b.Two common alternative analyses potential consequences can severely mislead researcher conflated effects areSeparate t tests time point.t test/post-hoc tests \\(\\texttt{auc}\\) (standard AUC)reason can mislead results conflate baseline effect interaction effect (Figure 13.5). baseline effect interaction effect physiological interest. difference mean AUC combines two effects. difference means post-baseline time point combines two effects. t-test AUC separate t-tests post-baseline time points conflate effects. conflated results muddle physiology. researcher wants simply conclude “knockout causes glucose intolerance” full AUC okay researcher recognize question trying answer. researcher asking, “difference treatment groups absorptive (post-baseline) state something different, , difference treatment groups fasted (baseline) state?”, researcher avoid t-tests AUC separate t-tests post-baseline times.Two common, alternative analyses can mislead model assumptions severely violated best practice models aretwo-way ANOVA treatment time factors, followed post-hoc tests. advantage analysis ability estimate effect baseline interaction effect. , correlated error due multiple measures individual violates independence assumption. Inference model generally optimisitic – CIs narrow p-values small. example pseudoreplication. correlated error can modeled GLS linear model Linear Mixed Model.repeated measures ANOVA treatment time factors, followed post-hoc tests. Like two-way ANOVA, repeated measures ANOVA can used estimate baseline interaction effects. Unlike two-way ANOVA, repeated measures ANOVA models correlated error. model correlated error unrealistic. better alternatives modeling correlated error GLS linear model Linear Mixed Model.","code":""},{"path":"issues.html","id":"auc-example-1-treatment-applied-prior-to-baseline","chapter":"13 Issues in inference","heading":"13.3.1.9 AUC Example 1 – Treatment applied prior to baseline","text":"Source: Innervation thermogenic adipose tissue via calsyntenin 3β–S100b axisSource data: Fig. 3f","code":""},{"path":"issues.html","id":"an-initial-plot","chapter":"13 Issues in inference","heading":"13.3.1.9.1 An initial plot","text":"","code":""},{"path":"issues.html","id":"inference-3","chapter":"13 Issues in inference","heading":"13.3.1.9.2 Inference","text":"change-score linear model (model 1a), estimates interaction effect (effect treatment difference change baseline post-baseline).NotesThe estimate average difference period difference average change baseline. knockout average change baseline 48.3 mg/dL larger average change baseline wildtype. period, blood glucose knockout 48.3 mg/dL larger expected difference mechanisms generating difference post-baseline mechanisms generating differences baseline.equivalent t test AUC adjusted baseline ($)NotesThe estimate effect average change baseline period times period. Can anyone look number claim sincerity, wow huge!. estimate change-score model, simply difference average change baseline period number interpretable.change-score model (analysis AUC adjusted baseline) estimate effect treatment baseline (effect fasted state). Researchers probably want . need linear model correlated error linear mixed model.Linear mixed model (model 1c), estimates interaction effect baseline effect.NotesThe treatment effect baseline first row m2_pairs contrast table linear mixed model.interaction effect (effect treatment change baseline) m2_ixn contrast table linear mixed model. estimate, SE, confidence intervals, t-value, p-value change score model m1_pairs.","code":"\nm1 <- lm(glucose_gtt_post - glucose_0 ~ treatment,\n         data = fig3f_wide)\n\nm1_emm <- emmeans(m1, specs = \"treatment\")\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\n\nm1_pairs %>%\n  kable(digits = 3) %>%\n  kable_styling()\n# increased digits to compare with lmm below\nm1_t <- t.test(iauc ~ treatment,\n       data = fig3f_wide,\n       var.equal = TRUE) %>%\n  tidy()\n\nm1_t[1, c(1, 4:8)]## # A tibble: 1 × 6\n##   estimate statistic p.value parameter conf.low conf.high\n##      <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n## 1   -5796.     -1.31   0.215        12  -15445.     3852.\nm2 <- lmer(glucose ~ treatment*time + (1|id),\n           data = fig3f_long)\nm2_emm <- emmeans(m2,\n                  specs = c(\"treatment\", \"time\"),\n                  lmer.df = \"Satterthwaite\")\nm2_pairs <- contrast(m2_emm,\n                     method = \"revpairwise\",\n                     simple = \"each\",\n                     combine = TRUE,\n                     adjust = \"none\") %>%\n  summary(infer = TRUE)\nm2_ixn <- contrast(m2_emm,\n                   interaction = \"trt.vs.ctrl\",\n                   adjust = \"none\") %>%\n  summary(infer = TRUE)\n\nm2_pairs %>%\n  kable(digits = 3) %>%\n  kable_styling()\nm2_ixn %>%\n  kable(digits = 3) %>%\n  kable_styling()\n# increased digits to compare to change-score model above"},{"path":"issues.html","id":"a-t-test-of-the-auc-or-separate-t-tests-at-each-time-point-result-in-ambiguous-inference","chapter":"13 Issues in inference","heading":"13.3.1.9.3 A t test of the AUC or separate t-tests at each time point result in ambiguous inference","text":"t-test AUCSeparate t-tests time point","code":"\nm3_t <- t.test(auc ~ treatment,\n       data = fig3f_wide,\n       var.equal = TRUE) %>%\n  tidy()\n\nm3_t[1, c(1, 4:8)]## # A tibble: 1 × 6\n##   estimate statistic p.value parameter conf.low conf.high\n##      <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n## 1   -19185     -3.52 0.00422        12  -31057.    -7313.\nm4_t0 <- t.test(time_0 ~ treatment, data = fig3f_wide, var.equal = TRUE)\nm4_t30 <- t.test(time_30 ~ treatment, data = fig3f_wide, var.equal = TRUE)\nm4_t60 <- t.test(time_60 ~ treatment, data = fig3f_wide, var.equal = TRUE)\nm4_t90 <- t.test(time_90 ~ treatment, data = fig3f_wide, var.equal = TRUE)\nm4_t120 <- t.test(time_120 ~ treatment, data = fig3f_wide, var.equal = TRUE)\n\nm4_t <- data.table(\n  Time = times,\n  p.value = c(m4_t0$p.value,\n              m4_t30$p.value,\n              m4_t60$p.value,\n              m4_t90$p.value,\n              m4_t120$p.value\n  )\n)\n\nm4_t %>%\n  kable %>%\n  kable_styling()"},{"path":"issues.html","id":"normalization-the-analysis-of-ratios","chapter":"13 Issues in inference","heading":"13.4 Normalization – the analysis of ratios","text":"","code":""},{"path":"issues.html","id":"kinds-of-ratios-in-experimental-biology","chapter":"13 Issues in inference","heading":"13.4.1 Kinds of ratios in experimental biology","text":"ratio density (count per length/area/volume) rate (count/time).Example: number marked cells per area tissue.Best practice: GLM count data offset model, offset denominator ratio.ratio relative standard (“normalized”).\nExample: expression focal mRNA relative expression standard mRNA thought affected treatment.\nBest practice: GLM count data offset model, offset denominator ratio.ratio proportion (percent).Example: Number marked cells per total number cells.Best practice: GLM logistic.ratio relative whole thing numerator thing denominator grow (allometric data).Example: adipose mass relative total lean body mass.Best practice: ANCOVA linear model.Alert! – known 100 years, repeatedly broadcasted, inference ratios allometric data range merely wrong (inferred effect size right direction, wrong) absurd (direction inferred effect opposite true effect).","code":""},{"path":"issues.html","id":"issues-offset","chapter":"13 Issues in inference","heading":"13.4.2 Example 1 – The ratio is a density (number of something per area)","text":"Researchers frequently count objects compare counts among treatments. problem often arises counts made samples different areas volumes tissue. consequence, variation treatment response confounded tissue size – samples higher counts may higher counts different response treatment, larger amount tissue, combination. common practice experimental biology adjust tissue size variation constructing ratio \\(\\frac{count}{area}\\) testing difference ratio using either linear model NHST (t-test/ANOVA) non-parametric NHST (Mann-Whitney-Wilcoxan). issue , initial counts kind count distribution (Poisson negative binomial) can sometimes look like sample normal distribution (see Introducing Generalized Linear Models using count data example). ratio kind ratio distribution hard model correctly.better practice model count using Generalized Linear Model (GLM) offset adjusts differences area sample. NHST ratios perform okay sense Type error close nominal relatively low power compared generalized linear model offset. researcher interested best practices including reporting uncertainty estimated effects, GLM useful confidence intervals – example CIs linear model assuming Normal error can often include absurd values ratios less zero.Source article (Fernández, Álvaro F., et al. “Disruption beclin 1–BCL2 autophagy regulatory complex promotes longevity mice.” Nature 558.7708 (2018): 136-140.)https://www.nature.com/articles/s41586-018-0162-7Public sourceSource data Fig. 3The example Fig 3b.Response variable – number TUNEL+ cells measured kidney tissue, positive marker indicates nuclear DNA damage.Background. experiments Figure 3 designed measure effect knock-mutation gene beclin 1 protein autophagy tissue health kidney heart. researchers interested autophagy evidence many non-mammalian model organisms increased autophagy reduces age-related damage tissues increases health lifespan. BCL2 autophagy inhibitor. Initial experiments showed knock-mutation beclin 1 inhibits BCL2. Inhibiting BCL2 knock-mutation increase autophagy , consequence, reduce age-related tissue damage.researchers measured Tunel+ cells 2 month old 20 month old mice order look age related effects. design factorial two factors (\\(\\texttt{age}\\) \\(\\texttt{genotype}\\). haven’t covered factorial designs text, limit analysis 20 month old mice.Design - single factor \\(\\texttt{genotype}\\) levels “WT” (wildtype) “KI” (knock-).code importing exp3b data Section 13.8.6 .","code":""},{"path":"issues.html","id":"fit-the-models","chapter":"13 Issues in inference","heading":"13.4.2.1 Fit the models","text":"","code":"\nexp3b_m1 <- lm(count_per_area ~ genotype, data = exp3b)\n\nexp3b_m2 <- glm.nb(positive_nuclei ~ genotype +\n                     offset(log(area_mm2)),\n                   data = exp3b)\n\nexp3b_m3 <- wilcox.test(count_per_area ~ genotype, data = exp3b)"},{"path":"issues.html","id":"check-the-models","chapter":"13 Issues in inference","heading":"13.4.2.2 Check the models","text":"","code":"\nggcheck_the_model(exp3b_m1)\nexp3b_m2_simulation <- simulateResiduals(fittedModel = exp3b_m2, n = 250)\nplot(exp3b_m2_simulation, asFactor = FALSE)"},{"path":"issues.html","id":"issues-exp3a-m3-inference","chapter":"13 Issues in inference","heading":"13.4.2.3 Inference from the model","text":"Notes","code":"\nexp3b_m2_coef <- cbind(coef(summary(exp3b_m2)),\n                       confint(exp3b_m2))\nexp3b_m2_emm <- emmeans(exp3b_m2,\n                        specs = c(\"genotype\"),\n                        type=\"response\")\nexp3b_m2_pairs <- contrast(exp3b_m2_emm,\n                             method = \"revpairwise\") %>%\n  summary(infer = TRUE)"},{"path":"issues.html","id":"plot-the-model-1","chapter":"13 Issues in inference","heading":"13.4.2.4 Plot the model","text":"code plot Section 13.8.7 .","code":""},{"path":"issues.html","id":"issues-size","chapter":"13 Issues in inference","heading":"13.4.3 Example 2 – The ratio is normalizing for size differences","text":"","code":"\ndata_from <- \"A big-data approach to understanding metabolic rate and response to obesity in laboratory mice\"\nfile_name <- \"mmpc_all_phases.csv\"\nfile_path <- here(data_folder, data_from, file_name)\n\ngeometric_mean <- function(x){\n  gm <- exp(mean(log(x)))\n  return(gm)\n}\nexp1 <- fread(file_path)\nexp1 <- exp1[acclimation == \"TRUE\", ]\n\nexp1[, resid_mass := total_mass - fat_mass - lean_mass]\nexp1[, nonfat_mass := total_mass - fat_mass]\n\nsize_cols <- c(\"fat_mass\", \"lean_mass\", \"resid_mass\")\nexp1[, size := apply(exp1[, .SD, .SDcols = size_cols], 1, geometric_mean)]\nsize_cols <- c(\"fat_mass\", \"lean_mass\", \"resid_mass\", \"total_mass\", \"size\")\ncor(exp1[, .SD, .SDcols = size_cols])##              fat_mass  lean_mass  resid_mass total_mass      size\n## fat_mass    1.0000000  0.6486331 -0.03980370 0.94096284 0.8192087\n## lean_mass   0.6486331  1.0000000 -0.41585035 0.78512117 0.4214359\n## resid_mass -0.0398037 -0.4158503  1.00000000 0.05819842 0.4595406\n## total_mass  0.9409628  0.7851212  0.05819842 1.00000000 0.8589070\n## size        0.8192087  0.4214359  0.45954060 0.85890700 1.0000000"},{"path":"issues.html","id":"dont-do-this-stuff","chapter":"13 Issues in inference","heading":"13.5 Don’t do this stuff","text":"","code":""},{"path":"issues.html","id":"normalize-the-response-so-that-all-control-values-are-equal-to-1.","chapter":"13 Issues in inference","heading":"13.5.1 Normalize the response so that all control values are equal to 1.","text":"many experiments, response variable measure units readily interpretable biologically. experiments, researchers often normalize response mean reference group.\\[\ny_i \\; \\mathrm{(normalized)} = \\frac{y_i}{\\overline{y}_{ref}}\n\\]normalized response variable multiple (fraction) mean response reference group.Less often, researchers normalize value reference within batch – example, mean control group within independent experiment – analyze batch means t-test ANOVA. goal -experiment_id normalization adjust variance among experiments, proper way Linear mixed model combining replicated experiments .Regardless goal, don’t . variance reference group n values 1. researcher telling t-test ANOVA natural variability reference group (values measured without measurement error). classical (Student) t-test ANOVA incorrectly small error variance (\\(\\sigma\\)) zero-variance go computation pooled (modeled) variance. consequence incorrectly small standard errors, confidence intervals, p-values, inflated false discovery.Let’s , show don’t . examples ’ve found literature archive batch-normalized data raw data ’m using fake data .best practice linear mixed model using mean-normalized response (paired t-test specific case ).common practice linear model using mean-normalized response (Student t-test specific case ). model violates independence assumption violation usually results less precision lower power.incorrect practice linear model (t-test ANOVA) using id-normalized response. model violates heterogeneity estimates error variance using group zero variation.summary table three models …plot mean-normalized id-normalized data modeled means CIs.\nFigure 13.6: Plot fake normalized data modeled means CI ) model m1, linear mixed model experiment_id random factor B) model m3, linear model response normalized experiment mean.\nNotes","code":"\nset.seed(9)\nn_exp <- 8 # number of experiments\nbeta_0 <- 100\nbeta_1 <- 1.5 # genotype effect as a multiple\nmu <- c(beta_0, beta_0*beta_1)\nsigma <- (beta_1 - 1)*beta_0 # iid error\nrho <- 0.7 # correlated error\nSigma <- matrix(c(sigma^2, rho*sigma^2, rho*sigma^2, sigma^2),\n                nrow = 2)\nfake_data <- data.table(NULL)\nfake_data[, genotype := rep(c(\"wt\", \"ko\"), each = n_exp)]\nfake_data[, genotype := factor(genotype,\n                               levels = c(\"wt\", \"ko\"))]\n\nfake_data[, experiment_id := rep(paste0(\"exp_\", 1:n_exp), 2)]\n\n# modeling the experiment means\nraw <- rmvnorm(n_exp, mean = mu, sigma = Sigma)\nfake_data[, y := c(raw)]\n\n# mean-normalize by the mean of the experiment means of wt group\nfake_data[, y_correct := c(raw/mean(raw[,1]))]\n\n# id-normalize by experiment mean of wt group for each experiment_id\nfake_data[, y_wrong := c(raw/raw[,1])]\nm1 <- lmer(y_correct ~ genotype +\n             (1|experiment_id),\n           data = fake_data)\n\nm1_emm <- emmeans(m1, specs = \"genotype\")\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\nm2 <- lm(y_correct ~ genotype,\n           data = fake_data)\n\nm2_emm <- emmeans(m2, specs = \"genotype\")\nm2_pairs <- contrast(m2_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\nm3 <- lm(y_wrong ~ genotype,\n           data = fake_data)\n\nm3_emm <- emmeans(m3, specs = \"genotype\")\nm3_pairs <- contrast(m3_emm,\n                     method = \"revpairwise\") %>%\n  summary(infer = TRUE)\ngg1 <- ggplot_the_response(m1, m1_emm, m1_pairs)\ngg2 <- ggplot_the_response(m3,\n                           m3_emm,\n                           m3_pairs,\n                           dots = \"jitter\")\nset.seed(1)\nplot_grid(gg1, gg2, ncol = 2, labels = \"AUTO\")"},{"path":"issues.html","id":"a-difference-in-significance-is-not-necessarily-significant","chapter":"13 Issues in inference","heading":"13.6 A difference in significance is not necessarily significant","text":"Thermal stress induces glycolytic beige fat formation via myogenic stateFigure 2j","code":""},{"path":"issues.html","id":"researcher-degrees-of-freedom","chapter":"13 Issues in inference","heading":"13.7 Researcher degrees of freedom","text":"","code":""},{"path":"issues.html","id":"hidden-code-5","chapter":"13 Issues in inference","heading":"13.8 Hidden code","text":"","code":""},{"path":"issues.html","id":"import-exp4d-vimentin-cell-count-data-replicate-experiments-example","chapter":"13 Issues in inference","heading":"13.8.1 Import exp4d vimentin cell count data (replicate experiments example)","text":"","code":"\ndata_from <- \"Distinct inflammatory and wound healing responses to complex caudal fin injuries of larval zebrafish\"\nfile_name <- \"elife-45976-fig4-data2-v1.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp4d_wide <- read_excel(file_path,\n                         sheet = \"Sheet1\",\n                         range = \"A16:C49\") %>%\n  data.table()\n\nexp4d_wide[, experiment_id := nafill(Replicate, type = \"locf\")]\nexp4d_wide[, experiment_id := paste0(\"Exp\", experiment_id)]\n\nold_cols <- c(\"STAT3 +/+\", \"STAT3 -/-\")\ngenotype_levels <- c(\"STAT3_wt\", \"STAT3_ko\")\nsetnames(exp4d_wide, old = old_cols, new = genotype_levels)\nexp4d <- melt(exp4d_wide,\n              id.vars = \"experiment_id\",\n              measure.vars = genotype_levels,\n              variable.name = \"genotype\",\n              value.name = \"vimentin_cells\") %>% # cell count\n  na.omit()\nexp4d[, genotype := factor(genotype,\n                           levels = genotype_levels)]"},{"path":"issues.html","id":"import-fig4c-data","chapter":"13 Issues in inference","heading":"13.8.2 Import Fig4c data","text":"","code":"\ndata_from <- \"Identification of osteoclast-osteoblast coupling factors in humans reveals links between bone and energy metabolism\"\nfile_name <- \"41467_2019_14003_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nfig4c_type <- c(\"text\", rep(\"numeric\", 8))\nfig4c <- read_excel(file_path,\n                   sheet = \"Fig4C and 4E\",\n                   range = \"A3:I48\",\n                   col_names = FALSE,\n                   col_types <- fig4c_type) %>%\n  data.table()\n\n# DPP4 (ng/mL), GLP-1 (pM), Glucose (mmol/L),   Insulin (uIU/mL)\nmeasures <- c(\"DPP4\", \"GLP1\", \"Glucose\", \"Insulin\")\n# post is 3 months post treatment\nperiod <- c(\"baseline\", \"post\")\nnew_colnames <- paste(rep(measures, 2),\n                      rep(period, each = 4),\n                      sep = \"_\")\n\nold_colnames <- colnames(fig4c)\nsetnames(fig4c,\n         old = old_colnames,\n         new = c(\"treatment\",\n                 new_colnames))\n\ntreatment_levels <- c(\"Placebo\", \"Denosumab\")\nfig4c[, treatment := factor(treatment,\n                            levels = treatment_levels)]\nfig4c[, id := paste0(\"human_\", .I)] # .I inserts row number\n#View(fig4c)\nfig4c_long <- melt(fig4c,\n                  id.vars = c(\"id\", \"treatment\"),\n                  measure.vars = c(\"DPP4_baseline\", \"DPP4_post\"),\n                  variable.name = \"time\",\n                  value.name = \"dpp4\")\n\nfig4c_long[, time := substr(as.character(time),\n                             6,\n                             nchar(as.character(time)))]\nfig4c_long[, time := factor(time,\n                             levels = c(\"baseline\", \"post\"))]\n\n# View(fig4c_long)"},{"path":"issues.html","id":"xx-males-fig1c","chapter":"13 Issues in inference","heading":"13.8.3 XX males fig1c","text":"","code":"\ndata_from <- \"XX sex chromosome complement promotes atherosclerosis in mice\"\nfile_name <- \"41467_2019_10462_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\nfig1c_1 <- read_excel(file_path,\n                         sheet = \"Figure 1C\",\n                         range = \"C3:G6\",\n                         col_names = FALSE) %>%\n  data.table()\nsetnames(fig1c_1,\n         old = colnames(fig1c_1),\n         new = paste(\"mouse\", 1:5))\nfig1c_1[, time := rep(c(\"baseline\", \"week_1\"), each = 2)]\nfig1c_1[, sex := rep(c(\"female\", \"male\"), 2)]\n\nfig1c_2 <- read_excel(file_path,\n                         sheet = \"Figure 1C\",\n                         range = \"I3:M6\",\n                         col_names = FALSE) %>%\n  data.table()\nsetnames(fig1c_2,\n         old = colnames(fig1c_2),\n         new = paste(\"mouse\", 6:10))\nfig1c_2[, time := rep(c(\"baseline\", \"week_1\"), each = 2)]\nfig1c_2[, sex := rep(c(\"female\", \"male\"), 2)]\n\nfig1c <- rbind(data.table(chromosome = \"xx\",\n                          melt(fig1c_1,\n                               id.vars = c(\"sex\", \"time\"),\n                               variable.name = \"id\",\n                               value.name = \"fat_mass\")),\n               data.table(chromosome = \"xy\",\n                          melt(fig1c_2,\n                               id.vars = c(\"sex\", \"time\"),\n                               variable.name = \"id\",\n                               value.name = \"fat_mass\")))\n# id is not unique\nfig1c[, id := paste(sex, id, sep=\"_\")] # now it is\n\nfig1c[, treatment := paste(sex, chromosome, sep = \"_\")]\n# unique(fig1c$treatment)\n# \"female_xx\" \"male_xx\"   \"female_xy\" \"male_xy\"\ntreatment_levels <- c(\"female_xx\", \"female_xy\", \"male_xx\", \"male_xy\")\nfig1c[, treatment := factor(treatment,\n                            levels = treatment_levels)]\n\nfig1c_wide <- dcast(fig1c, chromosome + sex + id + treatment ~ time, value.var = \"fat_mass\")\nfig1c_wide[, percent_change := (week_1 - baseline)/baseline*100]"},{"path":"issues.html","id":"generation-of-fake-data-to-illustrate-regression-to-the-mean","chapter":"13 Issues in inference","heading":"13.8.4 Generation of fake data to illustrate regression to the mean","text":"","code":"\nn_iter <- 1000\nn <- 6\nnp1 <- n + 1\nN <- n*2\nmu <- 30\nbeta_1 <- 5\nsigma_among <- 2\nsigma_within <- sigma_among/2\nrho <- sigma_among/(sigma_among + sigma_within)\nmax_baseline_d <- -9999\n\ntime_levels <- c(\"pre\", \"post\")\ntreatment_levels <- c(\"cn\",\"tr\")\nfake_data <- data.table(\n  treatment = rep(rep(treatment_levels, each = n), 2),\n  time = rep(time_levels, each = n*2),\n  id = rep(paste0(\"mouse_0\", 1:N), 2)\n)\nfake_data[, time := factor(time,\n                           levels = time_levels)]\nfake_data[, treatment := factor(treatment,\n                                levels = treatment_levels)]\ndiff_pre <- numeric(n_iter)\ndiff_post <- numeric(n_iter)\nixn <- numeric(n_iter)\nixn_p <- numeric(n_iter)\np_ancova <- numeric(n_iter)\np_change <- numeric(n_iter)\nseed_i <- 0\nfor(i in 1:n_iter){\n  seed_i <- seed_i + 1\n  set.seed(seed_i)\n  # the contribution to variance due to individual differences\n  mu_i <- rnorm(N, mean = mu, sd = sigma_among)\n  \n  # the contribution to variance due to variation within individual\n  mu_i_pre <- rnorm(N, sd = sigma_within)\n  mu_i_post <- rnorm(N, sd = sigma_within)\n  \n  pre <- mu_i + mu_i_pre\n  post <- mu_i + beta_1 + mu_i_post\n  fake_data[, weight := c(pre, post)]\n  fit <- lm(weight ~ treatment*time, data = fake_data)\n  diff_pre[i] <- mean(pre[np1:N]) - mean(pre[1:n])\n  diff_post[i] <- mean(post[np1:N]) - mean(post[1:n])\n  ixn[i] <- coef(summary(fit))[4, \"Estimate\"]\n  ixn_p[i] <- coef(summary(fit))[4, \"Pr(>|t|)\"]\n  \n  fake_data_wide <- dcast(fake_data,\n                          id + treatment ~ time,\n                          value.var = \"weight\")\n  fake_data_wide[, change := post-pre]\n  fake_data_wide[, percent := (post-pre)/pre*100]\n  p_ancova[i] <- coef(summary(lm(post ~ treatment + pre,\n                  data = fake_data_wide)))[2, \"Pr(>|t|)\"]\n  p_change[i] <- coef(summary(lm(change ~ treatment,\n                  data = fake_data_wide)))[2, \"Pr(>|t|)\"]\n  \n}\n\ndiverge_list <- which(abs(diff_pre) < 0.1 & abs(diff_post) > 1)\nconverge_list <- which(abs(diff_pre) > 1 & abs(diff_post) < 0.1)\np_list <- which(p_ancova > 0.1 & p_change < 0.01)\nkeep <- intersect(converge_list, p_list)\n\nmax_seed_con <- which(abs(ixn) == max(abs(ixn)[converge_list]))\n\n# convergence ixn\nset.seed(max_seed_con)\nmu_i <- rnorm(N, mean = mu, sd = sigma_among)\n# the contribution to variance due to variation within individual\nmu_i_pre <- rnorm(N, sd = sigma_within)\nmu_i_post <- rnorm(N, sd = sigma_within)\npre <- mu_i + mu_i_pre\npost <- mu_i + beta_1 + mu_i_post\nfake_data[, weight := c(pre, post)]\nfake_data_wide <- dcast(fake_data,\n                        id + treatment ~ time,\n                        value.var = \"weight\")\nfake_data_wide[, change := post-pre]\nfake_data_wide[, percent := (post-pre)/pre*100]\n# coef(summary(lm(post ~ treatment + pre,\n#                 data = fake_data_wide)))\n# coef(summary(lm(change ~ treatment,\n#                 data = fake_data_wide)))\n# coef(summary(lm(percent ~ treatment,\n#                 data = fake_data_wide)))\n\n\n# not much we can do about divergence so limit to convergence\n\nfake_means <- fake_data[, .(weight = mean(weight)),\n                        by = c(\"time\", \"treatment\")]"},{"path":"issues.html","id":"import-fig3f","chapter":"13 Issues in inference","heading":"13.8.5 Import fig3f","text":"**convert fig3f long format LMM","code":"\ndata_from <- \"Innervation of thermogenic adipose tissue via a calsyntenin 3β–S100b axis\"\nfile_name <- \"41586_2019_1156_MOESM5_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nfig3f_wide <- read_excel(file_path,\n                    sheet = \"Figure 3f\",\n                    range = \"H2:M15\",\n                    col_names = FALSE) %>%\n  data.table()\n\ntime_cols <- c(0, 30, 60, 90, 120)\nsetnames(fig3f_wide,\n         old = names(fig3f_wide),\n         new = c(\"id\", \n                 paste0(\"time_\", time_cols)))\n\ntreatment_levels <- c(\"WT\", \"KO\")\nfig3f_wide[, treatment := factor(substr(id, 1, 2),\n                                 treatment_levels)]\n\nfig3f_wide[, glucose_0 := time_0]\n\ntimes <- c(0, 30, 60, 90, 120)\nn_times <- length(times)\nperiod_full <- times[n_times] - times[1]\nperiod_post <- times[n_times] - times[2]\n\ntime_cols <- paste0(\"time_\", times)\nY <- fig3f_wide[, .SD, .SDcols = time_cols] %>%\n  as.matrix()\n\n# auc\nfig3f_wide[, auc := apply(Y, 1, trap.rule, x = times)]\n\n# mean response over full period\nfig3f_wide[, glucose_gtt_post := auc/period_full]\n\n# auc of the change. Equal to iauc of le Floch\nfig3f_wide[, iauc := apply(Y - Y[,1], 1, trap.rule, x = times)]\n\n# mean change over full period\nfig3f_wide[, glucose_gtt_change := iauc/period_full]\n\n# View(fig3f_wide)"},{"path":"issues.html","id":"issues-import-exp3b","chapter":"13 Issues in inference","heading":"13.8.6 Import exp3b","text":"","code":"\ndata_from <- \"Disruption of the beclin 1–BCL2 autophagy regulatory complex promotes longevity in mice\"\nfile_name <- \"41586_2018_162_MOESM5_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp3b <- read_excel(file_path,\n                         sheet = \"Fig. 3b\",\n                         range = \"H4:K50\",\n                         col_names = TRUE) %>%\n  clean_names() %>%\n  tidyr::fill(genotype) %>%\n  data.table()\nexp3b[, count_per_area := positive_nuclei/area_mm2]\ngenotype_levels <- c(\"WT\", \"KI\")\nexp3b[, genotype := factor(genotype,\n      levels = genotype_levels)]\nexp3b[, age := \"Old\"]"},{"path":"issues.html","id":"issues-exp3b-plot","chapter":"13 Issues in inference","heading":"13.8.7 Plot the model of exp3b (glm offset data)","text":"","code":"\ngg1 <- ggplot_the_effects(exp3b_m2,\n                 exp3b_m2_pairs,\n                 effect_label = \"Effect Ratio\")\n# gg1\n\nxdat_old <- expand.grid(\n  area_mm2 = seq(min(exp3b[age == \"Old\", area_mm2]),\n                 max(exp3b[age == \"Old\", area_mm2]),\n                 length.out = 50),\n  genotype = genotype_levels,\n  age = c(\"Old\")\n) %>%\n  data.table()\n\nxdat_old[, y := predict(exp3b_m2, xdat_old, type = \"response\")]\n\ngg2 <- ggplot(data = exp3b[age == \"Old\"],\n              aes(x = area_mm2,\n                  y = positive_nuclei,\n                  color = genotype)) +\n  geom_point() +\n  geom_path(data = xdat_old,\n            aes(x = area_mm2,\n                y = y,\n                color = genotype)) +\n  xlab(expression(paste(\"Area (\", mm^2, \")\"))) +\n  ylab(\"TUNEL+ nuclei\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n\n  theme_pubr() +\n  theme(legend.position = \"left\") +\n  NULL\n\n#gg2\n\nplot_grid(gg1, gg2,\n          nrow = 2,\n          rel_heights = c(0.4,1),\n          align = c(\"v\"),\n          axis = \"lr\")"},{"path":"part-v-more-than-one-x-multivariable-models.html","id":"part-v-more-than-one-x-multivariable-models","chapter":"Part V: More than one \\(X\\) – Multivariable Models","heading":"Part V: More than one \\(X\\) – Multivariable Models","text":"","code":""},{"path":"covariates.html","id":"covariates","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14 Linear models with added covariates (“ANCOVA”)","text":"general sense, Covariates simply \\(X\\) variables statistical model. data experiments, “covariates” typically refers continuous \\(X\\) variables added model increase precision treatment effects. observational designs, continuous categorical covariates might added model 1) increase predictive ability, 2) researcher interested specific conditional effects, 3) eliminate confounding. discussed later chapters. chapter, single, continuous covariate added linear model. Nothing fundamentally different covariate added categorical (\\(Sex\\) common categorical covariate) multiple covariates added.","code":""},{"path":"covariates.html","id":"adding-covariates-can-increases-the-precision-of-the-effect-of-interest","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.1 Adding covariates can increases the precision of the effect of interest","text":"use fake data introduce concept statistical elimination covariate statistical model. modeling effect new drug blood LDL-C levels. LDL kind lipoprotein, particles blood transport fats cholesterol different tissues. LDL-C cholesterol associated LDL particles. LDL-C considered “bad cholesterol” LDL believed transport cholesterol lipids arterial walls, basis atherosclerosis.Thirty applied biostats students recruited randomly assigned either “placebo” treatment level “drug” treatment level. response blood LDL-C concentration. drug manufacturer wants measure effect new drug ldlc.\nFigure 14.1: Effect drug therapy plasma LDL-C.\nlinear model fit simulated LDL-C data \\[\\begin{equation}\nldlc = \\beta_0 + \\beta_1 treatment_{drug} + \\varepsilon\n\\tag{14.1}\n\\end{equation}\\]\\(treatment_{drug}\\) dummy variable, set 0 \\(treatment = \\mathrm{''placebo''}\\) 1 \\(treatment = \\mathrm{''drug''}\\).coefficient table isThe response-effect plot shows large overlap LDL-C response treatment effect small relative noise. “effect drug (\\(p = .31\\))” incorrect interpretation p-value significance test estimate \\(\\beta_1\\). better interpretation , estimated effect -1.6 everything large, negative effects moderate positive effects consistent data.expert biologists, know LDL-C strongly correlated age large range age among Applied Bistats students. age contributes large fraction variance LDL-C among applied biostats students, age-related variance might mask effect drug. plot LDL-C vs. age, treatment assignment color coded. Remember, exact values LDL-C figure ?? .\nFigure 14.2: Linear regression \\(ldlc\\) age fit fake LDL-C data. points color coded treatment.\nregression line fit linear model\\[\\begin{equation}\nldlc = \\beta_0 + \\beta_1 age + \\varepsilon\n\\tag{14.2}\n\\end{equation}\\]points color-coded treatment level \\(treatment\\) model (14.2). color-coding makes clear “placebo” data points line, positive residuals model, “drug” data points line, negative residuals model. , specific value age, small overlap LDL-C values drug placebo.happening? Age contributing variance LDL-C, noise \\(\\varepsilon\\) model (14.1), added noise makes decreases precision estimate effect new drug relative placebo. view data Figure ??, age masking effect. somehow measure effect drug specific age, get precise estimate effect. ? three possible methods. third one use second useful understanding third.just analyze subset data, , cases value age nearly equal. throws away perfectly good data , consequently, greatly reduces sample size thus precision estimate effect.just analyze subset data, , cases value age nearly equal. throws away perfectly good data , consequently, greatly reduces sample size thus precision estimate effect.first fit model (14.2) use residuals fit new response variable estimate effect drug treatment (eye figure 14.2).first fit model (14.2) use residuals fit new response variable estimate effect drug treatment (eye figure 14.2).step 1:\\[\\begin{equation}\nldlc = \\beta_0 + \\beta_1 age + \\varepsilon\n\\end{equation}\\]step 2:\\[\\begin{equation}\nldlc\\_residual = \\beta_0 + \\beta_1 treatment_{drug} + \\varepsilon\n\\tag{14.3}\n\\end{equation}\\], use two-stage method useful introduce concept adjusting covariate linear model, covariate \\(age\\). , general, don’t – method usually “works” pretty well mean covariate (step 1 \\(X\\) variable) nearly treatment levels artifacts lead wrong inference introduced mean covariate far apart.\nFigure 14.3: Effect drug therapy plasma LDL-C using residuals. Don’t !\nFigure ??effect-response plot effect treatment LDL-C adjusted age using two-step method. Figure ??B original (adjust) effect-response plot. scale plots . means response axis length plots effects axis length plots. makes comparing two easy. Two patterns conspicuousThe age-adjusted means Figure ??apart (difference bigger) unadjusted means Figure ??B. seen response effects components plot.spread residual LDL-C measures within treatment level Figure ??less spread raw LDL-C measures Figure ??B.confidence interval effect (difference means) smaller using adjusted model (Figure ??) unadjusted model (Figure ??B)p-value effect (difference means) smaller using adjusted model (Figure ??) unadjusted model (Figure ??B)patterns quantified Estimates SEs coefficient table. , show comparison. Can match comparisons 1-4 statistic coefficient table?Coefficient table two-step, “adjusted” model (adjusting \\(age\\)):Coefficient table unadjusted model (adjusting \\(age\\)):clear plots tables two-stage adjustment increases precision estimates means differences means eliminating contribution Age variance LDL-C.best practice adjusting covariate (statistical elimination covariate) simply add covariate linear model.\\[\\begin{equation}\nldlc = \\beta_0 + \\beta_1 age + \\beta_2 treatment_{drug} + \\varepsilon\n\\tag{14.4}\n\\end{equation}\\], compare coefficient table model fit without covariate.adjusted effect larger (-4.5 vs. -1.6)adjusted SE difference smaller (0.8 vs. 1.5)adjusted CIs narrower (-6.1, -2.8 vs. -4.7, 1.6)p-value adjusted difference smaller (0.000007 vs. 0.31)plot model \nFigure 14.4: Effect drug therapy, adjusted age, plasma LDL-C.\n","code":"##               Estimate Std. Error t value Pr(>|t|)   2.5 %  97.5 %\n## (Intercept)    106.045      1.070  99.111    0.000 103.854 108.237\n## treatmentdrug   -1.554      1.513  -1.027    0.313  -4.654   1.546##               Estimate Std. Error t value Pr(>|t|)  2.5 % 97.5 %\n## (Intercept)      1.901      0.556   3.422    0.002  0.763  3.040\n## treatmentdrug   -3.803      0.786  -4.840    0.000 -5.412 -2.193##               Estimate Std. Error t value Pr(>|t|)   2.5 %  97.5 %\n## (Intercept)    106.045      1.070  99.111    0.000 103.854 108.237\n## treatmentdrug   -1.554      1.513  -1.027    0.313  -4.654   1.546##               Estimate Std. Error t value Pr(>|t|)  2.5 % 97.5 %\n## (Intercept)      75.60      3.260   23.20 2.28e-19 68.900  82.30\n## age               1.26      0.133    9.47 4.46e-10  0.988   1.53\n## treatmentdrug    -4.45      0.802   -5.55 6.94e-06 -6.090  -2.81##               Estimate Std. Error t value Pr(>|t|)  2.5 % 97.5 %\n## (Intercept)     106.00       1.07   99.10 3.36e-37 104.00 108.00\n## treatmentdrug    -1.55       1.51   -1.03 3.13e-01  -4.65   1.55"},{"path":"covariates.html","id":"understanding-a-linear-model-with-an-added-covariate-heart-necrosis-data","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2 Understanding a linear model with an added covariate – heart necrosis data","text":"1-Deoxydihydroceramide causes anoxic death impairing chaperonin-mediated protein foldingSource dataIn article, researchers investigating effects specific sphingolipids hypoxia (low O2) heart. hypoxia results necrosis (death) heart tissue. researchers specifically looking sphingolipid 1-deoxydihydroceramide (DoxDHCer), derived 1-deoxysphinganine6 (DoxSa).experiment Figure 4h, researchers measured effect three treatments necrosis.Vehicle – “cardioprotection” hypoxia-producing sphingolipids. “control”. expect necrotic area group.\n2 Myriocin – drug inhibits enzyme iniates sphingolipid production. drug provide protection hypoxia-producing sphingolipids. expect less necrotic area (, cardioprotection) group.Myriocin + DoxSa – DoxSa specific sphingolipid researchers believe cause hypoxia/necrosis. drug inhibit production sphingolipids added DoxSa reverses protective effect drug. reversal protection, supports hypothesis DoxSa sphingolipid causing hypoxia/necrosis.response (\\(Y\\)) variable \\(area\\_of\\_necrosis\\) – measured “area” necrotic tissue (area used sense “region” sense length times width). covariate \\(area\\_at\\_risk\\) – area heart tissue susceptible necrosis.","code":""},{"path":"covariates.html","id":"fit-the-model-5","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.1 Fit the model","text":"verbal model added covariate \\(area\\_of\\_necrosis \\sim area\\_at\\_risk + treatment\\;\\;\\;(\\mathcal{M}_1)\\).’ll name model \\(\\mathcal{M}_1\\). understand model coefficients, helps expand model \\(\\mathcal{M}_1\\) full linear model.\\[\\begin{equation}\narea\\_of\\_necrosis = \\beta_0 + \\beta_1 area\\_at\\_risk + \\beta_2 treatment_{myriosin} + \\beta_3 treatment_{Myriocin\\;+\\;DoxSa} + \\varepsilon\n\\tag{14.5}\n\\end{equation}\\]model includes parameters effects area risk (\\(\\beta_1\\)), myriosin treatment (\\(\\beta_2\\)) myriosin + DoxSa treatment (\\(\\beta_3\\)). explain interpretation effects “Interpretation model coefficients” .","code":"\nfig4h_m1 <- lm(area_of_necrosis ~ area_at_risk + treatment,\n               data = fig4h)"},{"path":"covariates.html","id":"plot-the-model-2","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.2 Plot the model","text":"\nFigure 14.5: effect treatment area necrosis, adjusted area risk.\n","code":""},{"path":"covariates.html","id":"interpretation-of-the-model-coefficients","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.3 Interpretation of the model coefficients","text":"estimate “(Intercept)” row (\\(b_0\\)) expected value reference (, Vehicle group) \\(area\\_at\\_risk=0\\). Using Figure 14.5, can visualized value \\(Y\\) Vehicle line (regression line reference group) crosses y-axis (\\(X = 0\\)). meaningful estimate since area risk hearts zero.estimate “area_at_risk” row (\\(b_1\\)) common slope three regression lines. might refer slope “effect” \\(area\\_at\\_risk\\) \\(area\\_of\\_necrosis\\) recommend using causal language \\(area\\_at\\_risk\\) randomly assigned – purpose model (necessarily linear models added covariate) interpretation improving inference expimental treatment factor.estimate “treatmentMyriocin” row (\\(b_2\\)) effect \\(Myriocin\\) \\(area\\_of\\_necrosis\\) conditional (adjusted ) \\(area\\_at\\_risk\\). frequent phrase “estimate controlling area risk” avoid “control” implies experimenter intervention true \\(area\\_at\\_risk\\). value difference elevation regression line reference group Myriocin group. difference equal difference conditional expectations specific value covariate.\\[\\begin{equation}\nb_2 = \\mathrm{E}(area\\_of\\_necrosis|treatment = \"Myriocin\", X = x) - \\mathrm{E}(area\\_of\\_necrosis|treatment = \"Vehicle\", X = x)\n\\end{equation}\\]estimate “treatmentMyriocin + DoxSa” row (\\(b_3\\)) effect \\(Myriocin + DoxSa\\) \\(area\\_of\\_necrosis\\) conditional (adjusted ) \\(area\\_at\\_risk\\). ’s interpretation similar \\(b_2\\).summarize interpretations given motivating hypothesis? results support hypothesis. DoxSa one sphingolipids inducing hypoxia/necrosis, ’d expect Myriocin + DoxSa line elevated Myriocin line. coefficient table “tests” prediction indirectly. explicit statistic prediction (Myriocin + DoxSa) - Myriocin contrast contrast table .","code":"\nfig4h_m1_coef <- cbind(coef(summary(fig4h_m1)),\n      confint(fig4h_m1))\nsignif(fig4h_m1_coef, digits = 3)##                           Estimate Std. Error t value Pr(>|t|)  2.5 % 97.5 %\n## (Intercept)                 -3.890     2.1200   -1.83 7.63e-02 -8.220  0.436\n## area_at_risk                 0.453     0.0587    7.73 8.28e-09  0.334  0.573\n## treatmentMyriocin           -3.110     1.2300   -2.53 1.64e-02 -5.620 -0.611\n## treatmentMyriocin + DoxSa   -3.150     1.3700   -2.31 2.78e-02 -5.930 -0.367"},{"path":"covariates.html","id":"everything-adds-up","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.4 Everything adds up","text":"Remember everything adds linear model. regression line line expected values (expectation \\(Y\\) conditional \\(X\\)). point Vehicle line \\(area\\_of\\_risk = 30\\) \\(b_0 + b_1 \\cdot 30\\). point “Myriocin” line \\(area\\_of\\_risk = 30\\) \\(b_0 + b_1 \\cdot 30 + b_2\\). , point “Myriocin + DoxSa” line \\(area\\_of\\_risk = 30\\) \\(b_0 + b_1 \\cdot 30 + b_3\\). Understanding components linear model add gives phenomenonal cosmic power statistical analysis.","code":""},{"path":"covariates.html","id":"interpretation-of-the-estimated-marginal-means","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.5 Interpretation of the estimated marginal means","text":"values column “emmean” expected values group (“conditional ”) \\(area\\_at\\_risk\\) equal mean \\(area\\_at\\_risk\\).","code":"\nfig4h_m1_emm <- emmeans(fig4h_m1, specs = \"treatment\")\nfig4h_m1_emm##  treatment        emmean    SE df lower.CL upper.CL\n##  Vehicle           11.98 0.839 32     10.3     13.7\n##  Myriocin           8.87 0.916 32      7.0     10.7\n##  Myriocin + DoxSa   8.83 1.045 32      6.7     11.0\n## \n## Confidence level used: 0.95"},{"path":"covariates.html","id":"interpretation-of-the-contrasts","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.6 Interpretation of the contrasts","text":"values column “estimate” differences estimated marginal means estimated marginal means table. first two contrasts equal coefficients \\(b_2\\) \\(b_3\\) coefficient table.","code":"\nfig4h_m1_pairs <- contrast(fig4h_m1_emm,\n                           method = \"revpairwise\",\n                           adjust = \"none\") %>%\n  summary(infer = TRUE)\nfig4h_m1_pairs##  contrast                      estimate   SE df lower.CL upper.CL t.ratio\n##  Myriocin - Vehicle             -3.1139 1.23 32    -5.62   -0.611  -2.534\n##  (Myriocin + DoxSa) - Vehicle   -3.1485 1.37 32    -5.93   -0.367  -2.306\n##  (Myriocin + DoxSa) - Myriocin  -0.0346 1.43 32    -2.95    2.881  -0.024\n##  p.value\n##   0.0164\n##   0.0278\n##   0.9809\n## \n## Confidence level used: 0.95"},{"path":"covariates.html","id":"adding-the-covariate-improves-inference","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.2.7 Adding the covariate improves inference","text":"Compare effects Model \\(\\mathcal{M}_1\\) (added covariate) effects linear model without added covariate.Inference two models different. model adjusting area risk (Model \\(\\mathcal{M}_1\\)), conclude “Unexpectedly, estimated Myriocin + DoxSa effect (-3.2 mg, 95% CI: -5.9, -0.4) effectively big Myriocin alone effect.” contrast, using model without covariate, conclude “Relative Myriocin effect, estimated Myriocin + DoxSa effect small (.29 mg), large uncertainty direction magnitude (95% CI: -4.1, 4.7).” (Note authors published different conclusion either . recover results leading conclusion using methods published authors.)","code":"##                           Estimate Std. Error t value Pr(>|t|) 2.5 % 97.5 %\n## (Intercept)                 11.200       1.39   8.070 2.60e-09  8.38 14.000\n## treatmentMyriocin           -3.660       2.04  -1.790 8.26e-02 -7.82  0.499\n## treatmentMyriocin + DoxSa    0.293       2.15   0.136 8.93e-01 -4.09  4.670"},{"path":"covariates.html","id":"understanding-interaction-effects-with-covariates","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3 Understanding interaction effects with covariates","text":"","code":""},{"path":"covariates.html","id":"fit-the-model-6","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3.1 Fit the model","text":"add continuous covariate model sometimes want model interaction categorical factor variable. interaction effect represents effect one variable changes given level second variable. verbal model added interaction effect \\(area\\_of\\_necrosis \\sim area\\_at\\_risk + treatment + area\\_at\\_risk \\times treatment\\;\\;\\;(\\mathcal{M}_2)\\)’ll refer Model \\(\\mathcal{M}_2\\). understand model coefficients, helps expand full linear model.\\[\\begin{align}\narea\\_of\\_necrosis = \\beta_0 &+ \\beta_1 area\\_at\\_risk + \\beta_2 treatment_{myriosin} + \\beta_3 treatment_{Myriocin\\;+\\;DoxSa}\\\\\n&+ \\beta_4 area\\_at\\_risk \\cdot treatment_{myriosin}\\\\\n&+ \\beta_5 area\\_at\\_risk \\cdot treatment_{Myriocin\\;+\\;DoxSa}\\\\\n&+ \\varepsilon\n\\tag{14.6}\n\\end{align}\\]addition effects myriosin (\\(\\beta_2\\)) myriosin + DoxSa (\\(\\beta_3\\)) treatments Model \\(\\mathcal{M}_1\\), Model \\(\\mathcal{M}_2\\) includes coefficients two interaction effects, interaction myriosin area risk (\\(\\beta_4\\)) interaction myriosin + DoxSa area risk (\\(\\beta_4\\)). parameters \\(\\beta_1\\) \\(\\beta_2\\) additive effects linear (additive) “\\(X\\)” variables. parameters \\(\\beta_3\\) \\(\\beta_4\\) non-additive effects linear “\\(X\\)” variables – effects coefficients \\(X\\) product two variables. can see model equation explain .goals researchers heart necrosis study, specifically want model interaction. Nevertheless, want plot interaction effect “Step 2: examine data” phase analysis sure additive model reasonable model given data.","code":"\n# see working in R to understand this model formula\nfig4h_m2 <- lm(area_of_necrosis ~ area_at_risk*treatment,\n               data = fig4h)"},{"path":"covariates.html","id":"plot-the-model-with-interaction-effect","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3.2 Plot the model with interaction effect","text":"\nFigure 14.6: effect treatment area necrosis, adjusted area risk.\n","code":""},{"path":"covariates.html","id":"interpretation-of-the-model-coefficients-1","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3.3 Interpretation of the model coefficients","text":"estimate “(Intercept)” row (\\(b_0\\)) expected value reference (, Vehicle group) \\(area\\_at\\_risk=0\\). interpretation intercept additive model (Model \\(\\mathcal{M}_1\\)) value different. slope coefficient (\\(b_1\\)) estimated Model \\(\\mathcal{M}_1\\) common slope (three groups) estimated pooling data three groups. slope coefficient non-additive model (Model \\(\\mathcal{M}_2\\)) computed just reference data. slope differs Model \\(\\mathcal{M}_1\\) Model \\(\\mathcal{M}_2\\), intercept differs (think necessarily related). Model \\(\\mathcal{M}_1\\), value inferential statistics meaningful parameterization slight modifications model can make meaningful.estimate “area_at_risk” row (\\(b_1\\)) slope regression line reference (Vehicle) group, , slope conditional \\(treatment = \\mathrm{''Vehicle''}\\)estimate “treatmentMyriocin” row (\\(b_2\\)) effect \\(Myriocin\\) \\(area\\_of\\_necrosis\\) conditional \\(area\\_at\\_risk = 0\\). Note difference interpretation \\(b_2\\) Model \\(\\mathcal{M}_1\\). difference interaction. effect \\(Myriocin\\) \\(area\\_of\\_necrosis\\) longer constant values \\(area\\_at\\_risk\\). easily seen Figure 14.6, effect Myriocin (vertical distance Vehicle Myriocin line) small small values area risk large large areas risk.estimate “treatmentMyriocin + DoxSa” row (\\(b_3\\)) effect \\(Myriocin + DoxSa\\) \\(area\\_of\\_necrosis\\) conditional \\(area\\_at\\_risk = 0\\). ’s interpretation similar \\(b_2\\).estimate “area_at_risk:treatmentMyriocin” row (\\(b_4\\)) interaction effect area risk Myriocin. ’s value difference slope regression line Myriocin points Figure 14.6 slope regression line reference (Vehicle) points Figure 14.6. Consequently, slope regression line Myriocin points \\(b_1 + b_4\\).estimate “area_at_risk:treatmentMyriocin + DoxSa” row (\\(b_5\\)) interaction effect area risk Myriocin + DoxSa ’s value difference slope regression line Myriocin + DoxSa points Figure 14.6 slope regression line reference (Vehicle) data Figure 14.6. Consequently, slope regression line Myriocin + DoxSa points \\(b_1 + b_5\\).Interaction effects differences slopes (also true interactions two factor variables, even though usually describe “differences differences”) difference easily seen inspection plot similar Figure 14.6. bigger interaction, less parallel regression line.","code":"\nfig4h_m2_coef <- cbind(coef(summary(fig4h_m2)),\n      confint(fig4h_m2))\nround(fig4h_m2_coef[,c(1,2)], digits = 2)##                                        Estimate Std. Error\n## (Intercept)                               -6.55       3.01\n## area_at_risk                               0.53       0.09\n## treatmentMyriocin                          4.90       4.90\n## treatmentMyriocin + DoxSa                 -1.17       5.28\n## area_at_risk:treatmentMyriocin            -0.25       0.15\n## area_at_risk:treatmentMyriocin + DoxSa    -0.06       0.14"},{"path":"covariates.html","id":"what-is-the-effect-of-a-treatment-if-interactions-are-modeled-it-depends.","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3.4 What is the effect of a treatment, if interactions are modeled? – it depends.","text":"useful way remember interaction “depends”. effect Myriocin area risk? interaction term linear model, answer “depends”. depends level \\(area\\_at\\_risk\\). small values area risk, effect Myriocin small. large values area risk, effect Myriocin large. Likewise, effect area risk necrosis? depends (interactions modeled). depends level treatment, example effect Myriocin group smaller (smaller slope) effect Vehicle group. “depends” hallmark interactions – effect one variable outcome depends level second variable.","code":""},{"path":"covariates.html","id":"which-model-do-we-use-mathcalm_1-or-mathcalm_2","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.3.5 Which model do we use, \\(\\mathcal{M}_1\\) or \\(\\mathcal{M}_2\\)?","text":"answer question depends partly goals. explicitly interested measuring interaction effect (perhaps theory predicts positive interaction) necesarily add interaction effects model. interested estimating treatment effect conditional covariate, don’t include interaction. Remember purpose initial explorations data (“Step 2 – examine data”) help decide model specify model formula. initial plot shows large interactions treatment levels covariate, adjusting covariate won’t work, least won’t work without additional complexity interpretation. “Large interactions” course raises question, large larege ignore interactions? Many textbooks biostatistics recommend using interaction p-value make decision. disagree. objective answer question. lose information models – science. researcher transparent decisions. additive (interactions) model used condition (adjust ) covariate, researcher use supplement plot model interactions evidence decision reasonable.","code":""},{"path":"covariates.html","id":"understanding-ancova-tables","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.4 Understanding ANCOVA tables","text":"","code":""},{"path":"covariates.html","id":"working-in-r-3","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5 Working in R","text":"","code":""},{"path":"covariates.html","id":"importing-the-heart-necrosis-data","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.1 Importing the heart necrosis data","text":"","code":"\ndata_folder <- \"data\"\ndata_from <- \"1-Deoxydihydroceramide causes anoxic death by impairing chaperonin-mediated protein folding\"\nfile_name <- \"42255_2019_123_MOESM7_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n  \nsheet_i <- \"Figure 4h\"\nfig4h_1 <- read_excel(file_path,\n                         sheet = sheet_i,\n                         range = \"A5:B19\",\n                         col_names = TRUE) %>%\n  clean_names() %>%\n  data.table()\nfig4h_2 <- read_excel(file_path,\n                         sheet = sheet_i,\n                         range = \"D5:E17\",\n                         col_names = TRUE) %>%\n  clean_names() %>%\n  data.table()\nfig4h_3 <- read_excel(file_path,\n                         sheet = sheet_i,\n                         range = \"G5:H15\",\n                         col_names = TRUE) %>%\n  clean_names() %>%\n  data.table()\n\nfig4h <- rbind(data.table(treatment = \"Vehicle\", fig4h_1),\n                    data.table(treatment = \"Myriocin\", fig4h_2),\n                    data.table(treatment = \"Myriocin + DoxSa\", fig4h_3))\n\ntreatment_levels <- c(\"Vehicle\", \"Myriocin\", \"Myriocin + DoxSa\")\nfig4h[, treatment := factor(treatment, levels = treatment_levels)]"},{"path":"covariates.html","id":"fitting-the-model","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.2 Fitting the model","text":"additive model, add covariate model formula using + operator. order variables doesn’t matter (right-hand-side model formula treatment + area_at_risk).nonadditive model interactions, add covariate model formula using * operator. ~ area_at_risk * treatment shortcut full model formula, 1 + area_at_risk + treatment + area_at_risk * treatment. R expands short formula full formula automatically. , order variables doesn’t matter (right-hand-side model formula treatment * area_at_risk).","code":"\nm1 <- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h)\nm2 <- lm(area_of_necrosis ~ area_at_risk * treatment, data = fig4h)"},{"path":"covariates.html","id":"using-the-emmeans-function-1","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.3 Using the emmeans function","text":"","code":"\nm1_emm <- emmeans(m1,\n        specs = c(\"treatment\"))\nm1_emm##  treatment        emmean    SE df lower.CL upper.CL\n##  Vehicle           11.98 0.839 32     10.3     13.7\n##  Myriocin           8.87 0.916 32      7.0     10.7\n##  Myriocin + DoxSa   8.83 1.045 32      6.7     11.0\n## \n## Confidence level used: 0.95\nm1_emm <- emmeans(m1,\n        specs = c(\"area_at_risk\", \"treatment\"))\nm1_emm##  area_at_risk treatment        emmean    SE df lower.CL upper.CL\n##            35 Vehicle           11.98 0.839 32     10.3     13.7\n##            35 Myriocin           8.87 0.916 32      7.0     10.7\n##            35 Myriocin + DoxSa   8.83 1.045 32      6.7     11.0\n## \n## Confidence level used: 0.95\n# recenter emms at the mean of the vehicle group\n\nvehicle_mean_x <- mean(fig4h[treatment == \"Vehicle\",\n                         area_at_risk])\nm1_emm_2 <- emmeans(m1,\n                    specs = c(\"area_at_risk\",\"treatment\"),\n                    at = list(area_at_risk = vehicle_mean_x))\nm1_emm_2##  area_at_risk treatment        emmean    SE df lower.CL upper.CL\n##          33.3 Vehicle           11.21 0.833 32     9.51    12.90\n##          33.3 Myriocin           8.09 0.903 32     6.25     9.93\n##          33.3 Myriocin + DoxSa   8.06 1.082 32     5.85    10.26\n## \n## Confidence level used: 0.95\nfig4h[, .(mean = mean(area_of_necrosis)),\n      by = treatment]##           treatment      mean\n## 1:          Vehicle 11.206580\n## 2:         Myriocin  7.546081\n## 3: Myriocin + DoxSa 11.499319"},{"path":"covariates.html","id":"ancova-tables","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.4 ANCOVA tables","text":"going report statistics ANCOVA table, sure report think reporting – easy get wrong R. One way easily get table using Type II III sums squares set contrast option factor variable contr.sum (tells R use deviation coding form model matrix linear model) use function Anova car package (base R anova).safest set lm function .coefficients linear model different interpretation given . Either understand new interpretation (given ) fit two models, one without reset contrasts one reset contrasts.estimated marginal mean table contrast table , regardless contrasts set linear model.Many websites show code changes default coding R session. strongly caution . don’t change back, likely misinterpret coefficients later analyses (see point 2).additive models, use Type II sum squares (Type III give results),non-additive models interactionsThe statistics interaction effect , regardless sum squares used.interpretation main effects problematic, best. discussed ANOVA chapter.statisticians recommend using Type III sums squares. effect $treatment* specific interpretation Type III – effect covariate = 0. can biologically relevant covariate data re-centered.","code":"\nAnova(m1_aov, type = 2) # be sure to use the correct model!## Anova Table (Type II tests)\n## \n## Response: area_of_necrosis\n##              Sum Sq Df F value   Pr(>F)    \n## treatment     82.04  2  4.2195  0.02364 *  \n## area_at_risk 580.25  1 59.6874 8.28e-09 ***\n## Residuals    311.09 32                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# for coefficients\nm2 <- lm(area_of_necrosis ~ treatment * area_at_risk,\n         data = fig4h)\n# for anova\nm2_aov <- lm(area_of_necrosis ~ treatment * area_at_risk,\n         data = fig4h,\n         contrasts = list(treatment = contr.sum))\nAnova(m2_aov, type = 2) # be sure to use the correct model!## Anova Table (Type II tests)\n## \n## Response: area_of_necrosis\n##                        Sum Sq Df F value    Pr(>F)    \n## treatment               82.04  2  4.3369   0.02216 *  \n## area_at_risk           580.25  1 61.3485 9.685e-09 ***\n## treatment:area_at_risk  27.34  2  1.4453   0.25163    \n## Residuals              283.75 30                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnova(m2_aov, type = 3) # be sure to use the correct model!## Anova Table (Type III tests)\n## \n## Response: area_of_necrosis\n##                        Sum Sq Df F value    Pr(>F)    \n## (Intercept)             56.06  1  5.9270   0.02107 *  \n## treatment               12.98  2  0.6863   0.51116    \n## area_at_risk           491.92  1 52.0087 5.012e-08 ***\n## treatment:area_at_risk  27.34  2  1.4453   0.25163    \n## Residuals              283.75 30                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"covariates.html","id":"plotting-the-model","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.5 Plotting the model","text":"","code":""},{"path":"covariates.html","id":"using-ggpubr","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.5.1 Using ggpubr","text":"ggpubr plot model interactions.","code":"\nggscatter(data = fig4h,\n          x = \"area_at_risk\",\n          y = \"area_of_necrosis\",\n          color = \"treatment\",\n          add = \"reg.line\",\n          ylab = \"Area of necrosis (mg)\",\n          xlab = \"Area at risk (mg)\",\n          palette = pal_okabe_ito\n)"},{"path":"covariates.html","id":"ggplot2","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.5.2 ggplot2","text":"","code":"\n# pal_batlow <- scico(n, alpha = NULL, begin = 0, end = 1, direction = 1, palette = \"batlow\")\n\nm1 <- lm(area_of_necrosis ~ area_at_risk + treatment,\n         data = fig4h)\n\ngg <- ggplot(fig4h, \n             aes(x = area_at_risk,\n                 y = area_of_necrosis,\n                 color = treatment)) +\n  geom_point(aes(color=treatment)) +\n  geom_smooth(method = \"lm\",\n              mapping = aes(y = predict(m1, fig4h))) +\n  scale_color_manual(values = pal_okabe_ito,\n                     name = NULL) +\n  # scale_color_manual(values = pal_batlow,\n  #   name = NULL) +\n  ylab(\"Area of necrosis (mg)\") +\n  xlab(\"Area at risk (mg)\") +\n  theme_pander() +\n  theme(legend.position=\"bottom\") +\n  NULL\ngg"},{"path":"covariates.html","id":"a-response-effects-plot-using-ggplot2","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.5.5.3 A response-effects plot using ggplot2","text":"","code":"\nm1 <- lm(area_of_necrosis ~ area_at_risk + treatment, data = fig4h)\nb <- coef(m1)\n\ngroups <- levels(fig4h$treatment)\nline_data <- fig4h[, .(min_x = min(area_at_risk),\n                       max_x = max(area_at_risk)), by = treatment]\nline_data[, dummy1 := ifelse(treatment == \"Myriocin\", 1, 0)]\nline_data[, dummy2 := ifelse(treatment == \"Myriocin + DoxSa\", 1, 0)]\n\nline_data[, y_min := b[1] + b[2]*min_x + b[3]*dummy1 + b[4]*dummy2]\nline_data[, y_max := b[1] + b[2]*max_x + b[3]*dummy1 + b[4]*dummy2]\n\ngg_response <- ggplot(data = fig4h,\n                      aes(x = area_at_risk,\n                          y = area_of_necrosis,\n                          color = treatment)) +\n  # points\n  geom_point(size = 2) +\n  \n  geom_segment(data = line_data,\n               aes(x = min_x,\n                   y = y_min,\n                   xend = max_x,\n                   yend = y_max)) +\n  scale_color_manual(values = pal_okabe_ito,\n                     name = NULL) +\n  ylab(\"Area of necrosis (mg)\") +\n  xlab(\"Area at risk (mg)\") +\n  theme_pubr() +\n  theme(legend.position=\"bottom\") +\n\n  NULL\n#gg_response\nm1_emm <- emmeans(m1, specs = \"treatment\")\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"none\") %>%\n  summary(infer = TRUE) %>%\n  data.table()\n# pvalString is from package lazyWeave\nm1_pairs[ , p_pretty := pvalString(p.value)]\n# also create a column with \"p-val: \"\nm1_pairs[ , pval_pretty := paste(\"p = \", p_pretty)]\n\ncontrast_order <- m1_pairs[, contrast]\nm1_pairs[, contrast := factor(contrast, contrast_order)]\n\ngg_effect <- ggplot(data = m1_pairs, \n                    aes(y = contrast,\n                        x = estimate)) +\n  # confidence level of effect\n  geom_errorbar(aes(xmin = lower.CL, \n                    xmax = upper.CL),\n                width = 0, \n                color = \"black\") +\n  # estimate of effect\n  geom_point(size = 3) +\n  \n  # draw a line at effect = 0\n  geom_vline(xintercept = 0, linetype = 2) +\n  \n  # p-value. The y coordinates are set by eye\n  annotate(geom = \"text\",\n           label = m1_pairs$pval_pretty,\n           y = 1:3,\n           x = 4.5) +\n  \n  # x-axis label and aesthetics\n  xlab(\"Effect (mg)\") +\n  ylab(\"Contrast\") +\n  coord_cartesian(xlim = c(-8,5.5)) +\n  scale_x_continuous(position=\"top\") +\n  \n  theme_pubr() +\n  # theme(axis.title.x = element_blank()) +\n  \n  NULL\n# gg_effect\ngg <- plot_grid(gg_effect,\n                gg_response,\n                nrow=2,\n                align = \"v\",\n                axis = \"rl\",\n                rel_heights = c(0.35, 1)\n                )\ngg"},{"path":"covariates.html","id":"best-practices","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.6 Best practices","text":"","code":""},{"path":"covariates.html","id":"do-not-use-a-ratio-of-partwhole-as-a-response-variable-instead-add-the-denominator-as-a-covariate","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.6.1 Do not use a ratio of part:whole as a response variable – instead add the denominator as a covariate","text":"","code":""},{"path":"covariates.html","id":"do-not-use-change-from-baseline-as-a-response-variable-instead-add-the-baseline-measure-as-a-covariate","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.6.2 Do not use change from baseline as a response variable – instead add the baseline measure as a covariate","text":"","code":""},{"path":"covariates.html","id":"do-not-test-for-balance-of-baseline-measures","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.6.3 Do not “test for balance” of baseline measures","text":"test null hypothesis difference mean baseline “test balance.” Researchers frequently test balance baseline use p-value test decide next step: 1) \\(p > 0.05\\), conclude pre-treatment means “differ” use something like simple t test post-treatment means, 2) \\(p < 0.05\\), use change score, percent change, response simple t-test, 3) \\(p < 0.05\\), use use linear model pre-treatment value covariate. , general, hypothesis tests used decide several ways proceed make sense. First, null-hypothesis significance test tell “difference” – null-hypothesis tests . Second, \\(p\\)-value initial test isn’t strictly valid take account decision step, minor. Third, doesn’t matter; always difference actual means initial measures , consequently, conditional expectation final measures, change measures, percent change dependent initial difference. , one initial measures, one use linear model adjusts baseline measures estimate treatment effect pre-post designs. , one isn’t planning taking initial measure, maybe , initial measure used linear model allows better estimate treatment effect, discussed Adding covariates can increases precision effect interest.","code":""},{"path":"covariates.html","id":"best-practices-2-use-a-covariate-instead-of-normalizing-a-response","chapter":"14 Linear models with added covariates (“ANCOVA”)","heading":"14.7 Best practices 2: Use a covariate instead of normalizing a response","text":"","code":""},{"path":"factorial.html","id":"factorial","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","text":"","code":"\n  # X <- model.matrix(m1)\n  # y_col <- insight::find_response(m1)\n  # y <- model.frame(m1)[, y_col]\n  # b <- (solve(t(X)%*%X)%*%t(X)%*%y)[,1]\n\n  emm_b <- function(m1_emm){\n    # works only for 2 x 2 and specs have to be in\n    # same order as model\n    if(!is.data.frame(m1_emm)){\n      m1_emm <- summary(m1_emm)\n    }\n    mu <- m1_emm[, \"emmean\"]\n    b <- c(mu[1],\n            mu[2] - mu[1],\n            mu[3] - mu[1],\n            mu[4])\n    b[4] <- b[4] - (b[1]+b[2]+b[3])\n    return(b)\n  }"},{"path":"factorial.html","id":"a-linear-model-with-crossed-factors-estimates-interaction-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.1 A linear model with crossed factors estimates interaction effects","text":"factorial experiment one two factor variables (categorical \\(X\\)) crossed, resulting group combination levels factor. specific combination different treatment. linear model crossed factors used estimate interaction effects, occur effect level one factor conditional level factors. Estimation interaction effect necessary inferences “Something different” – Estimation treatment effect relative control effect (Example 1 – TLR9-/- mice)“depends” – Estimation effect background condition effect (Example 2 – XX mice)“sum parts” – Estimation synergy, non-additive effect (Example 3 – plant root growth)Inferences like common experimental biology literature made using wrong statistics. correct statistic – interaction effect – easy compute rarely computed.Alert. NHST encourages thinking “isn’t interaction effect” dichotomous thinking can lead disasters biblical proportion, dogs cats living together. Interaction effects biology ubiquitous. Sometimes small enough ignore.","code":""},{"path":"factorial.html","id":"an-interaction-is-a-difference-in-simple-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.1.1 An interaction is a difference in simple effects","text":"chapter, ’ll describe interaction effect using different descriptions (Examples 1-3). , want emphasize interaction effect difference differences. clarify , ’ll introduce fake experiment. research group evidence certain gene product (CGP) intermediary intestinal microbiota obesity. researchers transfer feces either lean mice obese mice either wildtype mice CGP-/- mice investigate effect microbiota CGP body weight. design two factors (donor genotype), two levels (donor: “Lean”, “Obese; genotype:”WT”, “KO”), makes \\(2 \\times 2\\) (crossed factorial) design. \\(n=6\\) replicates treatment.good way visualize treatment combinations crossed design \\(m \\times p\\) table showing combinations \\(m\\) levels factor 1 (\\(\\texttt{donor}\\)) \\(p\\) levels factor 2 (\\(\\texttt{genotype}\\)) (15.1).\nTable 15.1: Mean body weight mice four treatment groups 2 x 2 factorial experiment.\nupper-left \\(2 \\times 2\\) part table contains fake mean body weight treatment group. means known cell means.first two elements “Effect” column contains difference two cells left – effects genotype conditional level donor.first two elements “Effect” row contains difference two cells – effects donor conditional level genotype.effects described items 2 3 known simple effects.value red difference two simple effects . also difference two simple effects left. differences equal. interaction effect.experiment single factor four levels, six pairwise comparisons may interest. \\(2 \\times 2\\) factorial experiment, four simple effects interaction effect pique interest researcher.fake experiment, want know effect obese donor treatment KO mice compared effect obese donor treatment WT mice. , want contrast two simple effects.\\[\n(\\operatorname{Obese/KO} - \\operatorname{Lean/KO}) - (\\operatorname{Obese/WT} - \\operatorname{Lean/WT})\n\\]contrast interaction effect. interaction effect difference differences.","code":""},{"path":"factorial.html","id":"a-linear-model-with-crossed-factors-includes-interaction-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.1.2 A linear model with crossed factors includes interaction effects","text":"factorial linear model fake data \\[\n\\begin{align}\n\\texttt{body_weight} &= \\beta_0 + \\beta_1 (\\texttt{donor}_{\\texttt{Obese}}) + \\beta_2 (\\texttt{genotype}_{\\texttt{KO}})\\ + \\\\\n&\\quad \\ \\beta_3 (\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}) + \\varepsilon\n\\end{align}\n\\]\nvariables?\\(\\texttt{donor}_{\\texttt{Obese}}\\) indicator variable “Obese” level factor \\(\\texttt{donor}\\). contains value 1 donor “Obese” 0 otherwise.\\(\\texttt{genotype}_{\\texttt{KO}}\\) indicator variable “KO” level factor \\(\\texttt{genotype}\\). contains value 1 genotype “KO” 0 otherwise.\\(\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}\\) indicator variable interaction “Obese” level \\(\\texttt{donor}\\) “KO” level \\(\\texttt{genotype}\\). “:” character indicate interaction follows R formula convention LME4 package. Many sources use \\(\\times\\) symbol instead colon. variable contains value 1 mouse assigned “Obese” “KO” 0 otherwise. value product values \\(\\texttt{donor}_{\\texttt{Obese}}\\) \\(\\texttt{genotype}_{\\texttt{KO}}\\).parameters?linear model set “Lean” reference level \\(\\texttt{donor}\\) “WT” reference level \\(\\texttt{genotype}\\). make Lean/WT mice control.\\(\\beta_0\\) true mean control, “Lean/WT” group.\\(\\beta_1\\) true effect donor WT mice (effect manipulating donor factor genotype factor). difference true means “Obese/WT” group “Lean/WT” group.mean “Obese/WT” group \\(\\beta_0 + \\beta_1\\). expectation start control add effect Obese donor.\\(\\beta_2\\) true effect genotype mice given feces lean donors (effect manipulating genotype factor donor factor). difference true means “Lean/KO” group “Lean/WT” group.mean “Lean/KO” group \\(\\beta_0 + \\beta_2\\). expectation start control add effect KO genotype.\\(\\beta_3\\) true interaction effect \\(\\texttt{donor}_{\\texttt{Obese}} : \\texttt{genotype}_{\\texttt{KO}}\\)mean Obese/KO group \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\). expected mean Obese/KO group, factors additive, means interaction effect zero, \\(\\beta_0 + \\beta_1 + \\beta_2\\). interaction effect difference actual mean Obese/KO group additive mean. interaction effect ’s left-, ’ve added Obese effect KO effect control.","code":""},{"path":"factorial.html","id":"factorial-experiments-are-frequently-analyzed-as-flattened-linear-models-in-the-experimental-biology-literature","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.1.3 factorial experiments are frequently analyzed as flattened linear models in the experimental biology literature","text":"Often, researchers analyze data factorial experiment one-way ANOVA followed pairwise tests (simple series separate t-tests). fake experiment, linear model \\[\n\\begin{align}\n\\texttt{body_weight} &= \\beta_0 + \\beta_1 (\\texttt{treatment}_{\\texttt{Obese}}) + \\beta_2 (\\texttt{treatment}_{\\texttt{KO}}) \\ + \\\\\n& \\quad \\ \\beta_3 (\\texttt{treatment}_{\\texttt{Obese + KO}}) + \\varepsilon\n\\end{align}\n\\]refer flattened model (table treatment combinations flattened single row). Inference factorial flattened model . can get pairwise contrasts interaction contrasts either model. said, factorial model nudges researchers think analysis factorial design, good, interaction effects explicit component factorial model.","code":""},{"path":"factorial.html","id":"example-1-estimation-of-a-treatment-effect-relative-to-a-control-effect-something-different-experiment-2j-glucose-uptake-data","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2 Example 1 – Estimation of a treatment effect relative to a control effect (“Something different”) (Experiment 2j glucose uptake data)","text":"introduce linear model crossed factors (categorical \\(X\\) variables), ’ll use data set experiments designed measure effect toll-like receptor protein TLR9 activation excercise-induced AMP-activated protein kinase (AMPK) downstream sequelae activation, including glucose transport (outside inside cell) skeletal muscle cells.Article source: TLR9 beclin 1 crosstalk regulates muscle AMPK activation exercisePublic sourceThe data multiple experiments Figure 2.Data source","code":""},{"path":"factorial.html","id":"twoway-understand1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2.1 Understand the experimental design","text":"Background. Exercise (muscle activity) stimulates AMPK activated glucose uptake (transport outside inside muscle cell). Instead natural exercise, can stimulate multiple systems, researchers directly activate muscle electrical stimulation.Research question two ways think question – produce answer.much TLR9 knockout inhibit expected increase glucose uptake following electrical stimulation? expected increase comes contrast positive negative controls. Call knockout-induced stimulation effect.much TLR9 knockout inhibit glucose uptake muscle stimulation compared effect TLR9 knockout muscle rest? Call stimulation-induced TLR9-/- effect.Response variable \\(\\texttt{glucose_uptake}\\) (nmol per mg protein per 15 min) – rate glucose transported cell.Factor 1 – \\(\\texttt{genotype}\\) (“WT”, “KO”).“WT” (reference level) – C57BL/6J mice intact TLR gene (TLR+/+)“KO” – TLR-/- mice C57BL/6J backgroundFactor 2 \\(\\texttt{stimulation}\\) (“Rest”, “Active”) – Two levels:“Rest” (reference level) – muscle stimulated.“Active” – electrical stimulation muscle induce contraction contractile-related cell changesThese two factors create three control treatments one focal treatment:WT/Rest – Negative control. Expect non-exercise (low) level uptake.WT/Rest – Negative control. Expect non-exercise (low) level uptake.WT/Active – Positive control. Expect high uptake.WT/Active – Positive control. Expect high uptake.KO/Rest – Method control. Unsure KO effect uptake Rest, need control.KO/Rest – Method control. Unsure KO effect uptake Rest, need control.KO/Active – Focal treatment. level KO/Rest TLR9-/- completely inhibits electrical stimulation glucose uptakeKO/Active – Focal treatment. level KO/Rest TLR9-/- completely inhibits electrical stimulation glucose uptakeDesign – \\(2 \\times 2\\), , two crossed factors two levels. results four groups, unique combination levels factor.Planned ContrastsThe two ways framing research question suggest either following sets contrasts. ways framing generate treatment contrast includes focal treatment control contrast include focal treatment. question pursued experiment addressed contrast treatment control contrasts, estimate interaction effect. difference inference two ways framing question – interaction effects equivalent. two framings simply give two different ways viewing interaction effect.Framing 1: much TLR9 knockout inhibit expected increase glucose uptake stimulation?(KO/Active - KO/Rest) – effect Stimulation KO mice. treatment contrast. TLR9 necessary glucose uptake, zero. positive, non-TLR9 paths.(KO/Active - KO/Rest) – effect Stimulation KO mice. treatment contrast. TLR9 necessary glucose uptake, zero. positive, non-TLR9 paths.(WT/Active - WT/Rest) – positive control contrast – know based prior knowledge want compare treatment effect . positive based prior knowledge.(WT/Active - WT/Rest) – positive control contrast – know based prior knowledge want compare treatment effect . positive based prior knowledge.need control expected increase 2 using contrast:(KO/Active - KO/Rest) - (WT/Active - WT/Rest) – contrast interaction effect. focal contrast need estimate knockout-induced stimulation effect.Framing 2: stimulation-induced TLR9-/- effect?(KO/Active - WT/Active) – effect KO muscle stimulation. treatment contrast. TLR9 necessary glucose uptake, big negative KO effect rest small.(KO/Active - WT/Active) – effect KO muscle stimulation. treatment contrast. TLR9 necessary glucose uptake, big negative KO effect rest small.(KO/Rest - WT/Rest) – effect KO muscle stimulated. methodological control contrast. TLR9-/- KO non-muscle-stimulation paths glucose uptake, something zero.(KO/Rest - WT/Rest) – effect KO muscle stimulated. methodological control contrast. TLR9-/- KO non-muscle-stimulation paths glucose uptake, something zero.need control (KO/Rest - WT/Rest) effect using contrast:(KO/Active - WT/Active) - (KO/Rest - WT/Rest) – contrast interaction effect. focal contrast need estimate stimulation-induced TLR9-/- effect.planned contrasts :(WT/Active - WT/Rest) – positive control contrast(KO/Rest - WT/Rest) – methodological control contrast(KO/Active - WT/Active) – treatment contrast(KO/Active - WT/Active) - (KO/Rest - WT/Rest) – interaction contrast","code":""},{"path":"factorial.html","id":"fit-the-linear-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2.2 Fit the linear model","text":"","code":"\nexp2j_m1 <- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)"},{"path":"factorial.html","id":"twoway-exp2j-inf","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2.3 Inference","text":"coefficient tableemmeans tableThe contrasts table","code":"\nexp2j_m1_coef <- tidy(exp2j_m1, conf.int = TRUE)\n\nexp2j_m1_coef %>%\n  kable(digits = c(1,2,3,1,4,2,2)) %>%\n  kable_styling()\nexp2j_m1_emm <- emmeans(exp2j_m1,\n                          specs = c(\"stimulation\", \"genotype\"))\nexp2j_m1_emm %>%\n  kable(digits = c(1,1,2,3,1,2,2)) %>%\n  kable_styling()\n# exp2j_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_rest <- c(1,0,0,0)\nwt_active <- c(0,1,0,0)\nko_rest <- c(0,0,1,0)\nko_active <- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# these planned contrasts are described above\n# 1. (WT/Active - WT/Rest) -- positive control contrast\n# 2. (KO/Rest - WT/Rest) -- methodological control contrast\n# 3. (KO/Active - WT/Active) -- treatment contrast\n# 4. (KO/Active - WT/Active) - (KO/Rest - WT/Rest) -- interaction \n\nexp2j_m1_planned <- contrast(\n  exp2j_m1_emm,\n  method = list(\n    \"(WT/Active - WT/Rest)\" = c(wt_active - wt_rest),\n    \"(KO/Rest - WT/Rest)\" = c(ko_rest - wt_rest),\n    \"(KO/Active - WT/Active)\" = c(ko_active - wt_active),\n    \"KO:Active Ixn\" = c(ko_active - wt_active) -\n      (ko_rest - wt_rest)\n  ),\n  adjust = \"none\"\n) %>%\n  summary(infer = TRUE)\n\n\nexp2j_m1_planned %>%\n  kable(digits = c(0,3,4,0,3,3,2,5)) %>%\n  kable_styling()\n# double check with automated contrasts\n# contrast(exp2b.2_m1_emm, method = c(\"revpairwise\"))"},{"path":"factorial.html","id":"plot-the-model-3","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2.4 Plot the model","text":"","code":"\nggplot_the_model(\n  exp2j_m1,\n  exp2j_m1_emm,\n  exp2j_m1_planned,\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = \"Glucose uptake\\n(nmol per mg protein\\nper 15 min)\",\n  effect_label = \"Difference in glucose uptake\",\n  rel_heights = c(0.44,1),\n)"},{"path":"factorial.html","id":"alternaplot-the-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.2.5 alternaPlot the model","text":"\nFigure 15.1: Dashed gray line expected additive mean KO/Active\n","code":"\nexp2j_m1_pairs <- contrast(exp2j_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") %>%\n  summary(infer = TRUE)\nb <- emm_b(exp2j_m1_emm)\ndodge_width <- 0.4\ngg <- ggplot_the_response(\n  exp2j_m1,\n  exp2j_m1_emm,\n  exp2j_m1_pairs[c(1,3,4),],\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = \"Glucose uptake\\n(nmol per mg protein\\nper 15 min)\",\n  y_pos = c(13.6, 13.2, 13.3)\n) +\n  geom_segment(x = 2 + dodge_width/2 - 0.05,\n               y = b[1] + b[2] + b[3],\n               xend = 2 + dodge_width/2 + 0.05,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 2.25,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"ixn p = \",\n                  fmt_p_value_rmd(exp2j_m1_planned[4,\"p.value\"])),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\")\n\ngg"},{"path":"factorial.html","id":"understanding-the-linear-model-with-crossed-factors-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3 Understanding the linear model with crossed factors 1","text":"Researchers experimental biology report almost exclusively p-values simple effects (Table 15.1) data factorial experiments. can computed even data flattened single “treatment” factor. interaction effect necessary make many inferences made researchers. see , need understand coefficients coefficient table .","code":""},{"path":"factorial.html","id":"twoway-what-coefs-are","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.1 What the coefficients are","text":"understand coefficients, helps use means emmeans table construct factorial table cell means\nTable 15.2: Cell mean table\nlinear model fit \\(\\texttt{exp2j}\\) data \\[\n\\begin{align}\n\\texttt{glucose_uptake} &= \\beta_{0} + \\beta_{1}(\\texttt{stimulation}_{\\texttt{Active}}) + \\beta_{2}(\\texttt{genotype}_{\\texttt{KO}}) \\ + \\\\\n&\\quad \\ \\beta_{3}(\\texttt{stimulation}_{\\texttt{Active}} : \\texttt{genotype}_{\\texttt{KO}}) + \\varepsilon\\\n\\end{align}\n\\]fit coefficients areExplainerUnderstand rows coefficient table . four parameters fit linear model – rows statistics estimates parameters. estimates coefficients model.\\(\\texttt{(Intercept)}\\) (\\(b_0\\)) mean \\(\\texttt{glucose_uptake}\\) reference level, set WT/Rest group (Figure 15.2). mean upper left cell Table 15.2.\\(\\texttt{stimulationActive}\\) coefficient (\\(b_2\\)) estimate added effect Stimulation genotype factor reference level, mean upper right cell minus mean upper left cell Table 15.2. coefficient mean – difference means (Figure 15.2). mean WT/Active group \\(b_0 + b_2\\).\\(\\texttt{genotypeKO}\\) coefficient (\\(b_1\\)) estimate added effect knocking TLR9 stimulation factor reference level, mean lower left cell minus mean upper left cell Table 15.2. coefficient mean – difference means (Figure 15.2). mean KO/Rest group \\(b_0 + b_1\\).\\(\\texttt{stimulationActive:genotypeKO}\\) coefficient (\\(b_3\\)) estimate interaction effect \\(\\texttt{genotype}\\) \\(\\texttt{stimulation}\\). mean lower right cell minus mean upper left cell. Instead, mean lower right cell minus expected mean lower right cell genotype activity treatment effects additive. expected additive-mean lower right cell (KO/Active) \\(b_0 + b_1 + b_2\\). mean KO/stimulation \\(b_0 + b_1 + b_2 + b_3\\) (Figure 15.2).interaction effect non-additive effect. Think . Adding Stimulation alone adds 3.45 nmol glucose per protein per 15 min uptake rate reference. Adding “KO” alone adds 0.78 glucose per protein per 15 min uptake rate reference. effects purely additive, adding Stimulation KO reference rate result mean 6.75 + 3.45 + 0.78 = 10.98 glucose per protein per 15 min. modeled mean KO/Active 8.67 glucose per protein per 15 min. difference observed - additive 8.67 - 10.98 = -2.31 glucose per protein per 15 min. Compare difference interaction coefficient coefficient table.Reinforce understanding “non-additive” item 5. interaction non-additive effect mean combined treatment something different just add KO Active effects. effect additive linear model. linear models – reference mean plus sum bunch effects.Understand rows . \\(\\texttt{stimulationActive}\\) row “stimulation” term Type III ANOVA table (ANOVA table produced GraphPad Prism JMP). p-values different p-values testing different hypotheses. coefficient table, \\(\\texttt{stimulationActive}\\) p-value testing difference means WT/Active WT/Rest. stimulation term Type III ANOVA table testing overall stimulation effect, estimated average two stimulation contrasts (WT/Active - WT/Rest) (KO/Active - KO/Rest). average two contrasts often interest (sometimes – see ).\nFigure 15.2: coefficients linear model two crossed factors, explained.\n","code":""},{"path":"factorial.html","id":"the-interaction-effect-is-something-different","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.2 The interaction effect is something different","text":"Item 6 coefficient table explainer stated “interaction non-additive effect mean combined treatment something different just add KO Active effects. something different interaction effect. interaction effect zero, expected effect stimulation KO mice expected effect stimulation WT mice (Figure 15.3. suggest underlying physiological changes Rest Active KO mice ”” physiological changes WT mice. , interaction, underlying physiological changes Rest Active KO mice “something different” physiological changes WT mice (Figure 15.3).biological reasons causing interaction effects highly variable makes Biology fun. Additive effects (interaction) may occur combined treatments act independently . might occur glucose uptake response knocking TLR9 opens path glucose uptake different independent paths activated electrial stimulation. Positive, synergistic interaction effects may occur combined treatments augment ’s ability affect response (see Example 3 ). occur glucose uptake response knocking TLR9 opens path glucose uptake different paths activated electrial stimulation also makes paths activated stimulation sensitive stimulation. Negative, antagonistic interaction effects may occur combined treatments interfere ’s ability affect response. occur glucose uptake response TLR9 path stimulation glucose uptake. Knocking TLR9 interferes path. TLR9 required, ’d expect interaction effect magnitude opposite sign control effect – , complete antagonism control effect. Previous experiments suggested negative interaction. measurement effect purpose experiment \\(\\texttt{exp2j}\\).\nFigure 15.3: interaction something different, . get observed difference (4), take expected difference (3) add something different (5).\n","code":""},{"path":"factorial.html","id":"why-we-want-to-compare-the-treatment-effect-to-a-control-effect","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.3 Why we want to compare the treatment effect to a control effect","text":"purpose experiment infer TLR9 role regulation muscle-stimulated glucose transport, , stimulation-induced TLR9-/- effect. TLR9 pathway muscle activity glucose transport, expect kind recovery baseline (Rest) values KO/Active group. TLR9 may also (alternatively) role non-stimulation-induced glucose uptake.make inference stimulation-induced TLR9-/- effect using experiment? Researchers typically look treatment effect(KO/Active - WT/Active)conclude stimulation-induced TLR9-/- effect ’s big (negative)treatment effect correct contrast inference, bother measures glucose uptake Rest? reason treatment effect alone wrong contrast inferring stimulation-induced TLR9 effect confounded control effect, contrast (KO/Rest - WT/Rest) (Figure 15.4).\nFigure 15.4: interaction effect stimulation-induced TLR9-/- effect\nunconfound inference, subtract confounder:(KO/Active - WT/Active) - (KO/Rest - WT/Rest)interaction effect. Consequences interpreting treatment effect KO/Active - WT/Active stimulation-induced TLR9-/- effect highlighted Figure 15.5. plots, data \\(\\texttt{exp2j}\\) values KO groups shifted create scenarios.Scenario 1. positive control big effect KO effect rest stimulation-induced TLR9-/- effect equal magnitude opposite direction Active effect WT (Figure 15.5A). stimulation-induced TLR9-/- effect conspicuous plot, sample means close true means high precision. simple effect measures stimulation-induced TLR9-/- effect correctly – simple effect equal magnitude (opposite sign) interaction effect. Many experiments literature pretty similar scenario.Scenario 2. positive control big effect KO negative effect rest zero stimulation-induced TLR9-/- effect – , interaction zero (Figure 15.5B). effect KO stimulation effect different occuring Rest. contraction induced TLR9 effect. Simple effect 1 inflates effect.Scenario 3. positive control big effect KO positive effect rest stimulation-induced TLR9-/- effect equal scenario 1 (Figure 15.5C). positive effect KO Rest masks stimulation-induced TLR9-/- effect. simple effect underestimates stimulation-induced TLR9-/- effect. similar happening Experiment 2j.\nFigure 15.5: Scenarios show consequence inferring stimulation-induced TLR9-/- effect simple effect (KO/Active - WT/Active). simple effect interaction effect lines extend KO/Active mean either KO/Rest mean (simple) expected mean KO/Active two factors additive (interaction).\n","code":""},{"path":"factorial.html","id":"the-order-of-the-factors-in-the-model-tells-the-same-story-differently","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.4 The order of the factors in the model tells the same story differently","text":"order factors model formula doesn’t matter coefficients, estimated marginal means, contrasts. can matter ANOVA () “tests ANOVA”. order matter researchers communicates results report (Figure 15.6). order also natural consequence two different ways framing research question.\nFigure 15.6: may take work plots show four means effects. difference communicate story others. ) order factors model stimulation * genotype. B) order factors model genotype * stimulation.\n","code":""},{"path":"factorial.html","id":"power-for-the-interaction-effect-is-less-than-that-for-simple-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.5 Power for the interaction effect is less than that for simple effects","text":"interaction contrast difference two simple contrasts , consequently, variance interaction contrast twice simple contrasts. , consequently, SE interaction estimate \\(\\sim 1.4 \\times\\) larger precision two simple effects form contrast (1.4 $sqrt{2}. exact value depend differences sample size among groups). consequence wider confidence intervals larger p-values interaction contrast compared simple contrast effect size.","code":""},{"path":"factorial.html","id":"planned-comparisons-vs.-post-hoc-tests","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.3.6 Planned comparisons vs. post-hoc tests","text":"contrasts computed planned based questions motivating experiment. planned contrasts contain three four simple effects. four simple effects areThere six pairwise comparisons experiment. Two simple effect:KO/Active - WT/RestWT/Active - KO/RestThese contrasts diagonal pairs cell-means table (Table 15.2). factorial design, generally interested diagonal contrasts. reported supplement. Recognize adjusting p-values multiple tests, care contrasts included computation adjustment, adjusted p-values conservative.","code":"\nexp2j_m1_pairs <- contrast(exp2j_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") %>%\n  summary(infer = TRUE)\nexp2j_m1_pairs %>%\n  kable(digits = c(1,1,1,2,3,1,2,2,1,6)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"example-2-estimation-of-the-effect-of-background-condition-on-an-effect-it-depends-experiment-3e-lesian-area-data","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4 Example 2: Estimation of the effect of background condition on an effect (“it depends”) (Experiment 3e lesian area data)","text":"","code":""},{"path":"factorial.html","id":"understand-the-experimental-design","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4.1 Understand the experimental design","text":"Research question\n1. effect X chromosome complement lipid-related disease markers?\n2. effect sex (female male gonad) lipid-related disease markers?\n3. conditional sex effect chromosome complement?\n4. conditional effect X chromosome complement level sex?Response variable \\(\\texttt{lesian_area}\\) – atherosclerotic lesian area aortic sinus.Factor 1 – \\(\\texttt{sex}\\) (“Female”, “Male”). typical sex factor merely observed manipulated factor. Sex determined presence absence SRY autosome using Four Core Genotype mouse model. SRY determines gonad develops (ovary testis). “Female” autosome SRY. “Male” autosome SRY.Factor 2 – \\(\\texttt{chromosome}\\) (“XX”, “XY”). sex chromosome complement observed manipulated. “XX”, neither sex chromosome SRY natural condition. “XY”, SRY removed Y chromosome.Design – \\(2 \\times 2\\), , two crossed factors two levels. results four groups, unique combination levels factor. “Female/XX” control. “Male/XX” adds autosomal SRY gene (testes instead ovary). “Female/XY” replaces “X” complement engineered Y complement. “Male/XY” autosomal SRY engineered Y complement.research question suggest following planned contrastsWhat effect \\(\\texttt{chromosome}\\) lipid-related disease markers?(Female/XX - Female/XY) – effect XX mice female gonad. hypothesis true, large, negative.(Male/XX - Male/XY) – effect XX mice male gonad. hypothesis true, large, negative.effect \\(\\texttt{sex}\\) lipid-related disease markers?(Male/XX - Female/XX) – effect sex XX mice.(Male/XY - Female/XY) – effect sex XY mice.conditional effects?(Male/XX - Male/XY) - (Female/XX - Female/XY) – Interaction.experiment, treatment contrast control contrast. Instead, four simple contrasts equal interest.","code":""},{"path":"factorial.html","id":"fit-the-linear-model-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4.2 Fit the linear model","text":"","code":"\nexp3e_m1 <- lm(lesian_area ~ sex*chromosome, data = exp3e)"},{"path":"factorial.html","id":"check-the-model-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4.3 Check the model","text":"","code":"\nggcheck_the_model(exp3e_m1)"},{"path":"factorial.html","id":"inference-from-the-model-2","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4.4 Inference from the model","text":"coefficient tableThe emmeans tableThe contrasts tableExplainerThe two simple effects interaction computed separately. want adjust three comparisons, use Holm method.magnitude estimated effect XX mice male gonads 1/2 mice female gonads. difference magnitude interaction.Don’t infer “effect” given p-value interaction. estimate interaction effect magnitude estimated effect XX mice male gonads 1/2 magnitude estimated effect XX mice female gonads. interaction p-value suggests caution confidence sign effect.","code":"\n# step 2 - get the coefficient table\nexp3e_m1_coef <- tidy(exp3e_m1, conf.int = TRUE)\n# step 3 - get the modeled means\nexp3e_m1_emm <- emmeans(exp3e_m1, specs = c(\"sex\", \"chromosome\"))\n\nexp3e_m1_emm %>%\n  summary() %>%\n  kable(digits = c(1,1,3,4,1,3,3)) %>%\n  kable_styling()\n# m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nfxx <- c(1,0,0,0)\nmxx <- c(0,1,0,0)\nfxy <- c(0,0,1,0)\nmxy <- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# the focal contrasts are in the understand the experiment section\n# 1. (FXY - FXX) \n# 2. (MXY - MXX)\n# 3. (MXX - FXX)\n# 4. (MXY - FXY)\n# 5. Interaction\n\nexp3e_m1_planned <- contrast(exp3e_m1_emm,\n                       method = list(\n                         \"FXY - FXX\" = c(fxy - fxx),\n                         \"MXY - MXX\" = c(mxy - mxx),\n                         \"MXX - FXX\" = c(mxx - fxx),\n                         \"MXY - FXY\" = c(mxy - fxy),\n                         \"Interaction\" = c(mxy - mxx) -\n                           c(fxy - fxx)\n                           \n                       ),\n                       adjust = \"none\"\n) %>%\n  summary(infer = TRUE)\n\n# check\n # exp3e_m1_ixn <- contrast(exp3e_m1_emm,\n #                           interaction = c(\"revpairwise\"),\n #                           by = NULL)\n\nexp3e_m1_planned %>%\n  kable(digits = c(0,3,4,0,3,3,2,5)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"plot-the-model-4","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.4.5 Plot the model","text":"\nFigure 15.7: Dashed gray line expected additive mean KO/Active\n","code":"\nexp3e_m1_plot <- ggplot_the_model(\n  fit = exp3e_m1,\n  fit_emm = exp3e_m1_emm,\n  fit_pairs = exp3e_m1_planned,\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = expression(paste(\"Lesian area (\", mm^2, \")\")),\n  effect_label = expression(paste(\"Effect (\", mm^2, \")\")),\n  contrast_rows = \"all\",\n  rel_heights = c(0.5,1)\n)\nexp3e_m1_plot\nexp3e_m1_pairs <- contrast(exp3e_m1_emm,\n                           method = \"revpairwise\",\n                           simple = \"each\",\n                           combine = TRUE,\n                           adjust = \"none\") %>%\n  summary(infer = TRUE)\nb <- emm_b(exp3e_m1_emm)\ndodge_width <- 0.4\ngg <- ggplot_the_response(\n  exp3e_m1,\n  exp3e_m1_emm,\n  exp3e_m1_pairs[1:4,],\n  palette = pal_okabe_ito_blue,\n  legend_position = \"bottom\",\n  y_label = expression(paste(\"Lesian area (\", mm^2, \")\")),\n  y_pos = c(0.75, 0.78, 0.72, 0.72)\n) +\n  geom_segment(x = 2 + dodge_width/2 - 0.05,\n               y = b[1] + b[2] + b[3],\n               xend = 2 + dodge_width/2 + 0.05,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 2.25,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"ixn p = \",\n                  fmt_p_value_rmd(exp3e_m1_planned[5,\"p.value\"])),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\")\n\ngg"},{"path":"factorial.html","id":"understanding-the-linear-model-with-crossed-factors-2","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5 Understanding the linear model with crossed factors 2","text":"","code":""},{"path":"factorial.html","id":"twoway-marginal-means","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.1 Conditional and marginal means","text":"\nTable 15.3: Conditional (white background) marginal (color background) means full factorial model fit lesian area data\nconditional means fit model shown upper left \\(2 \\times 2\\) block (white background) Table 15.3. means conditional level \\(\\texttt{sex}\\) \\(\\texttt{chromosome}\\). linear model two crossed factor (continuous covariates), conditional means equal sample means treatment. values last row column marginal means, means associated row column cells (values margins table). generally, marginal refers statistic averaged across multiple levels another variable. marginal means \\(\\texttt{chromosome}\\) levels (orange background) means “XX” “XY” rows. marginal means \\(\\texttt{sex}\\) levels (blue background) means “Female” “Male” columns. Note marginal means simple averages across cell means weighted averages weights sample size used compute conditional (cell) means.","code":""},{"path":"factorial.html","id":"simple-conditional-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.2 Simple (conditional) effects","text":"factorial experiment crossed B factors, effect factor (relative reference, another level factor ) p levels factor B. , effect factor B (relative reference, another level factor B) m levels factor . effects one factor levels factor called simple effects. prefer conditional effects, since value effect conditional level factor.mouse lesian area experiment, chromosome effect sex = Female different effect sex = Male. Similarly, sex effect chromosome = XX different effect chromosome = XY.\nTable 15.4: Conditional (“simple”) effects factorial model.\nfirst two rows conditional effects \\(\\texttt{sex}\\) levels \\(\\texttt{chromosome}\\). last two rows conditional effects \\(\\texttt{chromosome}\\) levels \\(\\texttt{sex}\\).help understand conditional effects, add \\(m \\times p\\) table treatment combination means (Table 15.5). values right-side column (orange) conditional effects \\(\\texttt{sex}\\) level \\(\\texttt{chromosome}\\) values difference means associated row. example, conditional effect \\(\\texttt{sex}\\) chromosome = XY 0.124 (second value orange column). values bottom row (blue) conditional effects \\(\\texttt{chromosome}\\) level \\(\\texttt{sex}\\). values difference means associated column. example, conditional effect \\(\\texttt{chromosome}\\) sex = Female -0.29 (first value blue row). Note first conditional effect factor corresponding row table coefficients fit model effects factor reference level factor.\nTable 15.5: Conditional (“simple”) effects sex (orange background) chromosome (blue background) lesian area. conditional means combination factor levels cells white background. simple effect difference means associated row column.\n","code":""},{"path":"factorial.html","id":"twoway-marginal-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.3 Marginal effects","text":"average conditional effects factor marginal effects, main effects ANOVA terminology.’m showing full output emmeans package output highlight warning inference “may misleading” interaction effect linear model. healthy warning follow .Table 15.6, add marginal effects table conditional effects (Table 15.5)\nTable 15.6: Marginal effects sex (orange) chromosome (blue) lesian area. Simple effects grey cells. conditional means combination factor levels white cells.\n","code":"## NOTE: Results may be misleading due to involvement in interactions##  contrast      estimate     SE df t.ratio p.value\n##  Male - Female   0.0529 0.0375 14   1.411  0.1800\n## \n## Results are averaged over the levels of: chromosome## NOTE: Results may be misleading due to involvement in interactions##  contrast estimate     SE df t.ratio p.value\n##  XY - XX    -0.219 0.0375 14  -5.844  <.0001\n## \n## Results are averaged over the levels of: sex"},{"path":"factorial.html","id":"the-additive-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.4 The additive model","text":"Marginal effects can useful summarizing general trend, , like average, might especially meaningful large heterogeneity simple effects, occurs interaction effect large.interaction effect small, want summarize results general trends (“sex , chromosome ”), best practice strategy refit new linear model estimates effects two factors interaction equal zero.\\[\n\\begin{equation}\n\\texttt{lesian_area} = \\beta_0 + \\beta_1 \\texttt{sex}_\\texttt{Male} + \\beta_2 \\texttt{chromosome}_\\texttt{XY} + \\varepsilon\n\\tag{15.1}\n\\end{equation}\n\\]Model @ref{eq:twoway-reduced} reduced model one terms removed model. particular reduced model often referred additive model, since excludes interaction term, non-additive effect (indicator variable product two “main” indicator variables). R, model isThe model coefficients additive model \nTable 15.7: Model coefficients additive model.\nExplainer\\(\\texttt{sexMale}\\) average two conditional effects “Male” (one chromosome = \"XX\" one chromosome = \"XY).\\(\\texttt{chromosomeXY}\\) average two conditional effects “XY” (one sex = \"Female\" one sex = \"Male).\\(\\texttt{(Intercept)}\\) expected value without added \\(\\texttt{sexMale}\\) \\(\\texttt{chromosomeXY}\\) effects. abstract “average” Female/XX average value Female/XX group.conditional effects reduced model \nTable 15.8: Conditional effects additive model.\nExplainerIn additive model, conditional effects one factor level factor. makes sense. model fit additive, interaction effect set zero model differences conditional effects among contrasts levels factor (otherwise, interaction). sensible way thinking , doesn’t make sense compute discuss conditional effects additive model. Instead, additive model automatically estimates marginal effects.Compare table marginal effects additive model table marginal effects full model. estimates chromosome effect t-values p-values differ different degrees freedom (full model estimates one parameter, interaction effect). estimates sex effect two tables imbalance sample size. computation marginal effect chromosome, two simple effects sample size 5 4. computation marginal effect sex, one simple effect sample size 5 5 simple effect 4 4.","code":"\nexp3e_m2 <- lm(lesian_area ~ sex + chromosome, data = exp3e)"},{"path":"factorial.html","id":"twoway-reduce","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.5 Reduce models for the right reason","text":"Unless one factor truly effect, always interaction. stated , interactions ubiquitous. interaction small, can make sense drop interaction term re-fit additive model estimate marginal effects order present simplified picture going , recognition estimates smoothing heterogenity conditional (simple) effects truly exist.Aided abetted statistics textbooks biologists, long history researchers dropping interaction effect interaction \\(p>0.05\\). good rule thumb , don’t make model decisions based p-values. doesn’t make sense.\\(p\\)-value arbitrary dichotomization continuous variable. make sense behave differently interaction \\(p=0.06\\) vs. \\(p=0.04\\), given two p-values effectively identical?\\(p\\)-value evidence effect zero, “doesn’t exist”, even effect “trivially small”. \\(p\\)-values function measurement error, sampling error, sample size, addition effect size.interaction p-value lesion-area data 0.078. refit additive model report simpler story “” chromosome effect “” sex effect? reduced model isn’t invalid useful. considerations.certainly real interaction two factors interaction reflects interesting biology.example, might report – additive effect main paper (since big chromosome complement effect story) conditional effects supplement, might work investigating underlying biology. maybe two sets p-values plot? lots unexplored ways provide “ways” looking results.Regardless, example, avoid reporting interaction effect conditional effects somewhere. estimated interaction effect (0.14 mm\\(^2\\)) moderately large relative four simple effects. ’s much bigger marginal effect sex (Table 15.8) 2/3 size marginal effect chromosome (Table 15.8).response plot models (Figure 15.8) can help understanding decision model report.\nFigure 15.8: . Conditional means p-values conditional effects. B. Marginal means p-values marginal effects.\n","code":""},{"path":"factorial.html","id":"the-marginal-means-of-an-additive-linear-model-with-two-factors-can-be-weird","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.5.6 The marginal means of an additive linear model with two factors can be weird","text":"better understand marginal effects computed additive model, let’s compare emmeans table factorial additive models.\n(#tab:twoway-exp3e_m1_emm)Conditional means lesian area data computed factorial model.\n\n(#tab:twoway-exp3e_m2_emm)Marginal means lesian area data computed additive model.\nExplainerThe means conditional means table (Table @ref(tab:twoway-exp3e_m1_emm)) equal sample means. conditional \\(\\texttt{sex}\\) \\(\\texttt{chromosome}\\).means marginal means table (Table @ref(tab:twoway-exp3e_m2_emm)) equal sample means. modeled means four groups model interaction effect, conditional effects one factor level factor. take difference Male - Female XX XY groups, . data measured interaction (even true interaction. remember, interaction ubiquitous biology). larger interaction, weird marginal means less compatible data.","code":""},{"path":"factorial.html","id":"example-3-estimation-of-synergy-more-than-the-sum-of-the-parts-experiment-1c-ja-data","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6 Example 3: Estimation of synergy (“More than the sum of the parts”) (Experiment 1c JA data)","text":"explain synergy estimated interaction effect, example uses data experiment measuring effect two defense signals defense response Maize plants. response herbivory insects, maize, plants, release multiple, chemical signals air (chemicals evaporate air known volatile compounds). chemicals signal plant, neighboring plants, secrete anti-herbivory hormones, including abcisic acid jasmonic acid. researchers investigated effects two volatile compounds, (Z)‐3‐hexenyl acetate (HAC) Indole, defense response without combination.example data come Figure 1c, effect HAC Indole tissue concentrations hormone jasmonic acid (JA). design fully crossed two factors, two levels: \\(\\texttt{hac}\\), levels “HAC-” “HAC+”, \\(\\texttt{indole}\\), levels (“Indole-” “Indole+”).","code":""},{"path":"factorial.html","id":"examine-the-data","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.1 Examine the data","text":"points box plot. control variance small. obvious implausible points. fit lm recognize small n warning inference.","code":"\nqplot(x = treatment, y = ja, data = exp1)"},{"path":"factorial.html","id":"fit-the-model-7","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.2 Fit the model","text":"","code":"\nexp1c_m1 <- lm(ja ~ hac * indole, data = exp1)"},{"path":"factorial.html","id":"model-check","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.3 Model check","text":"distribution looks like sample Normal. variance looks like increases mean. suggest gls modeling heterogeneity.","code":"\nggcheck_the_model(exp1c_m1)"},{"path":"factorial.html","id":"inference-from-the-model-3","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.4 Inference from the model","text":"","code":"\nexp1c_m1_coef <- tidy(exp1c_m1, conf.int = TRUE)\n\nexp1c_m1_coef %>%\n  kable(digits = 3) %>%\n  kable_styling()\nexp1c_m1_emm <- emmeans(exp1c_m1, specs = c(\"hac\", \"indole\"))\n\nexp1c_m1_emm %>%\n  kable(digits = c(1,1,1,2,1,1,1)) %>%\n  kable_styling()\n# exp1c_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nref <- c(1,0,0,0)\nhac <- c(0,1,0,0)\nindole <- c(0,0,1,0)\nhac_indole <- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# these planned contrasts are described above\n# 1. (hac+/indole- - hac-/indole-) # add hac\n# 2. (hac-/indole+ -  hac-/indole-) # add indole\n# 3. (hac+/indole+ - hac-/indole+) - (hac+/indole- - hac-/indole-) # Interaction\n\nexp1c_m1_planned <- contrast(\n  exp1c_m1_emm,\n  method = list(\n    \"hac+\" = c(hac - ref),\n    \"indole+\" = c(indole - ref),\n    \"HAC/Indole Ixn\" = c(hac_indole - indole) -\n      (hac - ref)\n  ),\n  adjust = \"none\"\n) %>%\n  summary(infer = TRUE)\n\nexp1c_m1_planned %>%\n  kable(digits = c(1,2,3,1,2,2,2,5)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"plot-the-model-5","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.5 Plot the model","text":"","code":""},{"path":"factorial.html","id":"alternative-plot","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.6.6 Alternative plot","text":"\nFigure 15.9: alternative plot showing estimate synergy. Gray, dashed line expected mean HAC + Indole group interaction zero.\n","code":""},{"path":"factorial.html","id":"understanding-the-linear-model-with-crossed-factors-3","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.7 Understanding the linear model with crossed factors 3","text":"","code":""},{"path":"factorial.html","id":"thinking-about-the-coefficients-of-the-linear-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.7.1 Thinking about the coefficients of the linear model","text":"\\[\n\\begin{equation}\n\\texttt{ja} = \\beta_0 + \\beta_1 (\\texttt{hac}_\\texttt{HAC+}) + \\beta_2 (\\texttt{indole}_\\texttt{Indole+}) + \\beta_3 (\\texttt{hac}_\\texttt{HAC+}:\\texttt{indole}_\\texttt{Indole+}) +\\varepsilon\n\\tag{15.2}\n\\end{equation}\n\\]linear model makes easy understand synergy \\(2 \\times 2\\) design. start mean reference (group without added HAC Indole), \\(\\beta_1\\) extra bit due adding HAC, \\(\\beta_2\\) extra bit due adding Indole, \\(\\beta_3\\) extra bit due synergy HAC Indole. positive \\(\\beta_3\\) means combined treatment effect sum parts.\nTable 15.9: Coefficient table factorial model\n, interaction non-additive effect. Adding HAC alone increases JA concentration 15.6 ng per g FW. Adding Indole alone increases JA concentration 13.1 ng per g FW. effects purely additive, adding HAC Indole Control mean result mean 43.9 + 15.6 + 13.1 = 72.6 ng per g FW HAC+Indole group. modeled mean 102.4 ng per g FW. difference (observed - additive) 102.4 - 72.6 = 29.8 ng per g FW. estimated interaction effect coefficient table.\nFigure 15.10: Synergy bit needed get HAC + Indole mean adding HAC effect Indole effect Control mean\n","code":""},{"path":"factorial.html","id":"issues-in-inference","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.8 Issues in inference","text":"","code":""},{"path":"factorial.html","id":"for-pairwise-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.8.1 For pairwise contrasts, it doesn’t matter if you fit a factorial or a flattened linear model","text":"Compare pairwise comparisons Experiment 2j glucose uptake data using factorial linear model flattened linear model.Factorial:Flattened:","code":"\nm1 <- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\n\nm1_emm <- emmeans(m1, specs = c(\"stimulation\", \"genotype\"))\n\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\") %>%\n  summary(infer = TRUE)\n\nm1_pairs %>%\n  kable(digits = c(3)) %>%\n  kable_styling()\nm2 <- lm(glucose_uptake ~ treatment, data = exp2j)\n\nm2_emm <- emmeans(m2, specs = c(\"treatment\"))\n\nm2_pairs <- contrast(m2_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\") %>%\n  summary(infer = TRUE)\n\nm2_pairs %>%\n  kable(digits = c(3)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"for-interaction-contrasts-it-doesnt-matter-if-you-fit-a-factorial-or-a-flattened-linear-model","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.8.2 For interaction contrasts, it doesn’t matter if you fit a factorial or a flattened linear model","text":"Factorial model:Flattened model:","code":"\n# using m1 from above\nm1_ixn <- contrast(m1_emm,\n                     interaction = \"revpairwise\") %>%\n  summary(infer = TRUE)\n\nm1_ixn %>%\n  kable(digits = c(3)) %>%\n  kable_styling()\n# need to compute the interaction as a contrast\n# using m2 from the previous chunk\n\n# m2_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_rest <- c(1,0,0,0)\nwt_active <- c(0,1,0,0)\nko_rest <- c(0,0,1,0)\nko_active <- c(0,0,0,1)\n\n# contrasts are the difference in the vectors created above\n# 4. (KO/Active - WT/Active) - (KO/Rest - WT/Rest) -- interaction \n\nm2_ixn <- contrast(m2_emm,\n                   method = list(\n                     \"KO:Active Ixn\" = c(ko_active - wt_active) -\n                       (ko_rest - wt_rest)\n                   )) %>%\n  summary(infer = TRUE)\n\nm2_ixn %>%\n  kable(digits = c(3)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"twoway-multiple-tests","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.8.3 Adjusting p-values for multiple tests","text":"Inflated Type error pairwise tests concern among statisticians many reviewers, readers, colleagues. frequency individual tests inflated batch pairwise tests “family-wise” error rate – given either six pairwise tests four simple-effect tests \\(2 \\times 2\\) design, frequency least one Type error within family tests? six pairwise tests \\(2 \\times 2\\) design, default adjustment Tukey HSD (Honestly Significant Difference) method. limit comparisons four simple effects, need adjustment, want adjust. six different adjustment methods (including adjustment) simple effects.\nTable 15.10: P-values seven different methods adjustment. Tukey-p-value six pairwise comparisons. p-values four simple effects.\nTable 15.10 shows p-values adjusted using seven different methods (including adjustment) Experiment 2j glucose uptake data.Tukey correction applied six pairwise tests code .Tukey – Tukey HSD adjustment corrects expected correlation among tests.corrections applied four simple effects.Bonferroni – Bonferroni adjustment assumes independence among tests. isn’t true posthoc tests every contrast includes group tests (, different contrasts common data). consequence correlation among tests Bonferroni adjustment conservative (actual rate Type error smaller nominal rate). Even independent test, Bonferroni method less powerful methods .Sidak – Sidak correction increased power relative Bonferroni explicitly account correlated tests. ’s type error rate conservative.Holm – Holm variant Bonferroni adjustment increased power relative Bonferroni explicitly account correlated tests. ’s type error rate conservative.Mvt – multivariate t adjustment corrects empirically measured correlation.FDR – FDR adjustment attempt correct Type error False Discovery Rate. difference concepts important.thoughts multiple adjustment, focussing highlighted row table.transparent best practice report , unadjusted p-values. Reporting unadjusted p-values allows reader compute whatever adjustment deem appropriate question addressing. Reporting p-values overwhelm principal message figure. Instead, report focal (planned comparison) p-values figure table p-values supplement. table every experiment supplement. point raises question “” means. designs, “” mean treatment vs. control comparisons. others, “” mean pairwise comparisons. others, “” mean pairwise comparisons interactions.Fisherian world-view p-values, just isn’t real difference inference \\(p = 0.02\\) (adjustment) \\(p = 0.086\\) (Tukey adjustment pairwise comparisons). Fisher expected decisions based multiple replications experiment, (nearly) \\(p < 0.05\\). Fisherian-world-view, , bigger ---science issues multiple testing making decisions entirely \\(p = 0.02\\) versus \\(p = 0.086\\).Neyman-Pearson testing world-view, important family tests. Using six pairwise comparisons four simple effects family software assigned rather mindless statistics.","code":""},{"path":"factorial.html","id":"two-way-anova","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.9 Two-way ANOVA","text":"wet-bench ecology/evolution experimental biologists ANOVA. Many textbooks, advisors, colleagues tell researchers report “tests ANOVA” – call pairwise contrasts – modern computing, pairwise contrasts ANOVA tables computed underlying regression model. ANOVA necessary inference, point return .Let’s use glucose uptake experiment (Example 1) explore ANOVA table.","code":""},{"path":"factorial.html","id":"how-to-read-a-two-way-anova-table","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.9.1 How to read a two-way ANOVA table","text":"\nTable 15.11: ANOVA table glucose uptake experiment (Example 1).\nExplainerThis ANOVA table computed using Type III sum squares match output statistics software including Graphpad Prism. relevance addressed .general, ANOVA table row term underlying linear model. ANOVA method decomposing total variance batches terms batch. linear model two crossed factors, term factor term interaction. \\(\\texttt{stimulation}\\) \\(\\texttt{genotype}\\) main (1st order) effects. \\(\\texttt{stimulation:genotype}\\) interaction (2nd order) effect. terms row names table.ANOVA functions R also include row variance residuals fit model. row can useful learning values ANOVA table one statistic row (residual df) especially useful reporting. Table 15.11, residual df included statistics term.Many ANOVA tables contain additional SS (sum squares) MSE (mean square error) columns. MSE variance term used compute F statistic. SS used compute MSE. Since columns used computation reporting, Table 15.11 excludes .df (Degree freedom) – term factor, df equal number levels (\\(k\\)) factor minus 1. Think way: contribution variance due factor function variability \\(k\\) level means around grand mean. many degrees independent variation level means , given know grand mean? answer \\(k-1\\) – values \\(k-1\\) level means written , \\(k\\)th level mean freedom vary; value \\(k\\bar{\\bar{Y}} - \\sum_i^{k-1}{Y_i}\\). interaction term, df product df factors interaction.F (F-value F-ratio) – test statistic. ratio MSE term divided MSE residual (strictly true “Fixed effects” ANOVA, ).p.value – p-value test statistic. F compared F-distribution, distribution F-values null.","code":""},{"path":"factorial.html","id":"what-do-the-main-effects-in-an-anova-table-mean","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.9.2 What do the main effects in an ANOVA table mean?","text":"common literature see researchers report rows ANOVA table stating something like “found effect stimulation glucose uptake (\\(F_{1,25} = 26.9\\), \\(p < 0.0001\\))”. Thinking main effect term ANOVA table “effect” can misleading. “effect” mean? Afterall, two effects \\(\\texttt{stimulation}\\) experiment, one wildtype mice one TLR9-/- mice.Main effects ANOVA tables average effects. main effect two-way ANOVA table “overall” marginal effect. Recall marginal effect level Factor average conditional effects level B (Section 15.5.3). \\(2 \\times 2\\) table, one marginal effect factor , consquence, p value main effect term \\(\\texttt{stimulation}\\) ANOVA table (Table 15.12) equal p-value marginal effect \\(\\texttt{stimulation}\\) contrast table (Table 15.13). “main term” effect stimulation illustrated Figure ??.\nTable 15.12: Reprinting ANOVA table glucose uptake experiment (Example 1) show decimal places p-value.\n\nTable 15.13: Marginal effect stimulation glucose uptake.\n, main effects ANOVA tables average effects. average effect want report? answer , depends. depend p-value interaction (answer many textbooks websites) although p-value , perhaps, irrelevant. Rather, depends research question motivating experiment. research question Experiment 2j specifically predicted different treatment (KO/Active - KO/Rest) control effect (WT/Active - WT/Rest). average two effects isn’t interest. inference average effect stimulation (p-value ANOVA table marginal effect size CIs) doesn’t answer question motivating experiment. general discussion might interested main effect term ANOVA table (better, marginal effects) section xxx section xxx.\nFigure 15.11: main effect stimulation ANOVA table tests average conditional effects stimulation. average conditional effects difference marginal means Active Rest.\n","code":""},{"path":"factorial.html","id":"more-issues-in-inference","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.10 More issues in inference","text":"","code":""},{"path":"factorial.html","id":"longitudinal-experiments-include-time-as-a-random-factor-better-than-repeated-measures-anova","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.10.1 Longitudinal experiments – include Time as a random factor (better than repeated measures ANOVA)","text":"","code":""},{"path":"factorial.html","id":"working-in-r-4","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11 Working in R","text":"","code":""},{"path":"factorial.html","id":"model-formula","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.1 Model formula","text":"linear model two crossed factors specified model formula y ~ *B \\(\\texttt{}\\) first factor, \\(\\texttt{B}\\) second factor. R expands formula y ~ 1 + + B + :B colon indicates interaction (multiplicative) effect.order factors model formula doesn’t matter values coefficients, estimated marginal means, contrasts. can matter ANOVA () “tests ANOVA”.additive model specified formula y ~ + B","code":"\nm1 <- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\nm1_b <- lm(glucose_uptake ~ genotype * stimulation, data = exp2j)\nm2 <- lm(glucose_uptake ~ stimulation + genotype, data = exp2j)"},{"path":"factorial.html","id":"using-the-emmeans-function-2","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.2 Using the emmeans function","text":"","code":""},{"path":"factorial.html","id":"conditional-means-table","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.2.1 Conditional means table","text":"NotesPrinting emmeans object displays useful information. , information includes confidence level used. object printed using kable() %>% kable_styling() (“Inference” “Understanding” sections ), table printed additional information lost.emmeans computes modeled means combinations levels factor variables specified specs.two factor variables model, passed specs, modeled means combinations levels two variables computed. one factor variable passed, marginal means (averaged levels missing factor) computed (see ).","code":"\n# model m1 fit above\n# m1 <- lm(glucose_uptake ~ stimulation * genotype, data = exp2j)\n\nm1_emm <- emmeans(m1, specs = c(\"stimulation\", \"genotype\"))\n\nm1_emm##  stimulation genotype emmean    SE df lower.CL upper.CL\n##  Rest        WT         6.75 0.419 25     5.89     7.61\n##  Active      WT        10.20 0.419 25     9.34    11.06\n##  Rest        KO         7.53 0.483 25     6.53     8.53\n##  Active      KO         8.67 0.447 25     7.75     9.59\n## \n## Confidence level used: 0.95"},{"path":"factorial.html","id":"marginal-means","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.2.2 Marginal means","text":"NotesIn model two crossed factors, y ~ * B, marginal means levels \\(\\texttt{}\\), averaged levels \\(\\texttt{B}\\) computed setting specs = \\(\\texttt{}\\) .Remember specs argument sets values predictors want mean. excluding \\(\\texttt{B}\\), don’t get means levels \\(\\texttt{}\\) level \\(\\texttt{B}\\) averaged across levels \\(\\texttt{B}\\).emmeans “knows” average across levels \\(\\texttt{B}\\) \\(\\texttt{B}\\) model.","code":"\nm1_emm_stimulation <- emmeans(m1, specs = c(\"stimulation\"))## NOTE: Results may be misleading due to involvement in interactions\nm1_emm_stimulation##  stimulation emmean    SE df lower.CL upper.CL\n##  Rest          7.14 0.320 25     6.48      7.8\n##  Active        9.43 0.306 25     8.80     10.1\n## \n## Results are averaged over the levels of: genotype \n## Confidence level used: 0.95"},{"path":"factorial.html","id":"contrasts","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3 Contrasts","text":"","code":""},{"path":"factorial.html","id":"twoway-working-planned","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.1 Planned contrasts","text":"","code":"\nwt_rest <- c(1,0,0,0)\nwt_active <- c(0,1,0,0)\nko_rest <- c(0,0,1,0)\nko_active <- c(0,0,0,1)\n\nm1_planned <- contrast(\n  m1_emm,\n  method = list(\n    \"(WT/Active - WT/Rest)\" = c(wt_active - wt_rest),\n    \"(KO/Rest - WT/Rest)\" = c(ko_rest - wt_rest),\n    \"(KO/Active - WT/Active)\" = c(ko_active - wt_active),\n    \"KO:Active Ixn\" = c(ko_active - wt_active) -\n      (ko_rest - wt_rest)\n  ),\n  adjust = \"none\"\n) %>%\n  summary(infer = TRUE)"},{"path":"factorial.html","id":"all-pairwise-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.2 All pairwise effects","text":"NotesNote printing contrast object displays useful information, including method adjustment multiple tests. object printed using kable() %>% kable_styling() (“Inference” “Understanding” sections ), table printed additional information lost.method argument used control set contrasts computed. See .adjust argument controls adjust multiple tests. method default adjustment method. See .level argument controls percentile boundaries confidence interval. default 0.95. Including argument value makes level transparent.","code":"\nm1_pairs <- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     adjust = \"tukey\",\n                     level = 0.95) %>%\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs##  contrast              estimate    SE df lower.CL upper.CL t.ratio p.value\n##  Active WT - Rest WT       3.45 0.592 25    1.820    5.076   5.826  <.0001\n##  Rest KO - Rest WT         0.78 0.639 25   -0.978    2.539   1.220  0.6202\n##  Rest KO - Active WT      -2.67 0.639 25   -4.427   -0.910  -4.173  0.0017\n##  Active KO - Rest WT       1.92 0.613 25    0.236    3.606   3.136  0.0212\n##  Active KO - Active WT    -1.53 0.613 25   -3.212    0.158  -2.493  0.0857\n##  Active KO - Rest KO       1.14 0.659 25   -0.671    2.953   1.733  0.3287\n## \n## Confidence level used: 0.95 \n## Conf-level adjustment: tukey method for comparing a family of 4 estimates \n## P value adjustment: tukey method for comparing a family of 4 estimates"},{"path":"factorial.html","id":"simple-effects","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.3 Simple effects","text":"","code":"\nm1_simple <- contrast(m1_emm,\n                     method = \"revpairwise\",\n                     simple = \"each\",\n                     combine = TRUE,\n                     adjust = \"fdr\") %>%\n  summary(infer = TRUE) # add the 95% CIs"},{"path":"factorial.html","id":"twoway-working-ixn","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.4 Interaction contrasts","text":"interaction contrasts can computed Planned contrasts using argument “interaction =”","code":"\nm1_ixn <- contrast(m1_emm,\n                     interaction = \"revpairwise\",\n                     adjust = \"none\") %>%\n  summary(infer = TRUE) # add the 95% CIs"},{"path":"factorial.html","id":"method-argument-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.5 Method argument","text":"Notesmethod = \"pairwise\" method = \"revpairwise\" compute pairwise comparisons. prefer “revpairwise” contrasts include reference direction non-reference minus reference.method = \"trt.vs.ctrl\" gives flattened picture model results constrains can infer results.","code":"\nm1_pairs <- contrast(m1_emm,\n                     method = \"pairwise\",\n                     adjust = \"fdr\") %>%\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs %>%\n  kable() %>%\n  kable_styling()\nm1_pairs <- contrast(m1_emm,\n                     method = \"trt.vs.ctrl\",\n                     adjust = \"none\") %>%\n  summary(infer = TRUE) # add the 95% CIs\n\nm1_pairs %>%\n  kable() %>%\n  kable_styling()"},{"path":"factorial.html","id":"adjustment-for-multiple-tests-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.3.6 Adjustment for multiple tests","text":"set adjust = argument “none” – adjustment“dunnettx” – Dunnett’s test method used comparing treatments single control. factorial design, makes sense flattened analysis.“tukey” – Tukey’s HSD method method used compare pairwise comparisons.“bonferroni” – Bonferroni general purpose method compare set multiple tests. test conservative. better method “holm”“holm” – Holm-Bonferroni general purpose method like Bonferroni powerful.“fdr” – controls false discovery rate Type error rate family tests. One might use exploratory experiment.“mvt” – based multivariate t distribution using covariance structure variables.","code":""},{"path":"factorial.html","id":"practice-safe-anova","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.4 Practice safe ANOVA","text":"Many researchers unaware different ways computing sum squares ANOVA table. F-value p-value functions sum squares. cells two-way ANOVA balanced (different sample sizes among cells), different ways computing sum squares can matter. great deal sound fury methods used avoided. many research questions experiments used address , arguments moot ANOVA table simply unnecessary inference. Instead, fit linear model compute contrasts interest.reviewer/advisor/boss wants ANOVA table wants table match Graphpad Prism JMP, need table computed Type III sum squares. using linear model compute ANOVA (remember ANOVA developed without fitting linear models), linear model needs fit using model matrix constructed indicator variables using effects coding. two safe ways R.","code":""},{"path":"factorial.html","id":"the-afex-aov_4-function-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.4.1 The afex aov_4 function","text":"NotesThe afex package three function names generating ANOVA table statistics – ’m using aov_4 functions uses linear model formula argument (specifically, used lme4 package), consistent rest text.formula includes addition random factor ((1|id)) even though really random factor model. See Models random factors – Blocking pseudoreplication information random factors. random factor (factor variable “id” created line fit model line) identifies individual mouse response variable measured. response measured individual mouse, “id” really random factor addition model formula necessary aov_4 function work.","code":"\n# .I is a data.table function that returns the row number\nexp2j[, fake_id := paste(\"mouse\", .I)]\n\nm1_aov4 <- aov_4(glucose_uptake ~ stimulation * genotype +\n                   (1|fake_id),\n                 data = exp2j)\n\nnice(m1_aov4, MSE = FALSE)[,-4] %>% # delete ges column\n  kable() %>%\n  kable_styling()"},{"path":"factorial.html","id":"twoway-car-anova","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.4.2 The car Anova function","text":"Notescar::Anova arguments reporting Type II Type III sum squares.Background: default model matrix lm function uses dummy (treatment) coding. Type 3 SS ANOVA (kind matches Graphpad Prism JMP), need tell lm use sum (deviation) coding.best practice method changing contrasts model matrix using contrasts argument within lm function, code fit m1_type3. safest practice sets contrasts specific fit.coefficients m1_type3 different using default contrasts. Except computing Type III ANOVA, text uses default contrasts throughout coefficients make sense kinds experiments experimental biology. using sum (“type III”) coding, intercept grand mean coefficients non-reference levels (effects) deviations grand mean. don’t find definition “effects” useful experiments biology (“useful” largely function “used ”).contrasts (differences means among pairs groups) contrast table , regardless contrast coding.Danger!. Many online sites suggest bit code Type III ANOVA using car::Anova:options(contrasts = c(\"contr.sum\", \"contr.poly\")’re reading book, almost certainly don’t want code resets R computes coefficients linear models SS ANOVA tables. effect future analysis contrasts set something else new R session started.","code":"\ntype3 <- list(stimulation = contr.sum,\n              genotype = contr.sum)\nm1_type3 <- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j,\n               contrasts = type3)\nAnova(m1_type3, type=\"3\")[-1, -1] %>% # delete row 1 and col 1\n  kable(digits = c(1,2,5)) %>%\n  kable_styling()"},{"path":"factorial.html","id":"better-to-avoid-these","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.11.5 Better to avoid these","text":"NotesNote differences afex table car Anova table. p-values interaction term – always case highest order interaction term model. p-values \\(\\texttt{stimulation}\\) \\(\\texttt{genotype}\\) terms different. difference function degree imbalance. , difference doesn’t make difference inference. data sets, difference catastrophic.Many introduction statistics textbooks websites teach base R aov function. function useful 1) data balanced 2) care ANOVA. Don’t use . need ANOVA table use either afex car Anova.base R anova useful know . Otherwise, use either afex car Anova.","code":"\nm1_aov <- aov(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)\nsummary(m1_aov)##                      Df Sum Sq Mean Sq F value   Pr(>F)    \n## stimulation           1  41.75   41.75  29.796 1.14e-05 ***\n## genotype              1   1.28    1.28   0.913   0.3485    \n## stimulation:genotype  1   9.51    9.51   6.789   0.0152 *  \n## Residuals            25  35.03    1.40                     \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# same as exp2j_m1 in the Example 1 section\nm1 <- lm(glucose_uptake ~ stimulation * genotype,\n               data = exp2j)\nanova(m1)## Analysis of Variance Table\n## \n## Response: glucose_uptake\n##                      Df Sum Sq Mean Sq F value    Pr(>F)    \n## stimulation           1 41.755  41.755 29.7959 1.143e-05 ***\n## genotype              1  1.279   1.279  0.9128   0.34853    \n## stimulation:genotype  1  9.514   9.514  6.7890   0.01523 *  \n## Residuals            25 35.034   1.401                      \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"factorial.html","id":"hidden-code-6","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.12 Hidden Code","text":"","code":""},{"path":"factorial.html","id":"import-exp2j-example-1","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.12.1 Import exp2j (Example 1)","text":"","code":"\ndata_from <- \"TLR9 and beclin 1 crosstalk regulates muscle AMPK activation in exercise\"\nfile_name <- \"41586_2020_1992_MOESM4_ESM.xlsx\"\n\nfile_path <- here(data_folder, data_from, file_name)\n\ntreatment_levels  <- c(\"WT Rest\",\n                       \"WT Active\",\n                       \"KO Rest\",\n                       \"KO Active\")\nexp2j_wide <- read_excel(file_path,\n                         sheet = \"2j\",\n                         range = \"A5:D13\",\n                         col_names = TRUE) %>%\n  data.table()\n\ncolnames(exp2j_wide) <- treatment_levels\n\nexp2j <- melt(exp2j_wide,\n              measure.vars = treatment_levels,\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\") %>%\n  na.omit() # danger!\n\nexp2j[, c(\"genotype\", \"stimulation\") := tstrsplit(treatment,\n                                                  \" \",\n                                                  fixed = TRUE)]\n\ngenotype_levels <- c(\"WT\", \"KO\")\nstimulation_levels <- c(\"Rest\", \"Active\")\nexp2j[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nexp2j[, stimulation := factor(stimulation,\n                              levels = stimulation_levels)]\n# View(exp2j)"},{"path":"factorial.html","id":"import-exp3e-lesian-area-data-example-2","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.12.2 Import exp3e lesian area data (Example 2)","text":"","code":"\ndata_from <- \"XX sex chromosome complement promotes atherosclerosis in mice\"\nfile_name <- \"41467_2019_10462_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\n# fig 3e\nexp3e_wide <- read_excel(file_path,\n                  sheet = \"Figure 3E\",\n                  range = \"A3:K4\",\n                  col_names = FALSE) %>%\n  data.table() %>%\n  transpose(make.names = 1)\n\nsex_levels <- colnames(exp3e_wide)\nexp3e <- melt(exp3e_wide,\n              measure.vars = sex_levels,\n              variable.name = \"sex\",\n              value.name = \"lesian_area\")\n\n# convert lesian_area to mm^2 from µm^2\nexp3e[, lesian_area := lesian_area/10^6]\n\n# convert sex variable to factor\nexp3e[, sex := factor(sex,\n                        levels = sex_levels)]\n\n# create chromosome column and convert to factor\nchromosome_levels <- c(\"XX\", \"XY\")\nexp3e[, chromosome := rep(rep(chromosome_levels, each = 5), 2)]\nexp3e[, chromosome := factor(chromosome,\n                             levels = chromosome_levels)]\n\n# researchers treatment levels\nexp3e[, treatment := rep(c(\"FXX\", \"FXY\", \"MXX\", \"MXY\"), each = 5)]\n\n# two rows with missing response so delte these rows as there is \n# no useful information in them\nexp3e <- na.omit(exp3e) # be careful with a global na.omit\n\n# View(exp3e) # highlight and run to see data"},{"path":"factorial.html","id":"import-exp1c-ja-data-example-3","chapter":"15 Linear models with two categorical \\(X\\) – Factorial linear models (“two-way ANOVA”)","heading":"15.12.3 Import Exp1c JA data (Example 3)","text":"","code":"\ndata_from <- \"Integration of two herbivore-induced plant volatiles results in synergistic effects on plant defense and resistance\"\nfile_name <- \"Data for Dryad.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp1 <- read_excel(file_path,\n                     sheet = \"Fig. 1\",\n                     range = \"A3:K23\", # 1 blank column\n                     col_names = TRUE) %>%\n  data.table() %>%\n  clean_names()\n\nsetnames(exp1, old = \"treat\", new = \"treatment\")\n\nexp1[treatment == \"Control\", c(\"hac\", \"indole\") := list(\"HAC-\", \"Indole-\")]\nexp1[treatment == \"HAC\", c(\"hac\", \"indole\") := list(\"HAC+\", \"Indole-\")]\nexp1[treatment == \"Indole\", c(\"hac\", \"indole\") := list(\"HAC-\", \"Indole+\")]\nexp1[treatment == \"HAC+Indole\", c(\"hac\", \"indole\") := list(\"HAC+\", \"Indole+\")]\n\ntreatment_levels <- c(\"Control\", \"HAC\", \"Indole\", \"HAC+Indole\")\nexp1[, treatment := factor(treatment,\n                           levels = treatment_levels)]\nexp1[, hac := factor(hac)] # levels in correct order\nexp1[, indole := factor(indole)] # levels in correct order\n# View(exp1)"},{"path":"part-vi-expanding-the-linear-model.html","id":"part-vi-expanding-the-linear-model","chapter":"Part VI – Expanding the Linear Model","heading":"Part VI – Expanding the Linear Model","text":"","code":""},{"path":"lmm.html","id":"lmm","chapter":"16 Models for non-independence – linear mixed models","heading":"16 Models for non-independence – linear mixed models","text":"Probably chapter book important best-practice analysis experimental data chapter. ? many experimental data violates assumption independence analysis using standard t-tests ANOVA always lead quantitative error inference (confidence intervals p-values) often lead qualitative errors inference (statements “significance”). Standard analysis non-independent data can lead absurdly liberal inference (p-values far lower data support), can also lead moderately conservative inference (p-values higher data supports). Liberal inference generates false discovery lures researchers dead-end research pathways. Conservative inference steers researchers away true discovery.Linear Mixed Models extension linear models appropriately adjust inferential statistics non-independent data. Paired t-tests Repeated Measures ANOVA classical tests special cases linear mixed models. Linear mixed models flexible classical tests models can include added covariates complex models generally. , linear mixed models can extended Generalized Linear Mixed Models counts, binary responses, skewed responses, ratios.introducing experimental designs generate non-independent data models used analyze , let’s explore two ubiquitous examples, one leads liberal inference one leads conservative inference.","code":""},{"path":"lmm.html","id":"liberal-inference-from-pseudoreplication","chapter":"16 Models for non-independence – linear mixed models","heading":"16.1 Liberal inference from pseudoreplication","text":"Researchers interested regulation repair DNA double-stranded breaks use proximity ligation assay (PLA) HeLa cells investigate number damage response events (“foci”) per cell without inhibitor transcription elongation (DRB). number foci fifty cells per treatment measured. experiment replicated three times. researchers use t-test compare effect DRB foci count naively include measures analysis.naive analysis? fifty measures per cell technical replicates values within cell independent share aspects cell environment shared values cells. Including technical replicates analysis without accounting non-independence kind pseudoreplicationTo show naive analysis results extremely liberal inference increase false discovery, simulate experiment using case effect DRB treatment, low p-value indicates false-discovery. simulation simplified two following conditions: 1) model pretends count data normally distributed (want focus pseudoreplication misspecified distribution) 2) model pretends values treatment within experiment independent (want focus pseudoreplication).naive analysis experiment, researcher finds effect treatment p-value 0.000073 uses small p-value justify decision move forward follow-experiments. low p-value supported data – discovery false. small p-value example “rare event”. fact, researcher repeats experiment 1000 times, median p-value 0.000195 72.3% 1000 p-values lead false discovery 0.05 used make decision move forward.example fake, see naive analysis lot: tumor area multiple tumors per mouse, islet area multiple islets per mouse, number vesicles docked membrane multiple cells per mouse, number mitochondria multiple cells per mouse, number neurites multiple neurons multiple cells per mouse, etc. etc. Indeed, ’m looking examples pseudoreplication teach, just look figures bunch points per treatment – something similar plot fake data experiment. Regardless, huge source false discovery disappear overnight.","code":""},{"path":"lmm.html","id":"conservative-inference-from-failure-to-identify-blocks","chapter":"16 Models for non-independence – linear mixed models","heading":"16.2 Conservative inference from failure to identify blocks","text":"Researchers frequently divide litter among treatment levels (“littermate control”) seen researchers use design increase statistical power decrease rate failed discovery. fake data example. Researchers interested sensory regulation wound healing use Nav1.8cre/Rosa26DTA mouse model investigate role dorsal root ganglion (location peripheral sensory neuron cell bodies) management. Two mice six litters sampled, one Nav1.8cre sib ablated dorsal root ganglion one Rosa26DTA “littermate control” intact dorsal root ganglion. researchers use naive t-test compare effect ablation muscle area (12 days wounding).naive analysis? mouse block values within block independent share aspects genetics maternal environment shared values blocks (mice). general, variation among blocks (mice) adds correlated noise data failure account blocks often lead conservative inference.show naive analysis results conservative inference increase failed discovery, simulate experiment using case effect ablation treatment, low p-value indicates true discovery.naive analysis experiment, p-value treatment 0.26 uses large p-value put grad student onto different project. bad, effect real. p-value paired t-test (values two mice within litter pair), equivalent special case linear mixed model, 0.0012. small p-value using model accounts non-independence example “rare event”. fact, researcher repeats experiment 1000 times, 72.5% paired t-test p-values less 0.05 12.1% classical t-test p-values less 0.05.example fake, see naive analysis lot – littermate controls , common examples , included replicated experiments (experiment block). certain instances researchers recognize non-independence use paired t-test vast majority blocked designs (occurring almost experimental biology papers) go unrecognized. huge source failed discovery disappear overnight.","code":""},{"path":"lmm.html","id":"introduction-to-models-for-non-independent-data-linear-mixed-models","chapter":"16 Models for non-independence – linear mixed models","heading":"16.3 Introduction to models for non-independent data (linear mixed models)","text":"chapter models correlated error, including linear models added random factors, known linear mixed models. classical hypothesis testing, paired t-test, repeated measures ANOVA, mixed-effect ANOVA equivalent specific cases linear mixed models. Linear mixed models used analyzing data composed subsets – batches – data measured “thing”, multiple measures within mouse, multiple mice within litter. Batched data results correlated error, violates key assumption linear models (“test” equivalents) muddles statistical inference unless correlated error modeled, explicitly implicitly. experimental designs (blocked designs), failure model correlated error reduces precision power, contributing reduced rates discovery confirmation. designs (nested designs), failure model correlated error results falsely high precision low p-values, leading increased rates false discovery. falsely high precision due pseudoreplication. think ’s fair infer experimental biology literature, experimental biologists don’t recognize ubiquitousness batched data correlated error. probably biggest issue inference field (far issue say, t-test non-normal data).mean “batch” can correlated error increase decrease false discovery? Consider experiment measure pancreatic islet area response two experimental factors: \\(\\texttt{genotype}\\) (WT, KO) \\(\\texttt{treatment}\\) (presence/absence drug believed agonist knocked protein). may seem like data experiment analyzed using ANOVA option GraphPad Prism (, advocated book, general linear model equivalent ANOVA), best practice statistical model actually depends experimental design. Experimental design matters different designs introduce different patterns correlated error due shared genetics environment. Recall inference linear model (including t-tests ANOVA) assumes independence (Chapter xxx) – , response value relationship value, due treatment. Lack independence results patterns correlation among residuals, correlated error.Something like first experiment (Design 1) necessary design use statistics covered book point, without extreme violation independence assumption. many (?) experiments experimental bench biology look like design Design 1 . Instead, many (?) experiments variants Designs 2-4, extreme violations independence assumption. Interestingly, violations result conservative statistics reduced, true discovery rate others result liberal statistics increased, false discovery rate.Design 1. design Figure @ref(fig:lmm-biological-replicates_1) factorial design two factors, \\(\\texttt{genotype}\\) \\(\\texttt{treatment}\\), two levels. Twenty mice sex, different litter unique dam sire mating, randomly sampled assigned one \\(genotype \\times treatment\\) combination (five mice per combination). mice housed individually (20 cages). pancreatic tissue mice prepared single batch area single islet measured mouse. entire experiment carried time component (tissue preparation, measuring) carried person (different people component). Completely Randomized Design (CRD). five replicate mice per treatment treatment replicates (often called biological replicates experimental biology. CRD batched data.Design 2. design Figure ??, four littermates randomly sampled five litters, different dam sire. Within litter, mice randomly assigned four treatment combinations (one per combination). litter randomly assigned cage single litter per cage. aspects design Design 1. Randomized Complete Block Design. five replicate mice per treatment treatment replicates. litter/cage combination type batch called block. blocked design typically functions reduce noise model fit (increases power) reduce number litters cages needed experiment. four measures Islet Area within litter/cage (one per mouse) independent . cage four mice litter mice share genetic maternal factors contribute mouse anatomy physiology shared mice litters. Additionally, cage unique set environmental factors contribute error variance measure response. cage shares cage-specific history temperature, humidity, food, light, interactions animal facilities staff, behavioral interactions among mice. response measures within litter/cage share component error variance unique litter/cage , consequence, error (residuals) within litter/cage similar residuals among litters/cages.Design 3. design Figure ?? exactly like Design 2, except researchers take three measures iselet area per mouse. three measures subsampled replicates. Experimental biologists often call technical replicates, especially multiple measures taken preparation. Subsampling kind nested design one variable nested within (opposed crossed ) another variable. , subsampled variable (subsample_id) nested within mouse_id variable. addition litter/cage batch, mouse batch. mouse unique set factors contribute error variance measures response mouse. response measures within mouse share component error variance unique mouse , consequence, error (residuals) within mouse similar residuals miceDesign 4. design Figure ?? variation Design 2, five treatment replicates combination housed together cage. design, litter batch cage batch different batches, unlike Design 2.experiments, systematic variation multiple levels: among treatments due treatment effects among batches due batch effects. Batches come lots flavors, including experiment, litter, cage, flask, plate, slide, donor, individual. among-treatment differences means fixed effects. among-batch differences random effect. assumption modeling random effects batches random sample batches sampled. often strictly true batches often convenience samples (example: human donors Type 2 diabetes beta cells hospital).variation among batches/lack independence within batches different consequences uncertainty estimate treatment effect. batches Experiment 1 contain treatment combinations. researcher interested treatment effect variation due differences among batches. batches nuissance factors add additional variance response, consequence estimates treatment effects less precise, unless variance due batches explicitly modeled. Modeling batch contains treatment combinations increase precision power.Batches contain least two treatment combinations known blocks. block contains treatment combinations complete block. block contains fewer combinations incomplete block. Including block structure design known blocking. Blocks non-experimental factors. Adding blocking factor statistical model used increase precision estimated treatment effect. Design 2 example randomized complete block design.Design 3, multiple measures per mouse design randomized complete block design subsampling. subsampling kind replication can used infer among treatment effect treatment assignment level subsamples. treatment replicates litters/cages, level treatment assignment randomized. statistical analysis measures subsampled design without modeling correlated error due subsampling kind pseudoreplication. Pseudoreplication results falsely precise standard errors false small p-values , consequently, increased rates false discovery.Design 4, treatment randomized batch, batch contains single treatment level. segregated experimental designs, variation among batches arises non-treatment related differences among batches confounds variation among batches due true treatment effect. Design 4 extreme example – single cage specific treatment combination. Imagine 1) true effect treatment combination zero 2) aggressive mouse control cage stimulates stress response mice stress response large effect value response variable measured researchers. researcher fooled thinking treatment caused difference response.designs, important researcher identify experimental unit measurement unit. experimental unit entity randomly assigned treatment. designs 1 – 3, experimental unit mouse. experiment 4, experimental unit cage. measurement unit entity measured. designs 1, 2, 4, measurement unit mouse. design 3, measurement unit specific islet measured.","code":""},{"path":"lmm.html","id":"experimental-designs-in-experimental-bench-biology","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4 Experimental designs in experimental bench biology","text":"Given basic principles , let’s consider kinds experimental designs seen experimental bench biology (Fig. ??).\nFigure 16.1: Experimental designs experimental bench biology. Images created BioRender.com\n\nFigure 16.2: Experimental designs experimental bench biology. Images created BioRender.com\n","code":""},{"path":"lmm.html","id":"notation-for-models","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.1 Notation for models","text":"= 1..t (treatments)j = 1..b (blocks)k = 1..r (experimental replications within block within CRD block structure)m = 1..s (subsamples technical replicates)","code":""},{"path":"lmm.html","id":"completely-randomized-design-crd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.2 Completely Randomized Design (CRD)","text":"Completely Randomized Design experiment Figure ??single factor, \\(\\texttt{treatment}\\) two levels (“Cn” “Tr”). Five mice randomly assigned treatment level. mouse bred different litter housed separate cage. researchers measure single value response variable mouse. five replicate mice per treatment treatment (biological) replicates. design completely randomized subgrouping due batches. kinds subgrouping design avoid?using single mouse per litter, litter batches subsets mice don’t share litter effects – common litter responses Cn Tr treatments. litter unique set factors contribute error variance measure response. Siblings dam sire share genetic variation non-siblings shared genetic variation contributes phenotypes (including response treatment) likely similar non-siblings. Siblings litter share history maternal factors (maternal effects, including epigenetic effects) specific pregnancy even history events leading pregnancy. shared non-genetic epigenetic variation contributes phenotypes (including response treatment) likely similar non-siblings. response measures within litter share genetic, maternal environmental, epigenetic components error variance unique litter , consequence, error (residuals) within litter similar residuals litters.housing mouse ’s cage, cage batch subsets mice don’t share cage effects – common cage responses Cn Tr treatments. stated earlier, cage unique set factors contribute error variance measure response. cage shares cage-specific history temperature, humidity, food, light, interactions animal facilities staff behavioral interactions among mice. response measures within cage share component error variance unique cage , consequence, error (residuals) within cage similar residuals cages.Examples:Ten mice separate litters sampled. Five mice randomly assigned control. Five mice randomly assigned treatment. single measure per mouse taken. Mouse experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), \\(s=1\\).Ten cell cultures created. Five cultures randomly assigned control five treatment. single measure per culture taken. Culture experimental unit. \\(t=2\\), \\(b=0\\), \\(r=5\\), \\(s=1\\).","code":""},{"path":"lmm.html","id":"completely-randomized-design-with-subsampling-crds-or-nested-design","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.3 Completely Randomized Design with Subsampling (CRDS) or “Nested Design”","text":"Completely Randomized Design Subsampling experiment Fig. ??B exactly like CRD except researchers measure multiple values response variable mouse condition (, different treatment time). multiple measures subsampled (technical) replicates.confuse subsampled replicates measures response different conditions mouse, example measure one brain slice control treatment measure second brain slice drug treatment. example kind Randomized Complete Block Design, outlined next core design chapter.confuse subsampled replicates measures response different times mouse, example, plasma glucose levels baseline five post-baseline time points. example kind longitudinal design, outlined thoroughly chapter Linear models longitudinal experiments.confuse subsampled replicates measures different response variables mouse, example measures weights five different skeletal muscles. example kind multiple response addressed xxx.technical replicates kind pseudoreplication. general linear model y ~ treatment fit data, including t-tests traditional ANOVA, falsely high precision falsely low p-values.Examples:Ten mice separate litters sampled. Five mice randomly assigned control. Five mice randomly assigned treatment. Multiple measures per mouse taken. Example: five measures Islet Area measured pancreas. Mouse experimental unit. measures technical replicate treatment randomly assigned islet whole mouse. \\(t=2\\), \\(b=0\\), \\(r=5\\), \\(s=5\\).Ten cell cultures created. Five cultures randomly assigned control five treatment. Multiple measures per culture taken. Example: Mitochondrial counts measured five cells culture. Culture experimental unit. counts technical replicate treatment randomly assigned cell whole culture. \\(t=2\\), \\(b=0\\), \\(r=5\\), \\(s=5\\).Notes:Subsampling can occur multiple levels. Example: Ten mice separate litters sampled. Five mice randomly assigned control. Five mice randomly assigned treatment. Five neurons slice brain tissue identified mouse. neuron, length five dendrite spines measured. five measures spine length “nested within” neuron five neurons “nested within” mouse. Nested subsampling can quickly lead massive pseudoreplication false discovery.","code":""},{"path":"lmm.html","id":"randomized-complete-block-design-rcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.4 Randomized Complete Block Design (RCBD)","text":"Randomized Complete Block Design experiment Figure ??C similar CRD except treatment combinations (two ) randomly assigned sibling mice within litter. , two mice litter randomly selected one randomly assigned “Cn” “Tr”. litter randomly assigned unique cage. researchers measure single value response variable mouse. five replicate mice per treatment treatment replicates. litters (cage) blocks. design, litter cage effects confounded consequence statistical model inference unless researchers want explicitly estimate effects separately. Compared CRD, design requires fewer resources (five litters instead ten, five cages instead ten). Compared general linear model fit data CRD (y ~ treatment), including t-tests traditional ANOVA, linear mixed model fit RCBD increased precision power. many researchers seem designing experiments similar (“littermate controls”), failing fit statistical model accounts batching taking advantage increased precision power.Examples:Ten mice sampled. mouse, one forelimb assigned control forelimb assigned treatment. single measure side taken. Limb experimental unit. Mouse block. \\(t=2\\), \\(b=10\\), \\(r=1\\), \\(s=1\\).Ten litters sampled. litter, one sib assigned control sib assigned treatment. single measure sib taken. Mouse experimental unit. Litter block. \\(t=2\\), \\(b=10\\), \\(r=1\\), \\(s=1\\).Two mice, separate litter sampled. One randomly assigned control treatment. single measure response variable taken per mouse. experiment replicated five times (five different days, newly made set reagents machine calibrations). Mouse experimental unit. Experiment block. \\(t=2\\), \\(b=5\\), \\(r=1\\), \\(s=1\\).","code":""},{"path":"lmm.html","id":"randomized-split-plot-design-rspd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.5 Randomized Split Plot Design (RSPD)","text":"Randomized Split Plot Design experiment Figure ??D similar RCBD except now second experimental factor crossed first experimental factor. individual mouse acts single experimental unit one factor (, \\(\\texttt{genotype}\\) levels “WT” “KO”) acts block second experimental factor (, \\(\\texttt{treatment}\\) levels “Cn” “Tr”). first factor (\\(\\texttt{genotype}\\)) main plot – levels factor randomly assigned main plots. second factor (\\(\\texttt{treatment}\\)) subplot – levels factor randomly assigned subplots. \\(\\texttt{Litter}\\) replicated block. Also Figure ??D, 2 x 2 RCBD two experimental factors shown comparison. 2 x 2 RCBD, four mice per block (litter) randomly assigned one 2 x 2 combinations \\(\\texttt{genotype}\\) \\(\\texttt{treatment}\\)).","code":""},{"path":"lmm.html","id":"generalized-randomized-complete-block-design-grcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.6 Generalized Randomized Complete Block Design (GRCBD)","text":"Generalized Randomized Complete Block Design experiment Figure ??E similar RCBD except two treatment replicates per block (litter/cage) assigned.Important somewhat intuitive treatment replicates within litter share common error variance, act like independent replicates. One consequence , sample size (\\(n\\)) five ten (\\(litters \\times treatment replicates\\)). general linear model y ~ treatment fit data, including t-tests traditional ANOVA, generally falsely high precision falsely low p-values.","code":""},{"path":"lmm.html","id":"nested-randomized-complete-block-design-nrcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.7 Nested Randomized Complete Block Design (NRCBD)","text":"Nested Randomized Complete Block Design experiment Figure ??F similar RCBD except now replicated experiments. nested block design litter (block) nested within experiment (block).","code":""},{"path":"lmm.html","id":"longitudinal-randomized-complete-block-design-lrcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.8 Longitudinal Randomized Complete Block Design (LRCBD)","text":"Longitudinal Randomized Complete Block Design experiment Figure ??G similar RCBD except multiple measures response variable taken, taken different time point, including baseline (time zero).","code":""},{"path":"lmm.html","id":"variations-due-to-multiple-measures-of-the-response-variable","chapter":"16 Models for non-independence – linear mixed models","heading":"16.4.9 Variations due to multiple measures of the response variable","text":"Similar CRDS , basic designs can include subsampling, resulting , example, RCBDS RSPDS. subsampling within subsampled units, can designated “SS”, example RCBDSS.Similar LRCBD , basic designs can include longitudinal sampling, resulting , example, LCRD LRSPD.","code":""},{"path":"lmm.html","id":"building-the-linear-mixed-model-for-clustered-data","chapter":"16 Models for non-independence – linear mixed models","heading":"16.5 Building the linear (mixed) model for clustered data","text":"Notationi = 1..t (treatments)j = 1..b (blocks)k = 1..r (experimental replications within block)m = 1..s (subsamples technical replicates)\\[y_i = \\beta_0 + \\beta_i treatment_i + (\\gamma_j block_j) + (\\gamma_{ij} block_j treatment_i) + \\varepsilon\\]","code":""},{"path":"lmm.html","id":"lmm-demo","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6 Example 1 – A random intercepts and slopes explainer (demo1)","text":"introduce linear mixed models, ’m using data Experiment 1g . design \\(2 \\times 2\\) factorial 4-5 mice per treatment combination. simplify explanation random intercepts random slopes linear models added random factors (linear mixed models), flatten analysis single treatment factor (\\(\\texttt{treatment}\\)) four levels (“Control”, “Tr1”, “Tr2”, “Tr3). response percent germinal centers (\\(\\texttt{gc}\\)) secondary lymphoid tissue. experiment replicated 4 times. replication batch. batch information variable \\(\\texttt{experiment_id}\\). detail isn’t necessary point.Figure 16.3A response plot linear model lm(gc ~ treatment) fit whole data set, ignoring fact data collected batches. complete pooling fit. Figure 16.3B response plot linear model lm(gc ~ treatment) fit means treatment combination experiment. means pooling fit.\nFigure 16.3: . Response plot linear model gc ~ treatment fit exp1g data. B. Response plot linear model gc ~ treatment fit experiment means exp1g data.\n","code":""},{"path":"lmm.html","id":"batched-measurements-result-in-clustered-residuals","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.1 Batched measurements result in clustered residuals","text":"Figure 16.4A plot residuals complete-pooling fit \\(\\texttt{experiment_id}\\). residuals clustered experiment. residuals experiment 1 positive. residuals experiment 2 negative. Residuals experiment 3 generally positive. Residuals experiment 4 seem pretty random. clustering experiment plot residuals means-pooling fit \\(\\texttt{experiment_id}\\) (Figure 16.4B). residuals independent either fit. asked guess sign residual gave information measure experiment 1, ’d correct 100% time. residuals independent, ’d correct, average, 50% time. Independent residuals randomly scattered zero within experiment (Figure 16.4C).\nFigure 16.4: . Residuals model fit demo1 data. B. Residuals model fit mean demo1 data.\n","code":""},{"path":"lmm.html","id":"lmm-corerr","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.2 Clustered residuals result in correlated error","text":"assumption inference linear model independence – response independently drawn distribution random values. Experiment 1g, experiments batches batched data results correlated error unless modeled. One way see correlated error use residuals means-pooled fit.aggregate data computing means treatment level within experiment.fit fixed effect model (model without added random factors) aggregated datacompute residuals modelcast (spread) residuals treatment column. creates 4 rows (experiment) \\(\\times\\) 4 columns (treatments) matrix residuals.Compute correlations among four treatment combination columns. correlated error due batch effect \\(\\texttt{experiment_id}\\).\nTable 16.1: Residuals fixed affect model fit aggregated data. residuals split treatment.\n’ve used GGally::ggpairs compute display correlations matrix. lower triangle matrix elements contains scatterplot residuals treatment combination defined row column headers. upper triangle elements contains Pearson correlation. four experiment residuals per treatment combination, large correlations common. correlations large, positive values. asterisks indicate values unexpected surprise null model correlation.explicitly model correlated error linear model correlated error using nlme::gls function, using model correlated error matches knowledge data generated (experiment batches). chapter, implicitly model correlated error using linear model added random factors – linear mixed model. explicitly model linear mixed model hierarchical levels variance.","code":""},{"path":"lmm.html","id":"in-blocked-designs-clustered-residuals-adds-a-variance-component-that-masks-treatment-effects","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.3 In blocked designs, clustered residuals adds a variance component that masks treatment effects","text":"variance among experiments within treatment much greater variance among treatment means. consequence , experiment effect masks effect treatment. can manually unmask bycompute experiment means across treatment combinations.create gc variable without variation among experiment means (“adjusted experiment_id”).\nFigure 16.5: Adjusting variance among experiments. black, dashed line grand-mean response. left panel, colored, dashed lines mean gc experiment, ignoring treatment. right panel, individual values shifted (adjusted) centering experiment means. effect reducing error variance – spread values around treatment means (large black dots).\nFigure 16.5, black dots modeled means treatment combination. small colored dots measured values response \\(\\texttt{experiment_id}\\) left panel experiment-adjusted values right panel. black, dashed line grand-mean response. colored, dashed lines means responses experiment. means equal right panel (covered black line) variation among means adjusted away. left error variation uncontaminated \\(\\texttt{experiment_id}\\).Experiment 1g, \\(\\texttt{experiment_id}\\) nuissance variable – adds noise. exercise , effects treatment variables adjusted elevation batch effects overal batch mean. Linear mixed models sophisticated . linear mixed model, effects treatment variables adjusted elevations batch effects intercept batch effects slopes (combination ). random intercepts random slopes.","code":""},{"path":"lmm.html","id":"linear-mixed-models-are-linear-models-with-added-random-factors","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.4 Linear mixed models are linear models with added random factors","text":"linear model adds combination random intercepts random slopes linear model.\\[\n\\begin{equation}\n\\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon\n\\tag{16.1}\n\\end{equation}\n\\]random intercept experiment j sum fixed intercept (\\(\\beta_0\\)) random intercept effect (\\(\\gamma_{0j}\\)). ’ve embedded within parentheses show combine random intercept. random slope batch j sum fixed slope (\\(\\beta_k\\)) non-reference level \\(k\\) random slope effect (\\(\\gamma_{kj}\\)). ’ve embedded within parentheses show combine random slopes.different \\(\\gamma_{0j}\\) experiment. different \\(\\gamma_{kj}\\) combination non-reference level experiment. \\(\\gamma_0j\\) \\(\\gamma_kj\\) modeled values experiment random draw infinite number experiments. \\(\\gamma_{0j}\\) \\(\\gamma_{kj}\\) random effects. contrast, \\(\\beta_0\\) three \\(\\beta_k\\) non-reference treatment levels experiments – \\(\\beta_0\\) known fixed effects (technically, \\(\\beta_0\\) mean effect).Model (16.1) fit Example 1 data using lme4::lmer()Notes(treatment | experiment_id) specifies random intercept levels \\(\\texttt{experiment_id}\\) \nrandom slope combinations levels \\(\\texttt{experiment_id}\\) non-reference levels \\(\\texttt{treatment}\\)","code":"\ndemo1_m1 <- lmer(gc ~ treatment +\n                   (treatment | experiment_id),\n                 data = demo1)"},{"path":"lmm.html","id":"what-the-random-effects-are","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.5 What the random effects are","text":"Random intercepts model batch effects reference treatment level. 16.6A illustrates random intercepts random intercept effects. large, colored dots modeled means experiment treatment combination. reference treatment level (“Control”), mean sum estimated fixed intercept (\\(b_0\\)), shown dashed gray line, estimated random intercept effect (\\(g_{0j}\\)) experiment j. random intercept effects vertical, colored lines.Random slopes model effect treatment batch effects non-reference treatment levels. 16.6B illustrates random slopes random slope effects, focusing slopes 2nd non-reference treatment level (“Tr2”). angled black line estimated fixed slope \\(b_2\\) level. colored lines random slopes experiment. pale, gray dots modeled means Tr2 level random slope effect – took large colored dots “Control” rigidly shifted black line “Tr2”. estimated random slope effects \\(\\mathrm{g}_{2j}\\) difference large, gray dots modeled means.\nFigure 16.6: random intercepts slopes . () random intercept batch \\(j\\) difference fixed intercept modeled mean batch \\(j\\) reference treatment level. random intercepts experiments 1 (\\(\\mathrm{g}_{0.1}\\)) 2 (\\(\\mathrm{g}_{0.2}\\)) shown brackets. (B) fixed slope “Tr2” illustrated bracket. large, grey dots expected values batch (experiment_id) “Tr2” treatment random slope effects zero. random slope batch \\(j\\) difference expected value batch j modeled mean batch \\(j\\). random slope experiment 2 “Tr2” (\\(\\mathrm{g}_{2.2}\\)) shown bracket.\n","code":""},{"path":"lmm.html","id":"in-a-blocked-design-a-linear-model-with-added-random-effects-increases-precision-of-treatment-effects","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.6 In a blocked design, a linear model with added random effects increases precision of treatment effects","text":"\\[\n\\begin{equation}\n\\texttt{gc}_{jk} = \\beta_{0} + \\beta_{k} \\texttt{treatment}_{k} +  (\\gamma_{0j} + \\gamma_{kj}\\texttt{treatment}_{k} + \\varepsilon)\n\\tag{16.2}\n\\end{equation}\n\\]random intercepts random slopes aren’t modeled, among-experiment variance shifted error variance intercept effects slope effects aren’t estimated absorbed error – everything Model (16.2) estimated residuals. consequence, estimate \\(\\sigma\\) (square root error variance) linear mixed model smaller linear model fixed effects.consequence smaller estimate \\(\\sigma\\) linear mixed model inference (confidence intervals p-values) depends number subsamples, variance random effects relative variance residual error, correlation among random effects.","code":"\n# sigma for the lmm\nm1 <- lmer(gc ~ treatment +\n                   (treatment | experiment_id),\n                 data = demo1)\nsummary(m1)$sigma## [1] 1.931339\n# sigma for the fixed lm\nm2 <- lm(gc ~ treatment,\n                 data = demo1)\nsummary(m2)$sigma## [1] 3.376402"},{"path":"lmm.html","id":"lmm-varcorr","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.7 The correlation among random intercepts and slopes","text":", linear mixed model fit experiment 1g data.\\[\n\\begin{equation}\n\\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon\n\\end{equation}\n\\]Think model generates data. four experiments, randomly draw four \\(\\gamma_{0j}\\) normal distribution variance \\(\\sigma_{0}^2\\). , non-reference treatment, randomly draw four \\(\\gamma_{0k}\\) normal distribution variance \\(\\sigma_{k}^2\\). gives us matrix four columns (one random intercept three random slopes) four rows (four experiments).randomly sample values, model needs variances (\\(\\sigma_k^2\\)) column (random effect) also correlation pair columns. correlations -diagonal elements correlation matrix random effects.models fit text, researcher doesn’t specify variances correlations. Instead, parameters estimated model. summary estimates variances random effects correlations among random effects linear mixed model fit experiment 1g data.first four values column “Std.Dev.” square roots estimated variances random effects given column “Name”. last value column “Std.Dev.” square roots estimate \\(\\sigma^2\\) (error variance). (lower) triangular matrix values “Corr” estimates correlations among random effects. variance random effects correlation among random effects creates correlated error described confuse different correlations (easy confuse, alone).compact way view variances correlations matrix random effect standard deviations diagonal correlations -diagonal. ’ll refer VarCorr matrix lme4 function used get values.\nTable 16.2: Varcorr matrix. Standard deviations random effects diagonal. Correlations random effects -diagonal.\n’s probably worth trying understand experimental reason underneath correlations among random effects. , researchers might want sleuth lab getting high random intercept slope variances, relative error variances, indicate potential sources improvement lab protocols.","code":"##  Groups        Name         Std.Dev. Corr                \n##  experiment_id (Intercept)  1.7710                       \n##                treatmentTr1 0.9642   -0.025              \n##                treatmentTr2 2.9026    0.458  0.876       \n##                treatmentTr3 2.9910    0.300  0.879  0.938\n##  Residual                   1.9313"},{"path":"lmm.html","id":"clustered-residuals-create-heterogeneity-among-treatments","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.8 Clustered residuals create heterogeneity among treatments","text":"variances four treatments areIn chapter xxx, stated heterogeneity variances can arise clustered data. clustered data generate heterogeneity? Let’s keep peeking linear mixed model fit experiment 1g data.\\[\n\\begin{equation}\n\\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon\n\\end{equation}\n\\]linear model fixed effects , expected variance treatment treatment \\(\\sigma^2\\). data batched, expected variances include components due batch batch components depend treatment. creates heterogeneity.understand , first rules expected variance. random variable \\(\\texttt{C}\\) sum two random variables \\(\\texttt{}\\) \\(\\texttt{B}\\). variances variables \\(\\sigma_{C}^2\\), \\(\\sigma_{}^2\\), \\(\\sigma_{B}^2\\).expected variance \\(\\texttt{C}\\) \\(\\texttt{}\\) \\(\\texttt{B}\\) independent (uncorrelated) \\(\\sigma_{C}^2 = \\sigma_{}^2 + \\sigma_{B}^2\\) (equation look familiar).expected variance \\(\\texttt{C}\\) \\(\\texttt{}\\) \\(\\texttt{B}\\) independent (correlated) \\(\\sigma_{C}^2 = \\sigma_{}^2 + \\sigma_{B}^2 + 2\\sigma_{}\\ \\sigma_{B}\\ \\rho_{,B}\\) \\(\\rho_{,B}\\) expected correlation \\(\\texttt{}\\) \\(\\texttt{B}\\) (equation might also look familiar).code better know expected variances sum two correlated random variables.Using rules standard deviations random effects given can compute expected variances treatment groups given fit model.variance reference (“Control”) group, need add \\(\\sigma^2\\) variance random intercept using rule #1 (residuals correlated random intercepts slopes). modeled variance less actual variance Control group.variance non-reference group, need add \\(\\sigma^2\\) variance random intercept variance random slope treatment component due correlation random slope treatment random intercept. Tr2, iswhich bit higher measured variance.","code":"\n# copy, paste, and explore\nn <- 10^4\nrho <- 0.6 # change this to any value between -1 and 1\nb <- sqrt(abs(rho))\nz <- rnorm(n)\nA <- b*z + sqrt(1-b^2)*rnorm(n)\nB <- sign(rho)*b*z + sqrt(1-b^2)*rnorm(n)\ncor(A,B) # should be close to rho\nC <- A + B\nsd(A)^2 # should be close to 1\nsd(B)^2 # should be close to 1\nsd(C)^2 # should be close to 1^2 + 1^2 + 2*1*1*rho\nsd(A)^2 + sd(B)^2 + 2*sd(A)*sd(B)*cor(A,B) # should equal previous line\nsummary(m1)$sigma^2 + 1.7710^2## [1] 6.866512\n# error + intercept + slope + cor(intercept, slope)\n(summary(m1)$sigma^2) + (1.7710^2) + (2.9026^2) + (2 * 1.7710 * 2.9026 * 0.458)## [1] 20.0003"},{"path":"lmm.html","id":"linear-mixed-models-are-flexible","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.9 Linear mixed models are flexible","text":"One look linear mixed model fit experiment 1g data.\\[\n\\begin{equation}\n\\texttt{gc}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{kj}) \\texttt{treatment}_{k} + \\varepsilon\n\\end{equation}\n\\]linear mixed model specifies random intercept random slope researcher might limit model random intercept , less commonly, random slope . researcher might replace random slope second random intercept captures variance batch treatment combinations like random slope. researcher might model structure (correlated error heterogeneity variances) residuals addition adding random factors model.","code":""},{"path":"lmm.html","id":"a-random-intercept-only-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.10 A random intercept only model","text":"NotesModel demo1_m2 specifies random intercept levels \\(\\texttt{experiment_id}\\). exclusion random slopes ia kind model simplification.experiments without subsampling, random slopes added model variation treatment batch combination.experiments subsampling, researcher might exclude random slope term several reasons, includingit culture many subfields include random intercept (, good reason)computation model fit returned convergence warningmodel comparison suggested model random slope complex given data. useful statistic comparing models different random effects specifications AIC, introduced Section 16.6.12 .","code":"\ndemo1_m2 <- lmer(gc ~ treatment +\n                   (1 | experiment_id),\n                 data = demo1)"},{"path":"lmm.html","id":"a-model-including-an-interaction-intercept","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.11 A model including an interaction intercept","text":"Notes(1 | experiment_id:treatment) models random intercept combinations levels \\(\\texttt{experiment_id}\\) \\(\\texttt{treatment}\\). interaction intercept alternative random slope modeling treatment-specific batch effects.","code":"\ndemo1_m3 <- lmer(gc ~ treatment +\n                   (1 | experiment_id) +\n                   (1 | experiment_id:treatment),\n                 data = demo1)"},{"path":"lmm.html","id":"lmm-demo-aic","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.12 AIC and model selection – which model to report?","text":"Three different linear mixed models fit Example 1 data: demo1_m1, demo1_m2, demo1_m3. model report? useful stastistic decision statistic known AIC.\nTable 16.3: AIC two alternative models fit Example 1 data\nNotesAIC (Akaike Information Criterion) relative measure model quality. Compare \\(R^2\\), absolute measure goodness fit. AIC formula two parts, one kind goodness fit (like \\(R^2\\)) penalty based number parameters model. goodness fit increases, AIC goes . number parameters increases, AIC goes . model lowest AIC highest quality model. actual value AIC, unlike \\(R^2\\), absolute meaning; meaningful relative AICs computed fits different models data.AIC three models suggests report Model demo1_m3.","code":""},{"path":"lmm.html","id":"the-specification-of-random-effects-matters","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.13 The specification of random effects matters","text":"Inference often different two models make different\nTable 16.4: Contrasts three linear mixed models fit Example 1 data.\nNotesInference intercept model (demo1_m2) almost certainly optimistic based simulations show intercept models can highly anti-conservative (narrow confidence intervals small p-values).Humans evolved make rational explanations – convince model smallest p-values scientifically rational model.","code":""},{"path":"lmm.html","id":"mixed-effect-and-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.14 Mixed Effect and Repeated Measures ANOVA","text":"Two-way mixed-effect ANOVA (fields call repeated measures ANOVA) equivalent Model demo1_m3 case. generally, two equal balanced designs – number subsamples treatment x batch combinations.Notes\n1. function afex::aov4 used specifying ANOVA models special cases linear mixed models.\n2. model formula Model demo1_m2 looks exactly like random intercepts slopes model (Model demo1_m1) .\n3. One can use either univariate multivariate model mixed repeated measures ANOVA – see section 16.7.11.\nTable 16.5: Contrasts two-way mixed effect ANOVA model demo1_m4.\n","code":"\ndemo1_m4 <- aov_4(gc ~ treatment + \n                   (treatment | experiment_id),\n                 data = demo1)"},{"path":"lmm.html","id":"pseudoreplication","chapter":"16 Models for non-independence – linear mixed models","heading":"16.6.15 Pseudoreplication","text":"","code":""},{"path":"lmm.html","id":"lmm-example2","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7 Example 2 – experiments without subsampling replication (exp6g)","text":"example introduces linear mixed models batches contain treatment levels single factor subsampling replication. example, batch individual mouse (\\(\\texttt{mouse_id}\\)). four measures response variable mouse, one measure per treatment level. subsampling replication, add random slope model single observation treatment level slope fit point reference level point non-reference level perfectly. However, can explicitly model variation correlated error heterogeneity variances among treatments alternative modeling random slope.Reversing model Parkinson’s disease situ converted nigral neuronsPublic sourceSource figure: Fig. 6gSource data: Source Data Fig. 6","code":""},{"path":"lmm.html","id":"understand-the-data","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.1 Understand the data","text":"study, researchers investigated effectiveness knocking protein PTBP1 induce astrocytes convert neurons motor processing region brain. Experimental lesions region brain model Parkinson’s disease. Experiment 6g, researchersGenerated lesion motor processing region using 6-hydroxydopamine (6-OHDA). lesion disrupts ability control contralateral (opposite side) forelimb.One month lesion, measured percent ipsilateral (side) forepaw touches (forelimb extending touching surface) test exploration new environment (“cylinder test”). expected percent intact mouse 50%. lesioned mouse, percent much greater 50% since less control contralateral limb. measure point treatment “Lesion”. positive control.Converted astrocytes lesion functional neurons knocking PTBP1.Two months knockdown, gave mouse saline remeasured percent ipsilateral touches cylinder test. knockdown worked expected, closer 50% ipsilateral touches. measure point treatment “Saline”. comparison Lesion focal test.Inhibited neuron action converted neurons using clozapine-N-oxide (CNO), suppresses neuron electrical activity. , remeasured percent ipsilateral touches cylinder test. CNO worked expected, much greater 50% ipsilateral touches since re-loss control contralateral limb. measure point treatment “CNO”. comparison Saline focal test.Allowed three days CNO degrade, , remeasured percent ipsilateral touches cylinder test. CNO degraded expected, converted neurons functional closer 50% ipsilateral touches. measure point treatment “Post_CNO”. comparison CNO focal test.design \\(4 \\times 1\\) – single treatment four levels (“Lesion”, “Saline”, “CNO”, “Post_CNO”)planned contrasts areSaline - Lesion. measures effect knockdown conversion astrocytes functional neurons.CNO - Saline. measures effect inhibiting converted neurons test neurons account effect contrast 1.Post_CNO - CNO. probing expectation contrast 2.","code":""},{"path":"lmm.html","id":"model-fit-and-inference","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.2 Model fit and inference","text":"","code":""},{"path":"lmm.html","id":"fit-the-model-8","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.2.1 Fit the model","text":"exp6g_m1b overparameterizes, report exp6g_m1a (see Alternative models exp6g )","code":"\nexp6g_m1a <- lmer(touch ~ treatment + (1|mouse_id), data = exp6g)\n\n# alt model\nexp6g_m1b <- lme(touch ~ treatment,\n                random = ~1|mouse_id,\n                correlation = corSymm(form = ~ 1 | mouse_id),\n                weights = varIdent(form = ~ 1 | treatment),\n                data = exp6g)\n\nAIC(exp6g_m1a, exp6g_m1b)##           df      AIC\n## exp6g_m1a  6 191.3367\n## exp6g_m1b 15 198.3888\n# report model a\nexp6g_m1 <- exp6g_m1a"},{"path":"lmm.html","id":"inference-from-the-model-4","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.2.2 Inference from the model","text":"","code":"\nexp6g_m1_coef <- cbind(coef(summary(exp6g_m1)),\n                       confint(exp6g_m1)[-c(1:2),])\n\n# exp5c_m1_coef %>%\n#   kable(digits = c(2,3,1,1,4,2,2)) %>%\n#   kable_styling()\nexp6g_m1_emm <- emmeans(exp6g_m1, specs = c(\"treatment\"))\n# exp6g_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nlesion <- c(1,0,0,0)\nsaline <- c(0,1,0,0)\ncno <- c(0,0,1,0)\npost_cno <- c(0,0,0,1)\n\nexp6g_m1_planned <- contrast(exp6g_m1_emm,\n                       method = list(\n                         \"Saline - Lesion\" = c(saline - lesion),\n                         \"CNO - Saline\" = c(cno - saline),\n                         \"Post_CNO - CNO\" = c(post_cno - cno)\n                       ),\n                       adjust = \"none\"\n) %>%\n  summary(infer = TRUE)"},{"path":"lmm.html","id":"lmm-exp6g-plotthemodel","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.2.3 Plot the model","text":"\nFigure 16.7: Treatment effect ipsilateral touch, percent touches. Gray dots connected lines individual mice.\n","code":""},{"path":"lmm.html","id":"alternaplot-the-model-1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.2.4 Alternaplot the model","text":"\nFigure 16.8: Ipsilateral touch (percent touches) response different treatments. Gray dots connected lines individual mice.\n","code":""},{"path":"lmm.html","id":"the-model-exp6g_m1-adds-a-random-intercept-but-not-a-random-slope","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.3 The model exp6g_m1 adds a random intercept but not a random slope","text":"model fit exp6g data \\[\n\\begin{equation}\n\\texttt{touch}_{jk} = (\\beta_{0} + \\gamma_{0j}) + (\\beta_{k} + \\gamma_{jk}) \\texttt{treatment}_{k} + \\varepsilon\n\\end{equation}\n\\]NotesAgain, experiments without subsampling, add random slope model (mouse, single observation treatment level slope fit two points perfectly).","code":""},{"path":"lmm.html","id":"the-fixed-effect-coefficients-of-model-exp6g_m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.4 The fixed effect coefficients of model exp6g_m1","text":"fixed effect coefficients model exp6g_m1 areNotesThe interpretation fixed effectcs coefficients usual interpretation (see coefficients linear model using dummy coding useful interpretation). Figure 16.9 reminder.\nFigure 16.9: Fixed effects estimated exp6g_m1. \\(b_0\\) modeled mean Lesian treatment. \\(b_1\\) difference (Saline - Lesian). \\(b_2\\) difference (CNO - Lesian). \\(b_3\\) difference (Post_CNO - Lesian).\n","code":""},{"path":"lmm.html","id":"the-random-intercept-coefficients-of-exp6g_m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.5 The random intercept coefficients of exp6g_m1","text":"\nTable 16.6: Random intercept coefficients (g_0j) exp6g_m1. mouse, random intercept sum fixed intercept (\\(b_0\\)) random intercept effect \\(g_{0j}\\).\nNotesThe random intercept effect \\(g_{0j}\\) difference modeled mean mouse j mean reference treatment. Unlike fixed intercept (\\(b_0\\)), \\(g_{0j}\\) effect.Figure ?? illustrates random intercept coefficients \\(g_{0j}\\) model exp6g_m1.random intercept coefficients often (always) treated source nuisance variation – , coefficients generally interest values typically reported.\nFigure 16.10: Random intercept effects model exp6g_m1. pale, colored dots measured percent ipsilateral touch values mouse treatment. dashed, gray lines modeled means treatment. dashed grey line reference level (“Lesion”) fixed intercept. dark, colored dots reference level random intercepts. value random intercept sum fixed intercept random intercept effect mouse. vertical, colored line segments reference level random intercept effects \\(g_{0j}\\). length segment residual dashed, gray line pale dot. random intercept effects mice 1 2 short see.\n","code":""},{"path":"lmm.html","id":"the-random-and-residual-variance-and-the-intraclass-correlation-of-model-exp6g_m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.6 The random and residual variance and the intraclass correlation of model exp6g_m1","text":"NotesThe first element “Std.Dev.” estimate \\(\\sqrt{\\sigma^2_{0j}}\\), standard deviation among donors due random effect \\(\\texttt{mouse_id}\\).second element “Std.Dev.” \\(\\sqrt{\\sigma^2_{0}}\\), estimate standard deviation error variance (\\(\\varepsilon^2\\)). Remember residuals model estimates \\(\\varepsilon\\).ratio \\(\\frac{\\sigma^2_{0j}}{\\sigma^2_{0j} + \\sigma^2_{0}}\\) (ratio among-block variance total random variance) known intraclass correlation. correlation estimate correlated error due -mouse clustering model fit without added random intercept. exp6g_m1, correlation 0.3.intraclass correlation ranges 0 1 makes pretty good qualitative indicator repeatability.","code":"##  Groups   Name        Std.Dev.\n##  mouse_id (Intercept) 4.9882  \n##  Residual             7.6155"},{"path":"lmm.html","id":"the-linear-mixed-model-exp6g_m1-increases-precision-of-treatment-effects-relative-to-a-fixed-effects-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.7 The linear mixed model exp6g_m1 increases precision of treatment effects, relative to a fixed effects model","text":"Let’s compare effects estimated linear mixed model exp6g_m1 linear model ignores donor (fixed effects model).\nFigure 16.11: . Inference linear mixed model blocking factor (mouse_id) added random intercept. B. Inference fixed effects model.\nFigure 16.11A plot effects linear mixed model exp6g_m1 models added variance due mouse. Figure 16.11B plot effects fixed effect model exp6g_m2 ignores added variance due mouse. 95% confidence intervals treatment effects model exp6g_m1 slightly smaller model exp6g_m2. Adding \\(\\texttt{mouse_id}\\) random factor linear model increases precision estimate treatment effects eliminating among-mouse component variance error variance.error variance (estimate $^2) linear mixed model fixed effects model isThe error variance 43% higher fixed effects model. linear mixed model, variance lost error shifted random intercept. can track shift table two variance components linear mixed model, shown , (values square roots variances).sum two variance components linear mixed model equal error variance fixed effects model:","code":"\nexp6g_m2 <- lm(touch ~ treatment, data = exp6g)\nsummary(exp6g_m1)$sigma^2## [1] 57.99549\nsummary(exp6g_m2)$sigma^2## [1] 82.8777\nVarCorr(exp6g_m1)##  Groups   Name        Std.Dev.\n##  mouse_id (Intercept) 4.9882  \n##  Residual             7.6155\nsum(as.data.frame(VarCorr(exp6g_m1))$vcov)## [1] 82.8777"},{"path":"lmm.html","id":"lmm-exp6g-alt","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.8 Alternative models for exp6g","text":"Linear mixed models flexible, topic advanced text. want focus alternative model relevance repeated measures ANOVA.NotesTwo models fit. two models fixed effect differ model random effect pattern correlations residuals. difference specification determines error variance degrees freedom computing uncertainty. ’ll return inference problem “model choose” moment. First, models differ?Model exp6g_m1a model analyzed explained . model specifies random intercept level \\(\\texttt{mouse_id}\\).Model exp6g_m1b random effects exp6g_m1a adds two additional arguments, correlation argument explicitly models correlated error residuals weights argument models heterogeneity residuals.Consider error (residuals) fixed effect model touch ~ treatment cast matrix residuals treatment column. matrix residuals \\(7 \\times 4\\) (7 mice, 4 treatments) matrix looks like table residuals Example 1 (Table 16.1).Model exp6g_m1a implicitly models compound symmetric correlated error. meansthe correlation every pair columns residual matrix .variances columns residual matrix .Model exp6g_m1a assumes zero correlation among columns residual matrix Model exp6g_m1a. source correlated residuals fixed model modeled random intercept.Model exp6g_m1b assume compound symmetric correlated error.argument correlation = corSymm(form = ~ 1 | mouse_id) model exp6g_m1b explicitly models different correlations pairs treatment combinations – , unstructured correlated error.argument weights = varIdent(form = ~ 1 | treatment) model exp6g_m1b explicitly models heterogeneity residuals among levels $.Model exp6g_m1b makes fewer assumptions inference. trade-estimation parameters potential overfitting.univariate model repeated measures ANOVA fit exp6g data equivalent model exp6g_m1a. See section 16.7.10.multivariate model repeated measures ANOVA fit exp6g data equivalent model exp6g_m1b. See section 16.7.11.\nTable 16.7: Planned contrasts two alternative models.\nPlanned comparisons two models given tables . effect estimates inference differs quantitatively among models (qualitatively) model models error. model report? One way evaluate models statistic known AIC.\nTable 16.8: AIC two alternative models fit exp6g data\nAIC (Akaike Information Criterion) relative measure model quality. Compare \\(R^2\\), absolute measure goodness fit. AIC formula two parts, one kind goodness fit (like \\(R^2\\)) penalty based number parameters model. goodness fit increases, AIC goes . number parameters increases, AIC goes . model lowest AIC highest quality model. actual number, however absolute meaning; meaningful relative AICs computed fits different models using data.AICs models exp6g_m1a models exp6g_m1b suggest model exp6g_m1b complex given data. relevance repeated measures ANOVA analysis next section.","code":"\nexp6g_m1a <- lmer(touch ~ treatment + (1 | mouse_id),\n                 data = exp6g)\nexp6g_m1b <- lme(touch ~ treatment,\n                random = ~1|mouse_id,\n                correlation = corSymm(form = ~ 1 | mouse_id),\n                weights = varIdent(form = ~ 1 | treatment),\n                data = exp6g)"},{"path":"lmm.html","id":"paired-t-tests-and-repeated-measures-anova-are-special-cases-of-linear-mixed-models","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.9 Paired t-tests and repeated measures ANOVA are special cases of linear mixed models","text":"paired t-test special case linear mixed model fit data randomized complete block design subsampling two treatment levels (see Lack independence Violations chapter). repeated measures ANOVA special case linear mixed model fit data randomized complete block design subsampling two treatment levels (see Lack independence Violations chapter).Experiment 6g randomized complete block design subsampling four treatments. “test” key traditional, experimental statistics textbook guide researcher analyze data using repeated measures ANOVA. two methods repeated measures anova, univariate model multivariate model.","code":""},{"path":"lmm.html","id":"lmm-exp6g-rmanova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.10 Classical (“univariate model”) repeated measures ANOVA of exp6g","text":"\nTable 16.9: Planned contrasts linear mixed model exp6g_m1a univariate model repeated measures ANOVA.\nNotesIn addition assumptions linear model outlined Violations chapter, classical (“univariate model”) repeated measures ANOVA assumes sphericity, equality variances pairwise differences among treatment combinations.univariate model classic RM ANOVA equivalent Model exp6g_m1a.","code":"\nexp6g_aov1 <- aov_4(touch ~ treatment +\n                    (treatment | mouse_id),\n                  data = exp6g)"},{"path":"lmm.html","id":"lmm-exp6g-rmanova-multi","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.11 “Multivariate model” repeated measures ANOVA","text":"\nTable 16.10: Planned contrasts linear mixed model exp6g_m1b multivariate model repeated measures ANOVA.\nNotesThe sphericity assumption classical repeated measures ANOVA relaxed model fit using “multivariate model”.multivariate model repeated measures ANOVA linear model multivariate response linear mixed model. multivariate model, treatment combination different response variable single row level random factor (\\(\\texttt{mouse_id}\\) Experiment 6g). multivariate model different way handling correlated error occurs conceiving design univariate.multivariate repeated measures ANOVA equivalent linear mixed model exp6g_m1b. SEs contrasts degrees freedom differ. increased df linear mixed model, CIs narrower p-value smaller – linear mixed model less conservative repeated measures ANOVA. correct degrees freedom linear mixed model like emmeans outputs one way compute (options others, none equivalent multivariate model RM-ANOVA).","code":""},{"path":"lmm.html","id":"linear-mixed-models-vs-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.12 Linear mixed models vs repeated measures ANOVA","text":"Many modern textbooks encourage researchers use linear mixed models instead repeated measures ANOVA randomized complete block designs (without subsampling) becauselinear mixed models exclude random units (subject/mouse/donor/cage) missing measures one treatment combinations. Example 2 (diHOME exp2a) – repeated measures ANOVA special case linear mixed model example kind missing data.subsampling, linear mixed models aggregate random-unit data, , linear mixed models simply compute means ignore variance sample within random unit.linear mixed models allow modeling additional sources correlated error, , experiment may two random factor variables (example, donor experiment).linear mixed models allow researcher model different patterns correlated error. especially important longitudinal experiments.linear mixed models can generalized model sampling non-normal distributions – generalized linear mixed models.","code":""},{"path":"lmm.html","id":"modeling-textttmouse_id-as-a-fixed-effect","chapter":"16 Models for non-independence – linear mixed models","heading":"16.7.13 Modeling \\(\\texttt{mouse_id}\\) as a fixed effect","text":"\\[\n\\begin{equation}\n\\texttt{touch} \\sim \\texttt{treatment + mouse_id}\n\\end{equation}\n\\tag{16.3}\n\\]Model (16.3) (using R formula syntax) linear model block \\(\\texttt{mouse_id}\\) added fixed covariate instead random intercept. coefficients fit model areNotesThe coefficients include slope seven non-reference levels mouse_id typically don’t care .randomized complete block design 1 replicate treatment combination per block, like Experiment 6g, inference treatment effects exactly model random intercept model exp6g_m1. Compare SE, CI p-value coefficients treatment effects linear mixed model exp6g_m1.randomized complete block design 1 replicate treatment combination per block, like Experiment 6g, inference treatment means differs model random intercept model – SE means fixed effect model smaller SE means linear mixed model. Compare SE CI (Intercept) (mean reference treatment combination) fixed effect linear mixed model.equivalence inference treatment effect fixed effect linear mixed model holds balanced randomized complete block designs – blocks contain treatment combinations subsampling replicate size treatment combinations blocks. means, outside special cases like Experiment 6g, choice adding blocking variable random fixed factor depends assumptions model.","code":"\nexp6g_m3 <- lm(touch ~ treatment + mouse_id,\n               data = exp6g)\n\nexp6g_m3_coef <- cbind(coef(summary(exp6g_m3)),\n                       confint(exp6g_m3))"},{"path":"lmm.html","id":"example-3-factorial-experiments-and-no-subsampling-replicates-exp5c","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8 Example 3 – Factorial experiments and no subsampling replicates (exp5c)","text":"Example 3 similar example 2 subsampling replication add random slopes linear mixed model. Example 3 differs design factorial – two, crossed fixed factors. Consequently, several alternative models different sets random intercepts. reported model includes two random intercepts, one models differences batch effects among treatment levels (treatment batch interactions). interaction intercept alternative random slope modeling treatment batch interactions.Transcriptomic profiling skeletal muscle adaptations exercise inactivitySource figure: Fig. 5cSource data: Source Data Fig. 5","code":""},{"path":"lmm.html","id":"understand-the-data-1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.1 Understand the data","text":"data Example 1 Figure 5c. Six muscle source cells used start six independent cultures. Cells culture treated either negative control (“Scr”) siRNA (“siNR4A3”) “silences” expression NR4A3 gene product cleaving mRNA. Glucose uptake two cell types measured rest (“Basal”) electrical pulse stimulation (“EPS”).design \\(2 \\times 2\\) Randomized complete block subsampling. two factors two levels: \\(\\texttt{treatment}\\) (“Scr”, “siNR4A3”) \\(\\texttt{activity}\\) (“Basal”, “EPS”). source cell block. four treatment combinations measured per block.","code":""},{"path":"lmm.html","id":"examine-the-data-1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.2 Examine the data","text":"plot shows strong donor effect.","code":""},{"path":"lmm.html","id":"lmm-exp5c-m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.3 Model fit and inference","text":"exp5c_m1b (equivalent univariate repeated measures ANOVA) singular fit. don’t use. Trivial difference AIC exp5c_m1a exp5c_m1c. Nothing VarCorr exp5c_m1c raises red flags. Report exp5c_m1c.","code":"\nexp5c_m1a <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor),\n                 data = exp5c)\n\nexp5c_m1b <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment) +\n                   (1 | donor:activity),\n                 data = exp5c)\n\nexp5c_m1c <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment),\n                 data = exp5c)\n\nexp5c_m1d <- lme(glucose_uptake ~ treatment * activity,\n                 random =  ~ 1 | donor,\n                 correlation = corSymm(form = ~ 1 | donor),\n                 weights = varIdent(form = ~ 1|t.by.a),\n                 data = exp5c)\n\n# check AIC\nAIC(exp5c_m1a, exp5c_m1b, exp5c_m1c, exp5c_m1d)##           df       AIC\n## exp5c_m1a  6  8.153813\n## exp5c_m1b  8 10.052676\n## exp5c_m1c  7  8.052676\n## exp5c_m1d 15 14.763024\n# check VarCorr model c\nVarCorr(exp5c_m1c) # fine##  Groups          Name        Std.Dev.\n##  donor:treatment (Intercept) 0.089519\n##  donor           (Intercept) 0.352097\n##  Residual                    0.090685\n# report 1c (based on AIC and VarCorr check)\nexp5c_m1 <- exp5c_m1c"},{"path":"lmm.html","id":"check-the-model-2","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.3.1 Check the model","text":"fine.","code":"\nggcheck_the_model(exp5c_m1)"},{"path":"lmm.html","id":"inference-from-the-model-5","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.3.2 Inference from the model","text":"","code":"\nexp5c_m1_coef <- cbind(coef(summary(exp5c_m1)),\n                       confint(exp5c_m1)[-c(1:3),])\nexp5c_m1_coef %>%\n  kable(digits = c(2,3,1,1,4,2,2)) %>%\n  kable_styling()\nexp5c_m1_emm <- emmeans(exp5c_m1, specs = c(\"treatment\", \"activity\"))\n# exp5c_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nscr_basal <- c(1,0,0,0)\nsiNR4A3_basal <- c(0,1,0,0)\nscr_eps <- c(0,0,1,0)\nsiNR4A3_eps <- c(0,0,0,1)\n\nexp5c_m1_planned <- contrast(exp5c_m1_emm,\n                       method = list(\n                         \"(Scr EPS) - (Scr Basal)\" = c(scr_eps - scr_basal),\n                         \"(siNR4A3 EPS) - (siNR4A3 Basal)\" = c(siNR4A3_eps - siNR4A3_basal),\n                         \"Interaction\" = c(siNR4A3_eps - siNR4A3_basal) -\n                           c(scr_eps - scr_basal)\n                           \n                       ),\n                       adjust = \"none\"\n) %>%\n  summary(infer = TRUE)"},{"path":"lmm.html","id":"lmm-exp5c-plotthemodel","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.3.3 Plot the model","text":"\nFigure 16.12: Treatment effect glucose uptake. Gray dots connected lines individual donors.\n","code":""},{"path":"lmm.html","id":"alternaplot-the-model-2","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.3.4 Alternaplot the model","text":"\nFigure 16.13: Glucose response different treatments. Gray dots connected lines individual donors. Dashed gray line expected additive mean “siNR4A3 EPS”.\nNotesMany researchers might look wide confidence intervals relative short distance means think “effect”. confidence intervals correct, simply meant tools inferring anything differences means. one many reasons plots means error bars can misleading inference, despite ubiquity use communicating results. , prefer effects--response plots, explicitly communicate correct inference effects.","code":""},{"path":"lmm.html","id":"why-we-care-about-modeling-batch-in-exp5c","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.4 Why we care about modeling batch in exp5c","text":"Figure 16.14 shows modeled means four treatment combinations individual values colored donor. pretty easy see glucose uptake values donors 4 5 well mean four treatments. , values donors 1, 2, 3 well mean four treatments. values donor 6 near mean four treatments.\nFigure 16.14: care blocking. black dots modeled means treatment combination. colored dots measured values response donor. position donor relative mean easy see data.\nLet’s compare effects estimated linear mixed model exp5c_m1 linear model ignores donor.\nFigure 16.15: . Inference linear mixed model blocking factor (donor) added random intercept. B. Inference fixed effects model.\nFigure 16.15A plot effects linear mixed model models correlated error due donor. Figure 16.15B plot effects fixed effect model ignores correlated error due donor. Adding \\(\\texttt{donor}\\) factor linear model increases precision estimate treatment effects eliminating among-donor component variance error variance.","code":"\nexp5c_m2 <- lm(glucose_uptake ~ treatment * activity,\n           data = exp5c)"},{"path":"lmm.html","id":"the-linear-mixed-model-exp5c_m1-adds-two-random-intercepts","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.5 The linear mixed model exp5c_m1 adds two random intercepts","text":"two random intercepts linear mixed model exp5c_m1 fit exp5c data (exp5c_m1 copied exp5c_m1c Model fit inference).\\[\n\\begin{align}\n\\texttt{glucose_uptake}_j = \\ &(\\beta_0 + \\gamma_{0j} + \\gamma_{0jk}) + \\beta_1 (\\texttt{treatment}_\\texttt{siNR4A3}) + \\beta_2 (\\texttt{activity}_\\texttt{EPS}) \\ + \\\\\n&\\beta_3 (\\texttt{treatment}_\\texttt{siNR4A3}:\\texttt{activity}_\\texttt{EPS}) + \\varepsilon\n\\end{align}\n\\tag{16.4}\n\\]random intercept effect \\(\\gamma_{0j}\\) models donor variation reference level. j indexes donor j. coefficient estimated donor.random intercept effect \\(\\gamma_{0jk}\\) models variation due \\(\\texttt{donor}\\) \\(\\texttt{treatment}\\) combinations. coefficient estimated 6 (donors) \\(\\times\\) 2 (levels \\(\\texttt{treatment}\\)). k indexes treatment level k. interaction intercept alternative random slope modeling treatment batch interactions. Unlike random slope, \\(\\gamma_{0jk}\\) coefficient donor reference level.","code":""},{"path":"lmm.html","id":"the-fixed-effect-coefficients-of-model-exp5c_m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.6 The fixed effect coefficients of model exp5c_m1","text":"fixed effect coefficients model exp5c_m1 areNotesThe interpretation fixed effectcs coefficients usual interpretation factorial linear model. Figure 16.16 reminder.\nFigure 16.16: Fixed effects estimated exp5c_m1. light gray point expected value GPR174- F treatment genotype sex additive.\n","code":""},{"path":"lmm.html","id":"the-random-effect-coefficients-of-model-exp5c_m1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.7 The random effect coefficients of model exp5c_m1","text":"\nTable 16.11: Random intercept effects donor (g_0j) donor:treatment combination (g_0jk) exp5c_m1. two random intercepts. random_intercept_j sum fixed intercept (b_0) g_0j. random_intercept_jk sum fixed intercept (b_0) g_0jk.\nNotesThe random intercept “random_intercept_j” sum fixed intercept \\(b_0\\) random intercept effect \\(g_{0j}\\) donor j. \\(g_{0j}\\) estimate \\(\\gamma_{0j}\\) Model (16.4). Note \\(g_{0j}\\) treatment levels within donor. illustration random effects similar Example 2 exp6g_m1 (Figure 16.10.random intercept “random_intercept_jk” sum fixed intercept \\(b_0\\) random intercept effect \\(g_{0jk}\\) donor j treatment level k (reference level equal 1). \\(g_{0jk}\\) estimate \\(\\gamma_{0jk}\\) Model (16.4). \\(g_{0jk}\\) differ combination \\(\\texttt{donor}\\) \\(\\texttt{treatment}\\).donor:treatment random intercepts (“random_intercept_jk”) shown dark, colored dots Figure 16.17A. distance dark colored dot dashed, gray line (modeled mean treatment:activity combination) random intercept effect estimate \\(\\gamma_{0jk}\\).set random intercept effects donor:treatment combinations (\\(g_{0jk}\\)) two activity levels. seen Figure 16.17A - compare pattern dark, colored dots two EPS treatments two Basal treatments.combined random intercept effects (\\(g_{0j} + g_{0jk}\\)) shown Figure 16.17B distance dark, colored dots dashed, gray lines.communication decision-making Figure 16.17. actual y-values dark, colored dots random intercept effect plus treatment-combination modeled mean. Thus, , dark colored dots equal “random_intercept_jk” reference level (since reference model mean fixed intercept).\nFigure 16.17: Random intercepts model exp5c_m1. pale, colored dots measured glucose uptake values donor treatment combination. dashed, gray lines modeled means treatment combination. () distance dark colored dot dashed, gray line random intercept effect \\(g_{0jk}\\). (B) distance dark colored dot dashed, gray line combined random intercept effect \\(g_{0j} + g_{0jk}\\).\n","code":""},{"path":"lmm.html","id":"lmm-exp5c-alt","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.8 Alternative models for exp5c","text":"NotesFour models fit. models specify fixed effects different random effects patterns correlated error. , care fixed effects – point experiment – specification random effects determines error variance degrees freedom computing uncertainty.Model exp5c_m1a specifies random intercept level \\(\\texttt{donor}\\).Model exp5c_m1b adds two additional random intercepts Model exp5c_m1a. code (1 | donor:treatment) adds intercept combination \\(\\texttt{donor}\\) \\(\\texttt{treatment}\\). code (1 | donor:activity) adds intercept combination \\(\\texttt{donor}\\) \\(\\texttt{activity}\\). model equivalent univariate model repeated measures ANOVA data (see Classical (“univariate model”) repeated measures ANOVA ).Model exp5c_m1c adds one additional random intercept ((1 | donor:treatment)). simplification Model exp5c_m1c. model reported explained .Model exp5c_m1d random effects exp5c_m1a models unstructured correlated error heterogeneity error, described Example 2: Alternative models exp6g. model equivalent multivariate model repeated measures ANOVA data (see “Multivariate model” repeated measures ANOVA ).lmer returns message “boundary (singular) fit: see ?isSingular” exp5c_m1b. model fit message means cautious (simply avoid) interpreting inferential statistics (SEs, CIs, p-values). Looking estimates estimated parameters random effects table (variances effects covariances among effects), Model exp5c_m1b estimates zero variance \\(\\texttt{donor:activity}\\) random intercept. suggests simplify exp5c_m1b removing (1 | donor:activity) model – exp5c_m1c. Think warning. fit using repeated measures ANOVA (next section), won’t get warning message still may overfitting.Planned comparisons four alternative models given Table 16.12. effect estimates inference differs slightly among models model partitions error variance either random effects residuals.\nTable 16.12: Planned contrasts four alternative models.\n\nTable 16.13: AIC four alternative models firt exp5c data\nuse AIC selecting among models different random effects described . AICs models suggest models exp5c_m1b exp5c_m1d complex given data. AICs models exp5c_m1a exp5c_m1c effectively . reported exp5c_m1c simply allow explain donor:treatment random effect.","code":"\nexp5c_m1a <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor),\n                 data = exp5c)\n\nexp5c_m1b <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment) +\n                   (1 | donor:activity),\n                 data = exp5c)## boundary (singular) fit: see help('isSingular')\nexp5c_m1c <- lmer(glucose_uptake ~ treatment * activity +\n                   (1 | donor) +\n                   (1 | donor:treatment),\n                 data = exp5c)\n\nexp5c_m1d <- lme(glucose_uptake ~ treatment * activity,\n                 random =  ~ 1 | donor,\n                 correlation = corSymm(form = ~ 1 | donor),\n                 weights = varIdent(form = ~ 1|t.by.a),\n                 data = exp5c)\nVarCorr(exp5c_m1b)##  Groups          Name        Std.Dev.\n##  donor:activity  (Intercept) 0.000000\n##  donor:treatment (Intercept) 0.089519\n##  donor           (Intercept) 0.352092\n##  Residual                    0.090686"},{"path":"lmm.html","id":"lmm-rmanova-5c","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.9 Classical (“univariate model”) repeated measures ANOVA","text":"\nTable 16.14: Planned contrasts linear mixed model exp5c_m1c univariate model repeated measures ANOVA.\nNotesIn addition assumptions linear model outlined Violations chapter, classical (“univariate model”) repeated measures ANOVA assumes sphericity, equality variances pairwise differences among treatment combinations.univariate model repeated measures ANOVA equivalent Model exp5c_m1c. Inference (SEs, CIs, p-values) differs slightly estimated variance \\(\\texttt{donor:activity}\\) random intercept Model exp5c_m1c zero. cases variances greater zero design balanced (block missing treatment combination), two tables identical.","code":"\nexp5c_aov1 <- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c)"},{"path":"lmm.html","id":"lmm-rmanova-5c-multi","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.10 “Multivariate model” repeated measures ANOVA of exp5c","text":"\nTable 16.15: Planned contrasts linear mixed model exp5c_m1d multivariate model repeated measures ANOVA.\nNotesThe sphericity assumption repeated measures ANOVA relaxed model fit using “multivariate model”.multivariate model repeated measures ANOVA linear model multivariate response linear mixed model. multivariate model, treatment combination different response variable single row level random factor (\\(\\texttt{donor}\\) Experiment 5c). multivariate model different way handling correlated error occurs conceiving design univariate.multivariate repeated measures ANOVA can specified using linear mixed model exp5c_m1d. Compared contrasts multivariate model repeated measures ANOVA , SEs contrasts degrees freedom differ. increased df linear mixed model, CIs narrower p-value smaller – linear mixed model less conservative repeated measures ANOVA. correct degrees freedom linear mixed model like emmeans outputs one way compute (options others, none equivalent multivariate model RM-ANOVA).","code":""},{"path":"lmm.html","id":"modeling-textttdonor-as-a-fixed-effect","chapter":"16 Models for non-independence – linear mixed models","heading":"16.8.11 Modeling \\(\\texttt{donor}\\) as a fixed effect","text":"\\[\n\\begin{equation}\n\\texttt{glucose_uptake} \\sim \\texttt{treatment * activity + donor}\n\\end{equation}\n\\tag{16.5}\n\\]Model (16.5) (using R formula syntax) linear model block \\(\\texttt{donor}\\) added fixed covariate instead random intercept.coefficients fit model areNotesThe coefficients include slope five non-reference levels donor. typically don’t care .\nTable 16.16: Planned contrasts linear mixed model exp5c_m1a fixed effect model exp5c_m3.\nNotesIn randomized complete block design 1 replicate treatment combination per block, like Experiment 5c, inference treatment effects exactly fixed effect model random intercept model exp5c_m1a.equivalence inference treatment effect fixed effect linear mixed model holds balanced randomized complete block designs – blocks contain treatment combinations subsampling replicate size treatment combinations blocks. means, outside special cases like Experiment 5c, choice adding blocking variable random fixed factor depends assumptions model.","code":"\nexp5c_m3 <- lm(glucose_uptake ~ treatment * activity + donor,\n               data = exp5c)"},{"path":"lmm.html","id":"lmm-example4","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9 Example 4 – Experiments with subsampling replication (exp1g)","text":"example design batches (independent experiments) contain treatment levels single factor subsampling replication. data used introduce linear mixed models Example 1. design experiment \\(2 \\times 2\\) factorial. Example 1 flattened analysis simplify explanation random intercepts random slopes. , data analyzed factorial model.GPR174–CCL21 module imparts sexual dimorphism humoral immunityPublic sourceSource figure: Fig. 1gSource data: Source Data Fig. 1","code":""},{"path":"lmm.html","id":"understand-the-data-2","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.1 Understand the data","text":"researchers paper interested discovering mechanisms causing lower antibody-mediated immune response males relative females. data Fig. 1 set experiments mice investigate G-protein coupled receptor protein GPR174 regulates formation B-cell germinal center secondary lymph tissue. GPR174 X-linked gene.Response variable \\(\\texttt{gc}\\) – germinal center size (%). units percent cells expressing germinal center markers.Factor 1 – \\(\\texttt{sex}\\) (“M”, “F”). Male (“M”) reference level.Factor 2 – \\(\\texttt{chromosome}\\) (“Gpr174+”, “Gpr174-”). “Gpr174-” GPR174 knockout. wildtype (“Gpr174+”) condition reference level.Design – \\(2 \\times 2\\), , two crossed factors two levels. results four groups, unique combination levels factor. “M Gpr174+” control. “M Gpr174+” knockout genotype males (“knockout added”). “F Gpr174+” wildtype female (“X chromosome added”). “F Gpr174-” knockout female (“knockout X chromosome added”.","code":""},{"path":"lmm.html","id":"examine-the-data-2","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.2 Examine the data","text":"","code":"\nggplot(data = exp1g,\n       aes(x = treatment,\n           y = gc,\n           color = experiment_id)) +\n  geom_point(position = position_dodge(0.4))"},{"path":"lmm.html","id":"fit-the-model-9","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.3 Fit the model","text":"","code":"\n# three slope parameters\nexp1g_m1a <- lmer(gc ~ genotype * sex +\n                   (genotype * sex | experiment_id),\n                 data = exp1g)\n\nVarCorr(exp1g_m1a) # looks fine##  Groups        Name                 Std.Dev. Corr                \n##  experiment_id (Intercept)          1.77082                      \n##                genotypeGpr174-      0.96192  -0.023              \n##                sexF                 2.90154   0.459  0.878       \n##                genotypeGpr174-:sexF 1.33169  -0.314 -0.625 -0.706\n##  Residual                           1.93155\n# one slope parameter but capturing all treatment combinations\nexp1g_m1b <- lmer(gc ~ genotype * sex +\n                   (treatment | experiment_id),\n                 data = exp1g)\n\n# intercept interactions\nexp1g_m1c <- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:genotype) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)\n\nVarCorr(exp1g_m1c) # id:genotype is low##  Groups                     Name        Std.Dev. \n##  experiment_id:genotype:sex (Intercept) 0.4046670\n##  experiment_id:sex          (Intercept) 1.6927499\n##  experiment_id:genotype     (Intercept) 0.0001216\n##  experiment_id              (Intercept) 2.5487069\n##  Residual                               1.9792428\n# drop id:genotype which has low variance\nexp1g_m1d <- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:sex:genotype),\n                 data = exp1g)\n\nAIC(exp1g_m1a, exp1g_m1b, exp1g_m1c, exp1g_m1d)##           df      AIC\n## exp1g_m1a 15 344.1579\n## exp1g_m1b 15 344.1529\n## exp1g_m1c  9 337.7602\n## exp1g_m1d  8 335.7602\n# go with exp1g_m1d.\nexp1g_m1 <- exp1g_m1d"},{"path":"lmm.html","id":"inference-from-the-model-6","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.4 Inference from the model","text":"NotesThe direction estimated effect opposite males females","code":"\nexp1g_m1_coef <- coef(summary(exp1g_m1))\nexp1g_m1_coef##                       Estimate Std. Error       df   t value    Pr(>|t|)\n## (Intercept)           8.435718   1.612602 4.638595  5.231123 0.004202015\n## genotypeGpr174-       2.568365   0.712122 5.760582  3.606637 0.012094327\n## sexF                  5.583860   1.397388 4.024010  3.995927 0.015995349\n## genotypeGpr174-:sexF -5.304752   1.013552 5.918159 -5.233823 0.002033657\n# order of factors reversed in specs because I want sex to be\n# main x-axis variable in plot\nexp1g_m1_emm <- emmeans(exp1g_m1, specs = c(\"sex\", \"genotype\"))\n# exp1g_m1_emm # print in console to get row numbers\n# set the mean as the row number from the emmeans table\nwt_m <- c(1,0,0,0)\nwt_f <- c(0,1,0,0)\nko_m <- c(0,0,1,0)\nko_f <- c(0,0,0,1)\n\n# simple effects within males and females + interaction \n# 1. (ko_m - wt_m) \n# 2. (ko_f - wt_f)\n\nexp1g_contrasts <- list(\n  \"(Gpr174- M) - (Gpr174+ M)\" = c(ko_m - wt_m),\n  \"(Gpr174- F) - (Gpr174+ F)\" = c(ko_f - wt_f),\n  \"Interaction\" = c(ko_f - wt_f) -\n    c(ko_m - wt_m)\n)\nexp1g_m1_planned <- contrast(exp1g_m1_emm,\n                       method = exp1g_contrasts,\n                       adjust = \"none\"\n) %>%\n  summary(infer = TRUE)"},{"path":"lmm.html","id":"plot-the-model-6","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.5 Plot the model","text":"\nFigure 16.18: Treatment effect germinal center (GC) formation. Small, pale, colored dots independent experiments. Intermediate size colored dots experiment means.\n","code":""},{"path":"lmm.html","id":"alternaplot-the-model-3","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.6 Alternaplot the model","text":"\nFigure 16.19: Germinal center (GC) formation response treatment. Small, pale, colored dots independent experiments. Intermediate size colored dots experiment means. Dashed gray line expected additive mean Gpr174- F\n","code":""},{"path":"lmm.html","id":"understanding-the-alternative-models","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.7 Understanding the alternative models","text":"NotesAll four models model fixed effects. models differ model random effects.Model exp1g_m1a fits one random intercept, two random slopes, one random interaction.\\(\\gamma_{0j}\\) – random intercept modeling batch effects \\(\\texttt{experiment_id}\\) intercept\\(\\gamma_{1j}\\) – random slope modeling effect non-reference level “Gpr174-” batch effect \\(\\texttt{experiment_id}\\). \\(\\texttt{experiment_id} \\times \\texttt{genotype}\\) interaction.\\(\\gamma_{2j}\\) – random slope modeling effect “F” (female) batch effect \\(\\texttt{experiment_id}\\). \\(\\texttt{experiment_id} \\times \\texttt{sex}\\) interaction.\\(\\gamma_{3j}\\) – random slope modeling effect interaction effect “Gpr174-” “F” batch effect \\(\\texttt{experiment_id}\\). \\(\\texttt{experiment_id} \\times \\texttt{sex} \\times \\texttt{genotype}\\) interaction.Model exp1g_m1b fits one random intercept three random slopes\\(\\gamma_{0j}\\) – random intercept modeling batch effects \\(\\texttt{experiment_id}\\) intercept. modeling thing \\(\\gamma_{0j}\\) Model exp1g_m1a.\\(\\gamma_{1j}\\) – random slope modeling effect “Gpr174- M” batch effect \\(\\texttt{experiment_id}\\). modeling thing \\(\\gamma_{1j}\\) Model exp1g_m1a.\\(\\gamma_{2j}\\) – random slope modeling effect “Gpr174+ F” batch effect \\(\\texttt{experiment_id}\\). modeling thing \\(\\gamma_{2j}\\) Model exp1g_m1a.\\(\\gamma_{3j}\\) – random slope modeling effect “M Gpr174- F” batch effect \\(\\texttt{experiment_id}\\). modeling added variance accounted random interaction \\(\\gamma_{2j}\\) Model exp1g_m1a different way.Model exp1g_m1c fits four random intercepts\\(\\gamma_{0j}\\) – random intercept modeling batch effects \\(\\texttt{experiment_id}\\) intercept. modeling thing \\(\\gamma_{0j}\\) Model exp1g_m1a.\\(\\gamma_{0jk}\\) – random intercept modeling effects combination \\(\\texttt{experiment_id}\\) \\(\\texttt{genotype}\\). similar variance modeled \\(\\gamma_{1j}\\) Model exp1g_m1a except draws \\(\\gamma_{0jk}\\) independent (uncorrelated) draws random intercepts.\\(\\gamma_{0jl}\\) – random intercept modeling effects combination \\(\\texttt{experiment_id}\\) \\(\\texttt{sex}\\). similar variance modeled \\(\\gamma_{2j}\\) Model exp1g_m1a except draws \\(\\gamma_{0jl}\\) independent (uncorrelated) draws random intercepts (review correlation among random intercepts slopes doesn’t make sense).\\(\\gamma_{0jkl}\\) – random intercept modeling effects combination \\(\\texttt{experiment_id}\\), \\(\\texttt{genotype}\\) \\(\\texttt{sex}\\). similar variance modeled \\(\\gamma_{3j}\\) Model exp1g_m1a except draws \\(\\gamma_{0jkl}\\) independent (uncorrelated) draws random intercepts.Model exp1g_m1d fits random intercepts Model exp1g_m1c excludes\n\\(\\gamma_{0jl}\\) (random intercept experiment_id genotyp combination. excluded low variance component fit model.","code":"\nexp1g_m1a <- lmer(gc ~ genotype * sex +\n                   (genotype * sex | experiment_id),\n                 data = exp1g)\n\nexp1g_m1b <- lmer(gc ~ genotype * sex +\n                   (treatment | experiment_id),\n                 data = exp1g)\n\nexp1g_m1c <- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:genotype) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)\n\nexp1g_m1d <- lmer(gc ~ genotype * sex +\n                   (1 | experiment_id) +\n                    (1 | experiment_id:sex) +\n                    (1 | experiment_id:genotype:sex),\n                 data = exp1g)"},{"path":"lmm.html","id":"the-varcorr-matrix-of-models-exp1g_m1a-and-exp1g_m1b","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.8 The VarCorr matrix of models exp1g_m1a and exp1g_m1b","text":"random effect similarity models exp1g_m1a exp1g_m1b can seen estimated variance components correlations among random effects.\nTable 16.17: Varcorr matrix. Standard deviations random effects diagonal. Correlations random effects -diagonal.\n","code":""},{"path":"lmm.html","id":"the-linear-mixed-model-has-more-precision-and-power-than-the-fixed-effect-model-of-batch-means","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.9 The linear mixed model has more precision and power than the fixed effect model of batch means","text":"\nTable 16.18: Planned contrasts linear mixed model exp1g_m1 fixed effects model experiment means exp1g_m2.\nNotesA fixed effects model fit batch-means pooled data strongly conservative result less discovery.Means pooling make data independent randomized complete block design. linear mixed model batch-means pooled data mixed-effect ANOVA (Next section. Also see Section 13.1.1 Issues chapter).","code":"\n# means pooling model\nexp1g_m2 <- lm(gc ~ sex * genotype,\n                data = exp1g_means)"},{"path":"lmm.html","id":"fixed-effect-models-and-pseudoreplication","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.10 Fixed effect models and pseudoreplication","text":"\n(#tab:lmm-exp1g_m3-compare)Planned contrasts linear mixed model exp1g_m1 fixed effects model exp1g_m3 complete pooling.\nNotesComplete pooling strongly anti-conservative result increased false discovery. Complete pooling type pseudoreplication – subsamples analyzed independent replicates. Subsamples independent.Experiment 1g data, 95% confidence intervals wider p-values larger complete pool model exp1g_m3 compared linear mixed model exp1g_m1. unusual result.","code":"\n# complete pooling model\nexp1g_m3 <- lm(gc ~ sex * genotype,\n                data = exp1g)"},{"path":"lmm.html","id":"mixed-effect-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.9.11 Mixed-effect ANOVA","text":"NotesThe formula format Example 3 repeated measures ANOVA RCB designs subsampling. biology, often called mixed effect ANOVA two fixed factors one random factor.data aggregated prior fitting model – means subsamples averaged within batch treatment combination.mixed-effect ANOVA equivalent linear mixed model exp1g_m1c number replicates treatment combination number subsamples treatment \\(\\texttt{experiment_id}\\) combinations. design balanced example.\n(#tab:lmm-exp1g_m1_aov-pairs)Planned contrasts mixed ANOVA compared lmm equivalent mixed ANOVA lowest AIC lmm.\n","code":"\nexp1g_m1_aov <- aov_4(gc ~ sex * genotype +\n                  (sex * genotype | experiment_id),\n                data = exp1g)"},{"path":"lmm.html","id":"statistical-models-for-experimental-designs","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10 Statistical models for experimental designs","text":"","code":""},{"path":"lmm.html","id":"models-for-completely-randomized-designs-crd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.1 Models for Completely Randomized Designs (CRD)","text":"","code":"\nlm0 <- lm(y ~ treatment,\n             data = figx)"},{"path":"lmm.html","id":"models-for-batched-data-crds-rcbd-rspd-grcbd-nrcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2 Models for batched data (CRDS, RCBD, RSPD, GRCBD, NRCBD)","text":"","code":""},{"path":"lmm.html","id":"linear-models-1","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.1 Linear models","text":"","code":""},{"path":"lmm.html","id":"fixed-effect-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.1.1 fixed effect model","text":"","code":"\nlm1 <- lm(y ~ treatment + block,\n             data = figx)"},{"path":"lmm.html","id":"linear-model-of-batch-means","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.1.2 linear model of batch means","text":"","code":"\nfigx_means <- figx[, .(y = mean(y)),\n               by = .(treatment, batch)]\nlm2 <- lm(y ~ treatment,\n             data = figx_means)"},{"path":"lmm.html","id":"linear-mixed-models-random-effects-models","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2 linear mixed models (“random effects” models)","text":"","code":""},{"path":"lmm.html","id":"random-intercept-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.1 Random intercept model","text":"","code":"\nlmm1 <- lmer(y ~ treatment + (1 | block),\n              data = figx)"},{"path":"lmm.html","id":"lmm-for-correlated-error","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.2 lmm for correlated error","text":"","code":"\nlmm2 <- lme(y ~ treatment,\n             random = ~1 | block,\n             correlation = corSymm(form = ~ 1 | block),\n             weights = varIdent(form = ~ 1 | treatment),\n             data = figx)"},{"path":"lmm.html","id":"random-intercept-and-slope-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.3 random intercept and slope model","text":"","code":"\nlmm3 <-  lmer(y ~ treatment + (treatment | block),\n               data = figx)"},{"path":"lmm.html","id":"random-interaction-intercept-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.4 random interaction intercept model","text":"","code":"\nlmm4 <-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment),\n               data = figx)"},{"path":"lmm.html","id":"random-interaction-model-with-subsampling","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.5 random interaction model with subsampling","text":"","code":"\nlmm5 <-  lmer(y ~ treatment + (1 | block) + (1 | block:treatment) + (1 | block:treatment:replicate),\n               data = figx)"},{"path":"lmm.html","id":"crd-split-plot-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.6 CRD split plot model","text":"","code":"\n# tr1 is the main plot\n# tr2 is the subplot\n# main_plot is tr1:rep, where rep is the rep id\nlmm6 <-  lmer(y ~ tr1 * tr2 + (1 | main_plot),\n               data = figx)"},{"path":"lmm.html","id":"rcbd-split-plot-model","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.2.7 RCBD split plot model","text":"","code":"\n# tr1 is the main plot\n# tr2 is the subplot\n# main_plot is tr1:block\nlmm7 <-  lmer(y ~ tr1 * tr2 + (1 | block) + (1 | block:tr1),\n               data = figx)\n\nlmm7 <-  lmer(y ~ tr1 * tr2 + block + (1 | block:tr1),\n               data = figx)\n# the two versions are numerically equivalent"},{"path":"lmm.html","id":"anova-models-mixed-models-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.3 ANOVA models (“mixed models”, “repeated measures ANOVA”)","text":"","code":""},{"path":"lmm.html","id":"multivariate-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.3.1 Multivariate repeated measures ANOVA","text":"","code":"\naov1 <- aov_4(y ~ treatment + (treatment | block),\n             data = figx)"},{"path":"lmm.html","id":"univariate-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.3.2 Univariate repeated measures ANOVA","text":"","code":"\naov2 <- aov_4(y ~ treatment + (treatment | block),\n           include_aov = TRUE,\n             data = figx)"},{"path":"lmm.html","id":"pairwise-paired-t-tests","chapter":"16 Models for non-independence – linear mixed models","heading":"16.10.2.3.3 Pairwise paired, t-tests","text":"","code":"\npptt <- pairwise_t_tests(y_col = \"y\",\n                         g_col = \"treatment\",\n                         id_col = \"block\",\n                         data = figx)"},{"path":"lmm.html","id":"which-model-and-why","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11 Which model and why?","text":"","code":""},{"path":"lmm.html","id":"crd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.1 CRD","text":"batch model!","code":""},{"path":"lmm.html","id":"crds","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.2 CRDS","text":"lmm1 – random intercept modellm2 – linear model batch meansNotesThese numerically equivalent result estimates, SE, CIs, p-valuesThese equivalent Nested t-test Nested ANOVA","code":""},{"path":"lmm.html","id":"rcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.3 RCBD","text":"Reasonable models:lmm1 – random intercept modellmm2 – lmm correlated errorlm1 – fixed effect modelaov1 – multivariate RM-ANOVAaov2 – univariate RM-ANOVApptt – pairwise, paired t-test(use lmm3 lmm4 replication block:treatment combination.)Assumptions:lmm1, lm1, aov2 assume\ncompound symmetry.\nsphericity.\ncompound symmetry.sphericity.lmm2, aov1, pptt assume compound symmetry sphericity.Notes:lmm1 standard. number treatments = 2, lmm1 eqivalent paired t-test.design balanced (blocks single value treatments) number treatments = 2 six methods numerically equivalent.design balanced number treatments > 2, \nlmm1, lm1, aov2 numerically equivalent\nlmm2, aov1, pptt result estimates SE lmm2 df, CIs, p-values lmm2 less conservative.\nlmm1, lm1, aov2 numerically equivalentlmm2, aov1, pptt result estimates SE lmm2 df, CIs, p-values lmm2 less conservative.number treatments = 2 design balanced (least one block missing value one treatment level) linear models (lm1, aov1, aov2, pptt) equivalent linear mixed models (lmm1, lmm2) differ linear models.treatment within block missing, whole block deleted RM-ANOVA models. reduces power (loss power depends partly number missing values).treatment within block missing, block deleted comparisons including missing treatment pairwise, paired t-tests. makes pptt powerful aov1.lmm models flexible – covariates can added can used generalized lmms non-normal distributions.lmm models especially lmm2 sometimes (often?) fail converge, especially small samples (small number blocks) among-block component variance small.Performance:\nTable 16.19: Type error rate RCBD statistical models different levels correlation structure among observations. Cor Error column average correlated error residuals fit simple linear model y ~ treatment. Model abbreviations text.\naov1, pptt, lmm2 well-behaved type error rates (lmm2 little anti-conservative) regardless correlation structure – False discoveries well controlled.lm1, lmm1, aov2 poorly-behaved type error rates block:treatment interaction (sim 1 2), resulting heterogenous correlation structure (remember moot two treatment levels) – consequence small block block:treatment variance small (small correlations residuals). block:treatment interaction, models conservative contrasts associated large residual correlation, resulting low power, anti-conservative contrasts associated small residual correlations. resulting high false discovery rates.Best Practices:number treatments = 2 \ndesign balanced: doesn’t matter method use.\ndesign balanced: use lmm1 lmm2\ndesign balanced: doesn’t matter method use.design balanced: use lmm1 lmm2If number treatments > 2 \ndesign balanced: use aov1/pptt (numerically equivalent). lmm1 standard assumes equal correlated error among treatment levels equal standard errors among contrasts. lmm2 explicitly models heterogeneity correlations variances 1) model often fails 2) slightly anti-conservative Type error.\ndesign balanced: lmm2 attractive model runs. pptt attractive missing values.\ndesign balanced: use aov1/pptt (numerically equivalent). lmm1 standard assumes equal correlated error among treatment levels equal standard errors among contrasts. lmm2 explicitly models heterogeneity correlations variances 1) model often fails 2) slightly anti-conservative Type error.design balanced: lmm2 attractive model runs. pptt attractive missing values.covariates, start lmm2. Use lmeControl lmm2 fails. fails, use pairwise lmm1, pairwise t-tests allows covariates added model. Note covariate modeled independently pair, effectively models consequence treatment:covariate interaction.","code":""},{"path":"lmm.html","id":"rspd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.4 RSPD","text":"Reasonable models:lmm4 – random interaction intercept modelAssumptions:Notes:Performance:\nTable 16.20: Type error rate RSPD statistical models different models random variance. design 2 (main plot: WT, KO) x 3 (subplot: Cn, Tr1, Tr2) ten blocks. four simulations component variance due block. Sim 1 includes block:main plot component, Sim 2 includes block:sub plot component, Sim 3 includes block:main block:sub components. Type error rates 9 simple effect contrasts averaged within two sets: contrast = main aggregate single main plot contrast (KO - WT) level subplot factor. contrast = sub aggregate three subplot contrasts (Tr1 - Cn, Tr2 - Cn, Tr2 - Tr1) level main plot factor. Cor Error column average correlated error residuals fit simple linear model y ~ treatment. Model abbreviations text.\n","code":""},{"path":"lmm.html","id":"rcbds","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.5 RCBDS","text":"lmm3 – random intercept slopes modellmm4 – random interaction intercept modellmm1 – random intercept model batch meanslmm2 – lmm correlated error batch meansaov1 – multivariate RM-ANOVAaov2 – univariate RM-ANOVApptt – pairwise, paired t-testNoteslmm3 “maximal model” – fits parameters fewest assumptions. lmm1 full data minimal model. ’m recommending pseudoreplication result anti-conservative inference. lmm4 less conservative lmm3 (makes assumptions).lmm1 lmm2 models RCBD designs using aggregated data - subsampled replicates within batch (block:treatment combination) averaged.design balanced\nlmm3 aov1 result estimates, SE, CIs, p-values\nlmm4, lmm1, aov2 result estimates, SE, CIs, p-values\nlmm2 estimates SE lmm3/aov1 df less conservative\nlmm3 aov1 result estimates, SE, CIs, p-valueslmm4, lmm1, aov2 result estimates, SE, CIs, p-valueslmm2 estimates SE lmm3/aov1 df less conservativeIf treatment within block missing, whole block deleted RM-ANOVA models. reduces powerthe lmm models flexible – covariates can added can used generalized lmms non-normal distributions.lmm models, especially lmm2 lmm3, sometimes (often?) fail converge, especially small samples (small number blocks) within block correlation smalllmm1, lmm4, aov2 less conservative (power higher type error/false positive rate)Best practicesIf balanced covariates use lmm3/lmm2/aov1 discovery expensive (want avoid false positives) lmm4/aov2/lmm1 discovery cheap (can afford false positives). lmm2 less conservative lmm3/aov1. Use lmm3 interested variance different levels /covariance structure.unbalanced, covariates, start lmm3 lmm2. Use lmeControl lmm2 fails. fail, go 1.","code":""},{"path":"lmm.html","id":"grcbd","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.6 GRCBD","text":"lmm3 – random intercept slopes modellmm4 – random interaction intercept modellmm1 – random intercept model block:treatment meanslmm2 – lmm correlated error block:treatment meansaov1 – multivariate RM-ANOVAaov2 – univariate RM-ANOVAlm1 – fixed effect model datapptt – pairwise, paired t-test block:treatment means","code":""},{"path":"lmm.html","id":"grcbds","chapter":"16 Models for non-independence – linear mixed models","heading":"16.11.7 GRCBDS","text":"lmm3 – random intercept slopes modellmm4 – random interaction intercept modellmm1 – random intercept model block:treatment meanslmm2 – lmm correlated error block:treatment meansaov1 – multivariate RM-ANOVAaov2 – univariate RM-ANOVAlm1 – fixed effect model block:treatment:rep meanspptt – pairwise, paired t-test block:treatment:rep means","code":""},{"path":"lmm.html","id":"working-in-r-5","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12 Working in R","text":"","code":""},{"path":"lmm.html","id":"plotting-models-fit-to-batched-data","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.1 Plotting models fit to batched data","text":"","code":""},{"path":"lmm.html","id":"models-without-subsampling---experiment-6g","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.1.1 Models without subsampling - Experiment 6g","text":"Data wrangling necessary plot:Experiments colored:Treatments colored:","code":"\n# convert contrast table to a data.table\nexp6g_m1_planned_dt <- data.table(exp6g_m1_planned)\n\n# create a pretty p-value column\nexp6g_m1_planned_dt[, pretty_p := pvalString(p.value)]\n\n# add group1 and group2 columns to exp1g_m1_planned\nexp6g_m1_planned_dt[, group1 := c(\"Saline\", \"CNO\", \"Post_CNO\")]\nexp6g_m1_planned_dt[, group2 := c(\"Lesion\", \"Saline\", \"CNO\")]\ngg1 <- ggplot(data = exp6g,\n              aes(x = treatment,\n                  y = touch,\n                  color = mouse_id)) +\n  geom_point(position = position_dodge(width = 0.2)) +\n  geom_line(aes(group = mouse_id),\n            position = position_dodge(width = 0.2),\n            color = \"gray80\") +\n  geom_point(data = summary(exp6g_m1_emm),\n             aes(y = emmean),\n             color = \"black\",\n             size = 3) +\n  geom_errorbar(data = summary(exp6g_m1_emm),\n             aes(y = emmean,\n                 ymin = lower.CL,\n                 ymax = upper.CL),\n             color = \"black\",\n             width = .05) +\n\n  # pvalue brackets\n  stat_pvalue_manual(exp6g_m1_planned_dt,\n                     label = \"pretty_p\",\n                     y.position = c(99,97,95),\n                     size = 2.5,\n                     tip.length = 0.01) +\n\n  ylab(\"Percent ipsilateral touch\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\ngg2 <- ggplot(data = exp6g,\n              aes(x = treatment,\n                  y = touch)) +\n  geom_point(aes(group = mouse_id),\n             position = position_dodge(width = 0.2),\n             color = \"gray\") +\n  geom_line(aes(group = mouse_id),\n            position = position_dodge(width = 0.2),\n            color = \"gray80\") +\n  geom_point(data = summary(exp6g_m1_emm),\n             aes(y = emmean,\n             color = treatment),\n             size = 3) +\n  geom_errorbar(data = summary(exp6g_m1_emm),\n             aes(y = emmean,\n                 ymin = lower.CL,\n                 ymax = upper.CL,\n             color = treatment),\n             width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(exp6g_m1_planned_dt,\n                     label = \"pretty_p\",\n                     y.position = c(99,97,95),\n                     size = 2.5,\n                     tip.length = 0.01) +\n\n  ylab(\"Percent ipsilateral touch\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n\n#gg2"},{"path":"lmm.html","id":"models-with-subsampling---experiment-1g","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.1.2 Models with subsampling - Experiment 1g","text":"Data wrangling necessary plot:","code":"\n# add treatment column to emmeans table\nexp1g_m1_emm_dt <- summary(exp1g_m1_emm) %>%\n  data.table\nexp1g_m1_emm_dt[, treatment := paste(genotype, sex)]\nexp1g_m1_emm_dt[, treatment := factor(treatment,\n                                      levels = levels(exp1g$treatment))]\n\n# create table of means for each treatment * experiment_id combination\nexp1g[, group_mean := predict(exp1g_m1)]\nexp1g_m1_emm2 <- exp1g[, .(group_mean = mean(group_mean)),\n                       by = .(treatment, experiment_id)]\n\n# add group1 and group2 columns to exp1g_m1_planned\nexp1g_m1_planned_dt <- data.table(exp1g_m1_planned)\nexp1g_m1_planned_dt[, pretty_p := pvalString(p.value)]\nexp1g_m1_planned_dt[, group1 := c(\"Gpr174- M\", \"Gpr174- F\", \"\")]\nexp1g_m1_planned_dt[, group2 := c(\"Gpr174+ M\", \"Gpr174+ F\", \"\")]\n# get coefficients of model\nb <- exp1g_m1_coef[, \"Estimate\"]\n\n# get interaction p\np_ixn <- exp1g_m1_planned_dt[contrast == \"Interaction\", pretty_p]\n\nexp1g_plot_a <- ggplot(data = exp1g,\n              aes(x = treatment,\n                  y = gc,\n                  color = experiment_id)) +\n  # modeled experiment by treatment means\n  geom_point(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                color = experiment_id),\n            position = position_dodge(width = 0.4),\n            alpha = 1,\n            size = 2) +\n  geom_line(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                group = experiment_id,\n                color = experiment_id),\n            position = position_dodge(width = 0.4)) +\n  \n  # raw data\n  geom_point(position = position_dodge(width = 0.4),\n             alpha = 0.3\n  ) +\n  \n  # modeled treatment means\n  geom_point(data = exp1g_m1_emm_dt,\n             aes(y = emmean),\n             color = \"black\",\n             size = 3) +\n  # geom_errorbar(data = exp1g_m1_emm_dt,\n  #            aes(y = emmean,\n  #                ymin = lower.CL,\n  #                ymax = upper.CL),\n  #             color = \"black\",\n  #           width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],\n                     label = \"pretty_p\",\n                     y.position = c(20,20),\n                     size = 2.5,\n                     tip.length = 0.01) +\n  \n  # additive line + interaction p bracket\n  geom_segment(x = 3.85,\n               y = b[1] + b[2] + b[3],\n               xend = 4.15,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 4.2,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"interaction\\np = \", p_ixn),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\") +\n  \n  ylab(\"GC (%)\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  coord_cartesian(xlim = c(1, 4.1)) +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n  \nexp1g_plot_a <- factor_wrap(exp1g_plot_a)\n\n#exp1g_plot_a\n# get coefficients of model\nb <- exp1g_m1_coef[, \"Estimate\"]\n\n# get interaction p\np_ixn <- exp1g_m1_planned_dt[contrast == \"Interaction\", pretty_p]\n\ndodge_width = 0.6\nexp1g_plot_b <- ggplot(data = exp1g,\n              aes(x = treatment,\n                  y = gc,\n                  group = experiment_id)) +\n  # modeled experiment by treatment means\n  geom_point(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                color = experiment_id),\n            position = position_dodge(width = dodge_width),\n            alpha = 1,\n            size = 2) +\n  geom_line(data = exp1g_m1_emm2,\n            aes(y = group_mean,\n                group = experiment_id,\n                color = experiment_id),\n            position = position_dodge(width = dodge_width)) +\n  \n  # raw data\n  geom_point(position = position_dodge(width = dodge_width),\n             color = \"gray80\"\n  ) +\n  \n  # modeled treatment means\n  geom_point(data = exp1g_m1_emm_dt,\n             aes(y = emmean),\n             color = \"black\",\n             size = 3) +\n  # geom_errorbar(data = exp1g_m1_emm_dt,\n  #            aes(y = emmean,\n  #                ymin = lower.CL,\n  #                ymax = upper.CL),\n  #             color = \"black\",\n  #           width = .05) +\n  \n  # pvalue brackets\n  stat_pvalue_manual(exp1g_m1_planned_dt[1:2],\n                     label = \"pretty_p\",\n                     y.position = c(20,20),\n                     size = 2.5,\n                     tip.length = 0.01) +\n  \n  # additive line + interaction p bracket\n  geom_segment(x = 4 - dodge_width/2,\n               y = b[1] + b[2] + b[3],\n               xend = 4 + dodge_width/2,\n               yend = b[1] + b[2] + b[3],\n               linetype = \"dashed\",\n               color = \"gray\") +\n  geom_bracket(\n    x = 4 + dodge_width/1.9,\n    y = b[1] + b[2] + b[3],\n    yend = b[1] + b[2] + b[3] + b[4],\n    label = paste0(\"interaction\\np = \", p_ixn),\n    text.size = 3,\n    text.hjust = 0,\n    color = \"black\") +\n  \n  ylab(\"GC (%)\") +\n  scale_color_manual(values = pal_okabe_ito_blue) +\n  theme_pubr() +\n  coord_cartesian(xlim = c(1, 4.2)) +\n  theme(\n    axis.title.x = element_blank(), # no x-axis title\n    legend.position = \"none\"\n  ) + \n  NULL\n  \nexp1g_plot_b <- factor_wrap(exp1g_plot_b)\n\n# exp1g_plot_b"},{"path":"lmm.html","id":"repeated-measures-anova-randomized-complete-block-with-no-subsampling","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.2 Repeated measures ANOVA (randomized complete block with no subsampling)","text":"Notesafex computes repeated measures anova model using aov (classical univariate repeated measures ANOVA) using lm multiple response variables (multivariate repeated measures ANOVA). writing, output aov default.Given one measure donor within \\(\\texttt{treatment} \\times \\texttt{activity}\\) combination, linear mixed model m2 equivalent univariate repeated measures model m1.model formula m1 looks like linear mixed model m3 two models equivalent.","code":"\n# this is the rm-ANOVA\nm1 <- aov_4(glucose_uptake ~ treatment * activity +\n              (treatment * activity | donor),\n            data = exp5c)\n\n# lmm equivalent\n\nm2 <- lmer(glucose_uptake ~ treatment * activity +\n         (1 | donor) +\n         (1 | donor:treatment) +\n         (1 | donor:activity),\n       data = exp5c)\n\n# random intercept and slope model that is *not* equivalent\n# this isn't solvable because there is no subsampling\n\nm3 <- lmer(glucose_uptake ~ treatment * activity +\n              (treatment * activity | donor),\n            data = exp5c)"},{"path":"lmm.html","id":"univariate-vs.-multivariate-repeated-measures-anova","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.2.1 univariate vs. multivariate repeated measures ANOVA","text":"Use multivariate model unless want replicate result someone used univariate model. default, aov_4 computes multivariate model (prior version xxx, default compute models).NotesThe include_aov = TRUE argument forces output aov_4 include univariate model.contrasts multivariate modelNotesIf rmANOVA model fit default specification (exp5c_aov1), can get multivariate output using model = \"multivariate\" argument within emmeans (contrast function!). , fit excluded univariate model (exp5c_aov2), don’t need model argument.contrasts univariate modelNotesPassing exp5c_aov1 emmeans return contrasts multivariate. change univariate model passing exp5c_aov1$aov.","code":"\n# default mode -- should be multivariate\nexp5c_aov <- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c)\n\n# force aov_4 to compute univariate model\nexp5c_aov1 <- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c,\n                  include_aov = TRUE)\n\n# explicitly exclude the univariate model\nexp5c_aov2 <- aov_4(glucose_uptake ~ treatment * activity +\n                    (treatment * activity | donor),\n                  data = exp5c,\n                  include_aov = FALSE)\n# These three should give equivalent results\n\n# exp5c_aov was fit with the default -- multivariate model only\nemmeans(exp5c_aov,\n        specs = c(\"treatment\", \"activity\")) %>%\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")##  activity treatment contrast      estimate     SE df t.ratio p.value\n##  Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n##  EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n##  .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n##  .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n# exp5c_aov included the univariate model but the default is still the multivariate model\nemmeans(exp5c_aov1,\n        specs = c(\"treatment\", \"activity\")) %>%\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")##  activity treatment contrast      estimate     SE df t.ratio p.value\n##  Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n##  EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n##  .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n##  .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n# passing `model = \"multivariate\"` makes the model choice transparent\nemmeans(exp5c_aov1,\n        specs = c(\"treatment\", \"activity\"),\n        model = \"multivariate\") %>%\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")##  activity treatment contrast      estimate     SE df t.ratio p.value\n##  Basal    .         siNR4A3 - Scr   0.0813 0.1012  5   0.804  0.4581\n##  EPS      .         siNR4A3 - Scr  -0.1858 0.0359  5  -5.180  0.0035\n##  .        Scr       EPS - Basal     0.1510 0.0395  5   3.821  0.0124\n##  .        siNR4A3   EPS - Basal    -0.1161 0.0626  5  -1.855  0.1228\n# get the univariate model results using $aov\nemmeans(exp5c_aov1$aov, specs = c(\"treatment\", \"activity\")) %>%\n  contrast(method = \"revpairwise\",\n           simple = \"each\",\n           combine = TRUE,\n           adjust = \"none\")##  activity treatment contrast      estimate     SE   df t.ratio p.value\n##  Basal    .         siNR4A3 - Scr   0.0813 0.0759 8.60   1.071  0.3132\n##  EPS      .         siNR4A3 - Scr  -0.1858 0.0759 8.60  -2.448  0.0380\n##  .        Scr       EPS - Basal     0.1510 0.0524 9.39   2.884  0.0173\n##  .        siNR4A3   EPS - Basal    -0.1161 0.0524 9.39  -2.218  0.0525"},{"path":"lmm.html","id":"the-anova-table","chapter":"16 Models for non-independence – linear mixed models","heading":"16.12.2.2 The ANOVA table","text":"NotesThe Greenhouse-Geiger (“GG”) correction default. correction = \"GG\" argument needed, makes script transparent.data Greenhouse-Geiger correction doesn’t make difference table.","code":"\nnice(exp5c_aov1, correction = \"GG\")## Anova Table (Type 3 tests)\n## \n## Response: glucose_uptake\n##               Effect   df  MSE       F   ges p.value\n## 1          treatment 1, 5 0.02    0.68  .006     .45\n## 2           activity 1, 5 0.01    0.30 .0006     .61\n## 3 treatment:activity 1, 5 0.01 10.37 *   .04     .02\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nnice(exp5c_aov1, correction = \"none\")## Anova Table (Type 3 tests)\n## \n## Response: glucose_uptake\n##               Effect   df  MSE       F   ges p.value\n## 1          treatment 1, 5 0.02    0.68  .006     .45\n## 2           activity 1, 5 0.01    0.30 .0006     .61\n## 3 treatment:activity 1, 5 0.01 10.37 *   .04     .02\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1"},{"path":"lmm.html","id":"hidden-code-7","chapter":"16 Models for non-independence – linear mixed models","heading":"16.13 Hidden code","text":"","code":""},{"path":"lmm.html","id":"import-exp5c","chapter":"16 Models for non-independence – linear mixed models","heading":"16.13.1 Import exp5c","text":"","code":"\ndata_from <- \"Transcriptomic profiling of skeletal muscle adaptations to exercise and inactivity\"\nfile_name <- \"41467_2019_13869_MOESM6_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp5c_wide <- read_excel(file_path,\n                         sheet = \"Fig5c\",\n                         range = \"A2:M3\",\n                         col_names = FALSE) %>%\n  data.table() %>%\n  transpose(make.names = 1)\n\nactivity_levels <- c(\"Basal\", \"EPS\")\ntreatment_levels <- names(exp5c_wide)\nexp5c_wide[, activity := rep(activity_levels, each = 6)]\nexp5c_wide[, activity := factor(activity, levels = activity_levels)]\n\nexp5c <- melt(exp5c_wide,\n              id.vars = \"activity\",\n              variable.name = \"treatment\",\n              value.name = \"glucose_uptake\")\nexp5c[, treatment := factor(treatment, levels = treatment_levels)]\n\nexp5c[, donor := rep(paste0(\"donor_\", 1:6), 4)]"},{"path":"lmm.html","id":"import-exp1g","chapter":"16 Models for non-independence – linear mixed models","heading":"16.13.2 Import exp1g","text":"","code":"\ndata_from <- \"A GPR174–CCL21 module imparts sexual dimorphism to humoral immunity\"\nfile_name <- \"41586_2019_1873_MOESM3_ESM.xlsx\"\nfile_path <- here(data_folder, data_from, file_name)\n\nexp1g_wide <- read_excel(file_path,\n                         sheet = \"Fig 1g\",\n                         range = \"B4:E25\",\n                         col_types = c(\"numeric\"),\n                         col_names = FALSE) %>%\n  data.table()\n\ngenotype_levels <- c(\"Gpr174+\", \"Gpr174-\")\nsex_levels <- c(\"M\", \"F\")\ng.by.s_levels <- do.call(paste, expand.grid(genotype_levels, sex_levels))\ncolnames(exp1g_wide) <- g.by.s_levels\n\nexp_levels <- paste0(\"exp_\", 1:4)\nexp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check!\nexp1g_wide[, experiment_id := factor(experiment_id)] #check!\n\nexp1g <- melt(exp1g_wide,\n              id.vars = \"experiment_id\",\n              measure.vars = g.by.s_levels,\n              variable.name = \"treatment\",\n              value.name = \"gc\") %>% # cell count\n  na.omit()\n\nexp1g[, c(\"genotype\", \"sex\"):= tstrsplit(treatment,\n                                             \" \",\n                                             fixed = TRUE)]\nexp1g[, genotype := factor(genotype,\n                           levels = genotype_levels)]\nexp1g[, sex := factor(sex,\n                           levels = sex_levels)]\n\nexp1g_means <- exp1g[, .(gc = mean(gc)),\n                     by = .(treatment, genotype, sex, experiment_id)]"}]
